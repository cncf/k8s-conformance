I0227 01:31:37.862814      23 e2e.go:126] Starting e2e run "66932bf2-ce63-47ef-bda3-5ca4c4ddd6c8" on Ginkgo node 1
Feb 27 01:31:37.879: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1677461497 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 27 01:31:37.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
E0227 01:31:37.985245      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Feb 27 01:31:37.985: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 27 01:31:38.004: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 27 01:31:38.075: INFO: 79 / 79 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 27 01:31:38.075: INFO: expected 17 pod replicas in namespace 'kube-system', 17 are Running and Ready.
Feb 27 01:31:38.075: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'eric-tm-external-connectivity-frontend-speaker' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kucero' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'subport-controller' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'subport-manager' (0 seconds elapsed)
Feb 27 01:31:38.083: INFO: e2e test version: v1.26.1
Feb 27 01:31:38.084: INFO: kube-apiserver version: v1.26.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Feb 27 01:31:38.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:31:38.088: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.104 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 27 01:31:37.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    E0227 01:31:37.985245      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Feb 27 01:31:37.985: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Feb 27 01:31:38.004: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Feb 27 01:31:38.075: INFO: 79 / 79 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Feb 27 01:31:38.075: INFO: expected 17 pod replicas in namespace 'kube-system', 17 are Running and Ready.
    Feb 27 01:31:38.075: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'eric-tm-external-connectivity-frontend-speaker' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kucero' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'subport-controller' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'subport-manager' (0 seconds elapsed)
    Feb 27 01:31:38.083: INFO: e2e test version: v1.26.1
    Feb 27 01:31:38.084: INFO: kube-apiserver version: v1.26.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Feb 27 01:31:38.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:31:38.088: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:31:38.112
Feb 27 01:31:38.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 01:31:38.113
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:31:38.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:31:38.152
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-e289f9cb-fc73-4cee-8e96-735ea9e29b87 02/27/23 01:31:38.154
STEP: Creating a pod to test consume configMaps 02/27/23 01:31:38.165
Feb 27 01:31:38.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab" in namespace "configmap-3952" to be "Succeeded or Failed"
Feb 27 01:31:38.241: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.569154ms
Feb 27 01:31:40.247: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008489939s
Feb 27 01:31:42.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Running", Reason="", readiness=false. Elapsed: 4.006517456s
Feb 27 01:31:44.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006265103s
STEP: Saw pod success 02/27/23 01:31:44.245
Feb 27 01:31:44.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab" satisfied condition "Succeeded or Failed"
Feb 27 01:31:44.248: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:31:44.261
Feb 27 01:31:44.278: INFO: Waiting for pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab to disappear
Feb 27 01:31:44.282: INFO: Pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:31:44.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3952" for this suite. 02/27/23 01:31:44.287
------------------------------
• [SLOW TEST] [6.182 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:31:38.112
    Feb 27 01:31:38.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 01:31:38.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:31:38.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:31:38.152
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-e289f9cb-fc73-4cee-8e96-735ea9e29b87 02/27/23 01:31:38.154
    STEP: Creating a pod to test consume configMaps 02/27/23 01:31:38.165
    Feb 27 01:31:38.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab" in namespace "configmap-3952" to be "Succeeded or Failed"
    Feb 27 01:31:38.241: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.569154ms
    Feb 27 01:31:40.247: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008489939s
    Feb 27 01:31:42.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Running", Reason="", readiness=false. Elapsed: 4.006517456s
    Feb 27 01:31:44.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006265103s
    STEP: Saw pod success 02/27/23 01:31:44.245
    Feb 27 01:31:44.245: INFO: Pod "pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab" satisfied condition "Succeeded or Failed"
    Feb 27 01:31:44.248: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:31:44.261
    Feb 27 01:31:44.278: INFO: Waiting for pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab to disappear
    Feb 27 01:31:44.282: INFO: Pod pod-configmaps-80337177-15cb-4ff8-b199-b4498206d1ab no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:31:44.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3952" for this suite. 02/27/23 01:31:44.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:31:44.295
Feb 27 01:31:44.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 01:31:44.295
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:31:44.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:31:44.407
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9922 02/27/23 01:31:44.416
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[] 02/27/23 01:31:44.492
Feb 27 01:31:44.495: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Feb 27 01:31:45.505: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9922 02/27/23 01:31:45.505
Feb 27 01:31:45.548: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9922" to be "running and ready"
Feb 27 01:31:45.551: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551694ms
Feb 27 01:31:45.551: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:31:47.554: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006126574s
Feb 27 01:31:47.554: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 01:31:47.554: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod1:[80]] 02/27/23 01:31:47.557
Feb 27 01:31:47.567: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 02/27/23 01:31:47.567
Feb 27 01:31:47.567: INFO: Creating new exec pod
Feb 27 01:31:47.575: INFO: Waiting up to 5m0s for pod "execpodh5mnm" in namespace "services-9922" to be "running"
Feb 27 01:31:47.577: INFO: Pod "execpodh5mnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.554009ms
Feb 27 01:31:49.583: INFO: Pod "execpodh5mnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008714107s
Feb 27 01:31:51.583: INFO: Pod "execpodh5mnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.00843814s
Feb 27 01:31:51.583: INFO: Pod "execpodh5mnm" satisfied condition "running"
Feb 27 01:31:52.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 01:31:52.795: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 01:31:52.795: INFO: stdout: ""
Feb 27 01:31:52.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
Feb 27 01:31:52.929: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
Feb 27 01:31:52.929: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9922 02/27/23 01:31:52.929
Feb 27 01:31:52.964: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9922" to be "running and ready"
Feb 27 01:31:52.967: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365134ms
Feb 27 01:31:52.967: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:31:54.972: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007631796s
Feb 27 01:31:54.972: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:31:56.972: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008175003s
Feb 27 01:31:56.972: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 01:31:56.972: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 01:31:56.975
Feb 27 01:31:56.987: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 01:31:56.987
Feb 27 01:31:57.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 01:31:58.127: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 01:31:58.127: INFO: stdout: ""
Feb 27 01:31:58.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
Feb 27 01:31:58.279: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
Feb 27 01:31:58.279: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9922 02/27/23 01:31:58.279
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod2:[80]] 02/27/23 01:31:58.291
Feb 27 01:31:59.314: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 02/27/23 01:31:59.314
Feb 27 01:32:00.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Feb 27 01:32:00.444: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Feb 27 01:32:00.444: INFO: stdout: ""
Feb 27 01:32:00.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
Feb 27 01:32:00.606: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
Feb 27 01:32:00.606: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9922 02/27/23 01:32:00.606
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[] 02/27/23 01:32:00.629
Feb 27 01:32:00.644: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 01:32:00.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9922" for this suite. 02/27/23 01:32:00.684
------------------------------
• [SLOW TEST] [16.408 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:31:44.295
    Feb 27 01:31:44.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 01:31:44.295
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:31:44.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:31:44.407
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9922 02/27/23 01:31:44.416
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[] 02/27/23 01:31:44.492
    Feb 27 01:31:44.495: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Feb 27 01:31:45.505: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9922 02/27/23 01:31:45.505
    Feb 27 01:31:45.548: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9922" to be "running and ready"
    Feb 27 01:31:45.551: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551694ms
    Feb 27 01:31:45.551: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:31:47.554: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006126574s
    Feb 27 01:31:47.554: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 01:31:47.554: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod1:[80]] 02/27/23 01:31:47.557
    Feb 27 01:31:47.567: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 02/27/23 01:31:47.567
    Feb 27 01:31:47.567: INFO: Creating new exec pod
    Feb 27 01:31:47.575: INFO: Waiting up to 5m0s for pod "execpodh5mnm" in namespace "services-9922" to be "running"
    Feb 27 01:31:47.577: INFO: Pod "execpodh5mnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.554009ms
    Feb 27 01:31:49.583: INFO: Pod "execpodh5mnm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008714107s
    Feb 27 01:31:51.583: INFO: Pod "execpodh5mnm": Phase="Running", Reason="", readiness=true. Elapsed: 4.00843814s
    Feb 27 01:31:51.583: INFO: Pod "execpodh5mnm" satisfied condition "running"
    Feb 27 01:31:52.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 01:31:52.795: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 01:31:52.795: INFO: stdout: ""
    Feb 27 01:31:52.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
    Feb 27 01:31:52.929: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
    Feb 27 01:31:52.929: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9922 02/27/23 01:31:52.929
    Feb 27 01:31:52.964: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9922" to be "running and ready"
    Feb 27 01:31:52.967: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365134ms
    Feb 27 01:31:52.967: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:31:54.972: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007631796s
    Feb 27 01:31:54.972: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:31:56.972: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008175003s
    Feb 27 01:31:56.972: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 01:31:56.972: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod1:[80] pod2:[80]] 02/27/23 01:31:56.975
    Feb 27 01:31:56.987: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 02/27/23 01:31:56.987
    Feb 27 01:31:57.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 01:31:58.127: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 01:31:58.127: INFO: stdout: ""
    Feb 27 01:31:58.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
    Feb 27 01:31:58.279: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
    Feb 27 01:31:58.279: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9922 02/27/23 01:31:58.279
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[pod2:[80]] 02/27/23 01:31:58.291
    Feb 27 01:31:59.314: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 02/27/23 01:31:59.314
    Feb 27 01:32:00.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Feb 27 01:32:00.444: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Feb 27 01:32:00.444: INFO: stdout: ""
    Feb 27 01:32:00.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9922 exec execpodh5mnm -- /bin/sh -x -c nc -v -z -w 2 10.100.150.86 80'
    Feb 27 01:32:00.606: INFO: stderr: "+ nc -v -z -w 2 10.100.150.86 80\nConnection to 10.100.150.86 80 port [tcp/http] succeeded!\n"
    Feb 27 01:32:00.606: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9922 02/27/23 01:32:00.606
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9922 to expose endpoints map[] 02/27/23 01:32:00.629
    Feb 27 01:32:00.644: INFO: successfully validated that service endpoint-test2 in namespace services-9922 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:32:00.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9922" for this suite. 02/27/23 01:32:00.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:32:00.703
Feb 27 01:32:00.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:32:00.704
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:32:00.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:32:00.73
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 02/27/23 01:32:00.733
Feb 27 01:32:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: mark a version not serverd 02/27/23 01:32:04.859
STEP: check the unserved version gets removed 02/27/23 01:32:04.88
STEP: check the other version is not changed 02/27/23 01:32:06.736
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:32:09.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9276" for this suite. 02/27/23 01:32:09.996
------------------------------
• [SLOW TEST] [9.301 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:32:00.703
    Feb 27 01:32:00.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:32:00.704
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:32:00.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:32:00.73
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 02/27/23 01:32:00.733
    Feb 27 01:32:00.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: mark a version not serverd 02/27/23 01:32:04.859
    STEP: check the unserved version gets removed 02/27/23 01:32:04.88
    STEP: check the other version is not changed 02/27/23 01:32:06.736
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:32:09.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9276" for this suite. 02/27/23 01:32:09.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:32:10.004
Feb 27 01:32:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 01:32:10.005
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:32:10.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:32:10.03
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8047 02/27/23 01:32:10.033
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-8047 02/27/23 01:32:10.04
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8047 02/27/23 01:32:10.058
Feb 27 01:32:10.064: INFO: Found 0 stateful pods, waiting for 1
Feb 27 01:32:20.068: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 01:32:20.068
Feb 27 01:32:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 01:32:20.219: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 01:32:20.219: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 01:32:20.219: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 01:32:20.224: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 01:32:30.229: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 01:32:30.229: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 01:32:30.243: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 27 01:32:30.243: INFO: ss-0  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  }]
Feb 27 01:32:30.243: INFO: 
Feb 27 01:32:30.243: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 27 01:32:31.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9969773s
Feb 27 01:32:32.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992964514s
Feb 27 01:32:33.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987986921s
Feb 27 01:32:34.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984139799s
Feb 27 01:32:35.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979951343s
Feb 27 01:32:36.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976272515s
Feb 27 01:32:37.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971987236s
Feb 27 01:32:38.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967977434s
Feb 27 01:32:39.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.068736ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8047 02/27/23 01:32:40.282
Feb 27 01:32:40.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 01:32:40.423: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 01:32:40.423: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 01:32:40.423: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 01:32:40.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 01:32:40.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 01:32:40.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 01:32:40.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 01:32:40.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 01:32:40.749: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 27 01:32:40.749: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 01:32:40.749: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 01:32:40.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 01:32:40.753: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 01:32:40.753: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 01:32:40.753
Feb 27 01:32:40.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 01:32:40.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 01:32:40.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 01:32:40.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 01:32:40.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 01:32:41.053: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 01:32:41.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 01:32:41.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 01:32:41.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 01:32:41.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 01:32:41.216: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 01:32:41.216: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 01:32:41.216: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 01:32:41.220: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 27 01:32:51.227: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 01:32:51.227: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 01:32:51.227: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 01:32:51.244: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 27 01:32:51.244: INFO: ss-0  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  }]
Feb 27 01:32:51.244: INFO: ss-1  worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
Feb 27 01:32:51.244: INFO: ss-2  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
Feb 27 01:32:51.244: INFO: 
Feb 27 01:32:51.244: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 27 01:32:52.248: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 27 01:32:52.248: INFO: ss-1  worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
Feb 27 01:32:52.248: INFO: ss-2  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
Feb 27 01:32:52.248: INFO: 
Feb 27 01:32:52.248: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 27 01:32:53.251: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990795406s
Feb 27 01:32:54.257: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986997425s
Feb 27 01:32:55.264: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981136817s
Feb 27 01:32:56.268: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.974147026s
Feb 27 01:32:57.272: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970288823s
Feb 27 01:32:58.276: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966452613s
Feb 27 01:32:59.281: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961346677s
Feb 27 01:33:00.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 956.746236ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8047 02/27/23 01:33:01.285
Feb 27 01:33:01.295: INFO: Scaling statefulset ss to 0
Feb 27 01:33:01.303: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 01:33:01.306: INFO: Deleting all statefulset in ns statefulset-8047
Feb 27 01:33:01.308: INFO: Scaling statefulset ss to 0
Feb 27 01:33:01.317: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 01:33:01.319: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8047" for this suite. 02/27/23 01:33:01.344
------------------------------
• [SLOW TEST] [51.350 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:32:10.004
    Feb 27 01:32:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 01:32:10.005
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:32:10.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:32:10.03
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8047 02/27/23 01:32:10.033
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-8047 02/27/23 01:32:10.04
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8047 02/27/23 01:32:10.058
    Feb 27 01:32:10.064: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 01:32:20.068: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 02/27/23 01:32:20.068
    Feb 27 01:32:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 01:32:20.219: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 01:32:20.219: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 01:32:20.219: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 01:32:20.224: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 01:32:30.229: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 01:32:30.229: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 01:32:30.243: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 27 01:32:30.243: INFO: ss-0  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  }]
    Feb 27 01:32:30.243: INFO: 
    Feb 27 01:32:30.243: INFO: StatefulSet ss has not reached scale 3, at 1
    Feb 27 01:32:31.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9969773s
    Feb 27 01:32:32.251: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992964514s
    Feb 27 01:32:33.256: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987986921s
    Feb 27 01:32:34.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984139799s
    Feb 27 01:32:35.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979951343s
    Feb 27 01:32:36.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976272515s
    Feb 27 01:32:37.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971987236s
    Feb 27 01:32:38.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967977434s
    Feb 27 01:32:39.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.068736ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8047 02/27/23 01:32:40.282
    Feb 27 01:32:40.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 01:32:40.423: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 01:32:40.423: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 01:32:40.423: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 01:32:40.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 01:32:40.575: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 01:32:40.575: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 01:32:40.575: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 01:32:40.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 01:32:40.749: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Feb 27 01:32:40.749: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 01:32:40.749: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 01:32:40.753: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 01:32:40.753: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 01:32:40.753: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 02/27/23 01:32:40.753
    Feb 27 01:32:40.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 01:32:40.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 01:32:40.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 01:32:40.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 01:32:40.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 01:32:41.053: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 01:32:41.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 01:32:41.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 01:32:41.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-8047 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 01:32:41.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 01:32:41.216: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 01:32:41.216: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 01:32:41.216: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 01:32:41.220: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Feb 27 01:32:51.227: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 01:32:51.227: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 01:32:51.227: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 01:32:51.244: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 27 01:32:51.244: INFO: ss-0  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:10 +0000 UTC  }]
    Feb 27 01:32:51.244: INFO: ss-1  worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
    Feb 27 01:32:51.244: INFO: ss-2  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
    Feb 27 01:32:51.244: INFO: 
    Feb 27 01:32:51.244: INFO: StatefulSet ss has not reached scale 0, at 3
    Feb 27 01:32:52.248: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Feb 27 01:32:52.248: INFO: ss-1  worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
    Feb 27 01:32:52.248: INFO: ss-2  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:32:30 +0000 UTC  }]
    Feb 27 01:32:52.248: INFO: 
    Feb 27 01:32:52.248: INFO: StatefulSet ss has not reached scale 0, at 2
    Feb 27 01:32:53.251: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990795406s
    Feb 27 01:32:54.257: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.986997425s
    Feb 27 01:32:55.264: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.981136817s
    Feb 27 01:32:56.268: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.974147026s
    Feb 27 01:32:57.272: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.970288823s
    Feb 27 01:32:58.276: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.966452613s
    Feb 27 01:32:59.281: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961346677s
    Feb 27 01:33:00.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 956.746236ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8047 02/27/23 01:33:01.285
    Feb 27 01:33:01.295: INFO: Scaling statefulset ss to 0
    Feb 27 01:33:01.303: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 01:33:01.306: INFO: Deleting all statefulset in ns statefulset-8047
    Feb 27 01:33:01.308: INFO: Scaling statefulset ss to 0
    Feb 27 01:33:01.317: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 01:33:01.319: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:01.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8047" for this suite. 02/27/23 01:33:01.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:01.355
Feb 27 01:33:01.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 01:33:01.356
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:01.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:01.374
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 01:33:01.402
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:33:01.72
STEP: Deploying the webhook pod 02/27/23 01:33:01.726
STEP: Wait for the deployment to be ready 02/27/23 01:33:01.736
Feb 27 01:33:01.744: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 01:33:03.755
STEP: Verifying the service has paired with the endpoint 02/27/23 01:33:03.773
Feb 27 01:33:04.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 02/27/23 01:33:04.778
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 01:33:04.779
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 01:33:04.779
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 01:33:04.779
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 01:33:04.78
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 01:33:04.78
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 01:33:04.781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:04.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3072" for this suite. 02/27/23 01:33:04.866
STEP: Destroying namespace "webhook-3072-markers" for this suite. 02/27/23 01:33:04.876
------------------------------
• [3.530 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:01.355
    Feb 27 01:33:01.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 01:33:01.356
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:01.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:01.374
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 01:33:01.402
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:33:01.72
    STEP: Deploying the webhook pod 02/27/23 01:33:01.726
    STEP: Wait for the deployment to be ready 02/27/23 01:33:01.736
    Feb 27 01:33:01.744: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 01:33:03.755
    STEP: Verifying the service has paired with the endpoint 02/27/23 01:33:03.773
    Feb 27 01:33:04.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 02/27/23 01:33:04.778
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 02/27/23 01:33:04.779
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 01:33:04.779
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 02/27/23 01:33:04.779
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 02/27/23 01:33:04.78
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 01:33:04.78
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 02/27/23 01:33:04.781
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:04.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3072" for this suite. 02/27/23 01:33:04.866
    STEP: Destroying namespace "webhook-3072-markers" for this suite. 02/27/23 01:33:04.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:04.886
Feb 27 01:33:04.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:33:04.886
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:04.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:04.916
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-a8dc5c1e-d84e-4650-8c7e-e5f99d416ded 02/27/23 01:33:04.919
STEP: Creating a pod to test consume secrets 02/27/23 01:33:04.929
Feb 27 01:33:04.941: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104" in namespace "projected-5069" to be "Succeeded or Failed"
Feb 27 01:33:04.949: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Pending", Reason="", readiness=false. Elapsed: 7.847838ms
Feb 27 01:33:06.952: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011354698s
Feb 27 01:33:08.953: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011913392s
STEP: Saw pod success 02/27/23 01:33:08.953
Feb 27 01:33:08.953: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104" satisfied condition "Succeeded or Failed"
Feb 27 01:33:08.956: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 01:33:08.964
Feb 27 01:33:08.978: INFO: Waiting for pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 to disappear
Feb 27 01:33:08.982: INFO: Pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:08.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5069" for this suite. 02/27/23 01:33:08.99
------------------------------
• [4.112 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:04.886
    Feb 27 01:33:04.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:33:04.886
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:04.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:04.916
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-a8dc5c1e-d84e-4650-8c7e-e5f99d416ded 02/27/23 01:33:04.919
    STEP: Creating a pod to test consume secrets 02/27/23 01:33:04.929
    Feb 27 01:33:04.941: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104" in namespace "projected-5069" to be "Succeeded or Failed"
    Feb 27 01:33:04.949: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Pending", Reason="", readiness=false. Elapsed: 7.847838ms
    Feb 27 01:33:06.952: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011354698s
    Feb 27 01:33:08.953: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011913392s
    STEP: Saw pod success 02/27/23 01:33:08.953
    Feb 27 01:33:08.953: INFO: Pod "pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104" satisfied condition "Succeeded or Failed"
    Feb 27 01:33:08.956: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 01:33:08.964
    Feb 27 01:33:08.978: INFO: Waiting for pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 to disappear
    Feb 27 01:33:08.982: INFO: Pod pod-projected-secrets-c5a000be-ffb1-4c93-85cd-6c00de377104 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:08.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5069" for this suite. 02/27/23 01:33:08.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:08.998
Feb 27 01:33:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 01:33:08.999
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:09.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:09.022
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 01:33:09.043
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 01:33:09.049
Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:09.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:33:09.056: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:10.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:33:10.065: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:11.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:33:11.066: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:12.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:33:12.065: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:13.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:33:13.069: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:33:14.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 01:33:14.065: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 02/27/23 01:33:14.068
STEP: DeleteCollection of the DaemonSets 02/27/23 01:33:14.073
STEP: Verify that ReplicaSets have been deleted 02/27/23 01:33:14.081
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Feb 27 01:33:14.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15186"},"items":null}

Feb 27 01:33:14.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15191"},"items":[{"metadata":{"name":"daemon-set-fjk5x","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"0e7c383c-b15b-4d67-9a93-3c736f2b83f3","resourceVersion":"15147","creationTimestamp":"2023-02-27T01:33:09Z","labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.21.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.21.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xkwpv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xkwpv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.5","podIP":"192.168.21.139","podIPs":[{"ip":"192.168.21.139"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://1e96fe8a268b09d16a66f6c30232530046109840ce5487257e5632a51489d7c3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gjc8d","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"bdea0e6b-ccb5-4775-843a-895494d17466","resourceVersion":"15186","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.128.15\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.128.15\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q68rx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q68rx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-losn7d81-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-losn7d81-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.22","podIP":"192.168.128.15","podIPs":[{"ip":"192.168.128.15"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://bf47ed8c029827ecef2405e9e30db67d459b9ffc85c8c2ace32455f5ea9207ee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nm8p2","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"84ce5257-0a5b-430c-aa8b-10639922094a","resourceVersion":"15189","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.226.73\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.226.73\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8mbsp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8mbsp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-58bsk712-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-58bsk712-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.15","podIP":"192.168.226.73","podIPs":[{"ip":"192.168.226.73"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://50ce906028aefd3837c7e0bfa43fd7917bc5daf43cb79a9c2888d4e84b071a7a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rqr58","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"4808fe1c-18c9-4ed9-b30f-d069d77683ce","resourceVersion":"15188","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4mfjr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4mfjr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.3","podIP":"192.168.214.143","podIPs":[{"ip":"192.168.214.143"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://ffa02498371d819657b4ebb36dd130953e5638b16fccbce2a0eed002b6edac70","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5325" for this suite. 02/27/23 01:33:14.268
------------------------------
• [SLOW TEST] [5.276 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:08.998
    Feb 27 01:33:08.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 01:33:08.999
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:09.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:09.022
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 01:33:09.043
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 01:33:09.049
    Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:09.053: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:09.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:33:09.056: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:10.061: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:10.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:33:10.065: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:11.063: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:11.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:33:11.066: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:12.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:12.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:33:12.065: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:13.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:13.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:33:13.069: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:14.062: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:33:14.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 01:33:14.065: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 02/27/23 01:33:14.068
    STEP: DeleteCollection of the DaemonSets 02/27/23 01:33:14.073
    STEP: Verify that ReplicaSets have been deleted 02/27/23 01:33:14.081
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Feb 27 01:33:14.140: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15186"},"items":null}

    Feb 27 01:33:14.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15191"},"items":[{"metadata":{"name":"daemon-set-fjk5x","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"0e7c383c-b15b-4d67-9a93-3c736f2b83f3","resourceVersion":"15147","creationTimestamp":"2023-02-27T01:33:09Z","labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.21.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.21.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xkwpv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xkwpv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.5","podIP":"192.168.21.139","podIPs":[{"ip":"192.168.21.139"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://1e96fe8a268b09d16a66f6c30232530046109840ce5487257e5632a51489d7c3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gjc8d","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"bdea0e6b-ccb5-4775-843a-895494d17466","resourceVersion":"15186","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.128.15\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.128.15\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q68rx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q68rx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-losn7d81-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-losn7d81-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.22","podIP":"192.168.128.15","podIPs":[{"ip":"192.168.128.15"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://bf47ed8c029827ecef2405e9e30db67d459b9ffc85c8c2ace32455f5ea9207ee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nm8p2","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"84ce5257-0a5b-430c-aa8b-10639922094a","resourceVersion":"15189","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.226.73\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.226.73\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8mbsp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8mbsp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-58bsk712-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-58bsk712-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.15","podIP":"192.168.226.73","podIPs":[{"ip":"192.168.226.73"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://50ce906028aefd3837c7e0bfa43fd7917bc5daf43cb79a9c2888d4e84b071a7a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rqr58","generateName":"daemon-set-","namespace":"daemonsets-5325","uid":"4808fe1c-18c9-4ed9-b30f-d069d77683ce","resourceVersion":"15188","creationTimestamp":"2023-02-27T01:33:09Z","deletionTimestamp":"2023-02-27T01:33:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"d5d68846d","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.143\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"61e21fea-151f-466a-8ba3-43183e0e8b06","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61e21fea-151f-466a-8ba3-43183e0e8b06\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-02-27T01:33:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4mfjr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4mfjr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:10Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-02-27T01:33:09Z"}],"hostIP":"10.0.10.3","podIP":"192.168.214.143","podIPs":[{"ip":"192.168.214.143"}],"startTime":"2023-02-27T01:33:09Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-02-27T01:33:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4","imageID":"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3","containerID":"containerd://ffa02498371d819657b4ebb36dd130953e5638b16fccbce2a0eed002b6edac70","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5325" for this suite. 02/27/23 01:33:14.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:14.275
Feb 27 01:33:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename watch 02/27/23 01:33:14.276
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:14.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:14.321
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 02/27/23 01:33:14.327
STEP: creating a new configmap 02/27/23 01:33:14.337
STEP: modifying the configmap once 02/27/23 01:33:14.353
STEP: closing the watch once it receives two notifications 02/27/23 01:33:14.414
Feb 27 01:33:14.414: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15201 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:14.414: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15203 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 02/27/23 01:33:14.414
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 01:33:14.518
STEP: deleting the configmap 02/27/23 01:33:14.519
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 01:33:14.552
Feb 27 01:33:14.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15206 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:14.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15210 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:14.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4452" for this suite. 02/27/23 01:33:14.559
------------------------------
• [0.296 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:14.275
    Feb 27 01:33:14.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename watch 02/27/23 01:33:14.276
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:14.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:14.321
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 02/27/23 01:33:14.327
    STEP: creating a new configmap 02/27/23 01:33:14.337
    STEP: modifying the configmap once 02/27/23 01:33:14.353
    STEP: closing the watch once it receives two notifications 02/27/23 01:33:14.414
    Feb 27 01:33:14.414: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15201 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:14.414: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15203 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 02/27/23 01:33:14.414
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 02/27/23 01:33:14.518
    STEP: deleting the configmap 02/27/23 01:33:14.519
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 02/27/23 01:33:14.552
    Feb 27 01:33:14.552: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15206 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:14.552: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4452  744250ce-1a1f-4696-b91f-d7358b24d318 15210 0 2023-02-27 01:33:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:14.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4452" for this suite. 02/27/23 01:33:14.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:14.572
Feb 27 01:33:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 01:33:14.573
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:14.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:14.64
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-6e00f5e7-a9c5-4553-9101-98915234f95d 02/27/23 01:33:14.642
STEP: Creating a pod to test consume configMaps 02/27/23 01:33:14.648
Feb 27 01:33:14.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b" in namespace "configmap-5790" to be "Succeeded or Failed"
Feb 27 01:33:14.687: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949231ms
Feb 27 01:33:16.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007390158s
Feb 27 01:33:18.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007205911s
STEP: Saw pod success 02/27/23 01:33:18.692
Feb 27 01:33:18.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b" satisfied condition "Succeeded or Failed"
Feb 27 01:33:18.694: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:33:18.701
Feb 27 01:33:18.718: INFO: Waiting for pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b to disappear
Feb 27 01:33:18.720: INFO: Pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:18.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5790" for this suite. 02/27/23 01:33:18.725
------------------------------
• [4.161 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:14.572
    Feb 27 01:33:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 01:33:14.573
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:14.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:14.64
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-6e00f5e7-a9c5-4553-9101-98915234f95d 02/27/23 01:33:14.642
    STEP: Creating a pod to test consume configMaps 02/27/23 01:33:14.648
    Feb 27 01:33:14.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b" in namespace "configmap-5790" to be "Succeeded or Failed"
    Feb 27 01:33:14.687: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949231ms
    Feb 27 01:33:16.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007390158s
    Feb 27 01:33:18.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007205911s
    STEP: Saw pod success 02/27/23 01:33:18.692
    Feb 27 01:33:18.692: INFO: Pod "pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b" satisfied condition "Succeeded or Failed"
    Feb 27 01:33:18.694: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:33:18.701
    Feb 27 01:33:18.718: INFO: Waiting for pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b to disappear
    Feb 27 01:33:18.720: INFO: Pod pod-configmaps-6267dc25-c617-4134-98a6-bd2bb2022c0b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:18.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5790" for this suite. 02/27/23 01:33:18.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:18.733
Feb 27 01:33:18.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 01:33:18.734
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:18.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:18.753
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 02/27/23 01:33:18.756
STEP: getting 02/27/23 01:33:18.798
STEP: listing in namespace 02/27/23 01:33:18.81
STEP: patching 02/27/23 01:33:18.828
STEP: deleting 02/27/23 01:33:18.856
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:18.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-4113" for this suite. 02/27/23 01:33:18.872
------------------------------
• [0.144 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:18.733
    Feb 27 01:33:18.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 01:33:18.734
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:18.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:18.753
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 02/27/23 01:33:18.756
    STEP: getting 02/27/23 01:33:18.798
    STEP: listing in namespace 02/27/23 01:33:18.81
    STEP: patching 02/27/23 01:33:18.828
    STEP: deleting 02/27/23 01:33:18.856
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:18.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-4113" for this suite. 02/27/23 01:33:18.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:18.878
Feb 27 01:33:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename watch 02/27/23 01:33:18.879
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:18.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:18.898
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 02/27/23 01:33:18.9
STEP: creating a watch on configmaps with label B 02/27/23 01:33:18.901
STEP: creating a watch on configmaps with label A or B 02/27/23 01:33:18.902
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.903
Feb 27 01:33:18.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15297 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:18.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15297 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.909
Feb 27 01:33:18.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15298 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:18.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15298 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 01:33:18.917
Feb 27 01:33:18.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15299 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:18.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15299 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.926
Feb 27 01:33:18.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15300 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:18.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15300 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 01:33:18.934
Feb 27 01:33:18.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15301 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:18.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15301 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 01:33:28.94
Feb 27 01:33:28.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15408 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:33:28.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15408 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 01:33:38.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4254" for this suite. 02/27/23 01:33:38.956
------------------------------
• [SLOW TEST] [20.087 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:18.878
    Feb 27 01:33:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename watch 02/27/23 01:33:18.879
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:18.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:18.898
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 02/27/23 01:33:18.9
    STEP: creating a watch on configmaps with label B 02/27/23 01:33:18.901
    STEP: creating a watch on configmaps with label A or B 02/27/23 01:33:18.902
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.903
    Feb 27 01:33:18.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15297 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:18.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15297 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.909
    Feb 27 01:33:18.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15298 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:18.917: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15298 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 02/27/23 01:33:18.917
    Feb 27 01:33:18.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15299 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:18.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15299 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 02/27/23 01:33:18.926
    Feb 27 01:33:18.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15300 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:18.934: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4254  a19d8ba8-250d-4c23-b9e8-8d1639f508a8 15300 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 02/27/23 01:33:18.934
    Feb 27 01:33:18.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15301 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:18.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15301 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 02/27/23 01:33:28.94
    Feb 27 01:33:28.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15408 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:33:28.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4254  c3315fe8-f1a4-416f-b92a-82fe97831de6 15408 0 2023-02-27 01:33:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-02-27 01:33:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:33:38.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4254" for this suite. 02/27/23 01:33:38.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:33:38.966
Feb 27 01:33:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 01:33:38.967
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:38.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:38.985
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e in namespace container-probe-8290 02/27/23 01:33:38.989
Feb 27 01:33:39.017: INFO: Waiting up to 5m0s for pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e" in namespace "container-probe-8290" to be "not pending"
Feb 27 01:33:39.022: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980755ms
Feb 27 01:33:41.031: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013639183s
Feb 27 01:33:41.031: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e" satisfied condition "not pending"
Feb 27 01:33:41.031: INFO: Started pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e in namespace container-probe-8290
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:33:41.031
Feb 27 01:33:41.037: INFO: Initial restart count of pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e is 0
Feb 27 01:34:01.082: INFO: Restart count of pod container-probe-8290/liveness-ef810c39-fafa-429e-b9df-2c416b238e4e is now 1 (20.044773973s elapsed)
STEP: deleting the pod 02/27/23 01:34:01.082
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 01:34:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8290" for this suite. 02/27/23 01:34:01.105
------------------------------
• [SLOW TEST] [22.145 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:33:38.966
    Feb 27 01:33:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 01:33:38.967
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:33:38.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:33:38.985
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e in namespace container-probe-8290 02/27/23 01:33:38.989
    Feb 27 01:33:39.017: INFO: Waiting up to 5m0s for pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e" in namespace "container-probe-8290" to be "not pending"
    Feb 27 01:33:39.022: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980755ms
    Feb 27 01:33:41.031: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013639183s
    Feb 27 01:33:41.031: INFO: Pod "liveness-ef810c39-fafa-429e-b9df-2c416b238e4e" satisfied condition "not pending"
    Feb 27 01:33:41.031: INFO: Started pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e in namespace container-probe-8290
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:33:41.031
    Feb 27 01:33:41.037: INFO: Initial restart count of pod liveness-ef810c39-fafa-429e-b9df-2c416b238e4e is 0
    Feb 27 01:34:01.082: INFO: Restart count of pod container-probe-8290/liveness-ef810c39-fafa-429e-b9df-2c416b238e4e is now 1 (20.044773973s elapsed)
    STEP: deleting the pod 02/27/23 01:34:01.082
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:34:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8290" for this suite. 02/27/23 01:34:01.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:34:01.112
Feb 27 01:34:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:34:01.113
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:01.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:01.13
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 02/27/23 01:34:01.133
Feb 27 01:34:01.162: INFO: Waiting up to 5m0s for pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269" in namespace "downward-api-3476" to be "Succeeded or Failed"
Feb 27 01:34:01.165: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.788645ms
Feb 27 01:34:03.170: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Running", Reason="", readiness=true. Elapsed: 2.007117224s
Feb 27 01:34:05.170: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Running", Reason="", readiness=false. Elapsed: 4.007839793s
Feb 27 01:34:07.171: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008042838s
STEP: Saw pod success 02/27/23 01:34:07.171
Feb 27 01:34:07.171: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269" satisfied condition "Succeeded or Failed"
Feb 27 01:34:07.173: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 container dapi-container: <nil>
STEP: delete the pod 02/27/23 01:34:07.181
Feb 27 01:34:07.191: INFO: Waiting for pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 to disappear
Feb 27 01:34:07.195: INFO: Pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 01:34:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3476" for this suite. 02/27/23 01:34:07.199
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:34:01.112
    Feb 27 01:34:01.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:34:01.113
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:01.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:01.13
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 02/27/23 01:34:01.133
    Feb 27 01:34:01.162: INFO: Waiting up to 5m0s for pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269" in namespace "downward-api-3476" to be "Succeeded or Failed"
    Feb 27 01:34:01.165: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.788645ms
    Feb 27 01:34:03.170: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Running", Reason="", readiness=true. Elapsed: 2.007117224s
    Feb 27 01:34:05.170: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Running", Reason="", readiness=false. Elapsed: 4.007839793s
    Feb 27 01:34:07.171: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008042838s
    STEP: Saw pod success 02/27/23 01:34:07.171
    Feb 27 01:34:07.171: INFO: Pod "downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269" satisfied condition "Succeeded or Failed"
    Feb 27 01:34:07.173: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 01:34:07.181
    Feb 27 01:34:07.191: INFO: Waiting for pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 to disappear
    Feb 27 01:34:07.195: INFO: Pod downward-api-4f90196b-0f18-4a82-9fb8-69824b09f269 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:34:07.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3476" for this suite. 02/27/23 01:34:07.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:34:07.205
Feb 27 01:34:07.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 01:34:07.206
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:07.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:07.239
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 02/27/23 01:34:07.244
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 01:34:07.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2089" for this suite. 02/27/23 01:34:07.256
------------------------------
• [0.060 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:34:07.205
    Feb 27 01:34:07.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 01:34:07.206
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:07.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:07.239
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 02/27/23 01:34:07.244
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:34:07.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2089" for this suite. 02/27/23 01:34:07.256
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:34:07.266
Feb 27 01:34:07.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 01:34:07.267
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:07.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:07.285
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 02/27/23 01:34:07.288
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_tcp@PTR;sleep 1; done
 02/27/23 01:34:07.314
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_tcp@PTR;sleep 1; done
 02/27/23 01:34:07.314
STEP: creating a pod to probe DNS 02/27/23 01:34:07.314
STEP: submitting the pod to kubernetes 02/27/23 01:34:07.314
Feb 27 01:34:07.346: INFO: Waiting up to 15m0s for pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e" in namespace "dns-5011" to be "running"
Feb 27 01:34:07.351: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189416ms
Feb 27 01:34:09.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558542s
Feb 27 01:34:11.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009425517s
Feb 27 01:34:13.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009582131s
Feb 27 01:34:15.356: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Running", Reason="", readiness=true. Elapsed: 8.010459332s
Feb 27 01:34:15.356: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e" satisfied condition "running"
STEP: retrieving the pod 02/27/23 01:34:15.356
STEP: looking for the results for each expected name from probers 02/27/23 01:34:15.36
Feb 27 01:34:15.365: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.369: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.373: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.397: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.404: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:15.420: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:20.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.429: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.435: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.451: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.454: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.457: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.461: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:20.477: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:25.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.428: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.438: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.453: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.455: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.460: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:25.476: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:30.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.432: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.435: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.439: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.455: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.458: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.460: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:30.481: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:35.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.431: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.435: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.438: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.454: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.457: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.459: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:35.475: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:40.437: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
Feb 27 01:34:40.479: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

Feb 27 01:34:45.524: INFO: DNS probes using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e succeeded

STEP: deleting the pod 02/27/23 01:34:45.525
STEP: deleting the test service 02/27/23 01:34:45.569
STEP: deleting the test headless service 02/27/23 01:34:45.622
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 01:34:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5011" for this suite. 02/27/23 01:34:45.655
------------------------------
• [SLOW TEST] [38.401 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:34:07.266
    Feb 27 01:34:07.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 01:34:07.267
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:07.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:07.285
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 02/27/23 01:34:07.288
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_tcp@PTR;sleep 1; done
     02/27/23 01:34:07.314
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5011.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5011.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5011.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.171.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.171.125_tcp@PTR;sleep 1; done
     02/27/23 01:34:07.314
    STEP: creating a pod to probe DNS 02/27/23 01:34:07.314
    STEP: submitting the pod to kubernetes 02/27/23 01:34:07.314
    Feb 27 01:34:07.346: INFO: Waiting up to 15m0s for pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e" in namespace "dns-5011" to be "running"
    Feb 27 01:34:07.351: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189416ms
    Feb 27 01:34:09.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558542s
    Feb 27 01:34:11.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009425517s
    Feb 27 01:34:13.355: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009582131s
    Feb 27 01:34:15.356: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e": Phase="Running", Reason="", readiness=true. Elapsed: 8.010459332s
    Feb 27 01:34:15.356: INFO: Pod "dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 01:34:15.356
    STEP: looking for the results for each expected name from probers 02/27/23 01:34:15.36
    Feb 27 01:34:15.365: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.369: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.373: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.397: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.400: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.404: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.407: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:15.420: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:20.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.429: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.435: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.451: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.454: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.457: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.461: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:20.477: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:25.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.428: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.438: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.453: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.455: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.460: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:25.476: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:30.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.432: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.435: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.439: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.455: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.458: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.460: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:30.481: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:35.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.431: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.435: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.438: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.454: INFO: Unable to read jessie_udp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.457: INFO: Unable to read jessie_tcp@dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.459: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.463: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:35.475: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_udp@dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@dns-test-service.dns-5011.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_udp@dns-test-service.dns-5011.svc.cluster.local jessie_tcp@dns-test-service.dns-5011.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:40.437: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local from pod dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e: the server could not find the requested resource (get pods dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e)
    Feb 27 01:34:40.479: INFO: Lookups using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-5011.svc.cluster.local]

    Feb 27 01:34:45.524: INFO: DNS probes using dns-5011/dns-test-abde5ae2-645c-4b70-b3d1-dacd17369b4e succeeded

    STEP: deleting the pod 02/27/23 01:34:45.525
    STEP: deleting the test service 02/27/23 01:34:45.569
    STEP: deleting the test headless service 02/27/23 01:34:45.622
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:34:45.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5011" for this suite. 02/27/23 01:34:45.655
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:34:45.668
Feb 27 01:34:45.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:34:45.668
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:45.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:45.688
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:34:45.691
Feb 27 01:34:45.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66" in namespace "projected-7651" to be "Succeeded or Failed"
Feb 27 01:34:45.738: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350191ms
Feb 27 01:34:47.741: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009049312s
Feb 27 01:34:49.743: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010487645s
STEP: Saw pod success 02/27/23 01:34:49.743
Feb 27 01:34:49.743: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66" satisfied condition "Succeeded or Failed"
Feb 27 01:34:49.746: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 container client-container: <nil>
STEP: delete the pod 02/27/23 01:34:49.754
Feb 27 01:34:49.768: INFO: Waiting for pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 to disappear
Feb 27 01:34:49.772: INFO: Pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 01:34:49.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7651" for this suite. 02/27/23 01:34:49.776
------------------------------
• [4.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:34:45.668
    Feb 27 01:34:45.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:34:45.668
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:45.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:45.688
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:34:45.691
    Feb 27 01:34:45.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66" in namespace "projected-7651" to be "Succeeded or Failed"
    Feb 27 01:34:45.738: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350191ms
    Feb 27 01:34:47.741: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009049312s
    Feb 27 01:34:49.743: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010487645s
    STEP: Saw pod success 02/27/23 01:34:49.743
    Feb 27 01:34:49.743: INFO: Pod "downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66" satisfied condition "Succeeded or Failed"
    Feb 27 01:34:49.746: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 container client-container: <nil>
    STEP: delete the pod 02/27/23 01:34:49.754
    Feb 27 01:34:49.768: INFO: Waiting for pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 to disappear
    Feb 27 01:34:49.772: INFO: Pod downwardapi-volume-7118ee76-bb23-41cf-921e-d4570c855d66 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:34:49.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7651" for this suite. 02/27/23 01:34:49.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:34:49.785
Feb 27 01:34:49.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 01:34:49.786
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:49.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:49.806
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 02/27/23 01:34:49.809
STEP: Creating a ResourceQuota 02/27/23 01:34:54.813
STEP: Ensuring resource quota status is calculated 02/27/23 01:34:54.822
STEP: Creating a ReplicaSet 02/27/23 01:34:56.827
STEP: Ensuring resource quota status captures replicaset creation 02/27/23 01:34:56.841
STEP: Deleting a ReplicaSet 02/27/23 01:34:58.845
STEP: Ensuring resource quota status released usage 02/27/23 01:34:58.853
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9336" for this suite. 02/27/23 01:35:00.863
------------------------------
• [SLOW TEST] [11.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:34:49.785
    Feb 27 01:34:49.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 01:34:49.786
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:34:49.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:34:49.806
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 02/27/23 01:34:49.809
    STEP: Creating a ResourceQuota 02/27/23 01:34:54.813
    STEP: Ensuring resource quota status is calculated 02/27/23 01:34:54.822
    STEP: Creating a ReplicaSet 02/27/23 01:34:56.827
    STEP: Ensuring resource quota status captures replicaset creation 02/27/23 01:34:56.841
    STEP: Deleting a ReplicaSet 02/27/23 01:34:58.845
    STEP: Ensuring resource quota status released usage 02/27/23 01:34:58.853
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9336" for this suite. 02/27/23 01:35:00.863
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:00.87
Feb 27 01:35:00.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 01:35:00.871
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:00.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:00.892
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 02/27/23 01:35:00.894
Feb 27 01:35:00.938: INFO: Waiting up to 5m0s for pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58" in namespace "var-expansion-2229" to be "Succeeded or Failed"
Feb 27 01:35:00.947: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Pending", Reason="", readiness=false. Elapsed: 9.098523ms
Feb 27 01:35:02.982: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044299288s
Feb 27 01:35:04.952: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01444058s
STEP: Saw pod success 02/27/23 01:35:04.952
Feb 27 01:35:04.952: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58" satisfied condition "Succeeded or Failed"
Feb 27 01:35:04.955: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 container dapi-container: <nil>
STEP: delete the pod 02/27/23 01:35:04.961
Feb 27 01:35:04.973: INFO: Waiting for pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 to disappear
Feb 27 01:35:04.976: INFO: Pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:04.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2229" for this suite. 02/27/23 01:35:04.98
------------------------------
• [4.116 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:00.87
    Feb 27 01:35:00.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 01:35:00.871
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:00.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:00.892
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 02/27/23 01:35:00.894
    Feb 27 01:35:00.938: INFO: Waiting up to 5m0s for pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58" in namespace "var-expansion-2229" to be "Succeeded or Failed"
    Feb 27 01:35:00.947: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Pending", Reason="", readiness=false. Elapsed: 9.098523ms
    Feb 27 01:35:02.982: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044299288s
    Feb 27 01:35:04.952: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01444058s
    STEP: Saw pod success 02/27/23 01:35:04.952
    Feb 27 01:35:04.952: INFO: Pod "var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58" satisfied condition "Succeeded or Failed"
    Feb 27 01:35:04.955: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 01:35:04.961
    Feb 27 01:35:04.973: INFO: Waiting for pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 to disappear
    Feb 27 01:35:04.976: INFO: Pod var-expansion-8bb7eefc-3c1d-4e70-ab42-a4a9fc432b58 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:04.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2229" for this suite. 02/27/23 01:35:04.98
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:04.986
Feb 27 01:35:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 01:35:04.987
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:05.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:05.009
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7146 02/27/23 01:35:05.011
STEP: changing the ExternalName service to type=NodePort 02/27/23 01:35:05.017
STEP: creating replication controller externalname-service in namespace services-7146 02/27/23 01:35:05.043
I0227 01:35:05.052809      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7146, replica count: 2
I0227 01:35:08.104422      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 01:35:08.104: INFO: Creating new exec pod
Feb 27 01:35:08.114: INFO: Waiting up to 5m0s for pod "execpodc6npj" in namespace "services-7146" to be "running"
Feb 27 01:35:08.119: INFO: Pod "execpodc6npj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925254ms
Feb 27 01:35:10.126: INFO: Pod "execpodc6npj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011850005s
Feb 27 01:35:10.126: INFO: Pod "execpodc6npj" satisfied condition "running"
Feb 27 01:35:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 27 01:35:11.262: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 01:35:11.262: INFO: stdout: ""
Feb 27 01:35:11.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.96.206.32 80'
Feb 27 01:35:11.396: INFO: stderr: "+ nc -v -z -w 2 10.96.206.32 80\nConnection to 10.96.206.32 80 port [tcp/http] succeeded!\n"
Feb 27 01:35:11.396: INFO: stdout: ""
Feb 27 01:35:11.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.15 30998'
Feb 27 01:35:11.533: INFO: stderr: "+ nc -v -z -w 2 10.0.10.15 30998\nConnection to 10.0.10.15 30998 port [tcp/*] succeeded!\n"
Feb 27 01:35:11.533: INFO: stdout: ""
Feb 27 01:35:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.5 30998'
Feb 27 01:35:11.670: INFO: stderr: "+ nc -v -z -w 2 10.0.10.5 30998\nConnection to 10.0.10.5 30998 port [tcp/*] succeeded!\n"
Feb 27 01:35:11.670: INFO: stdout: ""
Feb 27 01:35:11.670: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7146" for this suite. 02/27/23 01:35:11.717
------------------------------
• [SLOW TEST] [6.739 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:04.986
    Feb 27 01:35:04.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 01:35:04.987
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:05.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:05.009
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7146 02/27/23 01:35:05.011
    STEP: changing the ExternalName service to type=NodePort 02/27/23 01:35:05.017
    STEP: creating replication controller externalname-service in namespace services-7146 02/27/23 01:35:05.043
    I0227 01:35:05.052809      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7146, replica count: 2
    I0227 01:35:08.104422      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 01:35:08.104: INFO: Creating new exec pod
    Feb 27 01:35:08.114: INFO: Waiting up to 5m0s for pod "execpodc6npj" in namespace "services-7146" to be "running"
    Feb 27 01:35:08.119: INFO: Pod "execpodc6npj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.925254ms
    Feb 27 01:35:10.126: INFO: Pod "execpodc6npj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011850005s
    Feb 27 01:35:10.126: INFO: Pod "execpodc6npj" satisfied condition "running"
    Feb 27 01:35:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 27 01:35:11.262: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 01:35:11.262: INFO: stdout: ""
    Feb 27 01:35:11.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.96.206.32 80'
    Feb 27 01:35:11.396: INFO: stderr: "+ nc -v -z -w 2 10.96.206.32 80\nConnection to 10.96.206.32 80 port [tcp/http] succeeded!\n"
    Feb 27 01:35:11.396: INFO: stdout: ""
    Feb 27 01:35:11.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.15 30998'
    Feb 27 01:35:11.533: INFO: stderr: "+ nc -v -z -w 2 10.0.10.15 30998\nConnection to 10.0.10.15 30998 port [tcp/*] succeeded!\n"
    Feb 27 01:35:11.533: INFO: stdout: ""
    Feb 27 01:35:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-7146 exec execpodc6npj -- /bin/sh -x -c nc -v -z -w 2 10.0.10.5 30998'
    Feb 27 01:35:11.670: INFO: stderr: "+ nc -v -z -w 2 10.0.10.5 30998\nConnection to 10.0.10.5 30998 port [tcp/*] succeeded!\n"
    Feb 27 01:35:11.670: INFO: stdout: ""
    Feb 27 01:35:11.670: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7146" for this suite. 02/27/23 01:35:11.717
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:11.726
Feb 27 01:35:11.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:35:11.727
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:11.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:11.766
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Feb 27 01:35:11.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 01:35:13.761
Feb 27 01:35:13.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
Feb 27 01:35:14.339: INFO: stderr: ""
Feb 27 01:35:14.339: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 01:35:14.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 delete e2e-test-crd-publish-openapi-11-crds test-foo'
Feb 27 01:35:14.527: INFO: stderr: ""
Feb 27 01:35:14.527: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 27 01:35:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
Feb 27 01:35:15.030: INFO: stderr: ""
Feb 27 01:35:15.030: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 27 01:35:15.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 delete e2e-test-crd-publish-openapi-11-crds test-foo'
Feb 27 01:35:15.103: INFO: stderr: ""
Feb 27 01:35:15.103: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 01:35:15.103
Feb 27 01:35:15.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
Feb 27 01:35:15.607: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 01:35:15.607
Feb 27 01:35:15.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
Feb 27 01:35:15.786: INFO: rc: 1
Feb 27 01:35:15.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
Feb 27 01:35:15.954: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 01:35:15.955
Feb 27 01:35:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
Feb 27 01:35:16.117: INFO: rc: 1
Feb 27 01:35:16.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
Feb 27 01:35:16.278: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 02/27/23 01:35:16.278
Feb 27 01:35:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds'
Feb 27 01:35:16.435: INFO: stderr: ""
Feb 27 01:35:16.435: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 02/27/23 01:35:16.435
Feb 27 01:35:16.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.metadata'
Feb 27 01:35:16.587: INFO: stderr: ""
Feb 27 01:35:16.587: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 27 01:35:16.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec'
Feb 27 01:35:16.733: INFO: stderr: ""
Feb 27 01:35:16.733: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 27 01:35:16.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec.bars'
Feb 27 01:35:16.883: INFO: stderr: ""
Feb 27 01:35:16.883: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 01:35:16.883
Feb 27 01:35:16.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec.bars2'
Feb 27 01:35:17.035: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:19.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6509" for this suite. 02/27/23 01:35:19.236
------------------------------
• [SLOW TEST] [7.518 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:11.726
    Feb 27 01:35:11.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:35:11.727
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:11.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:11.766
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Feb 27 01:35:11.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 02/27/23 01:35:13.761
    Feb 27 01:35:13.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
    Feb 27 01:35:14.339: INFO: stderr: ""
    Feb 27 01:35:14.339: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 01:35:14.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 delete e2e-test-crd-publish-openapi-11-crds test-foo'
    Feb 27 01:35:14.527: INFO: stderr: ""
    Feb 27 01:35:14.527: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Feb 27 01:35:14.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
    Feb 27 01:35:15.030: INFO: stderr: ""
    Feb 27 01:35:15.030: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Feb 27 01:35:15.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 delete e2e-test-crd-publish-openapi-11-crds test-foo'
    Feb 27 01:35:15.103: INFO: stderr: ""
    Feb 27 01:35:15.103: INFO: stdout: "e2e-test-crd-publish-openapi-11-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 02/27/23 01:35:15.103
    Feb 27 01:35:15.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
    Feb 27 01:35:15.607: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 02/27/23 01:35:15.607
    Feb 27 01:35:15.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
    Feb 27 01:35:15.786: INFO: rc: 1
    Feb 27 01:35:15.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
    Feb 27 01:35:15.954: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 02/27/23 01:35:15.955
    Feb 27 01:35:15.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 create -f -'
    Feb 27 01:35:16.117: INFO: rc: 1
    Feb 27 01:35:16.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 --namespace=crd-publish-openapi-6509 apply -f -'
    Feb 27 01:35:16.278: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 02/27/23 01:35:16.278
    Feb 27 01:35:16.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds'
    Feb 27 01:35:16.435: INFO: stderr: ""
    Feb 27 01:35:16.435: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 02/27/23 01:35:16.435
    Feb 27 01:35:16.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.metadata'
    Feb 27 01:35:16.587: INFO: stderr: ""
    Feb 27 01:35:16.587: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Feb 27 01:35:16.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec'
    Feb 27 01:35:16.733: INFO: stderr: ""
    Feb 27 01:35:16.733: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Feb 27 01:35:16.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec.bars'
    Feb 27 01:35:16.883: INFO: stderr: ""
    Feb 27 01:35:16.883: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-11-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 02/27/23 01:35:16.883
    Feb 27 01:35:16.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6509 explain e2e-test-crd-publish-openapi-11-crds.spec.bars2'
    Feb 27 01:35:17.035: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:19.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6509" for this suite. 02/27/23 01:35:19.236
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:19.244
Feb 27 01:35:19.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 01:35:19.245
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:19.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:19.269
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 02/27/23 01:35:19.273
STEP: fetching the ConfigMap 02/27/23 01:35:19.277
STEP: patching the ConfigMap 02/27/23 01:35:19.279
STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 01:35:19.288
STEP: deleting the ConfigMap by collection with a label selector 02/27/23 01:35:19.292
STEP: listing all ConfigMaps in test namespace 02/27/23 01:35:19.3
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:19.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2211" for this suite. 02/27/23 01:35:19.306
------------------------------
• [0.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:19.244
    Feb 27 01:35:19.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 01:35:19.245
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:19.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:19.269
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 02/27/23 01:35:19.273
    STEP: fetching the ConfigMap 02/27/23 01:35:19.277
    STEP: patching the ConfigMap 02/27/23 01:35:19.279
    STEP: listing all ConfigMaps in all namespaces with a label selector 02/27/23 01:35:19.288
    STEP: deleting the ConfigMap by collection with a label selector 02/27/23 01:35:19.292
    STEP: listing all ConfigMaps in test namespace 02/27/23 01:35:19.3
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:19.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2211" for this suite. 02/27/23 01:35:19.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:19.313
Feb 27 01:35:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:35:19.313
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:19.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:19.331
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:35:19.334
Feb 27 01:35:19.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d" in namespace "projected-4583" to be "Succeeded or Failed"
Feb 27 01:35:19.377: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195457ms
Feb 27 01:35:21.383: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009140582s
Feb 27 01:35:23.382: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007913184s
STEP: Saw pod success 02/27/23 01:35:23.382
Feb 27 01:35:23.382: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d" satisfied condition "Succeeded or Failed"
Feb 27 01:35:23.385: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d container client-container: <nil>
STEP: delete the pod 02/27/23 01:35:23.392
Feb 27 01:35:23.407: INFO: Waiting for pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d to disappear
Feb 27 01:35:23.409: INFO: Pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:23.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4583" for this suite. 02/27/23 01:35:23.439
------------------------------
• [4.133 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:19.313
    Feb 27 01:35:19.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:35:19.313
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:19.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:19.331
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:35:19.334
    Feb 27 01:35:19.374: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d" in namespace "projected-4583" to be "Succeeded or Failed"
    Feb 27 01:35:19.377: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.195457ms
    Feb 27 01:35:21.383: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009140582s
    Feb 27 01:35:23.382: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007913184s
    STEP: Saw pod success 02/27/23 01:35:23.382
    Feb 27 01:35:23.382: INFO: Pod "downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d" satisfied condition "Succeeded or Failed"
    Feb 27 01:35:23.385: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d container client-container: <nil>
    STEP: delete the pod 02/27/23 01:35:23.392
    Feb 27 01:35:23.407: INFO: Waiting for pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d to disappear
    Feb 27 01:35:23.409: INFO: Pod downwardapi-volume-9e67006d-5240-44b2-9fab-5c8daab7603d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:23.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4583" for this suite. 02/27/23 01:35:23.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:23.446
Feb 27 01:35:23.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 01:35:23.447
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:23.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:23.468
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 02/27/23 01:35:23.47
Feb 27 01:35:23.473: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 02/27/23 01:35:23.473
Feb 27 01:35:23.478: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 02/27/23 01:35:23.478
Feb 27 01:35:23.487: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:23.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4719" for this suite. 02/27/23 01:35:23.491
------------------------------
• [0.064 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:23.446
    Feb 27 01:35:23.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 01:35:23.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:23.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:23.468
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 02/27/23 01:35:23.47
    Feb 27 01:35:23.473: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 02/27/23 01:35:23.473
    Feb 27 01:35:23.478: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 02/27/23 01:35:23.478
    Feb 27 01:35:23.487: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:23.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4719" for this suite. 02/27/23 01:35:23.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:23.511
Feb 27 01:35:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 01:35:23.512
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:23.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:23.532
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 02/27/23 01:35:23.534
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 02/27/23 01:35:23.541
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 02/27/23 01:35:23.541
STEP: creating a pod to probe DNS 02/27/23 01:35:23.541
STEP: submitting the pod to kubernetes 02/27/23 01:35:23.541
Feb 27 01:35:23.553: INFO: Waiting up to 15m0s for pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba" in namespace "dns-4192" to be "running"
Feb 27 01:35:23.560: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.504328ms
Feb 27 01:35:25.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013362185s
Feb 27 01:35:27.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012767993s
Feb 27 01:35:29.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.01353681s
Feb 27 01:35:29.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba" satisfied condition "running"
STEP: retrieving the pod 02/27/23 01:35:29.566
STEP: looking for the results for each expected name from probers 02/27/23 01:35:29.57
Feb 27 01:35:29.584: INFO: DNS probes using dns-4192/dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba succeeded

STEP: deleting the pod 02/27/23 01:35:29.584
STEP: deleting the test headless service 02/27/23 01:35:29.598
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4192" for this suite. 02/27/23 01:35:29.63
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:23.511
    Feb 27 01:35:23.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 01:35:23.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:23.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:23.532
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 02/27/23 01:35:23.534
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     02/27/23 01:35:23.541
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4192.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     02/27/23 01:35:23.541
    STEP: creating a pod to probe DNS 02/27/23 01:35:23.541
    STEP: submitting the pod to kubernetes 02/27/23 01:35:23.541
    Feb 27 01:35:23.553: INFO: Waiting up to 15m0s for pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba" in namespace "dns-4192" to be "running"
    Feb 27 01:35:23.560: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.504328ms
    Feb 27 01:35:25.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013362185s
    Feb 27 01:35:27.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012767993s
    Feb 27 01:35:29.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.01353681s
    Feb 27 01:35:29.566: INFO: Pod "dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 01:35:29.566
    STEP: looking for the results for each expected name from probers 02/27/23 01:35:29.57
    Feb 27 01:35:29.584: INFO: DNS probes using dns-4192/dns-test-58df4c41-e4ba-4540-8c8b-36b87a79b1ba succeeded

    STEP: deleting the pod 02/27/23 01:35:29.584
    STEP: deleting the test headless service 02/27/23 01:35:29.598
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:29.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4192" for this suite. 02/27/23 01:35:29.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:29.636
Feb 27 01:35:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 01:35:29.637
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:29.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:29.654
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Feb 27 01:35:29.686: INFO: Waiting up to 2m0s for pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" in namespace "var-expansion-7980" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 01:35:29.689: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751818ms
Feb 27 01:35:31.693: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283618s
Feb 27 01:35:31.693: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 01:35:31.693: INFO: Deleting pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" in namespace "var-expansion-7980"
Feb 27 01:35:31.703: INFO: Wait up to 5m0s for pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7980" for this suite. 02/27/23 01:35:33.715
------------------------------
• [4.085 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:29.636
    Feb 27 01:35:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 01:35:29.637
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:29.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:29.654
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Feb 27 01:35:29.686: INFO: Waiting up to 2m0s for pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" in namespace "var-expansion-7980" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 01:35:29.689: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751818ms
    Feb 27 01:35:31.693: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007283618s
    Feb 27 01:35:31.693: INFO: Pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 01:35:31.693: INFO: Deleting pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" in namespace "var-expansion-7980"
    Feb 27 01:35:31.703: INFO: Wait up to 5m0s for pod "var-expansion-851b707a-577c-44c4-a1a4-4564ecb999b1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7980" for this suite. 02/27/23 01:35:33.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:33.722
Feb 27 01:35:33.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 01:35:33.723
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.746
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 02/27/23 01:35:33.748
STEP: getting 02/27/23 01:35:33.769
STEP: listing 02/27/23 01:35:33.774
STEP: deleting 02/27/23 01:35:33.777
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:33.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-426" for this suite. 02/27/23 01:35:33.801
------------------------------
• [0.087 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:33.722
    Feb 27 01:35:33.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename csiinlinevolumes 02/27/23 01:35:33.723
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.746
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 02/27/23 01:35:33.748
    STEP: getting 02/27/23 01:35:33.769
    STEP: listing 02/27/23 01:35:33.774
    STEP: deleting 02/27/23 01:35:33.777
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:33.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-426" for this suite. 02/27/23 01:35:33.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:33.809
Feb 27 01:35:33.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:35:33.81
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.831
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-2g6tg"  02/27/23 01:35:33.834
Feb 27 01:35:33.839: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-2g6tg"  02/27/23 01:35:33.839
Feb 27 01:35:33.849: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:33.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1956" for this suite. 02/27/23 01:35:33.854
------------------------------
• [0.052 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:33.809
    Feb 27 01:35:33.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:35:33.81
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.831
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-2g6tg"  02/27/23 01:35:33.834
    Feb 27 01:35:33.839: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-2g6tg"  02/27/23 01:35:33.839
    Feb 27 01:35:33.849: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:33.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1956" for this suite. 02/27/23 01:35:33.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:33.862
Feb 27 01:35:33.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename events 02/27/23 01:35:33.863
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.88
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 02/27/23 01:35:33.882
STEP: listing events in all namespaces 02/27/23 01:35:33.889
STEP: listing events in test namespace 02/27/23 01:35:33.908
STEP: listing events with field selection filtering on source 02/27/23 01:35:33.911
STEP: listing events with field selection filtering on reportingController 02/27/23 01:35:33.914
STEP: getting the test event 02/27/23 01:35:33.916
STEP: patching the test event 02/27/23 01:35:33.919
STEP: getting the test event 02/27/23 01:35:33.928
STEP: updating the test event 02/27/23 01:35:33.93
STEP: getting the test event 02/27/23 01:35:33.936
STEP: deleting the test event 02/27/23 01:35:33.939
STEP: listing events in all namespaces 02/27/23 01:35:33.946
STEP: listing events in test namespace 02/27/23 01:35:33.97
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:33.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9317" for this suite. 02/27/23 01:35:33.979
------------------------------
• [0.123 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:33.862
    Feb 27 01:35:33.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename events 02/27/23 01:35:33.863
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:33.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:33.88
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 02/27/23 01:35:33.882
    STEP: listing events in all namespaces 02/27/23 01:35:33.889
    STEP: listing events in test namespace 02/27/23 01:35:33.908
    STEP: listing events with field selection filtering on source 02/27/23 01:35:33.911
    STEP: listing events with field selection filtering on reportingController 02/27/23 01:35:33.914
    STEP: getting the test event 02/27/23 01:35:33.916
    STEP: patching the test event 02/27/23 01:35:33.919
    STEP: getting the test event 02/27/23 01:35:33.928
    STEP: updating the test event 02/27/23 01:35:33.93
    STEP: getting the test event 02/27/23 01:35:33.936
    STEP: deleting the test event 02/27/23 01:35:33.939
    STEP: listing events in all namespaces 02/27/23 01:35:33.946
    STEP: listing events in test namespace 02/27/23 01:35:33.97
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:33.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9317" for this suite. 02/27/23 01:35:33.979
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:33.985
Feb 27 01:35:33.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 01:35:33.986
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:34.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:34.008
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 02/27/23 01:35:34.011
STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 01:35:34.016
STEP: patching the secret 02/27/23 01:35:34.054
STEP: deleting the secret using a LabelSelector 02/27/23 01:35:34.063
STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 01:35:34.07
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 01:35:34.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4626" for this suite. 02/27/23 01:35:34.081
------------------------------
• [0.104 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:33.985
    Feb 27 01:35:33.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 01:35:33.986
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:34.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:34.008
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 02/27/23 01:35:34.011
    STEP: listing secrets in all namespaces to ensure that there are more than zero 02/27/23 01:35:34.016
    STEP: patching the secret 02/27/23 01:35:34.054
    STEP: deleting the secret using a LabelSelector 02/27/23 01:35:34.063
    STEP: listing secrets in all namespaces, searching for label name and value in patch 02/27/23 01:35:34.07
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:35:34.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4626" for this suite. 02/27/23 01:35:34.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:35:34.09
Feb 27 01:35:34.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 01:35:34.091
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:34.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:34.11
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 02/27/23 01:35:51.117
STEP: Creating a ResourceQuota 02/27/23 01:35:56.122
STEP: Ensuring resource quota status is calculated 02/27/23 01:35:56.129
STEP: Creating a ConfigMap 02/27/23 01:35:58.133
STEP: Ensuring resource quota status captures configMap creation 02/27/23 01:35:58.146
STEP: Deleting a ConfigMap 02/27/23 01:36:00.151
STEP: Ensuring resource quota status released usage 02/27/23 01:36:00.157
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 01:36:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8576" for this suite. 02/27/23 01:36:02.168
------------------------------
• [SLOW TEST] [28.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:35:34.09
    Feb 27 01:35:34.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 01:35:34.091
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:35:34.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:35:34.11
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 02/27/23 01:35:51.117
    STEP: Creating a ResourceQuota 02/27/23 01:35:56.122
    STEP: Ensuring resource quota status is calculated 02/27/23 01:35:56.129
    STEP: Creating a ConfigMap 02/27/23 01:35:58.133
    STEP: Ensuring resource quota status captures configMap creation 02/27/23 01:35:58.146
    STEP: Deleting a ConfigMap 02/27/23 01:36:00.151
    STEP: Ensuring resource quota status released usage 02/27/23 01:36:00.157
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:36:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8576" for this suite. 02/27/23 01:36:02.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:36:02.175
Feb 27 01:36:02.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 01:36:02.176
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:36:02.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:36:02.199
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 in namespace container-probe-5678 02/27/23 01:36:02.201
Feb 27 01:36:02.237: INFO: Waiting up to 5m0s for pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656" in namespace "container-probe-5678" to be "not pending"
Feb 27 01:36:02.244: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656": Phase="Pending", Reason="", readiness=false. Elapsed: 6.980534ms
Feb 27 01:36:04.249: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656": Phase="Running", Reason="", readiness=true. Elapsed: 2.011936298s
Feb 27 01:36:04.249: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656" satisfied condition "not pending"
Feb 27 01:36:04.249: INFO: Started pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 in namespace container-probe-5678
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:36:04.249
Feb 27 01:36:04.253: INFO: Initial restart count of pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is 0
Feb 27 01:36:24.307: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 1 (20.054149094s elapsed)
Feb 27 01:36:44.379: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 2 (40.126015372s elapsed)
Feb 27 01:37:04.440: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 3 (1m0.18740745s elapsed)
Feb 27 01:37:24.494: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 4 (1m20.241273167s elapsed)
Feb 27 01:38:36.688: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 5 (2m32.435204356s elapsed)
STEP: deleting the pod 02/27/23 01:38:36.688
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:36.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5678" for this suite. 02/27/23 01:38:36.72
------------------------------
• [SLOW TEST] [154.551 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:36:02.175
    Feb 27 01:36:02.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 01:36:02.176
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:36:02.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:36:02.199
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 in namespace container-probe-5678 02/27/23 01:36:02.201
    Feb 27 01:36:02.237: INFO: Waiting up to 5m0s for pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656" in namespace "container-probe-5678" to be "not pending"
    Feb 27 01:36:02.244: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656": Phase="Pending", Reason="", readiness=false. Elapsed: 6.980534ms
    Feb 27 01:36:04.249: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656": Phase="Running", Reason="", readiness=true. Elapsed: 2.011936298s
    Feb 27 01:36:04.249: INFO: Pod "liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656" satisfied condition "not pending"
    Feb 27 01:36:04.249: INFO: Started pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 in namespace container-probe-5678
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:36:04.249
    Feb 27 01:36:04.253: INFO: Initial restart count of pod liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is 0
    Feb 27 01:36:24.307: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 1 (20.054149094s elapsed)
    Feb 27 01:36:44.379: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 2 (40.126015372s elapsed)
    Feb 27 01:37:04.440: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 3 (1m0.18740745s elapsed)
    Feb 27 01:37:24.494: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 4 (1m20.241273167s elapsed)
    Feb 27 01:38:36.688: INFO: Restart count of pod container-probe-5678/liveness-159cd6c2-af2d-4798-b80a-a0f73b46b656 is now 5 (2m32.435204356s elapsed)
    STEP: deleting the pod 02/27/23 01:38:36.688
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:36.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5678" for this suite. 02/27/23 01:38:36.72
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:36.726
Feb 27 01:38:36.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:38:36.726
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:36.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:36.771
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:38:36.774
Feb 27 01:38:36.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3" in namespace "downward-api-486" to be "Succeeded or Failed"
Feb 27 01:38:36.824: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.98611ms
Feb 27 01:38:38.830: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013637639s
Feb 27 01:38:40.829: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012948221s
STEP: Saw pod success 02/27/23 01:38:40.829
Feb 27 01:38:40.829: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3" satisfied condition "Succeeded or Failed"
Feb 27 01:38:40.834: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 container client-container: <nil>
STEP: delete the pod 02/27/23 01:38:40.843
Feb 27 01:38:40.866: INFO: Waiting for pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 to disappear
Feb 27 01:38:40.869: INFO: Pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-486" for this suite. 02/27/23 01:38:40.872
------------------------------
• [4.160 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:36.726
    Feb 27 01:38:36.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:38:36.726
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:36.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:36.771
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:38:36.774
    Feb 27 01:38:36.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3" in namespace "downward-api-486" to be "Succeeded or Failed"
    Feb 27 01:38:36.824: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.98611ms
    Feb 27 01:38:38.830: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013637639s
    Feb 27 01:38:40.829: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012948221s
    STEP: Saw pod success 02/27/23 01:38:40.829
    Feb 27 01:38:40.829: INFO: Pod "downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3" satisfied condition "Succeeded or Failed"
    Feb 27 01:38:40.834: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 container client-container: <nil>
    STEP: delete the pod 02/27/23 01:38:40.843
    Feb 27 01:38:40.866: INFO: Waiting for pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 to disappear
    Feb 27 01:38:40.869: INFO: Pod downwardapi-volume-a0e7e155-95e5-437a-8e71-5bdf300e25b3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-486" for this suite. 02/27/23 01:38:40.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:40.886
Feb 27 01:38:40.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 01:38:40.887
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:40.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:40.909
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 02/27/23 01:38:40.929
STEP: watching for Pod to be ready 02/27/23 01:38:40.963
Feb 27 01:38:40.965: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions []
Feb 27 01:38:40.973: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
Feb 27 01:38:41.007: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
Feb 27 01:38:41.500: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
Feb 27 01:38:41.937: INFO: Found Pod pod-test in namespace pods-9668 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 02/27/23 01:38:41.94
STEP: getting the Pod and ensuring that it's patched 02/27/23 01:38:41.95
STEP: replacing the Pod's status Ready condition to False 02/27/23 01:38:41.953
STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 01:38:41.964
STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 01:38:41.965
STEP: watching for the Pod to be deleted 02/27/23 01:38:41.977
Feb 27 01:38:41.979: INFO: observed event type MODIFIED
Feb 27 01:38:43.944: INFO: observed event type MODIFIED
Feb 27 01:38:44.947: INFO: observed event type MODIFIED
Feb 27 01:38:44.976: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9668" for this suite. 02/27/23 01:38:44.993
------------------------------
• [4.116 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:40.886
    Feb 27 01:38:40.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 01:38:40.887
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:40.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:40.909
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 02/27/23 01:38:40.929
    STEP: watching for Pod to be ready 02/27/23 01:38:40.963
    Feb 27 01:38:40.965: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Feb 27 01:38:40.973: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
    Feb 27 01:38:41.007: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
    Feb 27 01:38:41.500: INFO: observed Pod pod-test in namespace pods-9668 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
    Feb 27 01:38:41.937: INFO: Found Pod pod-test in namespace pods-9668 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:38:40 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 02/27/23 01:38:41.94
    STEP: getting the Pod and ensuring that it's patched 02/27/23 01:38:41.95
    STEP: replacing the Pod's status Ready condition to False 02/27/23 01:38:41.953
    STEP: check the Pod again to ensure its Ready conditions are False 02/27/23 01:38:41.964
    STEP: deleting the Pod via a Collection with a LabelSelector 02/27/23 01:38:41.965
    STEP: watching for the Pod to be deleted 02/27/23 01:38:41.977
    Feb 27 01:38:41.979: INFO: observed event type MODIFIED
    Feb 27 01:38:43.944: INFO: observed event type MODIFIED
    Feb 27 01:38:44.947: INFO: observed event type MODIFIED
    Feb 27 01:38:44.976: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9668" for this suite. 02/27/23 01:38:44.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:45.002
Feb 27 01:38:45.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename containers 02/27/23 01:38:45.003
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:45.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:45.025
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Feb 27 01:38:45.040: INFO: Waiting up to 5m0s for pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126" in namespace "containers-1932" to be "running"
Feb 27 01:38:45.043: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762997ms
Feb 27 01:38:47.052: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126": Phase="Running", Reason="", readiness=true. Elapsed: 2.011518169s
Feb 27 01:38:47.052: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:47.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1932" for this suite. 02/27/23 01:38:47.062
------------------------------
• [2.072 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:45.002
    Feb 27 01:38:45.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename containers 02/27/23 01:38:45.003
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:45.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:45.025
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Feb 27 01:38:45.040: INFO: Waiting up to 5m0s for pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126" in namespace "containers-1932" to be "running"
    Feb 27 01:38:45.043: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762997ms
    Feb 27 01:38:47.052: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126": Phase="Running", Reason="", readiness=true. Elapsed: 2.011518169s
    Feb 27 01:38:47.052: INFO: Pod "client-containers-9374edb6-1b46-4e5a-8da8-d1bb6980a126" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:47.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1932" for this suite. 02/27/23 01:38:47.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:47.074
Feb 27 01:38:47.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename podtemplate 02/27/23 01:38:47.075
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.1
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:47.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6085" for this suite. 02/27/23 01:38:47.131
------------------------------
• [0.067 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:47.074
    Feb 27 01:38:47.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename podtemplate 02/27/23 01:38:47.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.1
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:47.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6085" for this suite. 02/27/23 01:38:47.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:47.142
Feb 27 01:38:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sysctl 02/27/23 01:38:47.143
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.165
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 01:38:47.167
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:38:47.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1810" for this suite. 02/27/23 01:38:47.181
------------------------------
• [0.044 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:47.142
    Feb 27 01:38:47.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sysctl 02/27/23 01:38:47.143
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.165
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 02/27/23 01:38:47.167
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:38:47.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1810" for this suite. 02/27/23 01:38:47.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:38:47.187
Feb 27 01:38:47.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename subpath 02/27/23 01:38:47.187
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.209
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 01:38:47.212
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-6j75 02/27/23 01:38:47.227
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 01:38:47.227
Feb 27 01:38:47.277: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6j75" in namespace "subpath-1525" to be "Succeeded or Failed"
Feb 27 01:38:47.283: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446128ms
Feb 27 01:38:49.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 2.011420823s
Feb 27 01:38:51.291: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 4.014203128s
Feb 27 01:38:53.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 6.011029006s
Feb 27 01:38:55.289: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 8.011699832s
Feb 27 01:38:57.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 10.010807759s
Feb 27 01:38:59.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 12.011447541s
Feb 27 01:39:01.287: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 14.010071548s
Feb 27 01:39:03.289: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 16.012004276s
Feb 27 01:39:05.287: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 18.010178107s
Feb 27 01:39:07.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 20.011537817s
Feb 27 01:39:09.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=false. Elapsed: 22.010890645s
Feb 27 01:39:11.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011265295s
STEP: Saw pod success 02/27/23 01:39:11.288
Feb 27 01:39:11.288: INFO: Pod "pod-subpath-test-downwardapi-6j75" satisfied condition "Succeeded or Failed"
Feb 27 01:39:11.292: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-subpath-test-downwardapi-6j75 container test-container-subpath-downwardapi-6j75: <nil>
STEP: delete the pod 02/27/23 01:39:11.299
Feb 27 01:39:11.309: INFO: Waiting for pod pod-subpath-test-downwardapi-6j75 to disappear
Feb 27 01:39:11.311: INFO: Pod pod-subpath-test-downwardapi-6j75 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6j75 02/27/23 01:39:11.311
Feb 27 01:39:11.312: INFO: Deleting pod "pod-subpath-test-downwardapi-6j75" in namespace "subpath-1525"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 01:39:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1525" for this suite. 02/27/23 01:39:11.318
------------------------------
• [SLOW TEST] [24.136 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:38:47.187
    Feb 27 01:38:47.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename subpath 02/27/23 01:38:47.187
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:38:47.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:38:47.209
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 01:38:47.212
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-6j75 02/27/23 01:38:47.227
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 01:38:47.227
    Feb 27 01:38:47.277: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6j75" in namespace "subpath-1525" to be "Succeeded or Failed"
    Feb 27 01:38:47.283: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446128ms
    Feb 27 01:38:49.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 2.011420823s
    Feb 27 01:38:51.291: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 4.014203128s
    Feb 27 01:38:53.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 6.011029006s
    Feb 27 01:38:55.289: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 8.011699832s
    Feb 27 01:38:57.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 10.010807759s
    Feb 27 01:38:59.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 12.011447541s
    Feb 27 01:39:01.287: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 14.010071548s
    Feb 27 01:39:03.289: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 16.012004276s
    Feb 27 01:39:05.287: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 18.010178107s
    Feb 27 01:39:07.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=true. Elapsed: 20.011537817s
    Feb 27 01:39:09.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Running", Reason="", readiness=false. Elapsed: 22.010890645s
    Feb 27 01:39:11.288: INFO: Pod "pod-subpath-test-downwardapi-6j75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011265295s
    STEP: Saw pod success 02/27/23 01:39:11.288
    Feb 27 01:39:11.288: INFO: Pod "pod-subpath-test-downwardapi-6j75" satisfied condition "Succeeded or Failed"
    Feb 27 01:39:11.292: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-subpath-test-downwardapi-6j75 container test-container-subpath-downwardapi-6j75: <nil>
    STEP: delete the pod 02/27/23 01:39:11.299
    Feb 27 01:39:11.309: INFO: Waiting for pod pod-subpath-test-downwardapi-6j75 to disappear
    Feb 27 01:39:11.311: INFO: Pod pod-subpath-test-downwardapi-6j75 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-6j75 02/27/23 01:39:11.311
    Feb 27 01:39:11.312: INFO: Deleting pod "pod-subpath-test-downwardapi-6j75" in namespace "subpath-1525"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:39:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1525" for this suite. 02/27/23 01:39:11.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:39:11.323
Feb 27 01:39:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:39:11.323
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:11.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:11.349
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Feb 27 01:39:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 01:39:13.318
Feb 27 01:39:13.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 create -f -'
Feb 27 01:39:13.867: INFO: stderr: ""
Feb 27 01:39:13.867: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 01:39:13.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 delete e2e-test-crd-publish-openapi-453-crds test-cr'
Feb 27 01:39:13.968: INFO: stderr: ""
Feb 27 01:39:13.968: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 27 01:39:13.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 apply -f -'
Feb 27 01:39:14.575: INFO: stderr: ""
Feb 27 01:39:14.575: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 27 01:39:14.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 delete e2e-test-crd-publish-openapi-453-crds test-cr'
Feb 27 01:39:14.644: INFO: stderr: ""
Feb 27 01:39:14.644: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 01:39:14.644
Feb 27 01:39:14.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 explain e2e-test-crd-publish-openapi-453-crds'
Feb 27 01:39:15.111: INFO: stderr: ""
Feb 27 01:39:15.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-453-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:39:17.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6409" for this suite. 02/27/23 01:39:17.045
------------------------------
• [SLOW TEST] [5.731 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:39:11.323
    Feb 27 01:39:11.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:39:11.323
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:11.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:11.349
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Feb 27 01:39:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 01:39:13.318
    Feb 27 01:39:13.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 create -f -'
    Feb 27 01:39:13.867: INFO: stderr: ""
    Feb 27 01:39:13.867: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 01:39:13.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 delete e2e-test-crd-publish-openapi-453-crds test-cr'
    Feb 27 01:39:13.968: INFO: stderr: ""
    Feb 27 01:39:13.968: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Feb 27 01:39:13.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 apply -f -'
    Feb 27 01:39:14.575: INFO: stderr: ""
    Feb 27 01:39:14.575: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Feb 27 01:39:14.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 --namespace=crd-publish-openapi-6409 delete e2e-test-crd-publish-openapi-453-crds test-cr'
    Feb 27 01:39:14.644: INFO: stderr: ""
    Feb 27 01:39:14.644: INFO: stdout: "e2e-test-crd-publish-openapi-453-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 01:39:14.644
    Feb 27 01:39:14.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6409 explain e2e-test-crd-publish-openapi-453-crds'
    Feb 27 01:39:15.111: INFO: stderr: ""
    Feb 27 01:39:15.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-453-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:39:17.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6409" for this suite. 02/27/23 01:39:17.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:39:17.054
Feb 27 01:39:17.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:39:17.055
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:17.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:17.077
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  02/27/23 01:39:17.079
Feb 27 01:39:17.113: INFO: Waiting up to 5m0s for pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981" in namespace "svcaccounts-8756" to be "Succeeded or Failed"
Feb 27 01:39:17.117: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192291ms
Feb 27 01:39:19.121: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007956639s
Feb 27 01:39:21.124: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010237295s
STEP: Saw pod success 02/27/23 01:39:21.124
Feb 27 01:39:21.124: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981" satisfied condition "Succeeded or Failed"
Feb 27 01:39:21.127: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:39:21.142
Feb 27 01:39:21.165: INFO: Waiting for pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 to disappear
Feb 27 01:39:21.168: INFO: Pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 01:39:21.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8756" for this suite. 02/27/23 01:39:21.172
------------------------------
• [4.123 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:39:17.054
    Feb 27 01:39:17.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:39:17.055
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:17.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:17.077
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  02/27/23 01:39:17.079
    Feb 27 01:39:17.113: INFO: Waiting up to 5m0s for pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981" in namespace "svcaccounts-8756" to be "Succeeded or Failed"
    Feb 27 01:39:17.117: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192291ms
    Feb 27 01:39:19.121: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007956639s
    Feb 27 01:39:21.124: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010237295s
    STEP: Saw pod success 02/27/23 01:39:21.124
    Feb 27 01:39:21.124: INFO: Pod "test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981" satisfied condition "Succeeded or Failed"
    Feb 27 01:39:21.127: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:39:21.142
    Feb 27 01:39:21.165: INFO: Waiting for pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 to disappear
    Feb 27 01:39:21.168: INFO: Pod test-pod-3d8dd808-db9e-4161-84f5-6475a46e0981 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:39:21.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8756" for this suite. 02/27/23 01:39:21.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:39:21.178
Feb 27 01:39:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 01:39:21.179
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:21.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:21.204
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 01:39:21.207
Feb 27 01:39:21.226: INFO: Waiting up to 5m0s for pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7" in namespace "emptydir-6588" to be "Succeeded or Failed"
Feb 27 01:39:21.228: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534164ms
Feb 27 01:39:23.234: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008716829s
Feb 27 01:39:25.236: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010216408s
STEP: Saw pod success 02/27/23 01:39:25.236
Feb 27 01:39:25.236: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7" satisfied condition "Succeeded or Failed"
Feb 27 01:39:25.243: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 container test-container: <nil>
STEP: delete the pod 02/27/23 01:39:25.248
Feb 27 01:39:25.262: INFO: Waiting for pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 to disappear
Feb 27 01:39:25.264: INFO: Pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:39:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6588" for this suite. 02/27/23 01:39:25.268
------------------------------
• [4.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:39:21.178
    Feb 27 01:39:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 01:39:21.179
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:21.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:21.204
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 01:39:21.207
    Feb 27 01:39:21.226: INFO: Waiting up to 5m0s for pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7" in namespace "emptydir-6588" to be "Succeeded or Failed"
    Feb 27 01:39:21.228: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534164ms
    Feb 27 01:39:23.234: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008716829s
    Feb 27 01:39:25.236: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010216408s
    STEP: Saw pod success 02/27/23 01:39:25.236
    Feb 27 01:39:25.236: INFO: Pod "pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7" satisfied condition "Succeeded or Failed"
    Feb 27 01:39:25.243: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 container test-container: <nil>
    STEP: delete the pod 02/27/23 01:39:25.248
    Feb 27 01:39:25.262: INFO: Waiting for pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 to disappear
    Feb 27 01:39:25.264: INFO: Pod pod-09d7079e-1e15-451b-a2dd-cd81bdd7e2c7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:39:25.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6588" for this suite. 02/27/23 01:39:25.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:39:25.28
Feb 27 01:39:25.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:39:25.28
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:25.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:25.304
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Feb 27 01:39:25.334: INFO: created pod
Feb 27 01:39:25.334: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5370" to be "Succeeded or Failed"
Feb 27 01:39:25.337: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468506ms
Feb 27 01:39:27.341: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007125381s
Feb 27 01:39:29.343: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008990365s
STEP: Saw pod success 02/27/23 01:39:29.343
Feb 27 01:39:29.343: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Feb 27 01:39:59.343: INFO: polling logs
Feb 27 01:39:59.355: INFO: Pod logs: 
I0227 01:39:26.117053       1 log.go:198] OK: Got token
I0227 01:39:26.117102       1 log.go:198] validating with in-cluster discovery
I0227 01:39:26.117466       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0227 01:39:26.117503       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5370:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677462565, NotBefore:1677461965, IssuedAt:1677461965, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5370", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ba8dc636-0ee7-4e9d-af13-3cf15796fa30"}}}
I0227 01:39:26.128078       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0227 01:39:26.133358       1 log.go:198] OK: Validated signature on JWT
I0227 01:39:26.133423       1 log.go:198] OK: Got valid claims from token!
I0227 01:39:26.133444       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5370:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677462565, NotBefore:1677461965, IssuedAt:1677461965, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5370", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ba8dc636-0ee7-4e9d-af13-3cf15796fa30"}}}

Feb 27 01:39:59.355: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 01:39:59.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5370" for this suite. 02/27/23 01:39:59.37
------------------------------
• [SLOW TEST] [34.099 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:39:25.28
    Feb 27 01:39:25.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:39:25.28
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:25.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:25.304
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Feb 27 01:39:25.334: INFO: created pod
    Feb 27 01:39:25.334: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5370" to be "Succeeded or Failed"
    Feb 27 01:39:25.337: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468506ms
    Feb 27 01:39:27.341: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007125381s
    Feb 27 01:39:29.343: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008990365s
    STEP: Saw pod success 02/27/23 01:39:29.343
    Feb 27 01:39:29.343: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Feb 27 01:39:59.343: INFO: polling logs
    Feb 27 01:39:59.355: INFO: Pod logs: 
    I0227 01:39:26.117053       1 log.go:198] OK: Got token
    I0227 01:39:26.117102       1 log.go:198] validating with in-cluster discovery
    I0227 01:39:26.117466       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0227 01:39:26.117503       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5370:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677462565, NotBefore:1677461965, IssuedAt:1677461965, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5370", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ba8dc636-0ee7-4e9d-af13-3cf15796fa30"}}}
    I0227 01:39:26.128078       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0227 01:39:26.133358       1 log.go:198] OK: Validated signature on JWT
    I0227 01:39:26.133423       1 log.go:198] OK: Got valid claims from token!
    I0227 01:39:26.133444       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5370:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677462565, NotBefore:1677461965, IssuedAt:1677461965, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5370", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ba8dc636-0ee7-4e9d-af13-3cf15796fa30"}}}

    Feb 27 01:39:59.355: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:39:59.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5370" for this suite. 02/27/23 01:39:59.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:39:59.379
Feb 27 01:39:59.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 01:39:59.38
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:59.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:59.452
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6404 02/27/23 01:39:59.454
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 01:39:59.521
STEP: creating service externalsvc in namespace services-6404 02/27/23 01:39:59.521
STEP: creating replication controller externalsvc in namespace services-6404 02/27/23 01:39:59.638
I0227 01:39:59.659221      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6404, replica count: 2
I0227 01:40:02.709736      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 02/27/23 01:40:02.713
Feb 27 01:40:02.738: INFO: Creating new exec pod
Feb 27 01:40:02.774: INFO: Waiting up to 5m0s for pod "execpodslcpv" in namespace "services-6404" to be "running"
Feb 27 01:40:02.791: INFO: Pod "execpodslcpv": Phase="Pending", Reason="", readiness=false. Elapsed: 16.898966ms
Feb 27 01:40:04.795: INFO: Pod "execpodslcpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.021040292s
Feb 27 01:40:04.795: INFO: Pod "execpodslcpv" satisfied condition "running"
Feb 27 01:40:04.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6404 exec execpodslcpv -- /bin/sh -x -c nslookup nodeport-service.services-6404.svc.cluster.local'
Feb 27 01:40:04.951: INFO: stderr: "+ nslookup nodeport-service.services-6404.svc.cluster.local\n"
Feb 27 01:40:04.951: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-6404.svc.cluster.local\tcanonical name = externalsvc.services-6404.svc.cluster.local.\nName:\texternalsvc.services-6404.svc.cluster.local\nAddress: 10.98.234.168\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6404, will wait for the garbage collector to delete the pods 02/27/23 01:40:04.951
Feb 27 01:40:05.016: INFO: Deleting ReplicationController externalsvc took: 11.527629ms
Feb 27 01:40:05.117: INFO: Terminating ReplicationController externalsvc pods took: 100.654797ms
Feb 27 01:40:07.466: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 01:40:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6404" for this suite. 02/27/23 01:40:07.486
------------------------------
• [SLOW TEST] [8.127 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:39:59.379
    Feb 27 01:39:59.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 01:39:59.38
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:39:59.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:39:59.452
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-6404 02/27/23 01:39:59.454
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 01:39:59.521
    STEP: creating service externalsvc in namespace services-6404 02/27/23 01:39:59.521
    STEP: creating replication controller externalsvc in namespace services-6404 02/27/23 01:39:59.638
    I0227 01:39:59.659221      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6404, replica count: 2
    I0227 01:40:02.709736      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 02/27/23 01:40:02.713
    Feb 27 01:40:02.738: INFO: Creating new exec pod
    Feb 27 01:40:02.774: INFO: Waiting up to 5m0s for pod "execpodslcpv" in namespace "services-6404" to be "running"
    Feb 27 01:40:02.791: INFO: Pod "execpodslcpv": Phase="Pending", Reason="", readiness=false. Elapsed: 16.898966ms
    Feb 27 01:40:04.795: INFO: Pod "execpodslcpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.021040292s
    Feb 27 01:40:04.795: INFO: Pod "execpodslcpv" satisfied condition "running"
    Feb 27 01:40:04.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6404 exec execpodslcpv -- /bin/sh -x -c nslookup nodeport-service.services-6404.svc.cluster.local'
    Feb 27 01:40:04.951: INFO: stderr: "+ nslookup nodeport-service.services-6404.svc.cluster.local\n"
    Feb 27 01:40:04.951: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-6404.svc.cluster.local\tcanonical name = externalsvc.services-6404.svc.cluster.local.\nName:\texternalsvc.services-6404.svc.cluster.local\nAddress: 10.98.234.168\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6404, will wait for the garbage collector to delete the pods 02/27/23 01:40:04.951
    Feb 27 01:40:05.016: INFO: Deleting ReplicationController externalsvc took: 11.527629ms
    Feb 27 01:40:05.117: INFO: Terminating ReplicationController externalsvc pods took: 100.654797ms
    Feb 27 01:40:07.466: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:40:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6404" for this suite. 02/27/23 01:40:07.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:40:07.507
Feb 27 01:40:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename ingress 02/27/23 01:40:07.508
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:07.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:07.544
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 02/27/23 01:40:07.546
STEP: getting /apis/networking.k8s.io 02/27/23 01:40:07.549
STEP: getting /apis/networking.k8s.iov1 02/27/23 01:40:07.55
STEP: creating 02/27/23 01:40:07.551
STEP: getting 02/27/23 01:40:07.579
STEP: listing 02/27/23 01:40:07.588
STEP: watching 02/27/23 01:40:07.591
Feb 27 01:40:07.591: INFO: starting watch
STEP: cluster-wide listing 02/27/23 01:40:07.592
STEP: cluster-wide watching 02/27/23 01:40:07.598
Feb 27 01:40:07.598: INFO: starting watch
STEP: patching 02/27/23 01:40:07.599
STEP: updating 02/27/23 01:40:07.609
Feb 27 01:40:07.632: INFO: waiting for watch events with expected annotations
Feb 27 01:40:07.632: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 01:40:07.632
STEP: updating /status 02/27/23 01:40:07.643
STEP: get /status 02/27/23 01:40:07.659
STEP: deleting 02/27/23 01:40:07.662
STEP: deleting a collection 02/27/23 01:40:07.683
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Feb 27 01:40:07.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-7874" for this suite. 02/27/23 01:40:07.707
------------------------------
• [0.206 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:40:07.507
    Feb 27 01:40:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename ingress 02/27/23 01:40:07.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:07.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:07.544
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 02/27/23 01:40:07.546
    STEP: getting /apis/networking.k8s.io 02/27/23 01:40:07.549
    STEP: getting /apis/networking.k8s.iov1 02/27/23 01:40:07.55
    STEP: creating 02/27/23 01:40:07.551
    STEP: getting 02/27/23 01:40:07.579
    STEP: listing 02/27/23 01:40:07.588
    STEP: watching 02/27/23 01:40:07.591
    Feb 27 01:40:07.591: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 01:40:07.592
    STEP: cluster-wide watching 02/27/23 01:40:07.598
    Feb 27 01:40:07.598: INFO: starting watch
    STEP: patching 02/27/23 01:40:07.599
    STEP: updating 02/27/23 01:40:07.609
    Feb 27 01:40:07.632: INFO: waiting for watch events with expected annotations
    Feb 27 01:40:07.632: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 01:40:07.632
    STEP: updating /status 02/27/23 01:40:07.643
    STEP: get /status 02/27/23 01:40:07.659
    STEP: deleting 02/27/23 01:40:07.662
    STEP: deleting a collection 02/27/23 01:40:07.683
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:40:07.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-7874" for this suite. 02/27/23 01:40:07.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:40:07.714
Feb 27 01:40:07.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 01:40:07.715
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:07.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:07.743
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Feb 27 01:40:07.767: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 01:40:07.772
Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:07.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:40:07.779: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:08.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 01:40:08.789: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:09.791: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 01:40:09.791: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 02/27/23 01:40:09.807
STEP: Check that daemon pods images are updated. 02/27/23 01:40:09.823
Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-pkm6h. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:10.843: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:10.844: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:10.844: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:12.836: INFO: Pod daemon-set-n98wj is not available
Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:13.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:13.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:14.835: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:14.835: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:14.835: INFO: Pod daemon-set-rvn5p is not available
Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:15.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:16.836: INFO: Pod daemon-set-ktbd9 is not available
Feb 27 01:40:16.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.839: INFO: Pod daemon-set-gnmff is not available
Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 01:40:18.843
Feb 27 01:40:18.847: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.847: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.848: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:18.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:40:18.851: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:19.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:40:19.861: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:20.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 01:40:20.871: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:40:21.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 01:40:21.866: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 01:40:21.881
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5144, will wait for the garbage collector to delete the pods 02/27/23 01:40:21.881
Feb 27 01:40:21.943: INFO: Deleting DaemonSet.extensions daemon-set took: 8.677112ms
Feb 27 01:40:22.044: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.13106ms
Feb 27 01:40:24.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:40:24.048: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 01:40:24.051: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18753"},"items":null}

Feb 27 01:40:24.052: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18753"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:40:24.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5144" for this suite. 02/27/23 01:40:24.074
------------------------------
• [SLOW TEST] [16.367 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:40:07.714
    Feb 27 01:40:07.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 01:40:07.715
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:07.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:07.743
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Feb 27 01:40:07.767: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 01:40:07.772
    Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:07.777: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:07.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:40:07.779: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:08.785: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:08.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 01:40:08.789: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:09.786: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:09.791: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 01:40:09.791: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 02/27/23 01:40:09.807
    STEP: Check that daemon pods images are updated. 02/27/23 01:40:09.823
    Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-pkm6h. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:09.827: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:09.830: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:10.837: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:10.843: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:10.844: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:10.844: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:11.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:11.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-2nprk. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:12.836: INFO: Pod daemon-set-n98wj is not available
    Feb 27 01:40:12.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:12.842: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:13.836: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:13.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:13.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:14.835: INFO: Wrong image for pod: daemon-set-ktvw4. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:14.835: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:14.835: INFO: Pod daemon-set-rvn5p is not available
    Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:14.839: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:15.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:15.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:16.836: INFO: Pod daemon-set-ktbd9 is not available
    Feb 27 01:40:16.836: INFO: Wrong image for pod: daemon-set-rc5rp. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43, got: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4.
    Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:16.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:17.841: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.839: INFO: Pod daemon-set-gnmff is not available
    Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.843: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 02/27/23 01:40:18.843
    Feb 27 01:40:18.847: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.847: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.848: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:18.851: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:40:18.851: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:19.858: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:19.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:40:19.861: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:20.868: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:20.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 01:40:20.871: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:21.861: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:40:21.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 01:40:21.866: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 01:40:21.881
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5144, will wait for the garbage collector to delete the pods 02/27/23 01:40:21.881
    Feb 27 01:40:21.943: INFO: Deleting DaemonSet.extensions daemon-set took: 8.677112ms
    Feb 27 01:40:22.044: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.13106ms
    Feb 27 01:40:24.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:40:24.048: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 01:40:24.051: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18753"},"items":null}

    Feb 27 01:40:24.052: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18753"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:40:24.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5144" for this suite. 02/27/23 01:40:24.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:40:24.083
Feb 27 01:40:24.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename aggregator 02/27/23 01:40:24.083
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:24.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:24.105
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Feb 27 01:40:24.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 02/27/23 01:40:24.108
Feb 27 01:40:24.500: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 27 01:40:26.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:28.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:30.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:32.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:34.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:36.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:38.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:40.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:42.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:44.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:46.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 01:40:48.709: INFO: Waited 120.79313ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 01:40:48.758
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 01:40:48.763
STEP: List APIServices 02/27/23 01:40:48.772
Feb 27 01:40:48.777: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Feb 27 01:40:48.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-3309" for this suite. 02/27/23 01:40:49.006
------------------------------
• [SLOW TEST] [24.969 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:40:24.083
    Feb 27 01:40:24.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename aggregator 02/27/23 01:40:24.083
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:24.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:24.105
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Feb 27 01:40:24.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 02/27/23 01:40:24.108
    Feb 27 01:40:24.500: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Feb 27 01:40:26.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:28.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:30.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:32.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:34.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:36.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:38.575: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:40.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:42.577: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:44.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:46.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 1, 40, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b5d4b6bf8\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 01:40:48.709: INFO: Waited 120.79313ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 02/27/23 01:40:48.758
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 02/27/23 01:40:48.763
    STEP: List APIServices 02/27/23 01:40:48.772
    Feb 27 01:40:48.777: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:40:48.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-3309" for this suite. 02/27/23 01:40:49.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:40:49.053
Feb 27 01:40:49.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 01:40:49.054
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:49.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:49.083
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 02/27/23 01:40:49.085
Feb 27 01:40:49.086: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Feb 27 01:40:49.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:49.900: INFO: stderr: ""
Feb 27 01:40:49.900: INFO: stdout: "service/agnhost-replica created\n"
Feb 27 01:40:49.900: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Feb 27 01:40:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:50.658: INFO: stderr: ""
Feb 27 01:40:50.658: INFO: stdout: "service/agnhost-primary created\n"
Feb 27 01:40:50.658: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 27 01:40:50.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:51.438: INFO: stderr: ""
Feb 27 01:40:51.438: INFO: stdout: "service/frontend created\n"
Feb 27 01:40:51.438: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 27 01:40:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:51.601: INFO: stderr: ""
Feb 27 01:40:51.601: INFO: stdout: "deployment.apps/frontend created\n"
Feb 27 01:40:51.601: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 01:40:51.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:51.768: INFO: stderr: ""
Feb 27 01:40:51.768: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Feb 27 01:40:51.768: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 27 01:40:51.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
Feb 27 01:40:51.935: INFO: stderr: ""
Feb 27 01:40:51.935: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 02/27/23 01:40:51.935
Feb 27 01:40:51.935: INFO: Waiting for all frontend pods to be Running.
Feb 27 01:40:56.986: INFO: Waiting for frontend to serve content.
Feb 27 01:40:56.998: INFO: Trying to add a new entry to the guestbook.
Feb 27 01:40:57.009: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 02/27/23 01:40:57.016
Feb 27 01:40:57.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.121: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 01:40:57.122
Feb 27 01:40:57.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.211: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.211: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 01:40:57.211
Feb 27 01:40:57.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.307: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.307: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 01:40:57.307
Feb 27 01:40:57.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.377: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 01:40:57.377
Feb 27 01:40:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.446: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 02/27/23 01:40:57.446
Feb 27 01:40:57.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
Feb 27 01:40:57.510: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 01:40:57.510: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 01:40:57.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-925" for this suite. 02/27/23 01:40:57.515
------------------------------
• [SLOW TEST] [8.471 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:40:49.053
    Feb 27 01:40:49.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 01:40:49.054
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:49.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:49.083
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 02/27/23 01:40:49.085
    Feb 27 01:40:49.086: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Feb 27 01:40:49.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:49.900: INFO: stderr: ""
    Feb 27 01:40:49.900: INFO: stdout: "service/agnhost-replica created\n"
    Feb 27 01:40:49.900: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Feb 27 01:40:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:50.658: INFO: stderr: ""
    Feb 27 01:40:50.658: INFO: stdout: "service/agnhost-primary created\n"
    Feb 27 01:40:50.658: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Feb 27 01:40:50.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:51.438: INFO: stderr: ""
    Feb 27 01:40:51.438: INFO: stdout: "service/frontend created\n"
    Feb 27 01:40:51.438: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Feb 27 01:40:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:51.601: INFO: stderr: ""
    Feb 27 01:40:51.601: INFO: stdout: "deployment.apps/frontend created\n"
    Feb 27 01:40:51.601: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 01:40:51.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:51.768: INFO: stderr: ""
    Feb 27 01:40:51.768: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Feb 27 01:40:51.768: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Feb 27 01:40:51.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 create -f -'
    Feb 27 01:40:51.935: INFO: stderr: ""
    Feb 27 01:40:51.935: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 02/27/23 01:40:51.935
    Feb 27 01:40:51.935: INFO: Waiting for all frontend pods to be Running.
    Feb 27 01:40:56.986: INFO: Waiting for frontend to serve content.
    Feb 27 01:40:56.998: INFO: Trying to add a new entry to the guestbook.
    Feb 27 01:40:57.009: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 02/27/23 01:40:57.016
    Feb 27 01:40:57.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.121: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.121: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 01:40:57.122
    Feb 27 01:40:57.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.211: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.211: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 01:40:57.211
    Feb 27 01:40:57.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.307: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.307: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 01:40:57.307
    Feb 27 01:40:57.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.377: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 01:40:57.377
    Feb 27 01:40:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.446: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 02/27/23 01:40:57.446
    Feb 27 01:40:57.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-925 delete --grace-period=0 --force -f -'
    Feb 27 01:40:57.510: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 01:40:57.510: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:40:57.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-925" for this suite. 02/27/23 01:40:57.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:40:57.524
Feb 27 01:40:57.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:40:57.524
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:57.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:57.551
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Feb 27 01:40:57.593: INFO: Waiting up to 5m0s for pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a" in namespace "svcaccounts-7556" to be "running"
Feb 27 01:40:57.597: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55139ms
Feb 27 01:40:59.601: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008292251s
Feb 27 01:40:59.601: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a" satisfied condition "running"
STEP: reading a file in the container 02/27/23 01:40:59.601
Feb 27 01:40:59.601: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 02/27/23 01:40:59.742
Feb 27 01:40:59.742: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 02/27/23 01:40:59.942
Feb 27 01:40:59.942: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Feb 27 01:41:00.076: INFO: Got root ca configmap in namespace "svcaccounts-7556"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:00.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7556" for this suite. 02/27/23 01:41:00.081
------------------------------
• [2.564 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:40:57.524
    Feb 27 01:40:57.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 01:40:57.524
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:40:57.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:40:57.551
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Feb 27 01:40:57.593: INFO: Waiting up to 5m0s for pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a" in namespace "svcaccounts-7556" to be "running"
    Feb 27 01:40:57.597: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.55139ms
    Feb 27 01:40:59.601: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008292251s
    Feb 27 01:40:59.601: INFO: Pod "pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a" satisfied condition "running"
    STEP: reading a file in the container 02/27/23 01:40:59.601
    Feb 27 01:40:59.601: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 02/27/23 01:40:59.742
    Feb 27 01:40:59.742: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 02/27/23 01:40:59.942
    Feb 27 01:40:59.942: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7556 pod-service-account-e036481c-c1f0-4acb-96b5-7b57cb956d1a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Feb 27 01:41:00.076: INFO: Got root ca configmap in namespace "svcaccounts-7556"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:00.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7556" for this suite. 02/27/23 01:41:00.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:00.09
Feb 27 01:41:00.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:41:00.091
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:00.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:00.113
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:41:00.116
Feb 27 01:41:00.126: INFO: Waiting up to 5m0s for pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b" in namespace "downward-api-3898" to be "Succeeded or Failed"
Feb 27 01:41:00.129: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.61369ms
Feb 27 01:41:02.132: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006457042s
Feb 27 01:41:04.133: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006659656s
STEP: Saw pod success 02/27/23 01:41:04.133
Feb 27 01:41:04.133: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b" satisfied condition "Succeeded or Failed"
Feb 27 01:41:04.136: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b container client-container: <nil>
STEP: delete the pod 02/27/23 01:41:04.141
Feb 27 01:41:04.153: INFO: Waiting for pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b to disappear
Feb 27 01:41:04.157: INFO: Pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:04.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3898" for this suite. 02/27/23 01:41:04.162
------------------------------
• [4.079 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:00.09
    Feb 27 01:41:00.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:41:00.091
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:00.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:00.113
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:41:00.116
    Feb 27 01:41:00.126: INFO: Waiting up to 5m0s for pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b" in namespace "downward-api-3898" to be "Succeeded or Failed"
    Feb 27 01:41:00.129: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.61369ms
    Feb 27 01:41:02.132: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006457042s
    Feb 27 01:41:04.133: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006659656s
    STEP: Saw pod success 02/27/23 01:41:04.133
    Feb 27 01:41:04.133: INFO: Pod "downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b" satisfied condition "Succeeded or Failed"
    Feb 27 01:41:04.136: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b container client-container: <nil>
    STEP: delete the pod 02/27/23 01:41:04.141
    Feb 27 01:41:04.153: INFO: Waiting for pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b to disappear
    Feb 27 01:41:04.157: INFO: Pod downwardapi-volume-979fdd7b-7bcf-4c56-b49a-88cba9771e2b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:04.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3898" for this suite. 02/27/23 01:41:04.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:04.169
Feb 27 01:41:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 01:41:04.17
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:04.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:04.192
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-js559" 02/27/23 01:41:04.195
Feb 27 01:41:04.211: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-js559-4978" 02/27/23 01:41:04.211
Feb 27 01:41:04.221: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-js559-4978" 02/27/23 01:41:04.221
Feb 27 01:41:04.231: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8800" for this suite. 02/27/23 01:41:04.235
STEP: Destroying namespace "e2e-ns-js559-4978" for this suite. 02/27/23 01:41:04.244
------------------------------
• [0.082 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:04.169
    Feb 27 01:41:04.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 01:41:04.17
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:04.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:04.192
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-js559" 02/27/23 01:41:04.195
    Feb 27 01:41:04.211: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-js559-4978" 02/27/23 01:41:04.211
    Feb 27 01:41:04.221: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-js559-4978" 02/27/23 01:41:04.221
    Feb 27 01:41:04.231: INFO: Namespace "e2e-ns-js559-4978" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8800" for this suite. 02/27/23 01:41:04.235
    STEP: Destroying namespace "e2e-ns-js559-4978" for this suite. 02/27/23 01:41:04.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:04.253
Feb 27 01:41:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-runtime 02/27/23 01:41:04.254
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:04.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:04.276
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 02/27/23 01:41:04.278
STEP: wait for the container to reach Succeeded 02/27/23 01:41:04.288
STEP: get the container status 02/27/23 01:41:07.308
STEP: the container should be terminated 02/27/23 01:41:07.311
STEP: the termination message should be set 02/27/23 01:41:07.311
Feb 27 01:41:07.312: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 01:41:07.312
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:07.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7751" for this suite. 02/27/23 01:41:07.33
------------------------------
• [3.083 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:04.253
    Feb 27 01:41:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-runtime 02/27/23 01:41:04.254
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:04.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:04.276
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 02/27/23 01:41:04.278
    STEP: wait for the container to reach Succeeded 02/27/23 01:41:04.288
    STEP: get the container status 02/27/23 01:41:07.308
    STEP: the container should be terminated 02/27/23 01:41:07.311
    STEP: the termination message should be set 02/27/23 01:41:07.311
    Feb 27 01:41:07.312: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 01:41:07.312
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:07.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7751" for this suite. 02/27/23 01:41:07.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:07.336
Feb 27 01:41:07.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-pred 02/27/23 01:41:07.337
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:07.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:07.364
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 01:41:07.366: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 01:41:07.375: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 01:41:07.377: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
Feb 27 01:41:07.391: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:41:07.391: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:41:07.391: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container registry ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container sidecar ready: true, restart count 0
Feb 27 01:41:07.391: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Feb 27 01:41:07.391: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container controller ready: true, restart count 0
Feb 27 01:41:07.391: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:41:07.391: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:41:07.391: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:41:07.391: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:41:07.391: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:41:07.391: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:41:07.391: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:41:07.391: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container e2e ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:41:07.391: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:41:07.391: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
Feb 27 01:41:07.405: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:41:07.405: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container calicoctl ready: true, restart count 0
Feb 27 01:41:07.405: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:41:07.405: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:41:07.405: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:41:07.405: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:41:07.405: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 01:41:07.405: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 01:41:07.405: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:41:07.405: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Feb 27 01:41:07.405: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Feb 27 01:41:07.405: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:41:07.405: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 01:41:07.405: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:41:07.405: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
Feb 27 01:41:07.418: INFO: default-http-backend-67df9bcb5b-q8xpb from ingress-nginx started at 2023-02-27 01:07:17 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 27 01:41:07.418: INFO: nginx-ingress-controller-67d95699d-6kvs9 from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 01:41:07.418: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:41:07.418: INFO: ccd-license-consumer-69f48d6d8f-csqqf from kube-system started at 2023-02-27 01:17:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Feb 27 01:41:07.418: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:41:07.418: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:41:07.418: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:41:07.418: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:41:07.418: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:41:07.418: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:41:07.418: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:41:07.418: INFO: network-resources-injector-545655c748-tlvzk from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 01:41:07.418: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 01:41:07.418: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:41:07.418: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 01:09:23 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Feb 27 01:41:07.418: INFO: isp-logger-74bf5ff65d-xsw9s from monitoring started at 2023-02-27 01:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container isp-logger ready: true, restart count 0
Feb 27 01:41:07.418: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:41:07.418: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.418: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:41:07.418: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:41:07.418: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
Feb 27 01:41:07.432: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 01:41:07.432: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:41:07.432: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:41:07.432: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:41:07.432: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:41:07.432: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:41:07.432: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:41:07.432: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:41:07.432: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:41:07.432: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Feb 27 01:41:07.432: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Feb 27 01:41:07.432: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Feb 27 01:41:07.432: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:41:07.432: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:41:07.432: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:41:07.432: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 01:41:07.432
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17478adce48addd0], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling..] 02/27/23 01:41:07.555
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:08.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4763" for this suite. 02/27/23 01:41:08.564
------------------------------
• [1.236 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:07.336
    Feb 27 01:41:07.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-pred 02/27/23 01:41:07.337
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:07.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:07.364
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 01:41:07.366: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 01:41:07.375: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 01:41:07.377: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
    Feb 27 01:41:07.391: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container registry ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container sidecar ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container controller ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:41:07.391: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:41:07.391: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:41:07.391: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
    Feb 27 01:41:07.405: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container calicoctl ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:41:07.405: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:41:07.405: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:41:07.405: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
    Feb 27 01:41:07.418: INFO: default-http-backend-67df9bcb5b-q8xpb from ingress-nginx started at 2023-02-27 01:07:17 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container default-http-backend ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: nginx-ingress-controller-67d95699d-6kvs9 from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: ccd-license-consumer-69f48d6d8f-csqqf from kube-system started at 2023-02-27 01:17:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:41:07.418: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: network-resources-injector-545655c748-tlvzk from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 01:09:23 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: isp-logger-74bf5ff65d-xsw9s from monitoring started at 2023-02-27 01:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container isp-logger ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.418: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:41:07.418: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
    Feb 27 01:41:07.432: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:41:07.432: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:41:07.432: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:41:07.432: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:41:07.432: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 02/27/23 01:41:07.432
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17478adce48addd0], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling..] 02/27/23 01:41:07.555
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:08.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4763" for this suite. 02/27/23 01:41:08.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:08.572
Feb 27 01:41:08.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 01:41:08.573
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:08.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:08.634
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 01:41:08.637
Feb 27 01:41:08.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 01:41:08.747: INFO: stderr: ""
Feb 27 01:41:08.747: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 01:41:08.747
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 01:41:13.798
Feb 27 01:41:13.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 get pod e2e-test-httpd-pod -o json'
Feb 27 01:41:13.858: INFO: stderr: ""
Feb 27 01:41:13.858: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.214.163\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.214.163\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-02-27T01:41:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7165\",\n        \"resourceVersion\": \"19545\",\n        \"uid\": \"ec08387c-3149-4245-99a1-3c1ef4e6f64c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lkv4n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lkv4n\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://dd73455ca008bd19846c07fa34952169fa50318d2353a3287231f95568da37e7\",\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4\",\n                \"imageID\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T01:41:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.214.163\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.214.163\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T01:41:08Z\"\n    }\n}\n"
STEP: replace the image in the pod 02/27/23 01:41:13.858
Feb 27 01:41:13.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 replace -f -'
Feb 27 01:41:14.016: INFO: stderr: ""
Feb 27 01:41:14.016: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4 02/27/23 01:41:14.016
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Feb 27 01:41:14.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 delete pods e2e-test-httpd-pod'
Feb 27 01:41:15.336: INFO: stderr: ""
Feb 27 01:41:15.336: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:15.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7165" for this suite. 02/27/23 01:41:15.34
------------------------------
• [SLOW TEST] [6.775 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:08.572
    Feb 27 01:41:08.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 01:41:08.573
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:08.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:08.634
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 01:41:08.637
    Feb 27 01:41:08.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 01:41:08.747: INFO: stderr: ""
    Feb 27 01:41:08.747: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 02/27/23 01:41:08.747
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 01:41:13.798
    Feb 27 01:41:13.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 get pod e2e-test-httpd-pod -o json'
    Feb 27 01:41:13.858: INFO: stderr: ""
    Feb 27 01:41:13.858: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.214.163\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"192.168.214.163\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-02-27T01:41:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7165\",\n        \"resourceVersion\": \"19545\",\n        \"uid\": \"ec08387c-3149-4245-99a1-3c1ef4e6f64c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lkv4n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lkv4n\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-02-27T01:41:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://dd73455ca008bd19846c07fa34952169fa50318d2353a3287231f95568da37e7\",\n                \"image\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4\",\n                \"imageID\": \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-02-27T01:41:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.10.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.214.163\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.214.163\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-02-27T01:41:08Z\"\n    }\n}\n"
    STEP: replace the image in the pod 02/27/23 01:41:13.858
    Feb 27 01:41:13.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 replace -f -'
    Feb 27 01:41:14.016: INFO: stderr: ""
    Feb 27 01:41:14.016: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4 02/27/23 01:41:14.016
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Feb 27 01:41:14.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7165 delete pods e2e-test-httpd-pod'
    Feb 27 01:41:15.336: INFO: stderr: ""
    Feb 27 01:41:15.336: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:15.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7165" for this suite. 02/27/23 01:41:15.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:15.349
Feb 27 01:41:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:41:15.35
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:15.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:15.373
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-fc101e59-11d4-4183-b425-cc18b845b4ab 02/27/23 01:41:15.376
STEP: Creating a pod to test consume secrets 02/27/23 01:41:15.38
Feb 27 01:41:15.411: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7" in namespace "projected-8538" to be "Succeeded or Failed"
Feb 27 01:41:15.414: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818555ms
Feb 27 01:41:17.419: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008456239s
Feb 27 01:41:19.418: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124702s
STEP: Saw pod success 02/27/23 01:41:19.418
Feb 27 01:41:19.418: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7" satisfied condition "Succeeded or Failed"
Feb 27 01:41:19.422: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 01:41:19.427
Feb 27 01:41:19.440: INFO: Waiting for pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 to disappear
Feb 27 01:41:19.443: INFO: Pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8538" for this suite. 02/27/23 01:41:19.447
------------------------------
• [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:15.349
    Feb 27 01:41:15.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:41:15.35
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:15.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:15.373
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-fc101e59-11d4-4183-b425-cc18b845b4ab 02/27/23 01:41:15.376
    STEP: Creating a pod to test consume secrets 02/27/23 01:41:15.38
    Feb 27 01:41:15.411: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7" in namespace "projected-8538" to be "Succeeded or Failed"
    Feb 27 01:41:15.414: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818555ms
    Feb 27 01:41:17.419: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008456239s
    Feb 27 01:41:19.418: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007124702s
    STEP: Saw pod success 02/27/23 01:41:19.418
    Feb 27 01:41:19.418: INFO: Pod "pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7" satisfied condition "Succeeded or Failed"
    Feb 27 01:41:19.422: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 01:41:19.427
    Feb 27 01:41:19.440: INFO: Waiting for pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 to disappear
    Feb 27 01:41:19.443: INFO: Pod pod-projected-secrets-b0fd8e16-7f1b-4b2c-bdf1-4e33f4ebd6f7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:19.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8538" for this suite. 02/27/23 01:41:19.447
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:19.455
Feb 27 01:41:19.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 01:41:19.456
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:19.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:19.477
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 02/27/23 01:41:19.481
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +notcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_tcp@PTR;sleep 1; done
 02/27/23 01:41:19.5
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +notcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_tcp@PTR;sleep 1; done
 02/27/23 01:41:19.5
STEP: creating a pod to probe DNS 02/27/23 01:41:19.5
STEP: submitting the pod to kubernetes 02/27/23 01:41:19.501
Feb 27 01:41:19.515: INFO: Waiting up to 15m0s for pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb" in namespace "dns-5468" to be "running"
Feb 27 01:41:19.524: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423798ms
Feb 27 01:41:21.529: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014139451s
Feb 27 01:41:21.529: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb" satisfied condition "running"
STEP: retrieving the pod 02/27/23 01:41:21.529
STEP: looking for the results for each expected name from probers 02/27/23 01:41:21.533
Feb 27 01:41:21.537: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.541: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.544: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.547: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.554: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.563: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.569: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.582: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.584: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.586: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.589: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.592: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:21.612: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:26.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.621: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.624: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.627: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.638: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.655: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.657: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.660: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.663: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.673: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:26.689: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:31.621: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.625: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.629: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.633: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.640: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.649: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.653: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.673: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.679: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.682: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.685: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.687: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:31.706: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:36.619: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.625: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.629: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.639: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.657: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.659: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.663: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.671: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.674: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.678: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:36.690: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:41.620: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.624: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.641: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.643: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.659: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.663: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.666: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.668: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.671: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.673: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.690: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:41.719: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:46.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.623: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.626: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.629: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.637: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.664: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.669: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.677: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.680: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.684: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:46.706: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:51.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.630: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.647: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.662: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.665: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.673: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.683: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
Feb 27 01:41:51.694: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

Feb 27 01:41:56.689: INFO: DNS probes using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb succeeded

STEP: deleting the pod 02/27/23 01:41:56.689
STEP: deleting the test service 02/27/23 01:41:56.704
STEP: deleting the test headless service 02/27/23 01:41:56.76
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 01:41:56.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5468" for this suite. 02/27/23 01:41:56.809
------------------------------
• [SLOW TEST] [37.364 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:19.455
    Feb 27 01:41:19.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 01:41:19.456
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:19.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:19.477
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 02/27/23 01:41:19.481
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +notcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_tcp@PTR;sleep 1; done
     02/27/23 01:41:19.5
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5468.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5468.svc;check="$$(dig +notcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_udp@PTR;check="$$(dig +tcp +noall +answer +search 25.181.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.181.25_tcp@PTR;sleep 1; done
     02/27/23 01:41:19.5
    STEP: creating a pod to probe DNS 02/27/23 01:41:19.5
    STEP: submitting the pod to kubernetes 02/27/23 01:41:19.501
    Feb 27 01:41:19.515: INFO: Waiting up to 15m0s for pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb" in namespace "dns-5468" to be "running"
    Feb 27 01:41:19.524: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.423798ms
    Feb 27 01:41:21.529: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.014139451s
    Feb 27 01:41:21.529: INFO: Pod "dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 01:41:21.529
    STEP: looking for the results for each expected name from probers 02/27/23 01:41:21.533
    Feb 27 01:41:21.537: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.541: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.544: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.547: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.554: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.563: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.566: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.569: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.582: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.584: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.586: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.589: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.592: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.595: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.597: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.600: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:21.612: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:26.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.621: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.624: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.627: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.631: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.634: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.638: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.655: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.657: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.660: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.663: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.673: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:26.689: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:31.621: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.625: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.629: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.633: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.637: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.640: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.649: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.653: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.673: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.679: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.682: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.685: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.687: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:31.706: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:36.619: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.625: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.629: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.639: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.642: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.657: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.659: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.663: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.671: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.674: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.678: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:36.690: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:41.620: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.624: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.628: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.641: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.643: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.659: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.663: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.666: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.668: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.671: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.673: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.687: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.690: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:41.719: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:46.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.623: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.626: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.629: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.635: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.637: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.640: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.664: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.669: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.677: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.680: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.684: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.695: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:46.706: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:51.618: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.630: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.641: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.647: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.662: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.665: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.667: INFO: Unable to read jessie_udp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.670: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468 from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.673: INFO: Unable to read jessie_udp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.675: INFO: Unable to read jessie_tcp@dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.679: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.683: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc from pod dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb: the server could not find the requested resource (get pods dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb)
    Feb 27 01:41:51.694: INFO: Lookups using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5468 wheezy_tcp@dns-test-service.dns-5468 wheezy_udp@dns-test-service.dns-5468.svc wheezy_tcp@dns-test-service.dns-5468.svc wheezy_udp@_http._tcp.dns-test-service.dns-5468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5468 jessie_tcp@dns-test-service.dns-5468 jessie_udp@dns-test-service.dns-5468.svc jessie_tcp@dns-test-service.dns-5468.svc jessie_udp@_http._tcp.dns-test-service.dns-5468.svc jessie_tcp@_http._tcp.dns-test-service.dns-5468.svc]

    Feb 27 01:41:56.689: INFO: DNS probes using dns-5468/dns-test-f285d546-7062-4ae1-93b3-08aa297a06bb succeeded

    STEP: deleting the pod 02/27/23 01:41:56.689
    STEP: deleting the test service 02/27/23 01:41:56.704
    STEP: deleting the test headless service 02/27/23 01:41:56.76
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:41:56.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5468" for this suite. 02/27/23 01:41:56.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:41:56.82
Feb 27 01:41:56.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubelet-test 02/27/23 01:41:56.821
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:56.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:56.849
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 02/27/23 01:41:56.882
Feb 27 01:41:56.882: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157" in namespace "kubelet-test-7712" to be "completed"
Feb 27 01:41:56.891: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202433ms
Feb 27 01:41:58.895: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012256534s
Feb 27 01:42:00.896: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013450593s
Feb 27 01:42:00.896: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7712" for this suite. 02/27/23 01:42:00.907
------------------------------
• [4.094 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:41:56.82
    Feb 27 01:41:56.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 01:41:56.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:41:56.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:41:56.849
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 02/27/23 01:41:56.882
    Feb 27 01:41:56.882: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157" in namespace "kubelet-test-7712" to be "completed"
    Feb 27 01:41:56.891: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Pending", Reason="", readiness=false. Elapsed: 8.202433ms
    Feb 27 01:41:58.895: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012256534s
    Feb 27 01:42:00.896: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013450593s
    Feb 27 01:42:00.896: INFO: Pod "agnhost-host-aliasese5252f6a-c5d8-49a0-90fa-0f80ae48a157" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7712" for this suite. 02/27/23 01:42:00.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:00.915
Feb 27 01:42:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 01:42:00.916
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:00.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:00.948
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 02/27/23 01:42:00.95
STEP: Ensuring active pods == parallelism 02/27/23 01:42:00.96
STEP: Orphaning one of the Job's Pods 02/27/23 01:42:02.967
Feb 27 01:42:03.487: INFO: Successfully updated pod "adopt-release-277cm"
STEP: Checking that the Job readopts the Pod 02/27/23 01:42:03.487
Feb 27 01:42:03.487: INFO: Waiting up to 15m0s for pod "adopt-release-277cm" in namespace "job-4158" to be "adopted"
Feb 27 01:42:03.489: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.760069ms
Feb 27 01:42:05.494: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006842447s
Feb 27 01:42:05.494: INFO: Pod "adopt-release-277cm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 02/27/23 01:42:05.494
Feb 27 01:42:06.009: INFO: Successfully updated pod "adopt-release-277cm"
STEP: Checking that the Job releases the Pod 02/27/23 01:42:06.01
Feb 27 01:42:06.010: INFO: Waiting up to 15m0s for pod "adopt-release-277cm" in namespace "job-4158" to be "released"
Feb 27 01:42:06.012: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.725955ms
Feb 27 01:42:08.016: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006439396s
Feb 27 01:42:08.016: INFO: Pod "adopt-release-277cm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:08.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4158" for this suite. 02/27/23 01:42:08.021
------------------------------
• [SLOW TEST] [7.113 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:00.915
    Feb 27 01:42:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 01:42:00.916
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:00.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:00.948
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 02/27/23 01:42:00.95
    STEP: Ensuring active pods == parallelism 02/27/23 01:42:00.96
    STEP: Orphaning one of the Job's Pods 02/27/23 01:42:02.967
    Feb 27 01:42:03.487: INFO: Successfully updated pod "adopt-release-277cm"
    STEP: Checking that the Job readopts the Pod 02/27/23 01:42:03.487
    Feb 27 01:42:03.487: INFO: Waiting up to 15m0s for pod "adopt-release-277cm" in namespace "job-4158" to be "adopted"
    Feb 27 01:42:03.489: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.760069ms
    Feb 27 01:42:05.494: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006842447s
    Feb 27 01:42:05.494: INFO: Pod "adopt-release-277cm" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 02/27/23 01:42:05.494
    Feb 27 01:42:06.009: INFO: Successfully updated pod "adopt-release-277cm"
    STEP: Checking that the Job releases the Pod 02/27/23 01:42:06.01
    Feb 27 01:42:06.010: INFO: Waiting up to 15m0s for pod "adopt-release-277cm" in namespace "job-4158" to be "released"
    Feb 27 01:42:06.012: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.725955ms
    Feb 27 01:42:08.016: INFO: Pod "adopt-release-277cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006439396s
    Feb 27 01:42:08.016: INFO: Pod "adopt-release-277cm" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:08.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4158" for this suite. 02/27/23 01:42:08.021
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:08.028
Feb 27 01:42:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 01:42:08.029
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:08.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:08.053
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Feb 27 01:42:08.069: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 01:42:13.080: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 01:42:13.08
STEP: Scaling up "test-rs" replicaset  02/27/23 01:42:13.08
Feb 27 01:42:13.095: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 02/27/23 01:42:13.095
W0227 01:42:13.103086      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 01:42:13.104: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 01:42:13.145: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 01:42:13.166: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 01:42:13.177: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
Feb 27 01:42:14.683: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 2, AvailableReplicas 2
Feb 27 01:42:15.075: INFO: observed Replicaset test-rs in namespace replicaset-3926 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:15.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3926" for this suite. 02/27/23 01:42:15.08
------------------------------
• [SLOW TEST] [7.057 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:08.028
    Feb 27 01:42:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 01:42:08.029
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:08.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:08.053
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Feb 27 01:42:08.069: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 01:42:13.080: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 01:42:13.08
    STEP: Scaling up "test-rs" replicaset  02/27/23 01:42:13.08
    Feb 27 01:42:13.095: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 02/27/23 01:42:13.095
    W0227 01:42:13.103086      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 01:42:13.104: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 01:42:13.145: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 01:42:13.166: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 01:42:13.177: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 1, AvailableReplicas 1
    Feb 27 01:42:14.683: INFO: observed ReplicaSet test-rs in namespace replicaset-3926 with ReadyReplicas 2, AvailableReplicas 2
    Feb 27 01:42:15.075: INFO: observed Replicaset test-rs in namespace replicaset-3926 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:15.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3926" for this suite. 02/27/23 01:42:15.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:15.086
Feb 27 01:42:15.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 01:42:15.087
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.108
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 02/27/23 01:42:15.111
STEP: Getting a ResourceQuota 02/27/23 01:42:15.115
STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 01:42:15.117
STEP: Patching the ResourceQuota 02/27/23 01:42:15.121
STEP: Deleting a Collection of ResourceQuotas 02/27/23 01:42:15.126
STEP: Verifying the deleted ResourceQuota 02/27/23 01:42:15.135
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:15.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9794" for this suite. 02/27/23 01:42:15.142
------------------------------
• [0.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:15.086
    Feb 27 01:42:15.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 01:42:15.087
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.108
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 02/27/23 01:42:15.111
    STEP: Getting a ResourceQuota 02/27/23 01:42:15.115
    STEP: Listing all ResourceQuotas with LabelSelector 02/27/23 01:42:15.117
    STEP: Patching the ResourceQuota 02/27/23 01:42:15.121
    STEP: Deleting a Collection of ResourceQuotas 02/27/23 01:42:15.126
    STEP: Verifying the deleted ResourceQuota 02/27/23 01:42:15.135
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:15.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9794" for this suite. 02/27/23 01:42:15.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:15.15
Feb 27 01:42:15.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename discovery 02/27/23 01:42:15.151
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.17
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 02/27/23 01:42:15.173
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Feb 27 01:42:15.524: INFO: Checking APIGroup: apiregistration.k8s.io
Feb 27 01:42:15.525: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Feb 27 01:42:15.525: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Feb 27 01:42:15.525: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Feb 27 01:42:15.525: INFO: Checking APIGroup: apps
Feb 27 01:42:15.526: INFO: PreferredVersion.GroupVersion: apps/v1
Feb 27 01:42:15.526: INFO: Versions found [{apps/v1 v1}]
Feb 27 01:42:15.526: INFO: apps/v1 matches apps/v1
Feb 27 01:42:15.526: INFO: Checking APIGroup: events.k8s.io
Feb 27 01:42:15.527: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Feb 27 01:42:15.527: INFO: Versions found [{events.k8s.io/v1 v1}]
Feb 27 01:42:15.527: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Feb 27 01:42:15.527: INFO: Checking APIGroup: authentication.k8s.io
Feb 27 01:42:15.528: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Feb 27 01:42:15.528: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Feb 27 01:42:15.528: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Feb 27 01:42:15.528: INFO: Checking APIGroup: authorization.k8s.io
Feb 27 01:42:15.529: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Feb 27 01:42:15.529: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Feb 27 01:42:15.529: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Feb 27 01:42:15.529: INFO: Checking APIGroup: autoscaling
Feb 27 01:42:15.530: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Feb 27 01:42:15.530: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Feb 27 01:42:15.530: INFO: autoscaling/v2 matches autoscaling/v2
Feb 27 01:42:15.530: INFO: Checking APIGroup: batch
Feb 27 01:42:15.531: INFO: PreferredVersion.GroupVersion: batch/v1
Feb 27 01:42:15.531: INFO: Versions found [{batch/v1 v1}]
Feb 27 01:42:15.531: INFO: batch/v1 matches batch/v1
Feb 27 01:42:15.531: INFO: Checking APIGroup: certificates.k8s.io
Feb 27 01:42:15.532: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Feb 27 01:42:15.532: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Feb 27 01:42:15.532: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Feb 27 01:42:15.532: INFO: Checking APIGroup: networking.k8s.io
Feb 27 01:42:15.532: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Feb 27 01:42:15.532: INFO: Versions found [{networking.k8s.io/v1 v1}]
Feb 27 01:42:15.532: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Feb 27 01:42:15.532: INFO: Checking APIGroup: policy
Feb 27 01:42:15.533: INFO: PreferredVersion.GroupVersion: policy/v1
Feb 27 01:42:15.533: INFO: Versions found [{policy/v1 v1}]
Feb 27 01:42:15.533: INFO: policy/v1 matches policy/v1
Feb 27 01:42:15.533: INFO: Checking APIGroup: rbac.authorization.k8s.io
Feb 27 01:42:15.534: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Feb 27 01:42:15.534: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Feb 27 01:42:15.534: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Feb 27 01:42:15.534: INFO: Checking APIGroup: storage.k8s.io
Feb 27 01:42:15.535: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Feb 27 01:42:15.535: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Feb 27 01:42:15.535: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Feb 27 01:42:15.535: INFO: Checking APIGroup: admissionregistration.k8s.io
Feb 27 01:42:15.535: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Feb 27 01:42:15.535: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Feb 27 01:42:15.535: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Feb 27 01:42:15.535: INFO: Checking APIGroup: apiextensions.k8s.io
Feb 27 01:42:15.536: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Feb 27 01:42:15.536: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Feb 27 01:42:15.536: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Feb 27 01:42:15.536: INFO: Checking APIGroup: scheduling.k8s.io
Feb 27 01:42:15.536: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Feb 27 01:42:15.536: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Feb 27 01:42:15.536: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Feb 27 01:42:15.536: INFO: Checking APIGroup: coordination.k8s.io
Feb 27 01:42:15.537: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Feb 27 01:42:15.537: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Feb 27 01:42:15.537: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Feb 27 01:42:15.537: INFO: Checking APIGroup: node.k8s.io
Feb 27 01:42:15.538: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Feb 27 01:42:15.538: INFO: Versions found [{node.k8s.io/v1 v1}]
Feb 27 01:42:15.538: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Feb 27 01:42:15.538: INFO: Checking APIGroup: discovery.k8s.io
Feb 27 01:42:15.539: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Feb 27 01:42:15.539: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Feb 27 01:42:15.539: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Feb 27 01:42:15.539: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Feb 27 01:42:15.540: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Feb 27 01:42:15.540: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Feb 27 01:42:15.540: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Feb 27 01:42:15.540: INFO: Checking APIGroup: crd.projectcalico.org
Feb 27 01:42:15.541: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Feb 27 01:42:15.541: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Feb 27 01:42:15.541: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Feb 27 01:42:15.541: INFO: Checking APIGroup: k8s.cni.cncf.io
Feb 27 01:42:15.542: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Feb 27 01:42:15.542: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Feb 27 01:42:15.542: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Feb 27 01:42:15.542: INFO: Checking APIGroup: metrics.k8s.io
Feb 27 01:42:15.543: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Feb 27 01:42:15.543: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Feb 27 01:42:15.543: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:15.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-516" for this suite. 02/27/23 01:42:15.553
------------------------------
• [0.473 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:15.15
    Feb 27 01:42:15.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename discovery 02/27/23 01:42:15.151
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.17
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 02/27/23 01:42:15.173
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Feb 27 01:42:15.524: INFO: Checking APIGroup: apiregistration.k8s.io
    Feb 27 01:42:15.525: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Feb 27 01:42:15.525: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Feb 27 01:42:15.525: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Feb 27 01:42:15.525: INFO: Checking APIGroup: apps
    Feb 27 01:42:15.526: INFO: PreferredVersion.GroupVersion: apps/v1
    Feb 27 01:42:15.526: INFO: Versions found [{apps/v1 v1}]
    Feb 27 01:42:15.526: INFO: apps/v1 matches apps/v1
    Feb 27 01:42:15.526: INFO: Checking APIGroup: events.k8s.io
    Feb 27 01:42:15.527: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Feb 27 01:42:15.527: INFO: Versions found [{events.k8s.io/v1 v1}]
    Feb 27 01:42:15.527: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Feb 27 01:42:15.527: INFO: Checking APIGroup: authentication.k8s.io
    Feb 27 01:42:15.528: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Feb 27 01:42:15.528: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Feb 27 01:42:15.528: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Feb 27 01:42:15.528: INFO: Checking APIGroup: authorization.k8s.io
    Feb 27 01:42:15.529: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Feb 27 01:42:15.529: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Feb 27 01:42:15.529: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Feb 27 01:42:15.529: INFO: Checking APIGroup: autoscaling
    Feb 27 01:42:15.530: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Feb 27 01:42:15.530: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Feb 27 01:42:15.530: INFO: autoscaling/v2 matches autoscaling/v2
    Feb 27 01:42:15.530: INFO: Checking APIGroup: batch
    Feb 27 01:42:15.531: INFO: PreferredVersion.GroupVersion: batch/v1
    Feb 27 01:42:15.531: INFO: Versions found [{batch/v1 v1}]
    Feb 27 01:42:15.531: INFO: batch/v1 matches batch/v1
    Feb 27 01:42:15.531: INFO: Checking APIGroup: certificates.k8s.io
    Feb 27 01:42:15.532: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Feb 27 01:42:15.532: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Feb 27 01:42:15.532: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Feb 27 01:42:15.532: INFO: Checking APIGroup: networking.k8s.io
    Feb 27 01:42:15.532: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Feb 27 01:42:15.532: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Feb 27 01:42:15.532: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Feb 27 01:42:15.532: INFO: Checking APIGroup: policy
    Feb 27 01:42:15.533: INFO: PreferredVersion.GroupVersion: policy/v1
    Feb 27 01:42:15.533: INFO: Versions found [{policy/v1 v1}]
    Feb 27 01:42:15.533: INFO: policy/v1 matches policy/v1
    Feb 27 01:42:15.533: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Feb 27 01:42:15.534: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Feb 27 01:42:15.534: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Feb 27 01:42:15.534: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Feb 27 01:42:15.534: INFO: Checking APIGroup: storage.k8s.io
    Feb 27 01:42:15.535: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Feb 27 01:42:15.535: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Feb 27 01:42:15.535: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Feb 27 01:42:15.535: INFO: Checking APIGroup: admissionregistration.k8s.io
    Feb 27 01:42:15.535: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Feb 27 01:42:15.535: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Feb 27 01:42:15.535: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Feb 27 01:42:15.535: INFO: Checking APIGroup: apiextensions.k8s.io
    Feb 27 01:42:15.536: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Feb 27 01:42:15.536: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Feb 27 01:42:15.536: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Feb 27 01:42:15.536: INFO: Checking APIGroup: scheduling.k8s.io
    Feb 27 01:42:15.536: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Feb 27 01:42:15.536: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Feb 27 01:42:15.536: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Feb 27 01:42:15.536: INFO: Checking APIGroup: coordination.k8s.io
    Feb 27 01:42:15.537: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Feb 27 01:42:15.537: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Feb 27 01:42:15.537: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Feb 27 01:42:15.537: INFO: Checking APIGroup: node.k8s.io
    Feb 27 01:42:15.538: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Feb 27 01:42:15.538: INFO: Versions found [{node.k8s.io/v1 v1}]
    Feb 27 01:42:15.538: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Feb 27 01:42:15.538: INFO: Checking APIGroup: discovery.k8s.io
    Feb 27 01:42:15.539: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Feb 27 01:42:15.539: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Feb 27 01:42:15.539: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Feb 27 01:42:15.539: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Feb 27 01:42:15.540: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Feb 27 01:42:15.540: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Feb 27 01:42:15.540: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Feb 27 01:42:15.540: INFO: Checking APIGroup: crd.projectcalico.org
    Feb 27 01:42:15.541: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Feb 27 01:42:15.541: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Feb 27 01:42:15.541: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Feb 27 01:42:15.541: INFO: Checking APIGroup: k8s.cni.cncf.io
    Feb 27 01:42:15.542: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Feb 27 01:42:15.542: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Feb 27 01:42:15.542: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Feb 27 01:42:15.542: INFO: Checking APIGroup: metrics.k8s.io
    Feb 27 01:42:15.543: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Feb 27 01:42:15.543: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Feb 27 01:42:15.543: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:15.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-516" for this suite. 02/27/23 01:42:15.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:15.623
Feb 27 01:42:15.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-webhook 02/27/23 01:42:15.624
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.663
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 01:42:15.667
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 01:42:16.23
STEP: Deploying the custom resource conversion webhook pod 02/27/23 01:42:16.252
STEP: Wait for the deployment to be ready 02/27/23 01:42:16.268
Feb 27 01:42:16.274: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 01:42:18.283
STEP: Verifying the service has paired with the endpoint 02/27/23 01:42:18.301
Feb 27 01:42:19.301: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Feb 27 01:42:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Creating a v1 custom resource 02/27/23 01:42:21.896
STEP: v2 custom resource should be converted 02/27/23 01:42:21.905
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:42:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4912" for this suite. 02/27/23 01:42:22.506
------------------------------
• [SLOW TEST] [6.907 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:15.623
    Feb 27 01:42:15.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 01:42:15.624
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:15.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:15.663
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 01:42:15.667
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 01:42:16.23
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 01:42:16.252
    STEP: Wait for the deployment to be ready 02/27/23 01:42:16.268
    Feb 27 01:42:16.274: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 01:42:18.283
    STEP: Verifying the service has paired with the endpoint 02/27/23 01:42:18.301
    Feb 27 01:42:19.301: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Feb 27 01:42:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Creating a v1 custom resource 02/27/23 01:42:21.896
    STEP: v2 custom resource should be converted 02/27/23 01:42:21.905
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:42:22.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4912" for this suite. 02/27/23 01:42:22.506
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:42:22.53
Feb 27 01:42:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 01:42:22.531
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:22.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:22.559
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 01:43:22.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9244" for this suite. 02/27/23 01:43:22.606
------------------------------
• [SLOW TEST] [60.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:42:22.53
    Feb 27 01:42:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 01:42:22.531
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:42:22.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:42:22.559
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:43:22.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9244" for this suite. 02/27/23 01:43:22.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:43:22.614
Feb 27 01:43:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-pred 02/27/23 01:43:22.615
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:43:22.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:43:22.636
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 01:43:22.638: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 01:43:22.645: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 01:43:22.648: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
Feb 27 01:43:22.660: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:43:22.660: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:43:22.660: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container registry ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container sidecar ready: true, restart count 0
Feb 27 01:43:22.660: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Feb 27 01:43:22.660: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container controller ready: true, restart count 0
Feb 27 01:43:22.660: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:43:22.660: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:43:22.660: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:43:22.660: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:43:22.660: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:43:22.660: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:43:22.660: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:43:22.660: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container e2e ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:43:22.660: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:43:22.660: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
Feb 27 01:43:22.675: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:43:22.675: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container calicoctl ready: true, restart count 0
Feb 27 01:43:22.675: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:43:22.675: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:43:22.675: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:43:22.675: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:43:22.675: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 01:43:22.675: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 01:43:22.675: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:43:22.675: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Feb 27 01:43:22.675: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Feb 27 01:43:22.675: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:43:22.675: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 01:43:22.675: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:43:22.675: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
Feb 27 01:43:22.689: INFO: test-webserver-e27df5db-34d8-45d4-8f53-585801ba7a5c from container-probe-9244 started at 2023-02-27 01:42:22 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container test-webserver ready: false, restart count 0
Feb 27 01:43:22.689: INFO: default-http-backend-67df9bcb5b-q8xpb from ingress-nginx started at 2023-02-27 01:07:17 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 27 01:43:22.689: INFO: nginx-ingress-controller-67d95699d-6kvs9 from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 01:43:22.689: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:43:22.689: INFO: ccd-license-consumer-69f48d6d8f-csqqf from kube-system started at 2023-02-27 01:17:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Feb 27 01:43:22.689: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:43:22.689: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:43:22.689: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:43:22.689: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:43:22.689: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:43:22.689: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:43:22.689: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:43:22.689: INFO: network-resources-injector-545655c748-tlvzk from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 01:43:22.689: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 01:43:22.689: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:43:22.689: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 01:09:23 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Feb 27 01:43:22.689: INFO: isp-logger-74bf5ff65d-xsw9s from monitoring started at 2023-02-27 01:19:26 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container isp-logger ready: true, restart count 0
Feb 27 01:43:22.689: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:43:22.689: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.689: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:43:22.689: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 01:43:22.689: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
Feb 27 01:43:22.709: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 01:43:22.709: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 01:43:22.709: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 01:43:22.709: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 01:43:22.709: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container speaker ready: true, restart count 0
Feb 27 01:43:22.709: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 01:43:22.709: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 01:43:22.709: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container kucero ready: true, restart count 0
Feb 27 01:43:22.709: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 01:43:22.709: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Feb 27 01:43:22.709: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Feb 27 01:43:22.709: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Feb 27 01:43:22.709: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 01:43:22.709: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 01:43:22.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 01:43:22.709: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 01:43:22.709
Feb 27 01:43:22.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-649" to be "running"
Feb 27 01:43:22.742: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781236ms
Feb 27 01:43:24.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008941032s
Feb 27 01:43:26.745: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006616894s
Feb 27 01:43:26.745: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 01:43:26.749
STEP: Trying to apply a random label on the found node. 02/27/23 01:43:26.767
STEP: verifying the node has the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 95 02/27/23 01:43:26.779
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 01:43:26.782
W0227 01:43:26.793082      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Feb 27 01:43:26.793: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-649" to be "not pending"
Feb 27 01:43:26.796: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188734ms
Feb 27 01:43:28.800: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007322988s
Feb 27 01:43:28.800: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.5 on the node which pod4 resides and expect not scheduled 02/27/23 01:43:28.8
W0227 01:43:28.809004      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Feb 27 01:43:28.809: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-649" to be "not pending"
Feb 27 01:43:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043142ms
Feb 27 01:43:30.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011906955s
Feb 27 01:43:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011289215s
Feb 27 01:43:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010995697s
Feb 27 01:43:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010993309s
Feb 27 01:43:38.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009828809s
Feb 27 01:43:40.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011450678s
Feb 27 01:43:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010484516s
Feb 27 01:43:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011785731s
Feb 27 01:43:46.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010994364s
Feb 27 01:43:48.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016997036s
Feb 27 01:43:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011424302s
Feb 27 01:43:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010863536s
Feb 27 01:43:54.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011633224s
Feb 27 01:43:56.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010716947s
Feb 27 01:43:58.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012989184s
Feb 27 01:44:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011156011s
Feb 27 01:44:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012380971s
Feb 27 01:44:04.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010812467s
Feb 27 01:44:06.830: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021405189s
Feb 27 01:44:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010346985s
Feb 27 01:44:10.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012871762s
Feb 27 01:44:12.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011642838s
Feb 27 01:44:14.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010244649s
Feb 27 01:44:16.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011243243s
Feb 27 01:44:18.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015952015s
Feb 27 01:44:20.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01060702s
Feb 27 01:44:22.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011192656s
Feb 27 01:44:24.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011791384s
Feb 27 01:44:26.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0104809s
Feb 27 01:44:28.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012155353s
Feb 27 01:44:30.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011849468s
Feb 27 01:44:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011113351s
Feb 27 01:44:34.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012220986s
Feb 27 01:44:36.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012315214s
Feb 27 01:44:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010533179s
Feb 27 01:44:40.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012375906s
Feb 27 01:44:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009941056s
Feb 27 01:44:44.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013130248s
Feb 27 01:44:46.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016910935s
Feb 27 01:44:48.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009731317s
Feb 27 01:44:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011273826s
Feb 27 01:44:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010748084s
Feb 27 01:44:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01205433s
Feb 27 01:44:56.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017362625s
Feb 27 01:44:58.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010191214s
Feb 27 01:45:00.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010139744s
Feb 27 01:45:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010924125s
Feb 27 01:45:04.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011100491s
Feb 27 01:45:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010483627s
Feb 27 01:45:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010012122s
Feb 27 01:45:10.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011934281s
Feb 27 01:45:12.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01507725s
Feb 27 01:45:14.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01050367s
Feb 27 01:45:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009607609s
Feb 27 01:45:18.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010530184s
Feb 27 01:45:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011659795s
Feb 27 01:45:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010067712s
Feb 27 01:45:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010762926s
Feb 27 01:45:26.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01197219s
Feb 27 01:45:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010684902s
Feb 27 01:45:30.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.010089253s
Feb 27 01:45:32.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009854231s
Feb 27 01:45:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011255334s
Feb 27 01:45:36.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012317741s
Feb 27 01:45:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01031539s
Feb 27 01:45:40.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012888167s
Feb 27 01:45:42.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012810626s
Feb 27 01:45:44.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009803004s
Feb 27 01:45:46.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012448524s
Feb 27 01:45:48.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016584584s
Feb 27 01:45:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011276516s
Feb 27 01:45:52.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011204663s
Feb 27 01:45:54.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011055362s
Feb 27 01:45:56.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011040074s
Feb 27 01:45:58.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.01146574s
Feb 27 01:46:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011073198s
Feb 27 01:46:02.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.013257449s
Feb 27 01:46:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009584424s
Feb 27 01:46:06.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011649729s
Feb 27 01:46:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010269758s
Feb 27 01:46:10.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010082575s
Feb 27 01:46:12.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010344443s
Feb 27 01:46:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010914791s
Feb 27 01:46:16.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011971409s
Feb 27 01:46:18.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.018863074s
Feb 27 01:46:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.011771415s
Feb 27 01:46:22.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012858997s
Feb 27 01:46:24.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012547706s
Feb 27 01:46:26.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010874911s
Feb 27 01:46:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.010340134s
Feb 27 01:46:30.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01026937s
Feb 27 01:46:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.01126799s
Feb 27 01:46:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011500253s
Feb 27 01:46:36.834: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.024917895s
Feb 27 01:46:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.0103355s
Feb 27 01:46:40.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.011798412s
Feb 27 01:46:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010779454s
Feb 27 01:46:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011085861s
Feb 27 01:46:46.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.011149839s
Feb 27 01:46:48.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.010292922s
Feb 27 01:46:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.011255838s
Feb 27 01:46:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010837218s
Feb 27 01:46:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011985902s
Feb 27 01:46:56.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.011852188s
Feb 27 01:46:58.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.010167872s
Feb 27 01:47:00.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010740165s
Feb 27 01:47:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011935129s
Feb 27 01:47:04.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010905222s
Feb 27 01:47:06.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01448601s
Feb 27 01:47:08.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011157212s
Feb 27 01:47:10.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010506321s
Feb 27 01:47:12.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011207573s
Feb 27 01:47:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.011291722s
Feb 27 01:47:16.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.013761896s
Feb 27 01:47:18.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010520959s
Feb 27 01:47:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011721394s
Feb 27 01:47:22.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011572527s
Feb 27 01:47:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010159064s
Feb 27 01:47:26.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01728634s
Feb 27 01:47:28.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009559056s
Feb 27 01:47:30.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009255178s
Feb 27 01:47:32.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.010247406s
Feb 27 01:47:34.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010437224s
Feb 27 01:47:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.011414514s
Feb 27 01:47:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010052779s
Feb 27 01:47:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00939418s
Feb 27 01:47:42.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.00973548s
Feb 27 01:47:44.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009452507s
Feb 27 01:47:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009880593s
Feb 27 01:47:48.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.014311294s
Feb 27 01:47:50.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.010189293s
Feb 27 01:47:52.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009389154s
Feb 27 01:47:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012046969s
Feb 27 01:47:56.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010593855s
Feb 27 01:47:58.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01175777s
Feb 27 01:48:00.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.012498306s
Feb 27 01:48:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.011186381s
Feb 27 01:48:04.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.01016098s
Feb 27 01:48:06.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01602845s
Feb 27 01:48:08.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009847552s
Feb 27 01:48:10.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.011776099s
Feb 27 01:48:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009855473s
Feb 27 01:48:14.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012130631s
Feb 27 01:48:16.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009980915s
Feb 27 01:48:18.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.01823653s
Feb 27 01:48:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011778035s
Feb 27 01:48:22.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012476356s
Feb 27 01:48:24.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010958841s
Feb 27 01:48:26.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011410957s
Feb 27 01:48:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010515756s
Feb 27 01:48:28.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014452411s
STEP: removing the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 off the node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 01:48:28.823
STEP: verifying the node doesn't have the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 02/27/23 01:48:28.84
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-649" for this suite. 02/27/23 01:48:28.848
------------------------------
• [SLOW TEST] [306.244 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:43:22.614
    Feb 27 01:43:22.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-pred 02/27/23 01:43:22.615
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:43:22.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:43:22.636
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 01:43:22.638: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 01:43:22.645: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 01:43:22.648: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
    Feb 27 01:43:22.660: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container registry ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container sidecar ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container controller ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:43:22.660: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:43:22.660: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:43:22.660: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
    Feb 27 01:43:22.675: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container calicoctl ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:43:22.675: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:43:22.675: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.675: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:43:22.675: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
    Feb 27 01:43:22.689: INFO: test-webserver-e27df5db-34d8-45d4-8f53-585801ba7a5c from container-probe-9244 started at 2023-02-27 01:42:22 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container test-webserver ready: false, restart count 0
    Feb 27 01:43:22.689: INFO: default-http-backend-67df9bcb5b-q8xpb from ingress-nginx started at 2023-02-27 01:07:17 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container default-http-backend ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: nginx-ingress-controller-67d95699d-6kvs9 from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: ccd-license-consumer-69f48d6d8f-csqqf from kube-system started at 2023-02-27 01:17:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:43:22.689: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: network-resources-injector-545655c748-tlvzk from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 01:09:23 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: isp-logger-74bf5ff65d-xsw9s from monitoring started at 2023-02-27 01:19:26 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container isp-logger ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.689: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 01:43:22.689: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
    Feb 27 01:43:22.709: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 01:43:22.709: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 01:43:22.709: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 01:43:22.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 01:43:22.709: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 01:43:22.709
    Feb 27 01:43:22.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-649" to be "running"
    Feb 27 01:43:22.742: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781236ms
    Feb 27 01:43:24.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008941032s
    Feb 27 01:43:26.745: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006616894s
    Feb 27 01:43:26.745: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 01:43:26.749
    STEP: Trying to apply a random label on the found node. 02/27/23 01:43:26.767
    STEP: verifying the node has the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 95 02/27/23 01:43:26.779
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 02/27/23 01:43:26.782
    W0227 01:43:26.793082      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Feb 27 01:43:26.793: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-649" to be "not pending"
    Feb 27 01:43:26.796: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188734ms
    Feb 27 01:43:28.800: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007322988s
    Feb 27 01:43:28.800: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.10.5 on the node which pod4 resides and expect not scheduled 02/27/23 01:43:28.8
    W0227 01:43:28.809004      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Feb 27 01:43:28.809: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-649" to be "not pending"
    Feb 27 01:43:28.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043142ms
    Feb 27 01:43:30.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011906955s
    Feb 27 01:43:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011289215s
    Feb 27 01:43:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010995697s
    Feb 27 01:43:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010993309s
    Feb 27 01:43:38.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009828809s
    Feb 27 01:43:40.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011450678s
    Feb 27 01:43:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010484516s
    Feb 27 01:43:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011785731s
    Feb 27 01:43:46.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010994364s
    Feb 27 01:43:48.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016997036s
    Feb 27 01:43:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.011424302s
    Feb 27 01:43:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010863536s
    Feb 27 01:43:54.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011633224s
    Feb 27 01:43:56.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010716947s
    Feb 27 01:43:58.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012989184s
    Feb 27 01:44:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011156011s
    Feb 27 01:44:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012380971s
    Feb 27 01:44:04.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010812467s
    Feb 27 01:44:06.830: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.021405189s
    Feb 27 01:44:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010346985s
    Feb 27 01:44:10.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012871762s
    Feb 27 01:44:12.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011642838s
    Feb 27 01:44:14.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010244649s
    Feb 27 01:44:16.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.011243243s
    Feb 27 01:44:18.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.015952015s
    Feb 27 01:44:20.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01060702s
    Feb 27 01:44:22.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011192656s
    Feb 27 01:44:24.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.011791384s
    Feb 27 01:44:26.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0104809s
    Feb 27 01:44:28.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012155353s
    Feb 27 01:44:30.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011849468s
    Feb 27 01:44:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.011113351s
    Feb 27 01:44:34.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012220986s
    Feb 27 01:44:36.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.012315214s
    Feb 27 01:44:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010533179s
    Feb 27 01:44:40.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012375906s
    Feb 27 01:44:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009941056s
    Feb 27 01:44:44.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.013130248s
    Feb 27 01:44:46.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016910935s
    Feb 27 01:44:48.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009731317s
    Feb 27 01:44:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.011273826s
    Feb 27 01:44:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010748084s
    Feb 27 01:44:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.01205433s
    Feb 27 01:44:56.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017362625s
    Feb 27 01:44:58.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.010191214s
    Feb 27 01:45:00.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.010139744s
    Feb 27 01:45:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010924125s
    Feb 27 01:45:04.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.011100491s
    Feb 27 01:45:06.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010483627s
    Feb 27 01:45:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010012122s
    Feb 27 01:45:10.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011934281s
    Feb 27 01:45:12.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01507725s
    Feb 27 01:45:14.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01050367s
    Feb 27 01:45:16.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009607609s
    Feb 27 01:45:18.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.010530184s
    Feb 27 01:45:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011659795s
    Feb 27 01:45:22.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010067712s
    Feb 27 01:45:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010762926s
    Feb 27 01:45:26.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.01197219s
    Feb 27 01:45:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010684902s
    Feb 27 01:45:30.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.010089253s
    Feb 27 01:45:32.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009854231s
    Feb 27 01:45:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.011255334s
    Feb 27 01:45:36.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012317741s
    Feb 27 01:45:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01031539s
    Feb 27 01:45:40.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012888167s
    Feb 27 01:45:42.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012810626s
    Feb 27 01:45:44.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009803004s
    Feb 27 01:45:46.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012448524s
    Feb 27 01:45:48.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.016584584s
    Feb 27 01:45:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011276516s
    Feb 27 01:45:52.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.011204663s
    Feb 27 01:45:54.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011055362s
    Feb 27 01:45:56.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.011040074s
    Feb 27 01:45:58.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.01146574s
    Feb 27 01:46:00.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.011073198s
    Feb 27 01:46:02.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.013257449s
    Feb 27 01:46:04.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.009584424s
    Feb 27 01:46:06.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011649729s
    Feb 27 01:46:08.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.010269758s
    Feb 27 01:46:10.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.010082575s
    Feb 27 01:46:12.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010344443s
    Feb 27 01:46:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010914791s
    Feb 27 01:46:16.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.011971409s
    Feb 27 01:46:18.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.018863074s
    Feb 27 01:46:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.011771415s
    Feb 27 01:46:22.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.012858997s
    Feb 27 01:46:24.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012547706s
    Feb 27 01:46:26.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010874911s
    Feb 27 01:46:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.010340134s
    Feb 27 01:46:30.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01026937s
    Feb 27 01:46:32.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.01126799s
    Feb 27 01:46:34.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.011500253s
    Feb 27 01:46:36.834: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.024917895s
    Feb 27 01:46:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.0103355s
    Feb 27 01:46:40.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.011798412s
    Feb 27 01:46:42.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.010779454s
    Feb 27 01:46:44.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.011085861s
    Feb 27 01:46:46.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.011149839s
    Feb 27 01:46:48.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.010292922s
    Feb 27 01:46:50.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.011255838s
    Feb 27 01:46:52.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010837218s
    Feb 27 01:46:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011985902s
    Feb 27 01:46:56.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.011852188s
    Feb 27 01:46:58.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.010167872s
    Feb 27 01:47:00.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.010740165s
    Feb 27 01:47:02.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011935129s
    Feb 27 01:47:04.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010905222s
    Feb 27 01:47:06.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.01448601s
    Feb 27 01:47:08.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.011157212s
    Feb 27 01:47:10.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.010506321s
    Feb 27 01:47:12.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.011207573s
    Feb 27 01:47:14.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.011291722s
    Feb 27 01:47:16.822: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.013761896s
    Feb 27 01:47:18.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010520959s
    Feb 27 01:47:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.011721394s
    Feb 27 01:47:22.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.011572527s
    Feb 27 01:47:24.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010159064s
    Feb 27 01:47:26.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01728634s
    Feb 27 01:47:28.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009559056s
    Feb 27 01:47:30.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009255178s
    Feb 27 01:47:32.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.010247406s
    Feb 27 01:47:34.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.010437224s
    Feb 27 01:47:36.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.011414514s
    Feb 27 01:47:38.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010052779s
    Feb 27 01:47:40.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.00939418s
    Feb 27 01:47:42.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.00973548s
    Feb 27 01:47:44.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009452507s
    Feb 27 01:47:46.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009880593s
    Feb 27 01:47:48.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.014311294s
    Feb 27 01:47:50.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.010189293s
    Feb 27 01:47:52.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009389154s
    Feb 27 01:47:54.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.012046969s
    Feb 27 01:47:56.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010593855s
    Feb 27 01:47:58.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01175777s
    Feb 27 01:48:00.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.012498306s
    Feb 27 01:48:02.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.011186381s
    Feb 27 01:48:04.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.01016098s
    Feb 27 01:48:06.825: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.01602845s
    Feb 27 01:48:08.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009847552s
    Feb 27 01:48:10.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.011776099s
    Feb 27 01:48:12.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009855473s
    Feb 27 01:48:14.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012130631s
    Feb 27 01:48:16.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009980915s
    Feb 27 01:48:18.827: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.01823653s
    Feb 27 01:48:20.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.011778035s
    Feb 27 01:48:22.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.012476356s
    Feb 27 01:48:24.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010958841s
    Feb 27 01:48:26.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.011410957s
    Feb 27 01:48:28.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010515756s
    Feb 27 01:48:28.823: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.014452411s
    STEP: removing the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 off the node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 01:48:28.823
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-15e7fd7e-d691-48f8-b507-732d022d36b8 02/27/23 01:48:28.84
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:28.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-649" for this suite. 02/27/23 01:48:28.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:28.859
Feb 27 01:48:28.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 01:48:28.86
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:28.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:28.883
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 02/27/23 01:48:28.886
Feb 27 01:48:28.920: INFO: created test-pod-1
Feb 27 01:48:28.967: INFO: created test-pod-2
Feb 27 01:48:29.013: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 02/27/23 01:48:29.014
Feb 27 01:48:29.014: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3632' to be running and ready
Feb 27 01:48:29.037: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 01:48:29.037: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 01:48:29.037: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 27 01:48:29.037: INFO: 0 / 3 pods in namespace 'pods-3632' are running and ready (0 seconds elapsed)
Feb 27 01:48:29.037: INFO: expected 0 pod replicas in namespace 'pods-3632', 0 are Running and Ready.
Feb 27 01:48:29.037: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Feb 27 01:48:29.037: INFO: test-pod-1  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  }]
Feb 27 01:48:29.037: INFO: test-pod-2  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  }]
Feb 27 01:48:29.037: INFO: test-pod-3  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:29 +0000 UTC  }]
Feb 27 01:48:29.037: INFO: 
Feb 27 01:48:31.059: INFO: 3 / 3 pods in namespace 'pods-3632' are running and ready (2 seconds elapsed)
Feb 27 01:48:31.059: INFO: expected 0 pod replicas in namespace 'pods-3632', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 02/27/23 01:48:31.086
Feb 27 01:48:31.091: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 01:48:32.095: INFO: Pod quantity 3 is different from expected quantity 0
Feb 27 01:48:33.095: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3632" for this suite. 02/27/23 01:48:34.101
------------------------------
• [SLOW TEST] [5.256 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:28.859
    Feb 27 01:48:28.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 01:48:28.86
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:28.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:28.883
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 02/27/23 01:48:28.886
    Feb 27 01:48:28.920: INFO: created test-pod-1
    Feb 27 01:48:28.967: INFO: created test-pod-2
    Feb 27 01:48:29.013: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 02/27/23 01:48:29.014
    Feb 27 01:48:29.014: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3632' to be running and ready
    Feb 27 01:48:29.037: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 01:48:29.037: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 01:48:29.037: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Feb 27 01:48:29.037: INFO: 0 / 3 pods in namespace 'pods-3632' are running and ready (0 seconds elapsed)
    Feb 27 01:48:29.037: INFO: expected 0 pod replicas in namespace 'pods-3632', 0 are Running and Ready.
    Feb 27 01:48:29.037: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Feb 27 01:48:29.037: INFO: test-pod-1  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  }]
    Feb 27 01:48:29.037: INFO: test-pod-2  worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:28 +0000 UTC  }]
    Feb 27 01:48:29.037: INFO: test-pod-3  worker-pool1-losn7d81-n92-ci-ibd-23-jenkins  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-02-27 01:48:29 +0000 UTC  }]
    Feb 27 01:48:29.037: INFO: 
    Feb 27 01:48:31.059: INFO: 3 / 3 pods in namespace 'pods-3632' are running and ready (2 seconds elapsed)
    Feb 27 01:48:31.059: INFO: expected 0 pod replicas in namespace 'pods-3632', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 02/27/23 01:48:31.086
    Feb 27 01:48:31.091: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 01:48:32.095: INFO: Pod quantity 3 is different from expected quantity 0
    Feb 27 01:48:33.095: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3632" for this suite. 02/27/23 01:48:34.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:34.115
Feb 27 01:48:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:48:34.116
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:34.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:34.142
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 02/27/23 01:48:34.145
Feb 27 01:48:34.177: INFO: Waiting up to 5m0s for pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6" in namespace "downward-api-9142" to be "running and ready"
Feb 27 01:48:34.180: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213139ms
Feb 27 01:48:34.180: INFO: The phase of Pod labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:48:36.185: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008228415s
Feb 27 01:48:36.185: INFO: The phase of Pod labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6 is Running (Ready = true)
Feb 27 01:48:36.185: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6" satisfied condition "running and ready"
Feb 27 01:48:36.717: INFO: Successfully updated pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9142" for this suite. 02/27/23 01:48:40.742
------------------------------
• [SLOW TEST] [6.634 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:34.115
    Feb 27 01:48:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:48:34.116
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:34.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:34.142
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 02/27/23 01:48:34.145
    Feb 27 01:48:34.177: INFO: Waiting up to 5m0s for pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6" in namespace "downward-api-9142" to be "running and ready"
    Feb 27 01:48:34.180: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213139ms
    Feb 27 01:48:34.180: INFO: The phase of Pod labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:48:36.185: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008228415s
    Feb 27 01:48:36.185: INFO: The phase of Pod labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6 is Running (Ready = true)
    Feb 27 01:48:36.185: INFO: Pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6" satisfied condition "running and ready"
    Feb 27 01:48:36.717: INFO: Successfully updated pod "labelsupdate9268f080-76b1-45e3-8341-2e04fdcc33e6"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:40.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9142" for this suite. 02/27/23 01:48:40.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:40.75
Feb 27 01:48:40.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:48:40.751
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:40.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:40.776
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:48:40.778
Feb 27 01:48:40.811: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f" in namespace "downward-api-2923" to be "Succeeded or Failed"
Feb 27 01:48:40.815: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341008ms
Feb 27 01:48:42.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008386933s
Feb 27 01:48:44.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644499s
STEP: Saw pod success 02/27/23 01:48:44.82
Feb 27 01:48:44.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f" satisfied condition "Succeeded or Failed"
Feb 27 01:48:44.824: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f container client-container: <nil>
STEP: delete the pod 02/27/23 01:48:44.832
Feb 27 01:48:44.845: INFO: Waiting for pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f to disappear
Feb 27 01:48:44.848: INFO: Pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:44.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2923" for this suite. 02/27/23 01:48:44.852
------------------------------
• [4.110 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:40.75
    Feb 27 01:48:40.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:48:40.751
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:40.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:40.776
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:48:40.778
    Feb 27 01:48:40.811: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f" in namespace "downward-api-2923" to be "Succeeded or Failed"
    Feb 27 01:48:40.815: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.341008ms
    Feb 27 01:48:42.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008386933s
    Feb 27 01:48:44.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008644499s
    STEP: Saw pod success 02/27/23 01:48:44.82
    Feb 27 01:48:44.820: INFO: Pod "downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f" satisfied condition "Succeeded or Failed"
    Feb 27 01:48:44.824: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f container client-container: <nil>
    STEP: delete the pod 02/27/23 01:48:44.832
    Feb 27 01:48:44.845: INFO: Waiting for pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f to disappear
    Feb 27 01:48:44.848: INFO: Pod downwardapi-volume-b802d8fb-bd5e-48a3-a49b-a5ae0be3347f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:44.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2923" for this suite. 02/27/23 01:48:44.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:44.862
Feb 27 01:48:44.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 01:48:44.862
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:44.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:44.894
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Feb 27 01:48:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: creating the pod 02/27/23 01:48:44.897
STEP: submitting the pod to kubernetes 02/27/23 01:48:44.897
Feb 27 01:48:44.907: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4" in namespace "pods-6381" to be "running and ready"
Feb 27 01:48:44.910: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588281ms
Feb 27 01:48:44.910: INFO: The phase of Pod pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:48:46.914: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006381731s
Feb 27 01:48:46.914: INFO: The phase of Pod pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4 is Running (Ready = true)
Feb 27 01:48:46.914: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6381" for this suite. 02/27/23 01:48:46.933
------------------------------
• [2.083 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:44.862
    Feb 27 01:48:44.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 01:48:44.862
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:44.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:44.894
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Feb 27 01:48:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: creating the pod 02/27/23 01:48:44.897
    STEP: submitting the pod to kubernetes 02/27/23 01:48:44.897
    Feb 27 01:48:44.907: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4" in namespace "pods-6381" to be "running and ready"
    Feb 27 01:48:44.910: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588281ms
    Feb 27 01:48:44.910: INFO: The phase of Pod pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:48:46.914: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006381731s
    Feb 27 01:48:46.914: INFO: The phase of Pod pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4 is Running (Ready = true)
    Feb 27 01:48:46.914: INFO: Pod "pod-logs-websocket-fc70efed-d41b-4b3a-8ad5-2a50db08e6e4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:46.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6381" for this suite. 02/27/23 01:48:46.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:46.945
Feb 27 01:48:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 01:48:46.945
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:46.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:46.966
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 01:48:46.969
Feb 27 01:48:47.016: INFO: Waiting up to 5m0s for pod "pod-be337a78-e356-410f-b28f-1074cb368f97" in namespace "emptydir-6116" to be "Succeeded or Failed"
Feb 27 01:48:47.021: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952014ms
Feb 27 01:48:49.026: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00991994s
Feb 27 01:48:51.024: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008258966s
STEP: Saw pod success 02/27/23 01:48:51.024
Feb 27 01:48:51.024: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97" satisfied condition "Succeeded or Failed"
Feb 27 01:48:51.027: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-be337a78-e356-410f-b28f-1074cb368f97 container test-container: <nil>
STEP: delete the pod 02/27/23 01:48:51.032
Feb 27 01:48:51.049: INFO: Waiting for pod pod-be337a78-e356-410f-b28f-1074cb368f97 to disappear
Feb 27 01:48:51.052: INFO: Pod pod-be337a78-e356-410f-b28f-1074cb368f97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:48:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6116" for this suite. 02/27/23 01:48:51.058
------------------------------
• [4.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:46.945
    Feb 27 01:48:46.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 01:48:46.945
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:46.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:46.966
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 02/27/23 01:48:46.969
    Feb 27 01:48:47.016: INFO: Waiting up to 5m0s for pod "pod-be337a78-e356-410f-b28f-1074cb368f97" in namespace "emptydir-6116" to be "Succeeded or Failed"
    Feb 27 01:48:47.021: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952014ms
    Feb 27 01:48:49.026: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00991994s
    Feb 27 01:48:51.024: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008258966s
    STEP: Saw pod success 02/27/23 01:48:51.024
    Feb 27 01:48:51.024: INFO: Pod "pod-be337a78-e356-410f-b28f-1074cb368f97" satisfied condition "Succeeded or Failed"
    Feb 27 01:48:51.027: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-be337a78-e356-410f-b28f-1074cb368f97 container test-container: <nil>
    STEP: delete the pod 02/27/23 01:48:51.032
    Feb 27 01:48:51.049: INFO: Waiting for pod pod-be337a78-e356-410f-b28f-1074cb368f97 to disappear
    Feb 27 01:48:51.052: INFO: Pod pod-be337a78-e356-410f-b28f-1074cb368f97 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:48:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6116" for this suite. 02/27/23 01:48:51.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:48:51.069
Feb 27 01:48:51.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 01:48:51.07
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:51.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:51.095
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 02/27/23 01:48:51.098
Feb 27 01:48:51.108: INFO: Waiting up to 2m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570" to be "running"
Feb 27 01:48:51.111: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57883ms
Feb 27 01:48:53.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006016451s
Feb 27 01:48:55.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007066865s
Feb 27 01:48:57.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007449461s
Feb 27 01:48:59.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008256238s
Feb 27 01:49:01.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007807193s
Feb 27 01:49:03.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007454227s
Feb 27 01:49:05.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008142555s
Feb 27 01:49:07.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007743873s
Feb 27 01:49:09.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008643733s
Feb 27 01:49:11.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007382311s
Feb 27 01:49:13.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007722756s
Feb 27 01:49:15.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007801451s
Feb 27 01:49:17.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007759205s
Feb 27 01:49:19.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006419641s
Feb 27 01:49:21.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008231717s
Feb 27 01:49:23.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008794734s
Feb 27 01:49:25.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006086779s
Feb 27 01:49:27.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006433982s
Feb 27 01:49:29.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007542827s
Feb 27 01:49:31.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007553908s
Feb 27 01:49:33.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006625103s
Feb 27 01:49:35.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006682225s
Feb 27 01:49:37.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006633378s
Feb 27 01:49:39.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006896511s
Feb 27 01:49:41.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00819686s
Feb 27 01:49:43.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008278025s
Feb 27 01:49:45.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005822416s
Feb 27 01:49:47.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00657374s
Feb 27 01:49:49.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007616763s
Feb 27 01:49:51.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008104243s
Feb 27 01:49:53.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007700663s
Feb 27 01:49:55.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008025621s
Feb 27 01:49:57.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006514893s
Feb 27 01:49:59.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00737281s
Feb 27 01:50:01.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006690037s
Feb 27 01:50:03.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006877199s
Feb 27 01:50:05.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007378987s
Feb 27 01:50:07.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007766154s
Feb 27 01:50:09.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005998734s
Feb 27 01:50:11.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008327371s
Feb 27 01:50:13.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008321812s
Feb 27 01:50:15.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007318454s
Feb 27 01:50:17.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007915586s
Feb 27 01:50:19.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006423196s
Feb 27 01:50:21.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007728506s
Feb 27 01:50:23.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008262384s
Feb 27 01:50:25.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00752539s
Feb 27 01:50:27.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006797106s
Feb 27 01:50:29.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006609855s
Feb 27 01:50:31.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007853948s
Feb 27 01:50:33.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007406662s
Feb 27 01:50:35.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008169694s
Feb 27 01:50:37.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007458604s
Feb 27 01:50:39.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006824407s
Feb 27 01:50:41.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007575336s
Feb 27 01:50:43.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008701594s
Feb 27 01:50:45.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00701086s
Feb 27 01:50:47.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008774764s
Feb 27 01:50:49.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006628655s
Feb 27 01:50:51.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007650542s
Feb 27 01:50:51.119: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011018824s
STEP: updating the pod 02/27/23 01:50:51.119
Feb 27 01:50:51.638: INFO: Successfully updated pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba"
STEP: waiting for pod running 02/27/23 01:50:51.638
Feb 27 01:50:51.638: INFO: Waiting up to 2m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570" to be "running"
Feb 27 01:50:51.643: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.320759ms
Feb 27 01:50:53.647: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571085s
Feb 27 01:50:53.647: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 01:50:53.647
Feb 27 01:50:53.647: INFO: Deleting pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570"
Feb 27 01:50:53.656: INFO: Wait up to 5m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:25.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-570" for this suite. 02/27/23 01:51:25.669
------------------------------
• [SLOW TEST] [154.606 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:48:51.069
    Feb 27 01:48:51.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 01:48:51.07
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:48:51.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:48:51.095
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 02/27/23 01:48:51.098
    Feb 27 01:48:51.108: INFO: Waiting up to 2m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570" to be "running"
    Feb 27 01:48:51.111: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57883ms
    Feb 27 01:48:53.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006016451s
    Feb 27 01:48:55.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007066865s
    Feb 27 01:48:57.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007449461s
    Feb 27 01:48:59.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008256238s
    Feb 27 01:49:01.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007807193s
    Feb 27 01:49:03.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007454227s
    Feb 27 01:49:05.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008142555s
    Feb 27 01:49:07.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007743873s
    Feb 27 01:49:09.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008643733s
    Feb 27 01:49:11.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007382311s
    Feb 27 01:49:13.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007722756s
    Feb 27 01:49:15.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007801451s
    Feb 27 01:49:17.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007759205s
    Feb 27 01:49:19.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006419641s
    Feb 27 01:49:21.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008231717s
    Feb 27 01:49:23.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008794734s
    Feb 27 01:49:25.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006086779s
    Feb 27 01:49:27.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006433982s
    Feb 27 01:49:29.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007542827s
    Feb 27 01:49:31.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007553908s
    Feb 27 01:49:33.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006625103s
    Feb 27 01:49:35.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006682225s
    Feb 27 01:49:37.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006633378s
    Feb 27 01:49:39.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006896511s
    Feb 27 01:49:41.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00819686s
    Feb 27 01:49:43.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008278025s
    Feb 27 01:49:45.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005822416s
    Feb 27 01:49:47.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00657374s
    Feb 27 01:49:49.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007616763s
    Feb 27 01:49:51.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008104243s
    Feb 27 01:49:53.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007700663s
    Feb 27 01:49:55.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008025621s
    Feb 27 01:49:57.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006514893s
    Feb 27 01:49:59.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00737281s
    Feb 27 01:50:01.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006690037s
    Feb 27 01:50:03.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.006877199s
    Feb 27 01:50:05.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007378987s
    Feb 27 01:50:07.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007766154s
    Feb 27 01:50:09.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005998734s
    Feb 27 01:50:11.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008327371s
    Feb 27 01:50:13.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008321812s
    Feb 27 01:50:15.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007318454s
    Feb 27 01:50:17.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007915586s
    Feb 27 01:50:19.114: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006423196s
    Feb 27 01:50:21.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007728506s
    Feb 27 01:50:23.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008262384s
    Feb 27 01:50:25.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00752539s
    Feb 27 01:50:27.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006797106s
    Feb 27 01:50:29.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006609855s
    Feb 27 01:50:31.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007853948s
    Feb 27 01:50:33.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007406662s
    Feb 27 01:50:35.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008169694s
    Feb 27 01:50:37.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007458604s
    Feb 27 01:50:39.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006824407s
    Feb 27 01:50:41.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007575336s
    Feb 27 01:50:43.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008701594s
    Feb 27 01:50:45.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.00701086s
    Feb 27 01:50:47.117: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008774764s
    Feb 27 01:50:49.115: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006628655s
    Feb 27 01:50:51.116: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007650542s
    Feb 27 01:50:51.119: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011018824s
    STEP: updating the pod 02/27/23 01:50:51.119
    Feb 27 01:50:51.638: INFO: Successfully updated pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba"
    STEP: waiting for pod running 02/27/23 01:50:51.638
    Feb 27 01:50:51.638: INFO: Waiting up to 2m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570" to be "running"
    Feb 27 01:50:51.643: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.320759ms
    Feb 27 01:50:53.647: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571085s
    Feb 27 01:50:53.647: INFO: Pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 01:50:53.647
    Feb 27 01:50:53.647: INFO: Deleting pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" in namespace "var-expansion-570"
    Feb 27 01:50:53.656: INFO: Wait up to 5m0s for pod "var-expansion-17f7e120-0438-430a-b297-673028cc62ba" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:25.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-570" for this suite. 02/27/23 01:51:25.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:25.676
Feb 27 01:51:25.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 01:51:25.676
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:25.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:25.702
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-ac0863a6-dbbd-42f5-8b02-0f6f9db67f72 02/27/23 01:51:25.71
STEP: Creating secret with name s-test-opt-upd-8a91bbf7-daf3-45f1-a960-72272292c49e 02/27/23 01:51:25.715
STEP: Creating the pod 02/27/23 01:51:25.721
Feb 27 01:51:25.755: INFO: Waiting up to 5m0s for pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5" in namespace "secrets-6122" to be "running and ready"
Feb 27 01:51:25.759: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095652ms
Feb 27 01:51:25.759: INFO: The phase of Pod pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:51:27.764: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009152816s
Feb 27 01:51:27.764: INFO: The phase of Pod pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5 is Running (Ready = true)
Feb 27 01:51:27.764: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-ac0863a6-dbbd-42f5-8b02-0f6f9db67f72 02/27/23 01:51:27.791
STEP: Updating secret s-test-opt-upd-8a91bbf7-daf3-45f1-a960-72272292c49e 02/27/23 01:51:27.797
STEP: Creating secret with name s-test-opt-create-b3fa78dd-b913-4f95-ae47-d13c6cd7b96b 02/27/23 01:51:27.803
STEP: waiting to observe update in volume 02/27/23 01:51:27.807
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:31.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6122" for this suite. 02/27/23 01:51:31.841
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:25.676
    Feb 27 01:51:25.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 01:51:25.676
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:25.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:25.702
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-ac0863a6-dbbd-42f5-8b02-0f6f9db67f72 02/27/23 01:51:25.71
    STEP: Creating secret with name s-test-opt-upd-8a91bbf7-daf3-45f1-a960-72272292c49e 02/27/23 01:51:25.715
    STEP: Creating the pod 02/27/23 01:51:25.721
    Feb 27 01:51:25.755: INFO: Waiting up to 5m0s for pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5" in namespace "secrets-6122" to be "running and ready"
    Feb 27 01:51:25.759: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095652ms
    Feb 27 01:51:25.759: INFO: The phase of Pod pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:51:27.764: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009152816s
    Feb 27 01:51:27.764: INFO: The phase of Pod pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5 is Running (Ready = true)
    Feb 27 01:51:27.764: INFO: Pod "pod-secrets-fef24141-f3d3-4816-b65b-a470d00aefe5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-ac0863a6-dbbd-42f5-8b02-0f6f9db67f72 02/27/23 01:51:27.791
    STEP: Updating secret s-test-opt-upd-8a91bbf7-daf3-45f1-a960-72272292c49e 02/27/23 01:51:27.797
    STEP: Creating secret with name s-test-opt-create-b3fa78dd-b913-4f95-ae47-d13c6cd7b96b 02/27/23 01:51:27.803
    STEP: waiting to observe update in volume 02/27/23 01:51:27.807
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:31.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6122" for this suite. 02/27/23 01:51:31.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:31.848
Feb 27 01:51:31.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 01:51:31.849
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:31.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:31.875
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 02/27/23 01:51:31.877
Feb 27 01:51:31.909: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533" in namespace "emptydir-9882" to be "running"
Feb 27 01:51:31.912: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388535ms
Feb 27 01:51:33.917: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533": Phase="Running", Reason="", readiness=false. Elapsed: 2.007552622s
Feb 27 01:51:33.917: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533" satisfied condition "running"
STEP: Reading file content from the nginx-container 02/27/23 01:51:33.917
Feb 27 01:51:33.917: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9882 PodName:pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 01:51:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:51:33.918: INFO: ExecWithOptions: Clientset creation
Feb 27 01:51:33.918: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9882/pods/pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Feb 27 01:51:33.985: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:33.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9882" for this suite. 02/27/23 01:51:33.992
------------------------------
• [2.151 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:31.848
    Feb 27 01:51:31.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 01:51:31.849
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:31.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:31.875
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 02/27/23 01:51:31.877
    Feb 27 01:51:31.909: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533" in namespace "emptydir-9882" to be "running"
    Feb 27 01:51:31.912: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388535ms
    Feb 27 01:51:33.917: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533": Phase="Running", Reason="", readiness=false. Elapsed: 2.007552622s
    Feb 27 01:51:33.917: INFO: Pod "pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533" satisfied condition "running"
    STEP: Reading file content from the nginx-container 02/27/23 01:51:33.917
    Feb 27 01:51:33.917: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9882 PodName:pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 01:51:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:51:33.918: INFO: ExecWithOptions: Clientset creation
    Feb 27 01:51:33.918: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9882/pods/pod-sharedvolume-263f4649-0724-435c-9b4a-e7a86359a533/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Feb 27 01:51:33.985: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:33.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9882" for this suite. 02/27/23 01:51:33.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:34
Feb 27 01:51:34.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 01:51:34.001
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:34.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:34.022
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 02/27/23 01:51:34.024
Feb 27 01:51:34.035: INFO: Waiting up to 5m0s for pod "pod-qwk2m" in namespace "pods-6761" to be "running"
Feb 27 01:51:34.039: INFO: Pod "pod-qwk2m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253224ms
Feb 27 01:51:36.045: INFO: Pod "pod-qwk2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530874s
Feb 27 01:51:36.045: INFO: Pod "pod-qwk2m" satisfied condition "running"
STEP: patching /status 02/27/23 01:51:36.045
Feb 27 01:51:36.057: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6761" for this suite. 02/27/23 01:51:36.065
------------------------------
• [2.076 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:34
    Feb 27 01:51:34.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 01:51:34.001
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:34.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:34.022
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 02/27/23 01:51:34.024
    Feb 27 01:51:34.035: INFO: Waiting up to 5m0s for pod "pod-qwk2m" in namespace "pods-6761" to be "running"
    Feb 27 01:51:34.039: INFO: Pod "pod-qwk2m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253224ms
    Feb 27 01:51:36.045: INFO: Pod "pod-qwk2m": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530874s
    Feb 27 01:51:36.045: INFO: Pod "pod-qwk2m" satisfied condition "running"
    STEP: patching /status 02/27/23 01:51:36.045
    Feb 27 01:51:36.057: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:36.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6761" for this suite. 02/27/23 01:51:36.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:36.076
Feb 27 01:51:36.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-runtime 02/27/23 01:51:36.077
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:36.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:36.096
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 02/27/23 01:51:36.098
STEP: wait for the container to reach Failed 02/27/23 01:51:36.11
STEP: get the container status 02/27/23 01:51:40.128
STEP: the container should be terminated 02/27/23 01:51:40.131
STEP: the termination message should be set 02/27/23 01:51:40.131
Feb 27 01:51:40.132: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 02/27/23 01:51:40.132
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:40.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6198" for this suite. 02/27/23 01:51:40.151
------------------------------
• [4.082 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:36.076
    Feb 27 01:51:36.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-runtime 02/27/23 01:51:36.077
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:36.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:36.096
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 02/27/23 01:51:36.098
    STEP: wait for the container to reach Failed 02/27/23 01:51:36.11
    STEP: get the container status 02/27/23 01:51:40.128
    STEP: the container should be terminated 02/27/23 01:51:40.131
    STEP: the termination message should be set 02/27/23 01:51:40.131
    Feb 27 01:51:40.132: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 02/27/23 01:51:40.132
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:40.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6198" for this suite. 02/27/23 01:51:40.151
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:40.158
Feb 27 01:51:40.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 01:51:40.159
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:40.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:40.218
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 01:51:40.238
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:51:40.579
STEP: Deploying the webhook pod 02/27/23 01:51:40.587
STEP: Wait for the deployment to be ready 02/27/23 01:51:40.601
Feb 27 01:51:40.610: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 01:51:42.624
STEP: Verifying the service has paired with the endpoint 02/27/23 01:51:42.639
Feb 27 01:51:43.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 01:51:43.643
STEP: create a namespace for the webhook 02/27/23 01:51:43.671
STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 01:51:43.68
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:43.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8906" for this suite. 02/27/23 01:51:43.932
STEP: Destroying namespace "webhook-8906-markers" for this suite. 02/27/23 01:51:43.951
------------------------------
• [3.804 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:40.158
    Feb 27 01:51:40.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 01:51:40.159
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:40.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:40.218
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 01:51:40.238
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:51:40.579
    STEP: Deploying the webhook pod 02/27/23 01:51:40.587
    STEP: Wait for the deployment to be ready 02/27/23 01:51:40.601
    Feb 27 01:51:40.610: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 01:51:42.624
    STEP: Verifying the service has paired with the endpoint 02/27/23 01:51:42.639
    Feb 27 01:51:43.639: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 02/27/23 01:51:43.643
    STEP: create a namespace for the webhook 02/27/23 01:51:43.671
    STEP: create a configmap should be unconditionally rejected by the webhook 02/27/23 01:51:43.68
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:43.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8906" for this suite. 02/27/23 01:51:43.932
    STEP: Destroying namespace "webhook-8906-markers" for this suite. 02/27/23 01:51:43.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:43.963
Feb 27 01:51:43.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename events 02/27/23 01:51:43.964
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:43.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:43.992
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 02/27/23 01:51:43.997
Feb 27 01:51:44.004: INFO: created test-event-1
Feb 27 01:51:44.010: INFO: created test-event-2
Feb 27 01:51:44.015: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 02/27/23 01:51:44.015
STEP: delete collection of events 02/27/23 01:51:44.018
Feb 27 01:51:44.019: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 01:51:44.043
Feb 27 01:51:44.043: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 27 01:51:44.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1717" for this suite. 02/27/23 01:51:44.05
------------------------------
• [0.096 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:43.963
    Feb 27 01:51:43.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename events 02/27/23 01:51:43.964
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:43.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:43.992
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 02/27/23 01:51:43.997
    Feb 27 01:51:44.004: INFO: created test-event-1
    Feb 27 01:51:44.010: INFO: created test-event-2
    Feb 27 01:51:44.015: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 02/27/23 01:51:44.015
    STEP: delete collection of events 02/27/23 01:51:44.018
    Feb 27 01:51:44.019: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 01:51:44.043
    Feb 27 01:51:44.043: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:51:44.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1717" for this suite. 02/27/23 01:51:44.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:51:44.059
Feb 27 01:51:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pod-network-test 02/27/23 01:51:44.06
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:44.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:44.083
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4252 02/27/23 01:51:44.086
STEP: creating a selector 02/27/23 01:51:44.086
STEP: Creating the service pods in kubernetes 02/27/23 01:51:44.086
Feb 27 01:51:44.086: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 01:51:44.159: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4252" to be "running and ready"
Feb 27 01:51:44.164: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18535ms
Feb 27 01:51:44.164: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:51:46.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008198311s
Feb 27 01:51:46.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 01:51:48.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009027378s
Feb 27 01:51:48.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 01:51:50.169: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009944962s
Feb 27 01:51:50.169: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 01:51:52.169: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009522301s
Feb 27 01:51:52.169: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 01:51:54.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009027579s
Feb 27 01:51:54.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 01:51:56.171: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011244351s
Feb 27 01:51:56.171: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 01:51:56.171: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 01:51:56.174: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4252" to be "running and ready"
Feb 27 01:51:56.177: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.123742ms
Feb 27 01:51:56.177: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 01:51:58.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.007343418s
Feb 27 01:51:58.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 01:52:00.182: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.008025143s
Feb 27 01:52:00.182: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 01:52:02.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.007265174s
Feb 27 01:52:02.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 01:52:04.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007569814s
Feb 27 01:52:04.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Feb 27 01:52:06.182: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.008469239s
Feb 27 01:52:06.182: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 01:52:06.182: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 01:52:06.185: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4252" to be "running and ready"
Feb 27 01:52:06.189: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.241984ms
Feb 27 01:52:06.189: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 01:52:06.189: INFO: Pod "netserver-2" satisfied condition "running and ready"
Feb 27 01:52:06.191: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-4252" to be "running and ready"
Feb 27 01:52:06.195: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.564888ms
Feb 27 01:52:06.195: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Feb 27 01:52:06.195: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 01:52:06.199
W0227 01:52:06.233454      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Feb 27 01:52:06.233: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4252" to be "running"
Feb 27 01:52:06.238: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.320105ms
Feb 27 01:52:08.242: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382015s
Feb 27 01:52:08.242: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 01:52:08.246: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4252" to be "running"
Feb 27 01:52:08.249: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.701883ms
Feb 27 01:52:08.249: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 01:52:08.252: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 27 01:52:08.252: INFO: Going to poll 192.168.226.78 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 27 01:52:08.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.226.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 01:52:08.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:52:08.255: INFO: ExecWithOptions: Clientset creation
Feb 27 01:52:08.255: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.226.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 01:52:09.323: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 01:52:09.323: INFO: Going to poll 192.168.128.26 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 27 01:52:09.326: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.128.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 01:52:09.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:52:09.326: INFO: ExecWithOptions: Clientset creation
Feb 27 01:52:09.326: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.128.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 01:52:10.385: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 01:52:10.385: INFO: Going to poll 192.168.214.178 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 27 01:52:10.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.214.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 01:52:10.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:52:10.392: INFO: ExecWithOptions: Clientset creation
Feb 27 01:52:10.392: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.214.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 01:52:11.462: INFO: Found all 1 expected endpoints: [netserver-2]
Feb 27 01:52:11.462: INFO: Going to poll 192.168.21.160 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Feb 27 01:52:11.467: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.21.160 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 01:52:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:52:11.467: INFO: ExecWithOptions: Clientset creation
Feb 27 01:52:11.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.21.160+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 01:52:12.549: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:12.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4252" for this suite. 02/27/23 01:52:12.556
------------------------------
• [SLOW TEST] [28.504 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:51:44.059
    Feb 27 01:51:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 01:51:44.06
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:51:44.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:51:44.083
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4252 02/27/23 01:51:44.086
    STEP: creating a selector 02/27/23 01:51:44.086
    STEP: Creating the service pods in kubernetes 02/27/23 01:51:44.086
    Feb 27 01:51:44.086: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 01:51:44.159: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4252" to be "running and ready"
    Feb 27 01:51:44.164: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.18535ms
    Feb 27 01:51:44.164: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:51:46.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008198311s
    Feb 27 01:51:46.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 01:51:48.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009027378s
    Feb 27 01:51:48.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 01:51:50.169: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009944962s
    Feb 27 01:51:50.169: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 01:51:52.169: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009522301s
    Feb 27 01:51:52.169: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 01:51:54.168: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009027579s
    Feb 27 01:51:54.168: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 01:51:56.171: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011244351s
    Feb 27 01:51:56.171: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 01:51:56.171: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 01:51:56.174: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4252" to be "running and ready"
    Feb 27 01:51:56.177: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.123742ms
    Feb 27 01:51:56.177: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 01:51:58.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.007343418s
    Feb 27 01:51:58.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 01:52:00.182: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.008025143s
    Feb 27 01:52:00.182: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 01:52:02.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.007265174s
    Feb 27 01:52:02.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 01:52:04.181: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007569814s
    Feb 27 01:52:04.181: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Feb 27 01:52:06.182: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.008469239s
    Feb 27 01:52:06.182: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 01:52:06.182: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 01:52:06.185: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4252" to be "running and ready"
    Feb 27 01:52:06.189: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.241984ms
    Feb 27 01:52:06.189: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 01:52:06.189: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Feb 27 01:52:06.191: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-4252" to be "running and ready"
    Feb 27 01:52:06.195: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.564888ms
    Feb 27 01:52:06.195: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Feb 27 01:52:06.195: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 01:52:06.199
    W0227 01:52:06.233454      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Feb 27 01:52:06.233: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4252" to be "running"
    Feb 27 01:52:06.238: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.320105ms
    Feb 27 01:52:08.242: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382015s
    Feb 27 01:52:08.242: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 01:52:08.246: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4252" to be "running"
    Feb 27 01:52:08.249: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.701883ms
    Feb 27 01:52:08.249: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 01:52:08.252: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Feb 27 01:52:08.252: INFO: Going to poll 192.168.226.78 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 01:52:08.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.226.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 01:52:08.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:52:08.255: INFO: ExecWithOptions: Clientset creation
    Feb 27 01:52:08.255: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.226.78+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 01:52:09.323: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 01:52:09.323: INFO: Going to poll 192.168.128.26 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 01:52:09.326: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.128.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 01:52:09.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:52:09.326: INFO: ExecWithOptions: Clientset creation
    Feb 27 01:52:09.326: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.128.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 01:52:10.385: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 01:52:10.385: INFO: Going to poll 192.168.214.178 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 01:52:10.391: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.214.178 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 01:52:10.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:52:10.392: INFO: ExecWithOptions: Clientset creation
    Feb 27 01:52:10.392: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.214.178+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 01:52:11.462: INFO: Found all 1 expected endpoints: [netserver-2]
    Feb 27 01:52:11.462: INFO: Going to poll 192.168.21.160 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 01:52:11.467: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.21.160 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4252 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 01:52:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:52:11.467: INFO: ExecWithOptions: Clientset creation
    Feb 27 01:52:11.468: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4252/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.21.160+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 01:52:12.549: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:12.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4252" for this suite. 02/27/23 01:52:12.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:12.564
Feb 27 01:52:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 01:52:12.565
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:12.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:12.591
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 01:52:12.598
Feb 27 01:52:12.632: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6525" to be "running and ready"
Feb 27 01:52:12.635: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658022ms
Feb 27 01:52:12.635: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:52:14.640: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008051405s
Feb 27 01:52:14.640: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 01:52:14.640: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 02/27/23 01:52:14.644
Feb 27 01:52:14.652: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6525" to be "running and ready"
Feb 27 01:52:14.668: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.53237ms
Feb 27 01:52:14.668: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:52:16.672: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019583572s
Feb 27 01:52:16.672: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Feb 27 01:52:16.672: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 01:52:16.675
STEP: delete the pod with lifecycle hook 02/27/23 01:52:16.68
Feb 27 01:52:16.688: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 01:52:16.691: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 27 01:52:18.692: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 01:52:18.695: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 27 01:52:20.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 27 01:52:20.697: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6525" for this suite. 02/27/23 01:52:20.704
------------------------------
• [SLOW TEST] [8.147 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:12.564
    Feb 27 01:52:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 01:52:12.565
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:12.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:12.591
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 01:52:12.598
    Feb 27 01:52:12.632: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6525" to be "running and ready"
    Feb 27 01:52:12.635: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658022ms
    Feb 27 01:52:12.635: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:52:14.640: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008051405s
    Feb 27 01:52:14.640: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 01:52:14.640: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 02/27/23 01:52:14.644
    Feb 27 01:52:14.652: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6525" to be "running and ready"
    Feb 27 01:52:14.668: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 15.53237ms
    Feb 27 01:52:14.668: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:52:16.672: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019583572s
    Feb 27 01:52:16.672: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Feb 27 01:52:16.672: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 01:52:16.675
    STEP: delete the pod with lifecycle hook 02/27/23 01:52:16.68
    Feb 27 01:52:16.688: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 01:52:16.691: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 27 01:52:18.692: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 01:52:18.695: INFO: Pod pod-with-poststart-exec-hook still exists
    Feb 27 01:52:20.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Feb 27 01:52:20.697: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6525" for this suite. 02/27/23 01:52:20.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:20.712
Feb 27 01:52:20.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-runtime 02/27/23 01:52:20.713
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:20.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:20.735
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 02/27/23 01:52:20.738
STEP: wait for the container to reach Succeeded 02/27/23 01:52:20.769
STEP: get the container status 02/27/23 01:52:24.792
STEP: the container should be terminated 02/27/23 01:52:24.796
STEP: the termination message should be set 02/27/23 01:52:24.796
Feb 27 01:52:24.796: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 02/27/23 01:52:24.796
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6641" for this suite. 02/27/23 01:52:24.819
------------------------------
• [4.113 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:20.712
    Feb 27 01:52:20.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-runtime 02/27/23 01:52:20.713
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:20.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:20.735
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 02/27/23 01:52:20.738
    STEP: wait for the container to reach Succeeded 02/27/23 01:52:20.769
    STEP: get the container status 02/27/23 01:52:24.792
    STEP: the container should be terminated 02/27/23 01:52:24.796
    STEP: the termination message should be set 02/27/23 01:52:24.796
    Feb 27 01:52:24.796: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 02/27/23 01:52:24.796
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:24.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6641" for this suite. 02/27/23 01:52:24.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:24.827
Feb 27 01:52:24.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 01:52:24.827
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:24.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:24.854
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Feb 27 01:52:24.875: INFO: Create a RollingUpdate DaemonSet
Feb 27 01:52:24.881: INFO: Check that daemon pods launch on every node of the cluster
Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:24.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:52:24.889: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:25.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Feb 27 01:52:25.898: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:26.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 01:52:26.899: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Feb 27 01:52:26.899: INFO: Update the DaemonSet to trigger a rollout
Feb 27 01:52:26.908: INFO: Updating DaemonSet daemon-set
Feb 27 01:52:29.925: INFO: Roll back the DaemonSet before rollout is complete
Feb 27 01:52:29.939: INFO: Updating DaemonSet daemon-set
Feb 27 01:52:29.939: INFO: Make sure DaemonSet rollback is complete
Feb 27 01:52:29.944: INFO: Wrong image for pod: daemon-set-znzzd. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4, got: foo:non-existent.
Feb 27 01:52:29.944: INFO: Pod daemon-set-znzzd is not available
Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:35.956: INFO: Pod daemon-set-b2wls is not available
Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 01:52:35.966
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6919, will wait for the garbage collector to delete the pods 02/27/23 01:52:35.966
Feb 27 01:52:36.026: INFO: Deleting DaemonSet.extensions daemon-set took: 7.327664ms
Feb 27 01:52:36.127: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.103671ms
Feb 27 01:52:39.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 01:52:39.131: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 01:52:39.133: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24356"},"items":null}

Feb 27 01:52:39.136: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24356"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6919" for this suite. 02/27/23 01:52:39.159
------------------------------
• [SLOW TEST] [14.339 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:24.827
    Feb 27 01:52:24.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 01:52:24.827
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:24.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:24.854
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Feb 27 01:52:24.875: INFO: Create a RollingUpdate DaemonSet
    Feb 27 01:52:24.881: INFO: Check that daemon pods launch on every node of the cluster
    Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:24.887: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:24.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:52:24.889: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:25.895: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:25.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Feb 27 01:52:25.898: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:26.895: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:26.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 01:52:26.899: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Feb 27 01:52:26.899: INFO: Update the DaemonSet to trigger a rollout
    Feb 27 01:52:26.908: INFO: Updating DaemonSet daemon-set
    Feb 27 01:52:29.925: INFO: Roll back the DaemonSet before rollout is complete
    Feb 27 01:52:29.939: INFO: Updating DaemonSet daemon-set
    Feb 27 01:52:29.939: INFO: Make sure DaemonSet rollback is complete
    Feb 27 01:52:29.944: INFO: Wrong image for pod: daemon-set-znzzd. Expected: armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4, got: foo:non-existent.
    Feb 27 01:52:29.944: INFO: Pod daemon-set-znzzd is not available
    Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:29.951: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:30.962: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:31.981: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:32.961: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:33.965: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:34.962: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:35.956: INFO: Pod daemon-set-b2wls is not available
    Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 01:52:35.960: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 01:52:35.966
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6919, will wait for the garbage collector to delete the pods 02/27/23 01:52:35.966
    Feb 27 01:52:36.026: INFO: Deleting DaemonSet.extensions daemon-set took: 7.327664ms
    Feb 27 01:52:36.127: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.103671ms
    Feb 27 01:52:39.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 01:52:39.131: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 01:52:39.133: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24356"},"items":null}

    Feb 27 01:52:39.136: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24356"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:39.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6919" for this suite. 02/27/23 01:52:39.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:39.166
Feb 27 01:52:39.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 01:52:39.166
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:39.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:39.189
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 02/27/23 01:52:39.191
Feb 27 01:52:39.219: INFO: Waiting up to 5m0s for pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11" in namespace "pods-9803" to be "running and ready"
Feb 27 01:52:39.221: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417286ms
Feb 27 01:52:39.221: INFO: The phase of Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:52:41.226: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11": Phase="Running", Reason="", readiness=true. Elapsed: 2.007113652s
Feb 27 01:52:41.226: INFO: The phase of Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 is Running (Ready = true)
Feb 27 01:52:41.226: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11" satisfied condition "running and ready"
Feb 27 01:52:41.231: INFO: Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 has hostIP: 10.0.10.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:41.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9803" for this suite. 02/27/23 01:52:41.243
------------------------------
• [2.084 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:39.166
    Feb 27 01:52:39.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 01:52:39.166
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:39.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:39.189
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 02/27/23 01:52:39.191
    Feb 27 01:52:39.219: INFO: Waiting up to 5m0s for pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11" in namespace "pods-9803" to be "running and ready"
    Feb 27 01:52:39.221: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.417286ms
    Feb 27 01:52:39.221: INFO: The phase of Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:52:41.226: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11": Phase="Running", Reason="", readiness=true. Elapsed: 2.007113652s
    Feb 27 01:52:41.226: INFO: The phase of Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 is Running (Ready = true)
    Feb 27 01:52:41.226: INFO: Pod "pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11" satisfied condition "running and ready"
    Feb 27 01:52:41.231: INFO: Pod pod-hostip-08574553-2331-4c65-aab9-ede8f9b5aa11 has hostIP: 10.0.10.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:41.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9803" for this suite. 02/27/23 01:52:41.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:41.251
Feb 27 01:52:41.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:52:41.251
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:41.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:41.273
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 01:52:41.276
Feb 27 01:52:41.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 01:52:43.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:51.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1132" for this suite. 02/27/23 01:52:51.131
------------------------------
• [SLOW TEST] [9.888 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:41.251
    Feb 27 01:52:41.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 01:52:41.251
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:41.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:41.273
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 02/27/23 01:52:41.276
    Feb 27 01:52:41.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 01:52:43.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:51.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1132" for this suite. 02/27/23 01:52:51.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:51.14
Feb 27 01:52:51.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 01:52:51.14
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:51.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:51.165
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 01:52:51.17
Feb 27 01:52:51.204: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6997" to be "running and ready"
Feb 27 01:52:51.207: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021285ms
Feb 27 01:52:51.207: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:52:53.212: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007845671s
Feb 27 01:52:53.212: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 01:52:53.212: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 02/27/23 01:52:53.215
Feb 27 01:52:53.224: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6997" to be "running and ready"
Feb 27 01:52:53.229: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329103ms
Feb 27 01:52:53.229: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 01:52:55.234: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009231058s
Feb 27 01:52:55.234: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Feb 27 01:52:55.234: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 01:52:55.237
Feb 27 01:52:55.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 01:52:55.249: INFO: Pod pod-with-prestop-http-hook still exists
Feb 27 01:52:57.249: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 01:52:57.253: INFO: Pod pod-with-prestop-http-hook still exists
Feb 27 01:52:59.250: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 27 01:52:59.256: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 02/27/23 01:52:59.256
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 01:52:59.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6997" for this suite. 02/27/23 01:52:59.27
------------------------------
• [SLOW TEST] [8.138 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:51.14
    Feb 27 01:52:51.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 01:52:51.14
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:51.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:51.165
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 01:52:51.17
    Feb 27 01:52:51.204: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6997" to be "running and ready"
    Feb 27 01:52:51.207: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021285ms
    Feb 27 01:52:51.207: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:52:53.212: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007845671s
    Feb 27 01:52:53.212: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 01:52:53.212: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 02/27/23 01:52:53.215
    Feb 27 01:52:53.224: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6997" to be "running and ready"
    Feb 27 01:52:53.229: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329103ms
    Feb 27 01:52:53.229: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 01:52:55.234: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009231058s
    Feb 27 01:52:55.234: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Feb 27 01:52:55.234: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 01:52:55.237
    Feb 27 01:52:55.245: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 01:52:55.249: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 27 01:52:57.249: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 01:52:57.253: INFO: Pod pod-with-prestop-http-hook still exists
    Feb 27 01:52:59.250: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Feb 27 01:52:59.256: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 02/27/23 01:52:59.256
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:52:59.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6997" for this suite. 02/27/23 01:52:59.27
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:52:59.277
Feb 27 01:52:59.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:52:59.278
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:59.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:59.307
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-faaa8595-4de9-4a63-a0d8-75576c156908 02/27/23 01:52:59.31
STEP: Creating a pod to test consume secrets 02/27/23 01:52:59.32
Feb 27 01:52:59.352: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da" in namespace "projected-7284" to be "Succeeded or Failed"
Feb 27 01:52:59.358: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943026ms
Feb 27 01:53:01.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Running", Reason="", readiness=false. Elapsed: 2.01124474s
Feb 27 01:53:03.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010873588s
STEP: Saw pod success 02/27/23 01:53:03.363
Feb 27 01:53:03.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da" satisfied condition "Succeeded or Failed"
Feb 27 01:53:03.366: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 01:53:03.378
Feb 27 01:53:03.390: INFO: Waiting for pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da to disappear
Feb 27 01:53:03.393: INFO: Pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:03.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7284" for this suite. 02/27/23 01:53:03.397
------------------------------
• [4.124 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:52:59.277
    Feb 27 01:52:59.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:52:59.278
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:52:59.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:52:59.307
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-faaa8595-4de9-4a63-a0d8-75576c156908 02/27/23 01:52:59.31
    STEP: Creating a pod to test consume secrets 02/27/23 01:52:59.32
    Feb 27 01:52:59.352: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da" in namespace "projected-7284" to be "Succeeded or Failed"
    Feb 27 01:52:59.358: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.943026ms
    Feb 27 01:53:01.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Running", Reason="", readiness=false. Elapsed: 2.01124474s
    Feb 27 01:53:03.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010873588s
    STEP: Saw pod success 02/27/23 01:53:03.363
    Feb 27 01:53:03.363: INFO: Pod "pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da" satisfied condition "Succeeded or Failed"
    Feb 27 01:53:03.366: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 01:53:03.378
    Feb 27 01:53:03.390: INFO: Waiting for pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da to disappear
    Feb 27 01:53:03.393: INFO: Pod pod-projected-secrets-8e12a372-6fd0-4565-b06c-dfd46493e2da no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:03.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7284" for this suite. 02/27/23 01:53:03.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:03.402
Feb 27 01:53:03.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename subpath 02/27/23 01:53:03.403
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:03.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:03.424
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 01:53:03.426
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-c6v9 02/27/23 01:53:03.436
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 01:53:03.436
Feb 27 01:53:03.444: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c6v9" in namespace "subpath-7978" to be "Succeeded or Failed"
Feb 27 01:53:03.447: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9146ms
Feb 27 01:53:05.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007212034s
Feb 27 01:53:07.454: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 4.0095691s
Feb 27 01:53:09.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 6.007302388s
Feb 27 01:53:11.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008473806s
Feb 27 01:53:13.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.007278425s
Feb 27 01:53:15.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 12.007937995s
Feb 27 01:53:17.454: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009502164s
Feb 27 01:53:19.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 16.007682833s
Feb 27 01:53:21.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008820602s
Feb 27 01:53:23.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 20.00780579s
Feb 27 01:53:25.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008725708s
Feb 27 01:53:27.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008843375s
STEP: Saw pod success 02/27/23 01:53:27.453
Feb 27 01:53:27.453: INFO: Pod "pod-subpath-test-configmap-c6v9" satisfied condition "Succeeded or Failed"
Feb 27 01:53:27.457: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-c6v9 container test-container-subpath-configmap-c6v9: <nil>
STEP: delete the pod 02/27/23 01:53:27.463
Feb 27 01:53:27.482: INFO: Waiting for pod pod-subpath-test-configmap-c6v9 to disappear
Feb 27 01:53:27.485: INFO: Pod pod-subpath-test-configmap-c6v9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c6v9 02/27/23 01:53:27.485
Feb 27 01:53:27.485: INFO: Deleting pod "pod-subpath-test-configmap-c6v9" in namespace "subpath-7978"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7978" for this suite. 02/27/23 01:53:27.492
------------------------------
• [SLOW TEST] [24.098 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:03.402
    Feb 27 01:53:03.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename subpath 02/27/23 01:53:03.403
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:03.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:03.424
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 01:53:03.426
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-c6v9 02/27/23 01:53:03.436
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 01:53:03.436
    Feb 27 01:53:03.444: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c6v9" in namespace "subpath-7978" to be "Succeeded or Failed"
    Feb 27 01:53:03.447: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.9146ms
    Feb 27 01:53:05.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007212034s
    Feb 27 01:53:07.454: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 4.0095691s
    Feb 27 01:53:09.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 6.007302388s
    Feb 27 01:53:11.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008473806s
    Feb 27 01:53:13.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.007278425s
    Feb 27 01:53:15.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 12.007937995s
    Feb 27 01:53:17.454: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009502164s
    Feb 27 01:53:19.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 16.007682833s
    Feb 27 01:53:21.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008820602s
    Feb 27 01:53:23.452: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=true. Elapsed: 20.00780579s
    Feb 27 01:53:25.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008725708s
    Feb 27 01:53:27.453: INFO: Pod "pod-subpath-test-configmap-c6v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008843375s
    STEP: Saw pod success 02/27/23 01:53:27.453
    Feb 27 01:53:27.453: INFO: Pod "pod-subpath-test-configmap-c6v9" satisfied condition "Succeeded or Failed"
    Feb 27 01:53:27.457: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-c6v9 container test-container-subpath-configmap-c6v9: <nil>
    STEP: delete the pod 02/27/23 01:53:27.463
    Feb 27 01:53:27.482: INFO: Waiting for pod pod-subpath-test-configmap-c6v9 to disappear
    Feb 27 01:53:27.485: INFO: Pod pod-subpath-test-configmap-c6v9 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-c6v9 02/27/23 01:53:27.485
    Feb 27 01:53:27.485: INFO: Deleting pod "pod-subpath-test-configmap-c6v9" in namespace "subpath-7978"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7978" for this suite. 02/27/23 01:53:27.492
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:27.5
Feb 27 01:53:27.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 01:53:27.501
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:27.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:27.522
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 02/27/23 01:53:27.527
Feb 27 01:53:27.558: INFO: Waiting up to 5m0s for pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad" in namespace "var-expansion-8092" to be "Succeeded or Failed"
Feb 27 01:53:27.563: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521398ms
Feb 27 01:53:29.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010220134s
Feb 27 01:53:31.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010500964s
STEP: Saw pod success 02/27/23 01:53:31.568
Feb 27 01:53:31.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad" satisfied condition "Succeeded or Failed"
Feb 27 01:53:31.572: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad container dapi-container: <nil>
STEP: delete the pod 02/27/23 01:53:31.579
Feb 27 01:53:31.593: INFO: Waiting for pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad to disappear
Feb 27 01:53:31.596: INFO: Pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:31.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8092" for this suite. 02/27/23 01:53:31.6
------------------------------
• [4.106 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:27.5
    Feb 27 01:53:27.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 01:53:27.501
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:27.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:27.522
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 02/27/23 01:53:27.527
    Feb 27 01:53:27.558: INFO: Waiting up to 5m0s for pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad" in namespace "var-expansion-8092" to be "Succeeded or Failed"
    Feb 27 01:53:27.563: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.521398ms
    Feb 27 01:53:29.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010220134s
    Feb 27 01:53:31.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010500964s
    STEP: Saw pod success 02/27/23 01:53:31.568
    Feb 27 01:53:31.568: INFO: Pod "var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad" satisfied condition "Succeeded or Failed"
    Feb 27 01:53:31.572: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad container dapi-container: <nil>
    STEP: delete the pod 02/27/23 01:53:31.579
    Feb 27 01:53:31.593: INFO: Waiting for pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad to disappear
    Feb 27 01:53:31.596: INFO: Pod var-expansion-8b3e6e18-b48e-46b0-b84e-846e3b9826ad no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:31.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8092" for this suite. 02/27/23 01:53:31.6
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:31.606
Feb 27 01:53:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sysctl 02/27/23 01:53:31.607
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:31.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:31.626
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 01:53:31.634
STEP: Watching for error events or started pod 02/27/23 01:53:31.646
STEP: Waiting for pod completion 02/27/23 01:53:33.65
Feb 27 01:53:33.650: INFO: Waiting up to 3m0s for pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc" in namespace "sysctl-5332" to be "completed"
Feb 27 01:53:33.653: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.212504ms
Feb 27 01:53:35.658: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007795566s
Feb 27 01:53:35.658: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc" satisfied condition "completed"
STEP: Checking that the pod succeeded 02/27/23 01:53:35.662
STEP: Getting logs from the pod 02/27/23 01:53:35.662
STEP: Checking that the sysctl is actually updated 02/27/23 01:53:35.668
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:35.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5332" for this suite. 02/27/23 01:53:35.677
------------------------------
• [4.079 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:31.606
    Feb 27 01:53:31.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sysctl 02/27/23 01:53:31.607
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:31.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:31.626
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 02/27/23 01:53:31.634
    STEP: Watching for error events or started pod 02/27/23 01:53:31.646
    STEP: Waiting for pod completion 02/27/23 01:53:33.65
    Feb 27 01:53:33.650: INFO: Waiting up to 3m0s for pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc" in namespace "sysctl-5332" to be "completed"
    Feb 27 01:53:33.653: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.212504ms
    Feb 27 01:53:35.658: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007795566s
    Feb 27 01:53:35.658: INFO: Pod "sysctl-368d0102-5573-489e-9e3d-4641ba2f85dc" satisfied condition "completed"
    STEP: Checking that the pod succeeded 02/27/23 01:53:35.662
    STEP: Getting logs from the pod 02/27/23 01:53:35.662
    STEP: Checking that the sysctl is actually updated 02/27/23 01:53:35.668
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:35.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5332" for this suite. 02/27/23 01:53:35.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:35.687
Feb 27 01:53:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:53:35.688
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:35.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:35.711
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 02/27/23 01:53:35.714
Feb 27 01:53:35.724: INFO: Waiting up to 5m0s for pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1" in namespace "downward-api-9149" to be "Succeeded or Failed"
Feb 27 01:53:35.726: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166002ms
Feb 27 01:53:37.730: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006179754s
Feb 27 01:53:39.731: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007580155s
STEP: Saw pod success 02/27/23 01:53:39.731
Feb 27 01:53:39.731: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1" satisfied condition "Succeeded or Failed"
Feb 27 01:53:39.735: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 container dapi-container: <nil>
STEP: delete the pod 02/27/23 01:53:39.743
Feb 27 01:53:39.757: INFO: Waiting for pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 to disappear
Feb 27 01:53:39.761: INFO: Pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9149" for this suite. 02/27/23 01:53:39.765
------------------------------
• [4.084 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:35.687
    Feb 27 01:53:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:53:35.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:35.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:35.711
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 02/27/23 01:53:35.714
    Feb 27 01:53:35.724: INFO: Waiting up to 5m0s for pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1" in namespace "downward-api-9149" to be "Succeeded or Failed"
    Feb 27 01:53:35.726: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.166002ms
    Feb 27 01:53:37.730: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006179754s
    Feb 27 01:53:39.731: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007580155s
    STEP: Saw pod success 02/27/23 01:53:39.731
    Feb 27 01:53:39.731: INFO: Pod "downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1" satisfied condition "Succeeded or Failed"
    Feb 27 01:53:39.735: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 01:53:39.743
    Feb 27 01:53:39.757: INFO: Waiting for pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 to disappear
    Feb 27 01:53:39.761: INFO: Pod downward-api-9a23d1d9-4f67-48b8-aedc-8033b90133d1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9149" for this suite. 02/27/23 01:53:39.765
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:39.771
Feb 27 01:53:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 01:53:39.772
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:39.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:39.792
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 02/27/23 01:53:39.795
STEP: Creating a ResourceQuota 02/27/23 01:53:44.799
STEP: Ensuring resource quota status is calculated 02/27/23 01:53:44.805
STEP: Creating a Service 02/27/23 01:53:46.808
STEP: Creating a NodePort Service 02/27/23 01:53:46.841
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 01:53:46.883
STEP: Ensuring resource quota status captures service creation 02/27/23 01:53:46.947
STEP: Deleting Services 02/27/23 01:53:48.951
STEP: Ensuring resource quota status released usage 02/27/23 01:53:49.029
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:51.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9337" for this suite. 02/27/23 01:53:51.036
------------------------------
• [SLOW TEST] [11.275 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:39.771
    Feb 27 01:53:39.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 01:53:39.772
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:39.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:39.792
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 02/27/23 01:53:39.795
    STEP: Creating a ResourceQuota 02/27/23 01:53:44.799
    STEP: Ensuring resource quota status is calculated 02/27/23 01:53:44.805
    STEP: Creating a Service 02/27/23 01:53:46.808
    STEP: Creating a NodePort Service 02/27/23 01:53:46.841
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 02/27/23 01:53:46.883
    STEP: Ensuring resource quota status captures service creation 02/27/23 01:53:46.947
    STEP: Deleting Services 02/27/23 01:53:48.951
    STEP: Ensuring resource quota status released usage 02/27/23 01:53:49.029
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:51.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9337" for this suite. 02/27/23 01:53:51.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:51.047
Feb 27 01:53:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 01:53:51.048
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:51.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:51.079
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 01:53:51.082
Feb 27 01:53:51.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-8785 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4'
Feb 27 01:53:51.170: INFO: stderr: ""
Feb 27 01:53:51.170: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 01:53:51.17
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Feb 27 01:53:51.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-8785 delete pods e2e-test-httpd-pod'
Feb 27 01:53:53.012: INFO: stderr: ""
Feb 27 01:53:53.012: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:53.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8785" for this suite. 02/27/23 01:53:53.016
------------------------------
• [1.977 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:51.047
    Feb 27 01:53:51.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 01:53:51.048
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:51.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:51.079
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 01:53:51.082
    Feb 27 01:53:51.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-8785 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4'
    Feb 27 01:53:51.170: INFO: stderr: ""
    Feb 27 01:53:51.170: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 02/27/23 01:53:51.17
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Feb 27 01:53:51.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-8785 delete pods e2e-test-httpd-pod'
    Feb 27 01:53:53.012: INFO: stderr: ""
    Feb 27 01:53:53.012: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:53.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8785" for this suite. 02/27/23 01:53:53.016
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:53.024
Feb 27 01:53:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename podtemplate 02/27/23 01:53:53.025
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:53.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:53.056
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 02/27/23 01:53:53.058
STEP: Replace a pod template 02/27/23 01:53:53.063
Feb 27 01:53:53.071: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:53.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4787" for this suite. 02/27/23 01:53:53.076
------------------------------
• [0.059 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:53.024
    Feb 27 01:53:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename podtemplate 02/27/23 01:53:53.025
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:53.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:53.056
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 02/27/23 01:53:53.058
    STEP: Replace a pod template 02/27/23 01:53:53.063
    Feb 27 01:53:53.071: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:53.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4787" for this suite. 02/27/23 01:53:53.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:53.083
Feb 27 01:53:53.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 01:53:53.084
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:53.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:53.106
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 02/27/23 01:53:53.108
Feb 27 01:53:53.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949" in namespace "downward-api-9287" to be "Succeeded or Failed"
Feb 27 01:53:53.153: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070147ms
Feb 27 01:53:55.158: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Running", Reason="", readiness=false. Elapsed: 2.011231039s
Feb 27 01:53:57.159: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011713034s
STEP: Saw pod success 02/27/23 01:53:57.159
Feb 27 01:53:57.159: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949" satisfied condition "Succeeded or Failed"
Feb 27 01:53:57.162: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 container client-container: <nil>
STEP: delete the pod 02/27/23 01:53:57.166
Feb 27 01:53:57.179: INFO: Waiting for pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 to disappear
Feb 27 01:53:57.182: INFO: Pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 01:53:57.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9287" for this suite. 02/27/23 01:53:57.186
------------------------------
• [4.111 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:53.083
    Feb 27 01:53:53.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 01:53:53.084
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:53.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:53.106
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 02/27/23 01:53:53.108
    Feb 27 01:53:53.147: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949" in namespace "downward-api-9287" to be "Succeeded or Failed"
    Feb 27 01:53:53.153: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070147ms
    Feb 27 01:53:55.158: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Running", Reason="", readiness=false. Elapsed: 2.011231039s
    Feb 27 01:53:57.159: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011713034s
    STEP: Saw pod success 02/27/23 01:53:57.159
    Feb 27 01:53:57.159: INFO: Pod "downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949" satisfied condition "Succeeded or Failed"
    Feb 27 01:53:57.162: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 container client-container: <nil>
    STEP: delete the pod 02/27/23 01:53:57.166
    Feb 27 01:53:57.179: INFO: Waiting for pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 to disappear
    Feb 27 01:53:57.182: INFO: Pod downwardapi-volume-5a2884fe-c250-479a-aeda-27b4126cc949 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:53:57.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9287" for this suite. 02/27/23 01:53:57.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:53:57.195
Feb 27 01:53:57.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename containers 02/27/23 01:53:57.196
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:57.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:57.219
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 02/27/23 01:53:57.221
Feb 27 01:53:57.231: INFO: Waiting up to 5m0s for pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438" in namespace "containers-7903" to be "Succeeded or Failed"
Feb 27 01:53:57.234: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802723ms
Feb 27 01:53:59.239: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Running", Reason="", readiness=false. Elapsed: 2.007765719s
Feb 27 01:54:01.243: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012388228s
STEP: Saw pod success 02/27/23 01:54:01.243
Feb 27 01:54:01.243: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438" satisfied condition "Succeeded or Failed"
Feb 27 01:54:01.247: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:54:01.252
Feb 27 01:54:01.267: INFO: Waiting for pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 to disappear
Feb 27 01:54:01.269: INFO: Pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 01:54:01.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7903" for this suite. 02/27/23 01:54:01.275
------------------------------
• [4.086 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:53:57.195
    Feb 27 01:53:57.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename containers 02/27/23 01:53:57.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:53:57.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:53:57.219
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 02/27/23 01:53:57.221
    Feb 27 01:53:57.231: INFO: Waiting up to 5m0s for pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438" in namespace "containers-7903" to be "Succeeded or Failed"
    Feb 27 01:53:57.234: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802723ms
    Feb 27 01:53:59.239: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Running", Reason="", readiness=false. Elapsed: 2.007765719s
    Feb 27 01:54:01.243: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012388228s
    STEP: Saw pod success 02/27/23 01:54:01.243
    Feb 27 01:54:01.243: INFO: Pod "client-containers-50e54245-599d-4a4e-8312-d2008f071438" satisfied condition "Succeeded or Failed"
    Feb 27 01:54:01.247: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:54:01.252
    Feb 27 01:54:01.267: INFO: Waiting for pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 to disappear
    Feb 27 01:54:01.269: INFO: Pod client-containers-50e54245-599d-4a4e-8312-d2008f071438 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:54:01.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7903" for this suite. 02/27/23 01:54:01.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:54:01.282
Feb 27 01:54:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 01:54:01.283
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:01.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:01.309
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Feb 27 01:54:01.320: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 27 01:54:06.324: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 01:54:06.324
Feb 27 01:54:06.324: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 01:54:06.341
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 01:54:08.366: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9514  6f67cad6-0ee8-4858-aff7-ea026fb136f6 25359 1 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002975038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 01:54:06 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-85d9c778c5" has successfully progressed.,LastUpdateTime:2023-02-27 01:54:07 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 01:54:08.370: INFO: New ReplicaSet "test-cleanup-deployment-85d9c778c5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-85d9c778c5  deployment-9514  a7c7a05a-bfe3-4aec-b3d7-259547d8064c 25348 1 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6f67cad6-0ee8-4858-aff7-ea026fb136f6 0xc002975497 0xc002975498}] [] [{kube-controller-manager Update apps/v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f67cad6-0ee8-4858-aff7-ea026fb136f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 85d9c778c5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002975548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 01:54:08.372: INFO: Pod "test-cleanup-deployment-85d9c778c5-bzstv" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-85d9c778c5-bzstv test-cleanup-deployment-85d9c778c5- deployment-9514  bdc44a6c-47d6-45ba-a2ce-2db598f4f4c6 25347 0 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.167"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.167"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-deployment-85d9c778c5 a7c7a05a-bfe3-4aec-b3d7-259547d8064c 0xc002776d07 0xc002776d08}] [] [{kube-controller-manager Update v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7c7a05a-bfe3-4aec-b3d7-259547d8064c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbngg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbngg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.167,StartTime:2023-02-27 01:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 01:54:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://9e000f8a46e70f0d913466f411ea554629ffd195f0c1c42b543cd6e4ec96f010,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 01:54:08.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9514" for this suite. 02/27/23 01:54:08.377
------------------------------
• [SLOW TEST] [7.102 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:54:01.282
    Feb 27 01:54:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 01:54:01.283
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:01.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:01.309
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Feb 27 01:54:01.320: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Feb 27 01:54:06.324: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 01:54:06.324
    Feb 27 01:54:06.324: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 02/27/23 01:54:06.341
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 01:54:08.366: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9514  6f67cad6-0ee8-4858-aff7-ea026fb136f6 25359 1 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002975038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 01:54:06 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-85d9c778c5" has successfully progressed.,LastUpdateTime:2023-02-27 01:54:07 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 01:54:08.370: INFO: New ReplicaSet "test-cleanup-deployment-85d9c778c5" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-85d9c778c5  deployment-9514  a7c7a05a-bfe3-4aec-b3d7-259547d8064c 25348 1 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6f67cad6-0ee8-4858-aff7-ea026fb136f6 0xc002975497 0xc002975498}] [] [{kube-controller-manager Update apps/v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f67cad6-0ee8-4858-aff7-ea026fb136f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 85d9c778c5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002975548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 01:54:08.372: INFO: Pod "test-cleanup-deployment-85d9c778c5-bzstv" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-85d9c778c5-bzstv test-cleanup-deployment-85d9c778c5- deployment-9514  bdc44a6c-47d6-45ba-a2ce-2db598f4f4c6 25347 0 2023-02-27 01:54:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:85d9c778c5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.167"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.167"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-cleanup-deployment-85d9c778c5 a7c7a05a-bfe3-4aec-b3d7-259547d8064c 0xc002776d07 0xc002776d08}] [] [{kube-controller-manager Update v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7c7a05a-bfe3-4aec-b3d7-259547d8064c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 01:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 01:54:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbngg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbngg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 01:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.167,StartTime:2023-02-27 01:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 01:54:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://9e000f8a46e70f0d913466f411ea554629ffd195f0c1c42b543cd6e4ec96f010,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:54:08.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9514" for this suite. 02/27/23 01:54:08.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:54:08.384
Feb 27 01:54:08.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 01:54:08.385
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:08.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:08.404
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3841 02/27/23 01:54:08.407
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Feb 27 01:54:08.432: INFO: Found 0 stateful pods, waiting for 1
Feb 27 01:54:18.437: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 02/27/23 01:54:18.444
W0227 01:54:18.458033      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 01:54:18.465: INFO: Found 1 stateful pods, waiting for 2
Feb 27 01:54:28.471: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 01:54:28.471: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 02/27/23 01:54:28.486
STEP: Delete all of the StatefulSets 02/27/23 01:54:28.491
STEP: Verify that StatefulSets have been deleted 02/27/23 01:54:28.501
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 01:54:28.505: INFO: Deleting all statefulset in ns statefulset-3841
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 01:54:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3841" for this suite. 02/27/23 01:54:28.528
------------------------------
• [SLOW TEST] [20.158 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:54:08.384
    Feb 27 01:54:08.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 01:54:08.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:08.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:08.404
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3841 02/27/23 01:54:08.407
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Feb 27 01:54:08.432: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 01:54:18.437: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 02/27/23 01:54:18.444
    W0227 01:54:18.458033      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 01:54:18.465: INFO: Found 1 stateful pods, waiting for 2
    Feb 27 01:54:28.471: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 01:54:28.471: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 02/27/23 01:54:28.486
    STEP: Delete all of the StatefulSets 02/27/23 01:54:28.491
    STEP: Verify that StatefulSets have been deleted 02/27/23 01:54:28.501
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 01:54:28.505: INFO: Deleting all statefulset in ns statefulset-3841
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:54:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3841" for this suite. 02/27/23 01:54:28.528
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:54:28.542
Feb 27 01:54:28.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 01:54:28.543
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:28.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:28.567
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 01:54:28.571
Feb 27 01:54:28.602: INFO: Waiting up to 5m0s for pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c" in namespace "emptydir-3496" to be "Succeeded or Failed"
Feb 27 01:54:28.605: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165117ms
Feb 27 01:54:30.610: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007080832s
Feb 27 01:54:32.608: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005195096s
STEP: Saw pod success 02/27/23 01:54:32.608
Feb 27 01:54:32.608: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c" satisfied condition "Succeeded or Failed"
Feb 27 01:54:32.612: INFO: Trying to get logs from node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c container test-container: <nil>
STEP: delete the pod 02/27/23 01:54:32.62
Feb 27 01:54:32.638: INFO: Waiting for pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c to disappear
Feb 27 01:54:32.641: INFO: Pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:54:32.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3496" for this suite. 02/27/23 01:54:32.645
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:54:28.542
    Feb 27 01:54:28.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 01:54:28.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:28.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:28.567
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 01:54:28.571
    Feb 27 01:54:28.602: INFO: Waiting up to 5m0s for pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c" in namespace "emptydir-3496" to be "Succeeded or Failed"
    Feb 27 01:54:28.605: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165117ms
    Feb 27 01:54:30.610: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007080832s
    Feb 27 01:54:32.608: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005195096s
    STEP: Saw pod success 02/27/23 01:54:32.608
    Feb 27 01:54:32.608: INFO: Pod "pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c" satisfied condition "Succeeded or Failed"
    Feb 27 01:54:32.612: INFO: Trying to get logs from node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c container test-container: <nil>
    STEP: delete the pod 02/27/23 01:54:32.62
    Feb 27 01:54:32.638: INFO: Waiting for pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c to disappear
    Feb 27 01:54:32.641: INFO: Pod pod-2ad36e9a-db6d-4b1e-b84f-bea643a2a99c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:54:32.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3496" for this suite. 02/27/23 01:54:32.645
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:54:32.651
Feb 27 01:54:32.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename init-container 02/27/23 01:54:32.652
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:32.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:32.679
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 02/27/23 01:54:32.681
Feb 27 01:54:32.681: INFO: PodSpec: initContainers in spec.initContainers
Feb 27 01:55:20.223: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-76002186-5877-4058-94a6-07c4c37e180d", GenerateName:"", Namespace:"init-container-9997", SelfLink:"", UID:"6ddbb826-fcb3-43b4-a683-51bf2416b3a7", ResourceVersion:"25934", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"681150339"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0e40), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 54, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0e70), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 55, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0ea0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ktpwd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0014802e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002de7a20), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00565dc00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002de7ab0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002de7ad0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002de7ad8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002de7adc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0009beb00), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.3", PodIP:"192.168.214.139", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.214.139"}}, StartTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00565dce0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00565dd50)}, Ready:false, RestartCount:3, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", ImageID:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox@sha256:7ddd6b83e44b8f6e2f1fccace9562f5600b71e7717515ebd2131bdb94ad8634c", ContainerID:"containerd://d10b54efaa423ec73ba2a3124d6c2485fa94588ff5dccf5257e2f0ab5a5db85b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001480360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001480340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002de7b5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:55:20.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9997" for this suite. 02/27/23 01:55:20.228
------------------------------
• [SLOW TEST] [47.584 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:54:32.651
    Feb 27 01:54:32.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename init-container 02/27/23 01:54:32.652
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:54:32.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:54:32.679
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 02/27/23 01:54:32.681
    Feb 27 01:54:32.681: INFO: PodSpec: initContainers in spec.initContainers
    Feb 27 01:55:20.223: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-76002186-5877-4058-94a6-07c4c37e180d", GenerateName:"", Namespace:"init-container-9997", SelfLink:"", UID:"6ddbb826-fcb3-43b4-a683-51bf2416b3a7", ResourceVersion:"25934", Generation:0, CreationTimestamp:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"681150339"}, Annotations:map[string]string{"k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"192.168.214.139\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0e40), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 54, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0e70), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.February, 27, 1, 55, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a0ea0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ktpwd", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0014802e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ktpwd", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002de7a20), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00565dc00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002de7ab0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002de7ad0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002de7ad8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002de7adc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0009beb00), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.10.3", PodIP:"192.168.214.139", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.214.139"}}, StartTime:time.Date(2023, time.February, 27, 1, 54, 32, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00565dce0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00565dd50)}, Ready:false, RestartCount:3, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", ImageID:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox@sha256:7ddd6b83e44b8f6e2f1fccace9562f5600b71e7717515ebd2131bdb94ad8634c", ContainerID:"containerd://d10b54efaa423ec73ba2a3124d6c2485fa94588ff5dccf5257e2f0ab5a5db85b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001480360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001480340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002de7b5f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:55:20.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9997" for this suite. 02/27/23 01:55:20.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:55:20.235
Feb 27 01:55:20.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 01:55:20.235
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:20.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:20.259
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-13f50d99-0fe7-4840-b72a-f3f88dbaa955 02/27/23 01:55:20.261
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:55:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9979" for this suite. 02/27/23 01:55:20.267
------------------------------
• [0.045 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:55:20.235
    Feb 27 01:55:20.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 01:55:20.235
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:20.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:20.259
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-13f50d99-0fe7-4840-b72a-f3f88dbaa955 02/27/23 01:55:20.261
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:55:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9979" for this suite. 02/27/23 01:55:20.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:55:20.28
Feb 27 01:55:20.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:55:20.281
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:20.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:20.327
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-d2941f70-e16d-4207-bada-39aac5cd54a8 02/27/23 01:55:20.331
STEP: Creating a pod to test consume configMaps 02/27/23 01:55:20.336
Feb 27 01:55:20.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17" in namespace "projected-3425" to be "Succeeded or Failed"
Feb 27 01:55:20.373: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22264ms
Feb 27 01:55:22.378: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008561159s
Feb 27 01:55:24.392: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022386604s
STEP: Saw pod success 02/27/23 01:55:24.392
Feb 27 01:55:24.392: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17" satisfied condition "Succeeded or Failed"
Feb 27 01:55:24.399: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:55:24.443
Feb 27 01:55:24.454: INFO: Waiting for pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 to disappear
Feb 27 01:55:24.460: INFO: Pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:55:24.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3425" for this suite. 02/27/23 01:55:24.465
------------------------------
• [4.192 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:55:20.28
    Feb 27 01:55:20.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:55:20.281
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:20.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:20.327
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-d2941f70-e16d-4207-bada-39aac5cd54a8 02/27/23 01:55:20.331
    STEP: Creating a pod to test consume configMaps 02/27/23 01:55:20.336
    Feb 27 01:55:20.370: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17" in namespace "projected-3425" to be "Succeeded or Failed"
    Feb 27 01:55:20.373: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22264ms
    Feb 27 01:55:22.378: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008561159s
    Feb 27 01:55:24.392: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022386604s
    STEP: Saw pod success 02/27/23 01:55:24.392
    Feb 27 01:55:24.392: INFO: Pod "pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17" satisfied condition "Succeeded or Failed"
    Feb 27 01:55:24.399: INFO: Trying to get logs from node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:55:24.443
    Feb 27 01:55:24.454: INFO: Waiting for pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 to disappear
    Feb 27 01:55:24.460: INFO: Pod pod-projected-configmaps-4713635a-93b8-45f3-8269-77e4d6956c17 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:55:24.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3425" for this suite. 02/27/23 01:55:24.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:55:24.473
Feb 27 01:55:24.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 01:55:24.474
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:24.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:24.491
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 in namespace container-probe-7613 02/27/23 01:55:24.493
Feb 27 01:55:24.505: INFO: Waiting up to 5m0s for pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23" in namespace "container-probe-7613" to be "not pending"
Feb 27 01:55:24.508: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076467ms
Feb 27 01:55:26.512: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23": Phase="Running", Reason="", readiness=true. Elapsed: 2.006951476s
Feb 27 01:55:26.512: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23" satisfied condition "not pending"
Feb 27 01:55:26.512: INFO: Started pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 in namespace container-probe-7613
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:55:26.512
Feb 27 01:55:26.515: INFO: Initial restart count of pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 is 0
Feb 27 01:56:16.645: INFO: Restart count of pod container-probe-7613/busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 is now 1 (50.130122772s elapsed)
STEP: deleting the pod 02/27/23 01:56:16.645
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7613" for this suite. 02/27/23 01:56:16.661
------------------------------
• [SLOW TEST] [52.195 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:55:24.473
    Feb 27 01:55:24.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 01:55:24.474
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:55:24.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:55:24.491
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 in namespace container-probe-7613 02/27/23 01:55:24.493
    Feb 27 01:55:24.505: INFO: Waiting up to 5m0s for pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23" in namespace "container-probe-7613" to be "not pending"
    Feb 27 01:55:24.508: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076467ms
    Feb 27 01:55:26.512: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23": Phase="Running", Reason="", readiness=true. Elapsed: 2.006951476s
    Feb 27 01:55:26.512: INFO: Pod "busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23" satisfied condition "not pending"
    Feb 27 01:55:26.512: INFO: Started pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 in namespace container-probe-7613
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 01:55:26.512
    Feb 27 01:55:26.515: INFO: Initial restart count of pod busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 is 0
    Feb 27 01:56:16.645: INFO: Restart count of pod container-probe-7613/busybox-5e24de67-089d-4f9b-b81b-4a1782df1d23 is now 1 (50.130122772s elapsed)
    STEP: deleting the pod 02/27/23 01:56:16.645
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7613" for this suite. 02/27/23 01:56:16.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:16.668
Feb 27 01:56:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename watch 02/27/23 01:56:16.669
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:16.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:16.688
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 02/27/23 01:56:16.69
STEP: creating a new configmap 02/27/23 01:56:16.693
STEP: modifying the configmap once 02/27/23 01:56:16.696
STEP: changing the label value of the configmap 02/27/23 01:56:16.702
STEP: Expecting to observe a delete notification for the watched object 02/27/23 01:56:16.713
Feb 27 01:56:16.713: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26300 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:56:16.713: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26301 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:56:16.713: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26302 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 02/27/23 01:56:16.713
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 01:56:16.721
STEP: changing the label value of the configmap back 02/27/23 01:56:26.722
STEP: modifying the configmap a third time 02/27/23 01:56:26.733
STEP: deleting the configmap 02/27/23 01:56:26.745
STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 01:56:26.751
Feb 27 01:56:26.751: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26364 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:56:26.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26366 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 01:56:26.751: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26367 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:26.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-646" for this suite. 02/27/23 01:56:26.755
------------------------------
• [SLOW TEST] [10.094 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:16.668
    Feb 27 01:56:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename watch 02/27/23 01:56:16.669
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:16.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:16.688
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 02/27/23 01:56:16.69
    STEP: creating a new configmap 02/27/23 01:56:16.693
    STEP: modifying the configmap once 02/27/23 01:56:16.696
    STEP: changing the label value of the configmap 02/27/23 01:56:16.702
    STEP: Expecting to observe a delete notification for the watched object 02/27/23 01:56:16.713
    Feb 27 01:56:16.713: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26300 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:56:16.713: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26301 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:56:16.713: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26302 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 02/27/23 01:56:16.713
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 02/27/23 01:56:16.721
    STEP: changing the label value of the configmap back 02/27/23 01:56:26.722
    STEP: modifying the configmap a third time 02/27/23 01:56:26.733
    STEP: deleting the configmap 02/27/23 01:56:26.745
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 02/27/23 01:56:26.751
    Feb 27 01:56:26.751: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26364 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:56:26.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26366 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 01:56:26.751: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-646  6995a174-0496-4be7-9479-e55e26ffa12f 26367 0 2023-02-27 01:56:16 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-02-27 01:56:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:26.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-646" for this suite. 02/27/23 01:56:26.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:26.763
Feb 27 01:56:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 01:56:26.763
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:26.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:26.784
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 01:56:26.799
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:56:27.084
STEP: Deploying the webhook pod 02/27/23 01:56:27.092
STEP: Wait for the deployment to be ready 02/27/23 01:56:27.103
Feb 27 01:56:27.109: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 01:56:29.121
STEP: Verifying the service has paired with the endpoint 02/27/23 01:56:29.142
Feb 27 01:56:30.142: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Feb 27 01:56:30.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 01:56:30.656
Feb 27 01:56:30.675: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be denied by the webhook 02/27/23 01:56:30.785
STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 01:56:32.824
STEP: Updating the custom resource with disallowed data should be denied 02/27/23 01:56:32.831
STEP: Deleting the custom resource should be denied 02/27/23 01:56:32.838
STEP: Remove the offending key and value from the custom resource data 02/27/23 01:56:32.844
STEP: Deleting the updated custom resource should be successful 02/27/23 01:56:32.853
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:33.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9649" for this suite. 02/27/23 01:56:33.491
STEP: Destroying namespace "webhook-9649-markers" for this suite. 02/27/23 01:56:33.499
------------------------------
• [SLOW TEST] [6.757 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:26.763
    Feb 27 01:56:26.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 01:56:26.763
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:26.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:26.784
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 01:56:26.799
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:56:27.084
    STEP: Deploying the webhook pod 02/27/23 01:56:27.092
    STEP: Wait for the deployment to be ready 02/27/23 01:56:27.103
    Feb 27 01:56:27.109: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 01:56:29.121
    STEP: Verifying the service has paired with the endpoint 02/27/23 01:56:29.142
    Feb 27 01:56:30.142: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Feb 27 01:56:30.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 02/27/23 01:56:30.656
    Feb 27 01:56:30.675: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be denied by the webhook 02/27/23 01:56:30.785
    STEP: Creating a custom resource whose deletion would be denied by the webhook 02/27/23 01:56:32.824
    STEP: Updating the custom resource with disallowed data should be denied 02/27/23 01:56:32.831
    STEP: Deleting the custom resource should be denied 02/27/23 01:56:32.838
    STEP: Remove the offending key and value from the custom resource data 02/27/23 01:56:32.844
    STEP: Deleting the updated custom resource should be successful 02/27/23 01:56:32.853
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:33.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9649" for this suite. 02/27/23 01:56:33.491
    STEP: Destroying namespace "webhook-9649-markers" for this suite. 02/27/23 01:56:33.499
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:33.519
Feb 27 01:56:33.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 01:56:33.52
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:33.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:33.545
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 02/27/23 01:56:33.549
Feb 27 01:56:33.549: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-2470 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 02/27/23 01:56:33.595
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2470" for this suite. 02/27/23 01:56:33.608
------------------------------
• [0.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:33.519
    Feb 27 01:56:33.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 01:56:33.52
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:33.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:33.545
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 02/27/23 01:56:33.549
    Feb 27 01:56:33.549: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-2470 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 02/27/23 01:56:33.595
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2470" for this suite. 02/27/23 01:56:33.608
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:33.614
Feb 27 01:56:33.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 01:56:33.614
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:33.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:33.632
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 01:56:33.635
Feb 27 01:56:33.667: INFO: Waiting up to 5m0s for pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f" in namespace "emptydir-7529" to be "Succeeded or Failed"
Feb 27 01:56:33.670: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71241ms
Feb 27 01:56:35.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007525074s
Feb 27 01:56:37.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007624295s
STEP: Saw pod success 02/27/23 01:56:37.675
Feb 27 01:56:37.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f" satisfied condition "Succeeded or Failed"
Feb 27 01:56:37.679: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-47651be1-1c68-4cba-abee-e57d56426f7f container test-container: <nil>
STEP: delete the pod 02/27/23 01:56:37.694
Feb 27 01:56:37.709: INFO: Waiting for pod pod-47651be1-1c68-4cba-abee-e57d56426f7f to disappear
Feb 27 01:56:37.711: INFO: Pod pod-47651be1-1c68-4cba-abee-e57d56426f7f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:37.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7529" for this suite. 02/27/23 01:56:37.716
------------------------------
• [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:33.614
    Feb 27 01:56:33.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 01:56:33.614
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:33.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:33.632
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 01:56:33.635
    Feb 27 01:56:33.667: INFO: Waiting up to 5m0s for pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f" in namespace "emptydir-7529" to be "Succeeded or Failed"
    Feb 27 01:56:33.670: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71241ms
    Feb 27 01:56:35.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007525074s
    Feb 27 01:56:37.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007624295s
    STEP: Saw pod success 02/27/23 01:56:37.675
    Feb 27 01:56:37.675: INFO: Pod "pod-47651be1-1c68-4cba-abee-e57d56426f7f" satisfied condition "Succeeded or Failed"
    Feb 27 01:56:37.679: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-47651be1-1c68-4cba-abee-e57d56426f7f container test-container: <nil>
    STEP: delete the pod 02/27/23 01:56:37.694
    Feb 27 01:56:37.709: INFO: Waiting for pod pod-47651be1-1c68-4cba-abee-e57d56426f7f to disappear
    Feb 27 01:56:37.711: INFO: Pod pod-47651be1-1c68-4cba-abee-e57d56426f7f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:37.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7529" for this suite. 02/27/23 01:56:37.716
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:37.722
Feb 27 01:56:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 01:56:37.723
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:37.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:37.741
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-4e872120-8555-4e5a-9fd6-4780ba74d2ff 02/27/23 01:56:37.744
STEP: Creating a pod to test consume configMaps 02/27/23 01:56:37.748
Feb 27 01:56:37.761: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd" in namespace "projected-711" to be "Succeeded or Failed"
Feb 27 01:56:37.765: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.641146ms
Feb 27 01:56:39.771: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009320137s
Feb 27 01:56:41.770: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008329165s
STEP: Saw pod success 02/27/23 01:56:41.77
Feb 27 01:56:41.770: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd" satisfied condition "Succeeded or Failed"
Feb 27 01:56:41.773: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd container agnhost-container: <nil>
STEP: delete the pod 02/27/23 01:56:41.784
Feb 27 01:56:41.812: INFO: Waiting for pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd to disappear
Feb 27 01:56:41.815: INFO: Pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:41.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-711" for this suite. 02/27/23 01:56:41.819
------------------------------
• [4.104 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:37.722
    Feb 27 01:56:37.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 01:56:37.723
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:37.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:37.741
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-4e872120-8555-4e5a-9fd6-4780ba74d2ff 02/27/23 01:56:37.744
    STEP: Creating a pod to test consume configMaps 02/27/23 01:56:37.748
    Feb 27 01:56:37.761: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd" in namespace "projected-711" to be "Succeeded or Failed"
    Feb 27 01:56:37.765: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.641146ms
    Feb 27 01:56:39.771: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009320137s
    Feb 27 01:56:41.770: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008329165s
    STEP: Saw pod success 02/27/23 01:56:41.77
    Feb 27 01:56:41.770: INFO: Pod "pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd" satisfied condition "Succeeded or Failed"
    Feb 27 01:56:41.773: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 01:56:41.784
    Feb 27 01:56:41.812: INFO: Waiting for pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd to disappear
    Feb 27 01:56:41.815: INFO: Pod pod-projected-configmaps-37bea42c-8627-424e-a1c0-9c99e4430ebd no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:41.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-711" for this suite. 02/27/23 01:56:41.819
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:41.826
Feb 27 01:56:41.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 01:56:41.827
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:41.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:41.849
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 02/27/23 01:56:41.857
STEP: waiting for RC to be added 02/27/23 01:56:41.866
STEP: waiting for available Replicas 02/27/23 01:56:41.866
STEP: patching ReplicationController 02/27/23 01:56:44.492
STEP: waiting for RC to be modified 02/27/23 01:56:44.501
STEP: patching ReplicationController status 02/27/23 01:56:44.501
STEP: waiting for RC to be modified 02/27/23 01:56:44.525
STEP: waiting for available Replicas 02/27/23 01:56:44.525
STEP: fetching ReplicationController status 02/27/23 01:56:44.537
STEP: patching ReplicationController scale 02/27/23 01:56:44.541
STEP: waiting for RC to be modified 02/27/23 01:56:44.55
STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 01:56:44.551
STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 01:56:46.633
STEP: updating ReplicationController status 02/27/23 01:56:46.638
STEP: waiting for RC to be modified 02/27/23 01:56:46.647
STEP: listing all ReplicationControllers 02/27/23 01:56:46.647
STEP: checking that ReplicationController has expected values 02/27/23 01:56:46.65
STEP: deleting ReplicationControllers by collection 02/27/23 01:56:46.65
STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 01:56:46.663
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:46.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-400" for this suite. 02/27/23 01:56:46.704
------------------------------
• [4.885 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:41.826
    Feb 27 01:56:41.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 01:56:41.827
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:41.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:41.849
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 02/27/23 01:56:41.857
    STEP: waiting for RC to be added 02/27/23 01:56:41.866
    STEP: waiting for available Replicas 02/27/23 01:56:41.866
    STEP: patching ReplicationController 02/27/23 01:56:44.492
    STEP: waiting for RC to be modified 02/27/23 01:56:44.501
    STEP: patching ReplicationController status 02/27/23 01:56:44.501
    STEP: waiting for RC to be modified 02/27/23 01:56:44.525
    STEP: waiting for available Replicas 02/27/23 01:56:44.525
    STEP: fetching ReplicationController status 02/27/23 01:56:44.537
    STEP: patching ReplicationController scale 02/27/23 01:56:44.541
    STEP: waiting for RC to be modified 02/27/23 01:56:44.55
    STEP: waiting for ReplicationController's scale to be the max amount 02/27/23 01:56:44.551
    STEP: fetching ReplicationController; ensuring that it's patched 02/27/23 01:56:46.633
    STEP: updating ReplicationController status 02/27/23 01:56:46.638
    STEP: waiting for RC to be modified 02/27/23 01:56:46.647
    STEP: listing all ReplicationControllers 02/27/23 01:56:46.647
    STEP: checking that ReplicationController has expected values 02/27/23 01:56:46.65
    STEP: deleting ReplicationControllers by collection 02/27/23 01:56:46.65
    STEP: waiting for ReplicationController to have a DELETED watchEvent 02/27/23 01:56:46.663
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:46.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-400" for this suite. 02/27/23 01:56:46.704
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:46.711
Feb 27 01:56:46.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename tables 02/27/23 01:56:46.712
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:46.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:46.732
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2753" for this suite. 02/27/23 01:56:46.743
------------------------------
• [0.038 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:46.711
    Feb 27 01:56:46.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename tables 02/27/23 01:56:46.712
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:46.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:46.732
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:46.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2753" for this suite. 02/27/23 01:56:46.743
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:46.749
Feb 27 01:56:46.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 01:56:46.75
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:46.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:46.768
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-326484f7-240a-44d8-ab43-639277675cb7 02/27/23 01:56:46.77
STEP: Creating a pod to test consume secrets 02/27/23 01:56:46.775
Feb 27 01:56:46.804: INFO: Waiting up to 5m0s for pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c" in namespace "secrets-9037" to be "Succeeded or Failed"
Feb 27 01:56:46.812: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.165018ms
Feb 27 01:56:48.835: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03113573s
Feb 27 01:56:50.819: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014295398s
STEP: Saw pod success 02/27/23 01:56:50.819
Feb 27 01:56:50.819: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c" satisfied condition "Succeeded or Failed"
Feb 27 01:56:50.822: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 01:56:50.827
Feb 27 01:56:50.837: INFO: Waiting for pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c to disappear
Feb 27 01:56:50.840: INFO: Pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:50.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9037" for this suite. 02/27/23 01:56:50.845
------------------------------
• [4.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:46.749
    Feb 27 01:56:46.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 01:56:46.75
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:46.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:46.768
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-326484f7-240a-44d8-ab43-639277675cb7 02/27/23 01:56:46.77
    STEP: Creating a pod to test consume secrets 02/27/23 01:56:46.775
    Feb 27 01:56:46.804: INFO: Waiting up to 5m0s for pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c" in namespace "secrets-9037" to be "Succeeded or Failed"
    Feb 27 01:56:46.812: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.165018ms
    Feb 27 01:56:48.835: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03113573s
    Feb 27 01:56:50.819: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014295398s
    STEP: Saw pod success 02/27/23 01:56:50.819
    Feb 27 01:56:50.819: INFO: Pod "pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c" satisfied condition "Succeeded or Failed"
    Feb 27 01:56:50.822: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 01:56:50.827
    Feb 27 01:56:50.837: INFO: Waiting for pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c to disappear
    Feb 27 01:56:50.840: INFO: Pod pod-secrets-f647d033-18d0-4393-b1fb-ed9731364a4c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:50.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9037" for this suite. 02/27/23 01:56:50.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:50.852
Feb 27 01:56:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption 02/27/23 01:56:50.853
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:50.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:50.891
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 02/27/23 01:56:50.917
STEP: Updating PodDisruptionBudget status 02/27/23 01:56:52.926
STEP: Waiting for all pods to be running 02/27/23 01:56:52.96
Feb 27 01:56:52.966: INFO: running pods: 0 < 1
STEP: locating a running pod 02/27/23 01:56:54.97
STEP: Waiting for the pdb to be processed 02/27/23 01:56:54.982
STEP: Patching PodDisruptionBudget status 02/27/23 01:56:54.989
STEP: Waiting for the pdb to be processed 02/27/23 01:56:55.001
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 01:56:55.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-71" for this suite. 02/27/23 01:56:55.01
------------------------------
• [4.165 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:50.852
    Feb 27 01:56:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption 02/27/23 01:56:50.853
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:50.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:50.891
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 02/27/23 01:56:50.917
    STEP: Updating PodDisruptionBudget status 02/27/23 01:56:52.926
    STEP: Waiting for all pods to be running 02/27/23 01:56:52.96
    Feb 27 01:56:52.966: INFO: running pods: 0 < 1
    STEP: locating a running pod 02/27/23 01:56:54.97
    STEP: Waiting for the pdb to be processed 02/27/23 01:56:54.982
    STEP: Patching PodDisruptionBudget status 02/27/23 01:56:54.989
    STEP: Waiting for the pdb to be processed 02/27/23 01:56:55.001
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:56:55.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-71" for this suite. 02/27/23 01:56:55.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:56:55.018
Feb 27 01:56:55.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 01:56:55.019
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:55.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:55.039
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 02/27/23 01:56:55.042
STEP: Ensuring job reaches completions 02/27/23 01:56:55.048
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 01:57:05.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1523" for this suite. 02/27/23 01:57:05.06
------------------------------
• [SLOW TEST] [10.048 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:56:55.018
    Feb 27 01:56:55.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 01:56:55.019
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:56:55.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:56:55.039
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 02/27/23 01:56:55.042
    STEP: Ensuring job reaches completions 02/27/23 01:56:55.048
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:57:05.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1523" for this suite. 02/27/23 01:57:05.06
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:57:05.066
Feb 27 01:57:05.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 01:57:05.066
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:05.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:05.095
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 01:57:05.184
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:57:05.741
STEP: Deploying the webhook pod 02/27/23 01:57:05.753
STEP: Wait for the deployment to be ready 02/27/23 01:57:05.766
Feb 27 01:57:05.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 01:57:07.787
STEP: Verifying the service has paired with the endpoint 02/27/23 01:57:07.815
Feb 27 01:57:08.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Feb 27 01:57:08.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8115-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 01:57:09.332
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 01:57:09.347
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:57:11.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4042" for this suite. 02/27/23 01:57:11.971
STEP: Destroying namespace "webhook-4042-markers" for this suite. 02/27/23 01:57:11.984
------------------------------
• [SLOW TEST] [6.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:57:05.066
    Feb 27 01:57:05.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 01:57:05.066
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:05.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:05.095
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 01:57:05.184
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 01:57:05.741
    STEP: Deploying the webhook pod 02/27/23 01:57:05.753
    STEP: Wait for the deployment to be ready 02/27/23 01:57:05.766
    Feb 27 01:57:05.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 01:57:07.787
    STEP: Verifying the service has paired with the endpoint 02/27/23 01:57:07.815
    Feb 27 01:57:08.815: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Feb 27 01:57:08.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8115-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 01:57:09.332
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 01:57:09.347
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:57:11.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4042" for this suite. 02/27/23 01:57:11.971
    STEP: Destroying namespace "webhook-4042-markers" for this suite. 02/27/23 01:57:11.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:57:12.009
Feb 27 01:57:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 01:57:12.01
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:12.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:12.03
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 01:57:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6972" for this suite. 02/27/23 01:57:12.086
------------------------------
• [0.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:57:12.009
    Feb 27 01:57:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 01:57:12.01
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:12.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:12.03
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:57:12.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6972" for this suite. 02/27/23 01:57:12.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:57:12.094
Feb 27 01:57:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename taint-single-pod 02/27/23 01:57:12.095
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:12.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:12.121
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Feb 27 01:57:12.123: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 01:58:12.205: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Feb 27 01:58:12.209: INFO: Starting informer...
STEP: Starting pod... 02/27/23 01:58:12.209
Feb 27 01:58:12.450: INFO: Pod is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 01:58:12.45
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 01:58:12.464
STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 01:58:12.466
Feb 27 01:58:12.466: INFO: Pod wasn't evicted. Proceeding
Feb 27 01:58:12.467: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 01:58:12.48
STEP: Waiting some time to make sure that toleration time passed. 02/27/23 01:58:12.491
Feb 27 01:59:27.492: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 01:59:27.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8887" for this suite. 02/27/23 01:59:27.499
------------------------------
• [SLOW TEST] [135.411 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:57:12.094
    Feb 27 01:57:12.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename taint-single-pod 02/27/23 01:57:12.095
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:57:12.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:57:12.121
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Feb 27 01:57:12.123: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 01:58:12.205: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Feb 27 01:58:12.209: INFO: Starting informer...
    STEP: Starting pod... 02/27/23 01:58:12.209
    Feb 27 01:58:12.450: INFO: Pod is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 01:58:12.45
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 01:58:12.464
    STEP: Waiting short time to make sure Pod is queued for deletion 02/27/23 01:58:12.466
    Feb 27 01:58:12.466: INFO: Pod wasn't evicted. Proceeding
    Feb 27 01:58:12.467: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 01:58:12.48
    STEP: Waiting some time to make sure that toleration time passed. 02/27/23 01:58:12.491
    Feb 27 01:59:27.492: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 01:59:27.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8887" for this suite. 02/27/23 01:59:27.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 01:59:27.507
Feb 27 01:59:27.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename cronjob 02/27/23 01:59:27.508
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:59:27.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:59:27.528
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 02/27/23 01:59:27.531
STEP: Ensuring a job is scheduled 02/27/23 01:59:27.543
STEP: Ensuring exactly one is scheduled 02/27/23 02:00:01.557
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 02:00:01.562
STEP: Ensuring the job is replaced with a new one 02/27/23 02:00:01.566
STEP: Removing cronjob 02/27/23 02:01:01.572
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:01.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9141" for this suite. 02/27/23 02:01:01.585
------------------------------
• [SLOW TEST] [94.098 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 01:59:27.507
    Feb 27 01:59:27.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename cronjob 02/27/23 01:59:27.508
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 01:59:27.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 01:59:27.528
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 02/27/23 01:59:27.531
    STEP: Ensuring a job is scheduled 02/27/23 01:59:27.543
    STEP: Ensuring exactly one is scheduled 02/27/23 02:00:01.557
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 02:00:01.562
    STEP: Ensuring the job is replaced with a new one 02/27/23 02:00:01.566
    STEP: Removing cronjob 02/27/23 02:01:01.572
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:01.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9141" for this suite. 02/27/23 02:01:01.585
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:01.605
Feb 27 02:01:01.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 02:01:01.606
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:01.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:01.63
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Feb 27 02:01:01.632: INFO: Creating ReplicaSet my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35
Feb 27 02:01:01.644: INFO: Pod name my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Found 0 pods out of 1
Feb 27 02:01:06.649: INFO: Pod name my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Found 1 pods out of 1
Feb 27 02:01:06.649: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35" is running
Feb 27 02:01:06.649: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" in namespace "replicaset-8821" to be "running"
Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z": Phase="Running", Reason="", readiness=true. Elapsed: 3.279363ms
Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" satisfied condition "running"
Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:01 +0000 UTC Reason: Message:}])
Feb 27 02:01:06.652: INFO: Trying to dial the pod
Feb 27 02:01:11.665: INFO: Controller my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Got expected result from replica 1 [my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z]: "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:11.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8821" for this suite. 02/27/23 02:01:11.671
------------------------------
• [SLOW TEST] [10.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:01.605
    Feb 27 02:01:01.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 02:01:01.606
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:01.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:01.63
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Feb 27 02:01:01.632: INFO: Creating ReplicaSet my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35
    Feb 27 02:01:01.644: INFO: Pod name my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Found 0 pods out of 1
    Feb 27 02:01:06.649: INFO: Pod name my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Found 1 pods out of 1
    Feb 27 02:01:06.649: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35" is running
    Feb 27 02:01:06.649: INFO: Waiting up to 5m0s for pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" in namespace "replicaset-8821" to be "running"
    Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z": Phase="Running", Reason="", readiness=true. Elapsed: 3.279363ms
    Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" satisfied condition "running"
    Feb 27 02:01:06.652: INFO: Pod "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:01:01 +0000 UTC Reason: Message:}])
    Feb 27 02:01:06.652: INFO: Trying to dial the pod
    Feb 27 02:01:11.665: INFO: Controller my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35: Got expected result from replica 1 [my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z]: "my-hostname-basic-1d5f9d2b-f9f6-4479-985a-ba34f783ac35-25p7z", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:11.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8821" for this suite. 02/27/23 02:01:11.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:11.68
Feb 27 02:01:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:01:11.681
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:11.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:11.701
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-d054e4aa-5e71-46b9-aa8d-7fb576d1a360 02/27/23 02:01:11.703
STEP: Creating a pod to test consume secrets 02/27/23 02:01:11.708
Feb 27 02:01:11.747: INFO: Waiting up to 5m0s for pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8" in namespace "secrets-9759" to be "Succeeded or Failed"
Feb 27 02:01:11.751: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109443ms
Feb 27 02:01:13.757: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009290656s
Feb 27 02:01:15.755: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008155295s
STEP: Saw pod success 02/27/23 02:01:15.755
Feb 27 02:01:15.756: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8" satisfied condition "Succeeded or Failed"
Feb 27 02:01:15.759: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 container secret-env-test: <nil>
STEP: delete the pod 02/27/23 02:01:15.774
Feb 27 02:01:15.789: INFO: Waiting for pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 to disappear
Feb 27 02:01:15.791: INFO: Pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:15.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9759" for this suite. 02/27/23 02:01:15.796
------------------------------
• [4.121 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:11.68
    Feb 27 02:01:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:01:11.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:11.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:11.701
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-d054e4aa-5e71-46b9-aa8d-7fb576d1a360 02/27/23 02:01:11.703
    STEP: Creating a pod to test consume secrets 02/27/23 02:01:11.708
    Feb 27 02:01:11.747: INFO: Waiting up to 5m0s for pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8" in namespace "secrets-9759" to be "Succeeded or Failed"
    Feb 27 02:01:11.751: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109443ms
    Feb 27 02:01:13.757: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009290656s
    Feb 27 02:01:15.755: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008155295s
    STEP: Saw pod success 02/27/23 02:01:15.755
    Feb 27 02:01:15.756: INFO: Pod "pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8" satisfied condition "Succeeded or Failed"
    Feb 27 02:01:15.759: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 container secret-env-test: <nil>
    STEP: delete the pod 02/27/23 02:01:15.774
    Feb 27 02:01:15.789: INFO: Waiting for pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 to disappear
    Feb 27 02:01:15.791: INFO: Pod pod-secrets-0f7d7652-2e2d-4b27-af7e-8d8850721bf8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:15.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9759" for this suite. 02/27/23 02:01:15.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:15.802
Feb 27 02:01:15.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:01:15.803
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:15.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:15.823
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:01:15.825
Feb 27 02:01:15.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503" in namespace "projected-6174" to be "Succeeded or Failed"
Feb 27 02:01:15.845: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Pending", Reason="", readiness=false. Elapsed: 4.336885ms
Feb 27 02:01:17.850: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Running", Reason="", readiness=false. Elapsed: 2.009201099s
Feb 27 02:01:19.851: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010423595s
STEP: Saw pod success 02/27/23 02:01:19.851
Feb 27 02:01:19.852: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503" satisfied condition "Succeeded or Failed"
Feb 27 02:01:19.854: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 container client-container: <nil>
STEP: delete the pod 02/27/23 02:01:19.859
Feb 27 02:01:19.871: INFO: Waiting for pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 to disappear
Feb 27 02:01:19.875: INFO: Pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:19.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6174" for this suite. 02/27/23 02:01:19.881
------------------------------
• [4.087 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:15.802
    Feb 27 02:01:15.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:01:15.803
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:15.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:15.823
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:01:15.825
    Feb 27 02:01:15.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503" in namespace "projected-6174" to be "Succeeded or Failed"
    Feb 27 02:01:15.845: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Pending", Reason="", readiness=false. Elapsed: 4.336885ms
    Feb 27 02:01:17.850: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Running", Reason="", readiness=false. Elapsed: 2.009201099s
    Feb 27 02:01:19.851: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010423595s
    STEP: Saw pod success 02/27/23 02:01:19.851
    Feb 27 02:01:19.852: INFO: Pod "downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503" satisfied condition "Succeeded or Failed"
    Feb 27 02:01:19.854: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:01:19.859
    Feb 27 02:01:19.871: INFO: Waiting for pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 to disappear
    Feb 27 02:01:19.875: INFO: Pod downwardapi-volume-385655f2-9537-4b8e-ae48-36b8af620503 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:19.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6174" for this suite. 02/27/23 02:01:19.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:19.889
Feb 27 02:01:19.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:01:19.89
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:19.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:19.917
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:23.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3835" for this suite. 02/27/23 02:01:23.943
------------------------------
• [4.062 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:19.889
    Feb 27 02:01:19.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:01:19.89
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:19.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:19.917
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:23.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3835" for this suite. 02/27/23 02:01:23.943
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:23.951
Feb 27 02:01:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:01:23.952
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:23.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:23.995
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:01:23.998
Feb 27 02:01:24.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f" in namespace "projected-5351" to be "Succeeded or Failed"
Feb 27 02:01:24.015: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3825ms
Feb 27 02:01:26.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008672014s
Feb 27 02:01:28.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008392247s
STEP: Saw pod success 02/27/23 02:01:28.02
Feb 27 02:01:28.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f" satisfied condition "Succeeded or Failed"
Feb 27 02:01:28.024: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f container client-container: <nil>
STEP: delete the pod 02/27/23 02:01:28.031
Feb 27 02:01:28.046: INFO: Waiting for pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f to disappear
Feb 27 02:01:28.048: INFO: Pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:01:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5351" for this suite. 02/27/23 02:01:28.054
------------------------------
• [4.114 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:23.951
    Feb 27 02:01:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:01:23.952
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:23.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:23.995
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:01:23.998
    Feb 27 02:01:24.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f" in namespace "projected-5351" to be "Succeeded or Failed"
    Feb 27 02:01:24.015: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.3825ms
    Feb 27 02:01:26.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008672014s
    Feb 27 02:01:28.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008392247s
    STEP: Saw pod success 02/27/23 02:01:28.02
    Feb 27 02:01:28.020: INFO: Pod "downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f" satisfied condition "Succeeded or Failed"
    Feb 27 02:01:28.024: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f container client-container: <nil>
    STEP: delete the pod 02/27/23 02:01:28.031
    Feb 27 02:01:28.046: INFO: Waiting for pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f to disappear
    Feb 27 02:01:28.048: INFO: Pod downwardapi-volume-a228eca6-809a-473d-9f0e-9d710660af1f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:01:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5351" for this suite. 02/27/23 02:01:28.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:01:28.066
Feb 27 02:01:28.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename cronjob 02/27/23 02:01:28.067
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:28.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:28.085
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 02/27/23 02:01:28.087
STEP: Ensuring no jobs are scheduled 02/27/23 02:01:28.092
STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 02:06:28.099
STEP: Removing cronjob 02/27/23 02:06:28.101
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 02:06:28.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5979" for this suite. 02/27/23 02:06:28.118
------------------------------
• [SLOW TEST] [300.057 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:01:28.066
    Feb 27 02:01:28.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename cronjob 02/27/23 02:01:28.067
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:01:28.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:01:28.085
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 02/27/23 02:01:28.087
    STEP: Ensuring no jobs are scheduled 02/27/23 02:01:28.092
    STEP: Ensuring no job exists by listing jobs explicitly 02/27/23 02:06:28.099
    STEP: Removing cronjob 02/27/23 02:06:28.101
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:06:28.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5979" for this suite. 02/27/23 02:06:28.118
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:06:28.124
Feb 27 02:06:28.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 02:06:28.124
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:06:28.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:06:28.149
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 02/27/23 02:06:28.153
STEP: waiting for pod running 02/27/23 02:06:28.186
Feb 27 02:06:28.186: INFO: Waiting up to 2m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234" to be "running"
Feb 27 02:06:28.189: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.977993ms
Feb 27 02:06:30.196: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01007865s
Feb 27 02:06:30.197: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" satisfied condition "running"
STEP: creating a file in subpath 02/27/23 02:06:30.197
Feb 27 02:06:30.200: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3234 PodName:var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:06:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:06:30.201: INFO: ExecWithOptions: Clientset creation
Feb 27 02:06:30.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3234/pods/var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 02/27/23 02:06:30.274
Feb 27 02:06:30.277: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3234 PodName:var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:06:30.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:06:30.278: INFO: ExecWithOptions: Clientset creation
Feb 27 02:06:30.278: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3234/pods/var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 02/27/23 02:06:30.355
Feb 27 02:06:30.871: INFO: Successfully updated pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb"
STEP: waiting for annotated pod running 02/27/23 02:06:30.871
Feb 27 02:06:30.871: INFO: Waiting up to 2m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234" to be "running"
Feb 27 02:06:30.874: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Running", Reason="", readiness=true. Elapsed: 3.003798ms
Feb 27 02:06:30.874: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 02:06:30.874
Feb 27 02:06:30.874: INFO: Deleting pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234"
Feb 27 02:06:30.889: INFO: Wait up to 5m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:04.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3234" for this suite. 02/27/23 02:07:04.905
------------------------------
• [SLOW TEST] [36.789 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:06:28.124
    Feb 27 02:06:28.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 02:06:28.124
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:06:28.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:06:28.149
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 02/27/23 02:06:28.153
    STEP: waiting for pod running 02/27/23 02:06:28.186
    Feb 27 02:06:28.186: INFO: Waiting up to 2m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234" to be "running"
    Feb 27 02:06:28.189: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.977993ms
    Feb 27 02:06:30.196: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Running", Reason="", readiness=true. Elapsed: 2.01007865s
    Feb 27 02:06:30.197: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" satisfied condition "running"
    STEP: creating a file in subpath 02/27/23 02:06:30.197
    Feb 27 02:06:30.200: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3234 PodName:var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:06:30.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:06:30.201: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:06:30.201: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3234/pods/var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 02/27/23 02:06:30.274
    Feb 27 02:06:30.277: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3234 PodName:var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:06:30.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:06:30.278: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:06:30.278: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3234/pods/var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 02/27/23 02:06:30.355
    Feb 27 02:06:30.871: INFO: Successfully updated pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb"
    STEP: waiting for annotated pod running 02/27/23 02:06:30.871
    Feb 27 02:06:30.871: INFO: Waiting up to 2m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234" to be "running"
    Feb 27 02:06:30.874: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb": Phase="Running", Reason="", readiness=true. Elapsed: 3.003798ms
    Feb 27 02:06:30.874: INFO: Pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 02:06:30.874
    Feb 27 02:06:30.874: INFO: Deleting pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" in namespace "var-expansion-3234"
    Feb 27 02:06:30.889: INFO: Wait up to 5m0s for pod "var-expansion-7a4aa9ab-8a11-4418-8a4c-381eaebe4dcb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:04.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3234" for this suite. 02/27/23 02:07:04.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:04.913
Feb 27 02:07:04.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:07:04.914
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:04.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:04.937
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 02/27/23 02:07:04.941
STEP: watching for the Service to be added 02/27/23 02:07:04.957
Feb 27 02:07:04.960: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Feb 27 02:07:04.960: INFO: Service test-service-frhb4 created
STEP: Getting /status 02/27/23 02:07:04.96
Feb 27 02:07:04.966: INFO: Service test-service-frhb4 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 02/27/23 02:07:04.966
STEP: watching for the Service to be patched 02/27/23 02:07:04.974
Feb 27 02:07:04.976: INFO: observed Service test-service-frhb4 in namespace services-9907 with annotations: map[] & LoadBalancer: {[]}
Feb 27 02:07:04.976: INFO: Found Service test-service-frhb4 in namespace services-9907 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Feb 27 02:07:04.976: INFO: Service test-service-frhb4 has service status patched
STEP: updating the ServiceStatus 02/27/23 02:07:04.976
Feb 27 02:07:04.987: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 02/27/23 02:07:04.987
Feb 27 02:07:04.988: INFO: Observed Service test-service-frhb4 in namespace services-9907 with annotations: map[] & Conditions: {[]}
Feb 27 02:07:04.988: INFO: Observed event: &Service{ObjectMeta:{test-service-frhb4  services-9907  f25270e4-1529-406a-9c96-ff80dfc044b0 30586 0 2023-02-27 02:07:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.189.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.189.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Feb 27 02:07:04.988: INFO: Observed event: &Service{ObjectMeta:{test-service-frhb4  services-9907  f25270e4-1529-406a-9c96-ff80dfc044b0 30587 0 2023-02-27 02:07:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.189.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.189.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
Feb 27 02:07:04.989: INFO: Found Service test-service-frhb4 in namespace services-9907 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 02:07:04.989: INFO: Service test-service-frhb4 has service status updated
STEP: patching the service 02/27/23 02:07:04.989
STEP: watching for the Service to be patched 02/27/23 02:07:04.999
Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
Feb 27 02:07:05.002: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service:patched test-service-static:true]
Feb 27 02:07:05.002: INFO: Service test-service-frhb4 patched
STEP: deleting the service 02/27/23 02:07:05.002
STEP: watching for the Service to be deleted 02/27/23 02:07:05.043
Feb 27 02:07:05.045: INFO: Observed event: ADDED
Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
Feb 27 02:07:05.045: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Feb 27 02:07:05.045: INFO: Service test-service-frhb4 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:05.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9907" for this suite. 02/27/23 02:07:05.051
------------------------------
• [0.147 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:04.913
    Feb 27 02:07:04.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:07:04.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:04.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:04.937
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 02/27/23 02:07:04.941
    STEP: watching for the Service to be added 02/27/23 02:07:04.957
    Feb 27 02:07:04.960: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Feb 27 02:07:04.960: INFO: Service test-service-frhb4 created
    STEP: Getting /status 02/27/23 02:07:04.96
    Feb 27 02:07:04.966: INFO: Service test-service-frhb4 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 02/27/23 02:07:04.966
    STEP: watching for the Service to be patched 02/27/23 02:07:04.974
    Feb 27 02:07:04.976: INFO: observed Service test-service-frhb4 in namespace services-9907 with annotations: map[] & LoadBalancer: {[]}
    Feb 27 02:07:04.976: INFO: Found Service test-service-frhb4 in namespace services-9907 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Feb 27 02:07:04.976: INFO: Service test-service-frhb4 has service status patched
    STEP: updating the ServiceStatus 02/27/23 02:07:04.976
    Feb 27 02:07:04.987: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 02/27/23 02:07:04.987
    Feb 27 02:07:04.988: INFO: Observed Service test-service-frhb4 in namespace services-9907 with annotations: map[] & Conditions: {[]}
    Feb 27 02:07:04.988: INFO: Observed event: &Service{ObjectMeta:{test-service-frhb4  services-9907  f25270e4-1529-406a-9c96-ff80dfc044b0 30586 0 2023-02-27 02:07:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.189.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.189.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Feb 27 02:07:04.988: INFO: Observed event: &Service{ObjectMeta:{test-service-frhb4  services-9907  f25270e4-1529-406a-9c96-ff80dfc044b0 30587 0 2023-02-27 02:07:04 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-02-27 02:07:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.110.189.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.110.189.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
    Feb 27 02:07:04.989: INFO: Found Service test-service-frhb4 in namespace services-9907 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 02:07:04.989: INFO: Service test-service-frhb4 has service status updated
    STEP: patching the service 02/27/23 02:07:04.989
    STEP: watching for the Service to be patched 02/27/23 02:07:04.999
    Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
    Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
    Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
    Feb 27 02:07:05.002: INFO: observed Service test-service-frhb4 in namespace services-9907 with labels: map[test-service-static:true]
    Feb 27 02:07:05.002: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service:patched test-service-static:true]
    Feb 27 02:07:05.002: INFO: Service test-service-frhb4 patched
    STEP: deleting the service 02/27/23 02:07:05.002
    STEP: watching for the Service to be deleted 02/27/23 02:07:05.043
    Feb 27 02:07:05.045: INFO: Observed event: ADDED
    Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
    Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
    Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
    Feb 27 02:07:05.045: INFO: Observed event: MODIFIED
    Feb 27 02:07:05.045: INFO: Found Service test-service-frhb4 in namespace services-9907 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Feb 27 02:07:05.045: INFO: Service test-service-frhb4 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:05.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9907" for this suite. 02/27/23 02:07:05.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:05.06
Feb 27 02:07:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:07:05.061
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:05.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:05.094
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6080 02/27/23 02:07:05.098
STEP: creating service affinity-nodeport-transition in namespace services-6080 02/27/23 02:07:05.098
STEP: creating replication controller affinity-nodeport-transition in namespace services-6080 02/27/23 02:07:05.129
I0227 02:07:05.142708      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6080, replica count: 3
I0227 02:07:08.194821      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:07:08.208: INFO: Creating new exec pod
Feb 27 02:07:08.243: INFO: Waiting up to 5m0s for pod "execpod-affinitycqhbh" in namespace "services-6080" to be "running"
Feb 27 02:07:08.251: INFO: Pod "execpod-affinitycqhbh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.732061ms
Feb 27 02:07:10.255: INFO: Pod "execpod-affinitycqhbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01206668s
Feb 27 02:07:10.255: INFO: Pod "execpod-affinitycqhbh" satisfied condition "running"
Feb 27 02:07:11.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Feb 27 02:07:11.418: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Feb 27 02:07:11.418: INFO: stdout: ""
Feb 27 02:07:11.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.97.157.38 80'
Feb 27 02:07:11.553: INFO: stderr: "+ nc -v -z -w 2 10.97.157.38 80\nConnection to 10.97.157.38 80 port [tcp/http] succeeded!\n"
Feb 27 02:07:11.553: INFO: stdout: ""
Feb 27 02:07:11.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 32210'
Feb 27 02:07:11.684: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 32210\nConnection to 10.0.10.22 32210 port [tcp/*] succeeded!\n"
Feb 27 02:07:11.684: INFO: stdout: ""
Feb 27 02:07:11.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 32210'
Feb 27 02:07:11.825: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 32210\nConnection to 10.0.10.3 32210 port [tcp/*] succeeded!\n"
Feb 27 02:07:11.825: INFO: stdout: ""
Feb 27 02:07:11.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:32210/ ; done'
Feb 27 02:07:12.044: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n"
Feb 27 02:07:12.044: INFO: stdout: "\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9"
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
Feb 27 02:07:12.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:32210/ ; done'
Feb 27 02:07:12.251: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n"
Feb 27 02:07:12.251: INFO: stdout: "\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j"
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
Feb 27 02:07:12.251: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6080, will wait for the garbage collector to delete the pods 02/27/23 02:07:12.27
Feb 27 02:07:12.334: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.734819ms
Feb 27 02:07:12.435: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.179242ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6080" for this suite. 02/27/23 02:07:14.695
------------------------------
• [SLOW TEST] [9.643 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:05.06
    Feb 27 02:07:05.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:07:05.061
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:05.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:05.094
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6080 02/27/23 02:07:05.098
    STEP: creating service affinity-nodeport-transition in namespace services-6080 02/27/23 02:07:05.098
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6080 02/27/23 02:07:05.129
    I0227 02:07:05.142708      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6080, replica count: 3
    I0227 02:07:08.194821      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:07:08.208: INFO: Creating new exec pod
    Feb 27 02:07:08.243: INFO: Waiting up to 5m0s for pod "execpod-affinitycqhbh" in namespace "services-6080" to be "running"
    Feb 27 02:07:08.251: INFO: Pod "execpod-affinitycqhbh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.732061ms
    Feb 27 02:07:10.255: INFO: Pod "execpod-affinitycqhbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01206668s
    Feb 27 02:07:10.255: INFO: Pod "execpod-affinitycqhbh" satisfied condition "running"
    Feb 27 02:07:11.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Feb 27 02:07:11.418: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 02:07:11.418: INFO: stdout: ""
    Feb 27 02:07:11.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.97.157.38 80'
    Feb 27 02:07:11.553: INFO: stderr: "+ nc -v -z -w 2 10.97.157.38 80\nConnection to 10.97.157.38 80 port [tcp/http] succeeded!\n"
    Feb 27 02:07:11.553: INFO: stdout: ""
    Feb 27 02:07:11.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 32210'
    Feb 27 02:07:11.684: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 32210\nConnection to 10.0.10.22 32210 port [tcp/*] succeeded!\n"
    Feb 27 02:07:11.684: INFO: stdout: ""
    Feb 27 02:07:11.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 32210'
    Feb 27 02:07:11.825: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 32210\nConnection to 10.0.10.3 32210 port [tcp/*] succeeded!\n"
    Feb 27 02:07:11.825: INFO: stdout: ""
    Feb 27 02:07:11.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:32210/ ; done'
    Feb 27 02:07:12.044: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n"
    Feb 27 02:07:12.044: INFO: stdout: "\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-m8t46\naffinity-nodeport-transition-j6gb9"
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-m8t46
    Feb 27 02:07:12.044: INFO: Received response from host: affinity-nodeport-transition-j6gb9
    Feb 27 02:07:12.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-6080 exec execpod-affinitycqhbh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:32210/ ; done'
    Feb 27 02:07:12.251: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:32210/\n"
    Feb 27 02:07:12.251: INFO: stdout: "\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j\naffinity-nodeport-transition-46s9j"
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Received response from host: affinity-nodeport-transition-46s9j
    Feb 27 02:07:12.251: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6080, will wait for the garbage collector to delete the pods 02/27/23 02:07:12.27
    Feb 27 02:07:12.334: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.734819ms
    Feb 27 02:07:12.435: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.179242ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6080" for this suite. 02/27/23 02:07:14.695
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:14.704
Feb 27 02:07:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:07:14.705
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:14.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:14.745
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Feb 27 02:07:14.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3907" for this suite. 02/27/23 02:07:17.904
------------------------------
• [3.207 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:14.704
    Feb 27 02:07:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:07:14.705
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:14.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:14.745
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Feb 27 02:07:14.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3907" for this suite. 02/27/23 02:07:17.904
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:17.911
Feb 27 02:07:17.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:07:17.912
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:17.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:17.936
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 02/27/23 02:07:17.938
Feb 27 02:07:17.983: INFO: Waiting up to 5m0s for pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65" in namespace "downward-api-7753" to be "Succeeded or Failed"
Feb 27 02:07:17.985: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331645ms
Feb 27 02:07:19.989: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006737786s
Feb 27 02:07:21.990: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007238499s
STEP: Saw pod success 02/27/23 02:07:21.99
Feb 27 02:07:21.990: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65" satisfied condition "Succeeded or Failed"
Feb 27 02:07:21.993: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 container dapi-container: <nil>
STEP: delete the pod 02/27/23 02:07:22.003
Feb 27 02:07:22.016: INFO: Waiting for pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 to disappear
Feb 27 02:07:22.018: INFO: Pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:22.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7753" for this suite. 02/27/23 02:07:22.024
------------------------------
• [4.118 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:17.911
    Feb 27 02:07:17.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:07:17.912
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:17.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:17.936
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 02/27/23 02:07:17.938
    Feb 27 02:07:17.983: INFO: Waiting up to 5m0s for pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65" in namespace "downward-api-7753" to be "Succeeded or Failed"
    Feb 27 02:07:17.985: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.331645ms
    Feb 27 02:07:19.989: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006737786s
    Feb 27 02:07:21.990: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007238499s
    STEP: Saw pod success 02/27/23 02:07:21.99
    Feb 27 02:07:21.990: INFO: Pod "downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65" satisfied condition "Succeeded or Failed"
    Feb 27 02:07:21.993: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 02:07:22.003
    Feb 27 02:07:22.016: INFO: Waiting for pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 to disappear
    Feb 27 02:07:22.018: INFO: Pod downward-api-1d52e9a9-a976-41cb-95d7-924d5b427b65 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:22.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7753" for this suite. 02/27/23 02:07:22.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:22.03
Feb 27 02:07:22.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context 02/27/23 02:07:22.031
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:22.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:22.054
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 02:07:22.057
Feb 27 02:07:22.066: INFO: Waiting up to 5m0s for pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632" in namespace "security-context-5118" to be "Succeeded or Failed"
Feb 27 02:07:22.070: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685057ms
Feb 27 02:07:24.075: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00907028s
Feb 27 02:07:26.076: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009554549s
STEP: Saw pod success 02/27/23 02:07:26.076
Feb 27 02:07:26.076: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632" satisfied condition "Succeeded or Failed"
Feb 27 02:07:26.080: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 container test-container: <nil>
STEP: delete the pod 02/27/23 02:07:26.086
Feb 27 02:07:26.097: INFO: Waiting for pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 to disappear
Feb 27 02:07:26.100: INFO: Pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:26.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5118" for this suite. 02/27/23 02:07:26.104
------------------------------
• [4.083 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:22.03
    Feb 27 02:07:22.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context 02/27/23 02:07:22.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:22.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:22.054
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 02:07:22.057
    Feb 27 02:07:22.066: INFO: Waiting up to 5m0s for pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632" in namespace "security-context-5118" to be "Succeeded or Failed"
    Feb 27 02:07:22.070: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Pending", Reason="", readiness=false. Elapsed: 3.685057ms
    Feb 27 02:07:24.075: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00907028s
    Feb 27 02:07:26.076: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009554549s
    STEP: Saw pod success 02/27/23 02:07:26.076
    Feb 27 02:07:26.076: INFO: Pod "security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632" satisfied condition "Succeeded or Failed"
    Feb 27 02:07:26.080: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:07:26.086
    Feb 27 02:07:26.097: INFO: Waiting for pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 to disappear
    Feb 27 02:07:26.100: INFO: Pod security-context-8896097e-ac38-4e29-a37a-1fe0c6f36632 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:26.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5118" for this suite. 02/27/23 02:07:26.104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:26.113
Feb 27 02:07:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 02:07:26.114
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:26.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:26.136
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 02/27/23 02:07:26.139
STEP: Verify that the required pods have come up 02/27/23 02:07:26.146
Feb 27 02:07:26.148: INFO: Pod name sample-pod: Found 0 pods out of 3
Feb 27 02:07:31.154: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 02/27/23 02:07:31.154
Feb 27 02:07:31.157: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 02/27/23 02:07:31.157
STEP: DeleteCollection of the ReplicaSets 02/27/23 02:07:31.162
STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 02:07:31.172
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9651" for this suite. 02/27/23 02:07:31.189
------------------------------
• [SLOW TEST] [5.095 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:26.113
    Feb 27 02:07:26.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 02:07:26.114
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:26.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:26.136
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 02/27/23 02:07:26.139
    STEP: Verify that the required pods have come up 02/27/23 02:07:26.146
    Feb 27 02:07:26.148: INFO: Pod name sample-pod: Found 0 pods out of 3
    Feb 27 02:07:31.154: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 02/27/23 02:07:31.154
    Feb 27 02:07:31.157: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 02/27/23 02:07:31.157
    STEP: DeleteCollection of the ReplicaSets 02/27/23 02:07:31.162
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 02/27/23 02:07:31.172
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9651" for this suite. 02/27/23 02:07:31.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:31.209
Feb 27 02:07:31.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 02:07:31.209
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:31.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:31.234
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Feb 27 02:07:31.270: INFO: Waiting up to 2m0s for pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" in namespace "var-expansion-5551" to be "container 0 failed with reason CreateContainerConfigError"
Feb 27 02:07:31.275: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72494ms
Feb 27 02:07:33.279: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530917s
Feb 27 02:07:33.279: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Feb 27 02:07:33.279: INFO: Deleting pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" in namespace "var-expansion-5551"
Feb 27 02:07:33.289: INFO: Wait up to 5m0s for pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:37.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5551" for this suite. 02/27/23 02:07:37.304
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:31.209
    Feb 27 02:07:31.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 02:07:31.209
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:31.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:31.234
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Feb 27 02:07:31.270: INFO: Waiting up to 2m0s for pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" in namespace "var-expansion-5551" to be "container 0 failed with reason CreateContainerConfigError"
    Feb 27 02:07:31.275: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72494ms
    Feb 27 02:07:33.279: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008530917s
    Feb 27 02:07:33.279: INFO: Pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Feb 27 02:07:33.279: INFO: Deleting pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" in namespace "var-expansion-5551"
    Feb 27 02:07:33.289: INFO: Wait up to 5m0s for pod "var-expansion-a3069cb3-8ee8-4261-b6ab-afbf499d4fbc" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:37.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5551" for this suite. 02/27/23 02:07:37.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:37.311
Feb 27 02:07:37.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption 02/27/23 02:07:37.312
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:37.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:37.334
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 02/27/23 02:07:37.341
STEP: Waiting for all pods to be running 02/27/23 02:07:37.417
Feb 27 02:07:37.424: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:39.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9092" for this suite. 02/27/23 02:07:39.437
------------------------------
• [2.133 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:37.311
    Feb 27 02:07:37.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption 02/27/23 02:07:37.312
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:37.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:37.334
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 02/27/23 02:07:37.341
    STEP: Waiting for all pods to be running 02/27/23 02:07:37.417
    Feb 27 02:07:37.424: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:39.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9092" for this suite. 02/27/23 02:07:39.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:39.446
Feb 27 02:07:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:07:39.447
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:39.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:39.467
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 02/27/23 02:07:39.47
STEP: getting /apis/node.k8s.io 02/27/23 02:07:39.472
STEP: getting /apis/node.k8s.io/v1 02/27/23 02:07:39.473
STEP: creating 02/27/23 02:07:39.474
STEP: watching 02/27/23 02:07:39.488
Feb 27 02:07:39.488: INFO: starting watch
STEP: getting 02/27/23 02:07:39.496
STEP: listing 02/27/23 02:07:39.498
STEP: patching 02/27/23 02:07:39.501
STEP: updating 02/27/23 02:07:39.508
Feb 27 02:07:39.513: INFO: waiting for watch events with expected annotations
STEP: deleting 02/27/23 02:07:39.513
STEP: deleting a collection 02/27/23 02:07:39.523
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1696" for this suite. 02/27/23 02:07:39.54
------------------------------
• [0.099 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:39.446
    Feb 27 02:07:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:07:39.447
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:39.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:39.467
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 02/27/23 02:07:39.47
    STEP: getting /apis/node.k8s.io 02/27/23 02:07:39.472
    STEP: getting /apis/node.k8s.io/v1 02/27/23 02:07:39.473
    STEP: creating 02/27/23 02:07:39.474
    STEP: watching 02/27/23 02:07:39.488
    Feb 27 02:07:39.488: INFO: starting watch
    STEP: getting 02/27/23 02:07:39.496
    STEP: listing 02/27/23 02:07:39.498
    STEP: patching 02/27/23 02:07:39.501
    STEP: updating 02/27/23 02:07:39.508
    Feb 27 02:07:39.513: INFO: waiting for watch events with expected annotations
    STEP: deleting 02/27/23 02:07:39.513
    STEP: deleting a collection 02/27/23 02:07:39.523
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1696" for this suite. 02/27/23 02:07:39.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:39.546
Feb 27 02:07:39.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:07:39.546
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:39.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:39.567
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6817/secret-test-03873ebc-4969-4b67-81aa-1525df789a36 02/27/23 02:07:39.569
STEP: Creating a pod to test consume secrets 02/27/23 02:07:39.574
Feb 27 02:07:39.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d" in namespace "secrets-6817" to be "Succeeded or Failed"
Feb 27 02:07:39.585: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091104ms
Feb 27 02:07:41.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00708929s
Feb 27 02:07:43.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006512052s
STEP: Saw pod success 02/27/23 02:07:43.59
Feb 27 02:07:43.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d" satisfied condition "Succeeded or Failed"
Feb 27 02:07:43.593: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d container env-test: <nil>
STEP: delete the pod 02/27/23 02:07:43.598
Feb 27 02:07:43.613: INFO: Waiting for pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d to disappear
Feb 27 02:07:43.617: INFO: Pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:43.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6817" for this suite. 02/27/23 02:07:43.622
------------------------------
• [4.082 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:39.546
    Feb 27 02:07:39.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:07:39.546
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:39.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:39.567
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6817/secret-test-03873ebc-4969-4b67-81aa-1525df789a36 02/27/23 02:07:39.569
    STEP: Creating a pod to test consume secrets 02/27/23 02:07:39.574
    Feb 27 02:07:39.583: INFO: Waiting up to 5m0s for pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d" in namespace "secrets-6817" to be "Succeeded or Failed"
    Feb 27 02:07:39.585: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091104ms
    Feb 27 02:07:41.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00708929s
    Feb 27 02:07:43.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006512052s
    STEP: Saw pod success 02/27/23 02:07:43.59
    Feb 27 02:07:43.590: INFO: Pod "pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d" satisfied condition "Succeeded or Failed"
    Feb 27 02:07:43.593: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d container env-test: <nil>
    STEP: delete the pod 02/27/23 02:07:43.598
    Feb 27 02:07:43.613: INFO: Waiting for pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d to disappear
    Feb 27 02:07:43.617: INFO: Pod pod-configmaps-52ceeb0f-21e7-434c-81a8-2d4f9dd1c85d no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:43.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6817" for this suite. 02/27/23 02:07:43.622
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:43.628
Feb 27 02:07:43.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:07:43.629
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:43.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:43.65
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:07:43.668
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:07:43.834
STEP: Deploying the webhook pod 02/27/23 02:07:43.842
STEP: Wait for the deployment to be ready 02/27/23 02:07:43.854
Feb 27 02:07:43.863: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:07:45.873
STEP: Verifying the service has paired with the endpoint 02/27/23 02:07:45.889
Feb 27 02:07:46.889: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 02:07:46.892
STEP: create a pod that should be denied by the webhook 02/27/23 02:07:46.907
STEP: create a pod that causes the webhook to hang 02/27/23 02:07:46.94
STEP: create a configmap that should be denied by the webhook 02/27/23 02:07:56.956
STEP: create a configmap that should be admitted by the webhook 02/27/23 02:07:56.969
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 02:07:56.978
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 02:07:56.99
STEP: create a namespace that bypass the webhook 02/27/23 02:07:56.995
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 02:07:57.003
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:57.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-820" for this suite. 02/27/23 02:07:57.088
STEP: Destroying namespace "webhook-820-markers" for this suite. 02/27/23 02:07:57.102
------------------------------
• [SLOW TEST] [13.491 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:43.628
    Feb 27 02:07:43.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:07:43.629
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:43.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:43.65
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:07:43.668
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:07:43.834
    STEP: Deploying the webhook pod 02/27/23 02:07:43.842
    STEP: Wait for the deployment to be ready 02/27/23 02:07:43.854
    Feb 27 02:07:43.863: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:07:45.873
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:07:45.889
    Feb 27 02:07:46.889: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 02:07:46.892
    STEP: create a pod that should be denied by the webhook 02/27/23 02:07:46.907
    STEP: create a pod that causes the webhook to hang 02/27/23 02:07:46.94
    STEP: create a configmap that should be denied by the webhook 02/27/23 02:07:56.956
    STEP: create a configmap that should be admitted by the webhook 02/27/23 02:07:56.969
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 02:07:56.978
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 02/27/23 02:07:56.99
    STEP: create a namespace that bypass the webhook 02/27/23 02:07:56.995
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 02/27/23 02:07:57.003
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:57.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-820" for this suite. 02/27/23 02:07:57.088
    STEP: Destroying namespace "webhook-820-markers" for this suite. 02/27/23 02:07:57.102
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:57.12
Feb 27 02:07:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 02:07:57.121
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:57.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:57.165
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3774" 02/27/23 02:07:57.169
Feb 27 02:07:57.184: INFO: Namespace "namespaces-3774" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"66932bf2-ce63-47ef-bda3-5ca4c4ddd6c8", "kubernetes.io/metadata.name":"namespaces-3774", "namespaces-3774":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:07:57.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3774" for this suite. 02/27/23 02:07:57.194
------------------------------
• [0.083 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:57.12
    Feb 27 02:07:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 02:07:57.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:57.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:57.165
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3774" 02/27/23 02:07:57.169
    Feb 27 02:07:57.184: INFO: Namespace "namespaces-3774" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"66932bf2-ce63-47ef-bda3-5ca4c4ddd6c8", "kubernetes.io/metadata.name":"namespaces-3774", "namespaces-3774":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:07:57.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3774" for this suite. 02/27/23 02:07:57.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:07:57.204
Feb 27 02:07:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:07:57.205
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:57.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:57.251
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 02:07:57.253
Feb 27 02:07:57.282: INFO: Waiting up to 5m0s for pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158" in namespace "emptydir-2036" to be "Succeeded or Failed"
Feb 27 02:07:57.296: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Pending", Reason="", readiness=false. Elapsed: 14.831885ms
Feb 27 02:07:59.301: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019129189s
Feb 27 02:08:01.302: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020892891s
STEP: Saw pod success 02/27/23 02:08:01.302
Feb 27 02:08:01.303: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158" satisfied condition "Succeeded or Failed"
Feb 27 02:08:01.307: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 container test-container: <nil>
STEP: delete the pod 02/27/23 02:08:01.314
Feb 27 02:08:01.325: INFO: Waiting for pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 to disappear
Feb 27 02:08:01.327: INFO: Pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:01.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2036" for this suite. 02/27/23 02:08:01.332
------------------------------
• [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:07:57.204
    Feb 27 02:07:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:07:57.205
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:07:57.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:07:57.251
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 02/27/23 02:07:57.253
    Feb 27 02:07:57.282: INFO: Waiting up to 5m0s for pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158" in namespace "emptydir-2036" to be "Succeeded or Failed"
    Feb 27 02:07:57.296: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Pending", Reason="", readiness=false. Elapsed: 14.831885ms
    Feb 27 02:07:59.301: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019129189s
    Feb 27 02:08:01.302: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020892891s
    STEP: Saw pod success 02/27/23 02:08:01.302
    Feb 27 02:08:01.303: INFO: Pod "pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158" satisfied condition "Succeeded or Failed"
    Feb 27 02:08:01.307: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:08:01.314
    Feb 27 02:08:01.325: INFO: Waiting for pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 to disappear
    Feb 27 02:08:01.327: INFO: Pod pod-3e64b4a7-0808-4668-aa1e-b8b86b6ce158 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:01.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2036" for this suite. 02/27/23 02:08:01.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:01.341
Feb 27 02:08:01.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:08:01.342
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:01.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:01.36
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:08:01.363
Feb 27 02:08:01.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a" in namespace "downward-api-3524" to be "Succeeded or Failed"
Feb 27 02:08:01.382: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.631815ms
Feb 27 02:08:03.386: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009279826s
Feb 27 02:08:05.387: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009950478s
STEP: Saw pod success 02/27/23 02:08:05.387
Feb 27 02:08:05.387: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a" satisfied condition "Succeeded or Failed"
Feb 27 02:08:05.390: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a container client-container: <nil>
STEP: delete the pod 02/27/23 02:08:05.394
Feb 27 02:08:05.405: INFO: Waiting for pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a to disappear
Feb 27 02:08:05.409: INFO: Pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:05.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3524" for this suite. 02/27/23 02:08:05.414
------------------------------
• [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:01.341
    Feb 27 02:08:01.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:08:01.342
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:01.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:01.36
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:08:01.363
    Feb 27 02:08:01.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a" in namespace "downward-api-3524" to be "Succeeded or Failed"
    Feb 27 02:08:01.382: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.631815ms
    Feb 27 02:08:03.386: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009279826s
    Feb 27 02:08:05.387: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009950478s
    STEP: Saw pod success 02/27/23 02:08:05.387
    Feb 27 02:08:05.387: INFO: Pod "downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a" satisfied condition "Succeeded or Failed"
    Feb 27 02:08:05.390: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a container client-container: <nil>
    STEP: delete the pod 02/27/23 02:08:05.394
    Feb 27 02:08:05.405: INFO: Waiting for pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a to disappear
    Feb 27 02:08:05.409: INFO: Pod downwardapi-volume-aca72af9-b98a-4f6c-af6a-f504865fa91a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:05.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3524" for this suite. 02/27/23 02:08:05.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:05.42
Feb 27 02:08:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:08:05.421
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:05.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:05.444
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-04b722a7-40f2-4b68-941a-6f16088c99c8 02/27/23 02:08:05.446
STEP: Creating a pod to test consume configMaps 02/27/23 02:08:05.451
Feb 27 02:08:05.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72" in namespace "configmap-4819" to be "Succeeded or Failed"
Feb 27 02:08:05.466: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501289ms
Feb 27 02:08:07.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0097719s
Feb 27 02:08:09.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010112001s
STEP: Saw pod success 02/27/23 02:08:09.471
Feb 27 02:08:09.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72" satisfied condition "Succeeded or Failed"
Feb 27 02:08:09.476: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:08:09.482
Feb 27 02:08:09.495: INFO: Waiting for pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 to disappear
Feb 27 02:08:09.498: INFO: Pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4819" for this suite. 02/27/23 02:08:09.502
------------------------------
• [4.089 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:05.42
    Feb 27 02:08:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:08:05.421
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:05.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:05.444
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-04b722a7-40f2-4b68-941a-6f16088c99c8 02/27/23 02:08:05.446
    STEP: Creating a pod to test consume configMaps 02/27/23 02:08:05.451
    Feb 27 02:08:05.461: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72" in namespace "configmap-4819" to be "Succeeded or Failed"
    Feb 27 02:08:05.466: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Pending", Reason="", readiness=false. Elapsed: 4.501289ms
    Feb 27 02:08:07.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0097719s
    Feb 27 02:08:09.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010112001s
    STEP: Saw pod success 02/27/23 02:08:09.471
    Feb 27 02:08:09.471: INFO: Pod "pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72" satisfied condition "Succeeded or Failed"
    Feb 27 02:08:09.476: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:08:09.482
    Feb 27 02:08:09.495: INFO: Waiting for pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 to disappear
    Feb 27 02:08:09.498: INFO: Pod pod-configmaps-1ad07ba0-2c46-40b7-bc52-f72f603e5f72 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4819" for this suite. 02/27/23 02:08:09.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:09.509
Feb 27 02:08:09.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:08:09.51
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:09.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:09.543
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 02/27/23 02:08:09.546
Feb 27 02:08:09.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3693 cluster-info'
Feb 27 02:08:09.606: INFO: stderr: ""
Feb 27 02:08:09.606: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:09.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3693" for this suite. 02/27/23 02:08:09.611
------------------------------
• [0.109 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:09.509
    Feb 27 02:08:09.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:08:09.51
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:09.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:09.543
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 02/27/23 02:08:09.546
    Feb 27 02:08:09.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3693 cluster-info'
    Feb 27 02:08:09.606: INFO: stderr: ""
    Feb 27 02:08:09.606: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:09.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3693" for this suite. 02/27/23 02:08:09.611
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:09.618
Feb 27 02:08:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 02:08:09.619
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:09.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:09.648
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Feb 27 02:08:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: creating the pod 02/27/23 02:08:09.651
STEP: submitting the pod to kubernetes 02/27/23 02:08:09.651
Feb 27 02:08:09.666: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67" in namespace "pods-3233" to be "running and ready"
Feb 27 02:08:09.675: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67": Phase="Pending", Reason="", readiness=false. Elapsed: 8.622497ms
Feb 27 02:08:09.675: INFO: The phase of Pod pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:08:11.681: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.01480348s
Feb 27 02:08:11.681: INFO: The phase of Pod pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67 is Running (Ready = true)
Feb 27 02:08:11.681: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:11.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3233" for this suite. 02/27/23 02:08:11.756
------------------------------
• [2.152 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:09.618
    Feb 27 02:08:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 02:08:09.619
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:09.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:09.648
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Feb 27 02:08:09.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: creating the pod 02/27/23 02:08:09.651
    STEP: submitting the pod to kubernetes 02/27/23 02:08:09.651
    Feb 27 02:08:09.666: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67" in namespace "pods-3233" to be "running and ready"
    Feb 27 02:08:09.675: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67": Phase="Pending", Reason="", readiness=false. Elapsed: 8.622497ms
    Feb 27 02:08:09.675: INFO: The phase of Pod pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:08:11.681: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.01480348s
    Feb 27 02:08:11.681: INFO: The phase of Pod pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67 is Running (Ready = true)
    Feb 27 02:08:11.681: INFO: Pod "pod-exec-websocket-d610aba5-84ee-4397-baeb-17f6d2876f67" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:11.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3233" for this suite. 02/27/23 02:08:11.756
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:11.771
Feb 27 02:08:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:08:11.772
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:11.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:11.807
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5631 02/27/23 02:08:11.812
STEP: creating service affinity-clusterip in namespace services-5631 02/27/23 02:08:11.813
STEP: creating replication controller affinity-clusterip in namespace services-5631 02/27/23 02:08:11.829
I0227 02:08:11.835766      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5631, replica count: 3
I0227 02:08:14.886359      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:08:14.891: INFO: Creating new exec pod
Feb 27 02:08:14.924: INFO: Waiting up to 5m0s for pod "execpod-affinityj79vv" in namespace "services-5631" to be "running"
Feb 27 02:08:14.932: INFO: Pod "execpod-affinityj79vv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778575ms
Feb 27 02:08:16.937: INFO: Pod "execpod-affinityj79vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012839099s
Feb 27 02:08:16.937: INFO: Pod "execpod-affinityj79vv" satisfied condition "running"
Feb 27 02:08:17.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Feb 27 02:08:18.061: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Feb 27 02:08:18.061: INFO: stdout: ""
Feb 27 02:08:18.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c nc -v -z -w 2 10.105.16.137 80'
Feb 27 02:08:18.192: INFO: stderr: "+ nc -v -z -w 2 10.105.16.137 80\nConnection to 10.105.16.137 80 port [tcp/http] succeeded!\n"
Feb 27 02:08:18.192: INFO: stdout: ""
Feb 27 02:08:18.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.16.137:80/ ; done'
Feb 27 02:08:18.400: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n"
Feb 27 02:08:18.400: INFO: stdout: "\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj"
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
Feb 27 02:08:18.400: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5631, will wait for the garbage collector to delete the pods 02/27/23 02:08:18.411
Feb 27 02:08:18.473: INFO: Deleting ReplicationController affinity-clusterip took: 8.831426ms
Feb 27 02:08:18.573: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.556121ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:20.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5631" for this suite. 02/27/23 02:08:20.722
------------------------------
• [SLOW TEST] [8.961 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:11.771
    Feb 27 02:08:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:08:11.772
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:11.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:11.807
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5631 02/27/23 02:08:11.812
    STEP: creating service affinity-clusterip in namespace services-5631 02/27/23 02:08:11.813
    STEP: creating replication controller affinity-clusterip in namespace services-5631 02/27/23 02:08:11.829
    I0227 02:08:11.835766      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5631, replica count: 3
    I0227 02:08:14.886359      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:08:14.891: INFO: Creating new exec pod
    Feb 27 02:08:14.924: INFO: Waiting up to 5m0s for pod "execpod-affinityj79vv" in namespace "services-5631" to be "running"
    Feb 27 02:08:14.932: INFO: Pod "execpod-affinityj79vv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778575ms
    Feb 27 02:08:16.937: INFO: Pod "execpod-affinityj79vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.012839099s
    Feb 27 02:08:16.937: INFO: Pod "execpod-affinityj79vv" satisfied condition "running"
    Feb 27 02:08:17.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Feb 27 02:08:18.061: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Feb 27 02:08:18.061: INFO: stdout: ""
    Feb 27 02:08:18.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c nc -v -z -w 2 10.105.16.137 80'
    Feb 27 02:08:18.192: INFO: stderr: "+ nc -v -z -w 2 10.105.16.137 80\nConnection to 10.105.16.137 80 port [tcp/http] succeeded!\n"
    Feb 27 02:08:18.192: INFO: stdout: ""
    Feb 27 02:08:18.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-5631 exec execpod-affinityj79vv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.16.137:80/ ; done'
    Feb 27 02:08:18.400: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.16.137:80/\n"
    Feb 27 02:08:18.400: INFO: stdout: "\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj\naffinity-clusterip-fs4dj"
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Received response from host: affinity-clusterip-fs4dj
    Feb 27 02:08:18.400: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5631, will wait for the garbage collector to delete the pods 02/27/23 02:08:18.411
    Feb 27 02:08:18.473: INFO: Deleting ReplicationController affinity-clusterip took: 8.831426ms
    Feb 27 02:08:18.573: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.556121ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:20.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5631" for this suite. 02/27/23 02:08:20.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:20.732
Feb 27 02:08:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context-test 02/27/23 02:08:20.733
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:20.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:20.756
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Feb 27 02:08:20.791: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104" in namespace "security-context-test-8873" to be "Succeeded or Failed"
Feb 27 02:08:20.795: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773624ms
Feb 27 02:08:22.798: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007061392s
Feb 27 02:08:24.802: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010881123s
Feb 27 02:08:24.802: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:24.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8873" for this suite. 02/27/23 02:08:24.813
------------------------------
• [4.087 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:20.732
    Feb 27 02:08:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context-test 02/27/23 02:08:20.733
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:20.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:20.756
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Feb 27 02:08:20.791: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104" in namespace "security-context-test-8873" to be "Succeeded or Failed"
    Feb 27 02:08:20.795: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773624ms
    Feb 27 02:08:22.798: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007061392s
    Feb 27 02:08:24.802: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010881123s
    Feb 27 02:08:24.802: INFO: Pod "alpine-nnp-false-9c9a5146-96e6-4c1b-a5ff-9c5a36f69104" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:24.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8873" for this suite. 02/27/23 02:08:24.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:24.82
Feb 27 02:08:24.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:08:24.821
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:24.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:24.849
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-30b3b969-a6e8-4f68-a26c-fc55f2b76f49 02/27/23 02:08:24.852
STEP: Creating a pod to test consume configMaps 02/27/23 02:08:24.858
Feb 27 02:08:24.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35" in namespace "configmap-2262" to be "Succeeded or Failed"
Feb 27 02:08:24.874: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.838459ms
Feb 27 02:08:26.878: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009402877s
Feb 27 02:08:28.879: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01031302s
STEP: Saw pod success 02/27/23 02:08:28.879
Feb 27 02:08:28.879: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35" satisfied condition "Succeeded or Failed"
Feb 27 02:08:28.882: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 container configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 02:08:28.888
Feb 27 02:08:28.898: INFO: Waiting for pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 to disappear
Feb 27 02:08:28.900: INFO: Pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:28.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2262" for this suite. 02/27/23 02:08:28.905
------------------------------
• [4.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:24.82
    Feb 27 02:08:24.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:08:24.821
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:24.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:24.849
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-30b3b969-a6e8-4f68-a26c-fc55f2b76f49 02/27/23 02:08:24.852
    STEP: Creating a pod to test consume configMaps 02/27/23 02:08:24.858
    Feb 27 02:08:24.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35" in namespace "configmap-2262" to be "Succeeded or Failed"
    Feb 27 02:08:24.874: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.838459ms
    Feb 27 02:08:26.878: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009402877s
    Feb 27 02:08:28.879: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01031302s
    STEP: Saw pod success 02/27/23 02:08:28.879
    Feb 27 02:08:28.879: INFO: Pod "pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35" satisfied condition "Succeeded or Failed"
    Feb 27 02:08:28.882: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 container configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:08:28.888
    Feb 27 02:08:28.898: INFO: Waiting for pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 to disappear
    Feb 27 02:08:28.900: INFO: Pod pod-configmaps-be6a7f62-9002-470c-afd9-4e49c5806c35 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:28.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2262" for this suite. 02/27/23 02:08:28.905
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:28.913
Feb 27 02:08:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:08:28.914
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:28.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:28.942
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2175 02/27/23 02:08:28.944
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 02:08:28.963
STEP: creating service externalsvc in namespace services-2175 02/27/23 02:08:28.963
STEP: creating replication controller externalsvc in namespace services-2175 02/27/23 02:08:28.982
I0227 02:08:28.996334      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2175, replica count: 2
I0227 02:08:32.047936      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 02/27/23 02:08:32.052
Feb 27 02:08:32.075: INFO: Creating new exec pod
Feb 27 02:08:32.111: INFO: Waiting up to 5m0s for pod "execpoddfqbl" in namespace "services-2175" to be "running"
Feb 27 02:08:32.119: INFO: Pod "execpoddfqbl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.314543ms
Feb 27 02:08:34.125: INFO: Pod "execpoddfqbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.014169946s
Feb 27 02:08:34.125: INFO: Pod "execpoddfqbl" satisfied condition "running"
Feb 27 02:08:34.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-2175 exec execpoddfqbl -- /bin/sh -x -c nslookup clusterip-service.services-2175.svc.cluster.local'
Feb 27 02:08:34.271: INFO: stderr: "+ nslookup clusterip-service.services-2175.svc.cluster.local\n"
Feb 27 02:08:34.271: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-2175.svc.cluster.local\tcanonical name = externalsvc.services-2175.svc.cluster.local.\nName:\texternalsvc.services-2175.svc.cluster.local\nAddress: 10.110.241.173\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2175, will wait for the garbage collector to delete the pods 02/27/23 02:08:34.271
Feb 27 02:08:34.348: INFO: Deleting ReplicationController externalsvc took: 22.537911ms
Feb 27 02:08:34.549: INFO: Terminating ReplicationController externalsvc pods took: 200.089345ms
Feb 27 02:08:36.786: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:36.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2175" for this suite. 02/27/23 02:08:36.812
------------------------------
• [SLOW TEST] [7.908 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:28.913
    Feb 27 02:08:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:08:28.914
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:28.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:28.942
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2175 02/27/23 02:08:28.944
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 02/27/23 02:08:28.963
    STEP: creating service externalsvc in namespace services-2175 02/27/23 02:08:28.963
    STEP: creating replication controller externalsvc in namespace services-2175 02/27/23 02:08:28.982
    I0227 02:08:28.996334      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2175, replica count: 2
    I0227 02:08:32.047936      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 02/27/23 02:08:32.052
    Feb 27 02:08:32.075: INFO: Creating new exec pod
    Feb 27 02:08:32.111: INFO: Waiting up to 5m0s for pod "execpoddfqbl" in namespace "services-2175" to be "running"
    Feb 27 02:08:32.119: INFO: Pod "execpoddfqbl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.314543ms
    Feb 27 02:08:34.125: INFO: Pod "execpoddfqbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.014169946s
    Feb 27 02:08:34.125: INFO: Pod "execpoddfqbl" satisfied condition "running"
    Feb 27 02:08:34.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-2175 exec execpoddfqbl -- /bin/sh -x -c nslookup clusterip-service.services-2175.svc.cluster.local'
    Feb 27 02:08:34.271: INFO: stderr: "+ nslookup clusterip-service.services-2175.svc.cluster.local\n"
    Feb 27 02:08:34.271: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-2175.svc.cluster.local\tcanonical name = externalsvc.services-2175.svc.cluster.local.\nName:\texternalsvc.services-2175.svc.cluster.local\nAddress: 10.110.241.173\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2175, will wait for the garbage collector to delete the pods 02/27/23 02:08:34.271
    Feb 27 02:08:34.348: INFO: Deleting ReplicationController externalsvc took: 22.537911ms
    Feb 27 02:08:34.549: INFO: Terminating ReplicationController externalsvc pods took: 200.089345ms
    Feb 27 02:08:36.786: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:36.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2175" for this suite. 02/27/23 02:08:36.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:36.822
Feb 27 02:08:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename prestop 02/27/23 02:08:36.822
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:36.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:36.857
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9031 02/27/23 02:08:36.86
STEP: Waiting for pods to come up. 02/27/23 02:08:36.882
Feb 27 02:08:36.882: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9031" to be "running"
Feb 27 02:08:36.891: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35241ms
Feb 27 02:08:38.900: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.017633793s
Feb 27 02:08:38.900: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9031 02/27/23 02:08:38.904
Feb 27 02:08:38.911: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9031" to be "running"
Feb 27 02:08:38.914: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279537ms
Feb 27 02:08:40.919: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00792872s
Feb 27 02:08:40.919: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 02/27/23 02:08:40.919
Feb 27 02:08:45.941: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 02/27/23 02:08:45.941
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Feb 27 02:08:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9031" for this suite. 02/27/23 02:08:45.972
------------------------------
• [SLOW TEST] [9.155 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:36.822
    Feb 27 02:08:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename prestop 02/27/23 02:08:36.822
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:36.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:36.857
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9031 02/27/23 02:08:36.86
    STEP: Waiting for pods to come up. 02/27/23 02:08:36.882
    Feb 27 02:08:36.882: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9031" to be "running"
    Feb 27 02:08:36.891: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.35241ms
    Feb 27 02:08:38.900: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.017633793s
    Feb 27 02:08:38.900: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9031 02/27/23 02:08:38.904
    Feb 27 02:08:38.911: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9031" to be "running"
    Feb 27 02:08:38.914: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.279537ms
    Feb 27 02:08:40.919: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.00792872s
    Feb 27 02:08:40.919: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 02/27/23 02:08:40.919
    Feb 27 02:08:45.941: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 02/27/23 02:08:45.941
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:08:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9031" for this suite. 02/27/23 02:08:45.972
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:08:45.978
Feb 27 02:08:45.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename cronjob 02/27/23 02:08:45.979
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:45.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:45.996
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 02/27/23 02:08:45.998
STEP: Ensuring more than one job is running at a time 02/27/23 02:08:46.005
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 02:10:02.009
STEP: Removing cronjob 02/27/23 02:10:02.013
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 02:10:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5768" for this suite. 02/27/23 02:10:02.024
------------------------------
• [SLOW TEST] [76.056 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:08:45.978
    Feb 27 02:08:45.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename cronjob 02/27/23 02:08:45.979
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:08:45.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:08:45.996
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 02/27/23 02:08:45.998
    STEP: Ensuring more than one job is running at a time 02/27/23 02:08:46.005
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 02/27/23 02:10:02.009
    STEP: Removing cronjob 02/27/23 02:10:02.013
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:10:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5768" for this suite. 02/27/23 02:10:02.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:10:02.035
Feb 27 02:10:02.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 02:10:02.036
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:10:02.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:10:02.078
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 in namespace container-probe-4252 02/27/23 02:10:02.081
Feb 27 02:10:02.115: INFO: Waiting up to 5m0s for pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51" in namespace "container-probe-4252" to be "not pending"
Feb 27 02:10:02.119: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21694ms
Feb 27 02:10:04.124: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.008647439s
Feb 27 02:10:04.124: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51" satisfied condition "not pending"
Feb 27 02:10:04.124: INFO: Started pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 in namespace container-probe-4252
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:10:04.124
Feb 27 02:10:04.129: INFO: Initial restart count of pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 is 0
STEP: deleting the pod 02/27/23 02:14:04.777
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:04.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4252" for this suite. 02/27/23 02:14:04.799
------------------------------
• [SLOW TEST] [242.770 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:10:02.035
    Feb 27 02:10:02.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 02:10:02.036
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:10:02.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:10:02.078
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 in namespace container-probe-4252 02/27/23 02:10:02.081
    Feb 27 02:10:02.115: INFO: Waiting up to 5m0s for pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51" in namespace "container-probe-4252" to be "not pending"
    Feb 27 02:10:02.119: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51": Phase="Pending", Reason="", readiness=false. Elapsed: 3.21694ms
    Feb 27 02:10:04.124: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51": Phase="Running", Reason="", readiness=true. Elapsed: 2.008647439s
    Feb 27 02:10:04.124: INFO: Pod "liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51" satisfied condition "not pending"
    Feb 27 02:10:04.124: INFO: Started pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 in namespace container-probe-4252
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:10:04.124
    Feb 27 02:10:04.129: INFO: Initial restart count of pod liveness-c5aed8e4-bfde-4fac-93f9-60811e251b51 is 0
    STEP: deleting the pod 02/27/23 02:14:04.777
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:04.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4252" for this suite. 02/27/23 02:14:04.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:04.805
Feb 27 02:14:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:14:04.806
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:04.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:04.828
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 02/27/23 02:14:04.831
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 02:14:04.836
STEP: delete the deployment 02/27/23 02:14:05.344
STEP: wait for all rs to be garbage collected 02/27/23 02:14:05.352
STEP: expected 0 rs, got 1 rs 02/27/23 02:14:05.355
STEP: expected 0 pods, got 2 pods 02/27/23 02:14:05.359
STEP: Gathering metrics 02/27/23 02:14:05.874
Feb 27 02:14:05.906: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 02:14:05.909: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 3.063222ms
Feb 27 02:14:05.909: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 02:14:05.909: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 02:14:06.214: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:06.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7151" for this suite. 02/27/23 02:14:06.219
------------------------------
• [1.419 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:04.805
    Feb 27 02:14:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:14:04.806
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:04.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:04.828
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 02/27/23 02:14:04.831
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 02:14:04.836
    STEP: delete the deployment 02/27/23 02:14:05.344
    STEP: wait for all rs to be garbage collected 02/27/23 02:14:05.352
    STEP: expected 0 rs, got 1 rs 02/27/23 02:14:05.355
    STEP: expected 0 pods, got 2 pods 02/27/23 02:14:05.359
    STEP: Gathering metrics 02/27/23 02:14:05.874
    Feb 27 02:14:05.906: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 02:14:05.909: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 3.063222ms
    Feb 27 02:14:05.909: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 02:14:05.909: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 02:14:06.214: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:06.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7151" for this suite. 02/27/23 02:14:06.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:06.225
Feb 27 02:14:06.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:14:06.225
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:06.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:06.245
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1942 02/27/23 02:14:06.247
STEP: creating service affinity-clusterip-transition in namespace services-1942 02/27/23 02:14:06.247
STEP: creating replication controller affinity-clusterip-transition in namespace services-1942 02/27/23 02:14:06.264
I0227 02:14:06.270447      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1942, replica count: 3
I0227 02:14:09.321263      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:14:09.326: INFO: Creating new exec pod
Feb 27 02:14:09.360: INFO: Waiting up to 5m0s for pod "execpod-affinitysdtcv" in namespace "services-1942" to be "running"
Feb 27 02:14:09.370: INFO: Pod "execpod-affinitysdtcv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.52242ms
Feb 27 02:14:11.373: INFO: Pod "execpod-affinitysdtcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013662911s
Feb 27 02:14:11.373: INFO: Pod "execpod-affinitysdtcv" satisfied condition "running"
Feb 27 02:14:12.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Feb 27 02:14:12.530: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Feb 27 02:14:12.530: INFO: stdout: ""
Feb 27 02:14:12.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c nc -v -z -w 2 10.105.128.116 80'
Feb 27 02:14:12.680: INFO: stderr: "+ nc -v -z -w 2 10.105.128.116 80\nConnection to 10.105.128.116 80 port [tcp/http] succeeded!\n"
Feb 27 02:14:12.680: INFO: stdout: ""
Feb 27 02:14:12.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.128.116:80/ ; done'
Feb 27 02:14:12.895: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n"
Feb 27 02:14:12.895: INFO: stdout: "\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg"
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
Feb 27 02:14:12.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.128.116:80/ ; done'
Feb 27 02:14:13.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n"
Feb 27 02:14:13.096: INFO: stdout: "\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr"
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
Feb 27 02:14:13.096: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1942, will wait for the garbage collector to delete the pods 02/27/23 02:14:13.114
Feb 27 02:14:13.180: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.42769ms
Feb 27 02:14:13.281: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.220582ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1942" for this suite. 02/27/23 02:14:15.53
------------------------------
• [SLOW TEST] [9.314 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:06.225
    Feb 27 02:14:06.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:14:06.225
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:06.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:06.245
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1942 02/27/23 02:14:06.247
    STEP: creating service affinity-clusterip-transition in namespace services-1942 02/27/23 02:14:06.247
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1942 02/27/23 02:14:06.264
    I0227 02:14:06.270447      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1942, replica count: 3
    I0227 02:14:09.321263      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:14:09.326: INFO: Creating new exec pod
    Feb 27 02:14:09.360: INFO: Waiting up to 5m0s for pod "execpod-affinitysdtcv" in namespace "services-1942" to be "running"
    Feb 27 02:14:09.370: INFO: Pod "execpod-affinitysdtcv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.52242ms
    Feb 27 02:14:11.373: INFO: Pod "execpod-affinitysdtcv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013662911s
    Feb 27 02:14:11.373: INFO: Pod "execpod-affinitysdtcv" satisfied condition "running"
    Feb 27 02:14:12.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Feb 27 02:14:12.530: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Feb 27 02:14:12.530: INFO: stdout: ""
    Feb 27 02:14:12.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c nc -v -z -w 2 10.105.128.116 80'
    Feb 27 02:14:12.680: INFO: stderr: "+ nc -v -z -w 2 10.105.128.116 80\nConnection to 10.105.128.116 80 port [tcp/http] succeeded!\n"
    Feb 27 02:14:12.680: INFO: stdout: ""
    Feb 27 02:14:12.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.128.116:80/ ; done'
    Feb 27 02:14:12.895: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n"
    Feb 27 02:14:12.895: INFO: stdout: "\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg\naffinity-clusterip-transition-srvwc\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-7w9gg"
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-srvwc
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:12.895: INFO: Received response from host: affinity-clusterip-transition-7w9gg
    Feb 27 02:14:12.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-1942 exec execpod-affinitysdtcv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.128.116:80/ ; done'
    Feb 27 02:14:13.096: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.128.116:80/\n"
    Feb 27 02:14:13.096: INFO: stdout: "\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr\naffinity-clusterip-transition-cwfcr"
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Received response from host: affinity-clusterip-transition-cwfcr
    Feb 27 02:14:13.096: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1942, will wait for the garbage collector to delete the pods 02/27/23 02:14:13.114
    Feb 27 02:14:13.180: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.42769ms
    Feb 27 02:14:13.281: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.220582ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1942" for this suite. 02/27/23 02:14:15.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:15.539
Feb 27 02:14:15.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename subpath 02/27/23 02:14:15.54
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:15.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:15.577
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 02:14:15.579
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-5x5d 02/27/23 02:14:15.592
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:14:15.592
Feb 27 02:14:15.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5x5d" in namespace "subpath-3216" to be "Succeeded or Failed"
Feb 27 02:14:15.627: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205837ms
Feb 27 02:14:17.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006202413s
Feb 27 02:14:19.632: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 4.00713594s
Feb 27 02:14:21.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 6.006551936s
Feb 27 02:14:23.630: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 8.005777958s
Feb 27 02:14:25.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 10.006925692s
Feb 27 02:14:27.630: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 12.005928823s
Feb 27 02:14:29.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 14.00661273s
Feb 27 02:14:31.632: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 16.007795795s
Feb 27 02:14:33.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 18.006271348s
Feb 27 02:14:35.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 20.006649108s
Feb 27 02:14:37.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=false. Elapsed: 22.006458763s
Feb 27 02:14:39.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006957342s
STEP: Saw pod success 02/27/23 02:14:39.631
Feb 27 02:14:39.631: INFO: Pod "pod-subpath-test-configmap-5x5d" satisfied condition "Succeeded or Failed"
Feb 27 02:14:39.636: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-5x5d container test-container-subpath-configmap-5x5d: <nil>
STEP: delete the pod 02/27/23 02:14:39.647
Feb 27 02:14:39.659: INFO: Waiting for pod pod-subpath-test-configmap-5x5d to disappear
Feb 27 02:14:39.662: INFO: Pod pod-subpath-test-configmap-5x5d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5x5d 02/27/23 02:14:39.662
Feb 27 02:14:39.662: INFO: Deleting pod "pod-subpath-test-configmap-5x5d" in namespace "subpath-3216"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3216" for this suite. 02/27/23 02:14:39.671
------------------------------
• [SLOW TEST] [24.141 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:15.539
    Feb 27 02:14:15.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename subpath 02/27/23 02:14:15.54
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:15.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:15.577
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 02:14:15.579
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-5x5d 02/27/23 02:14:15.592
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:14:15.592
    Feb 27 02:14:15.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5x5d" in namespace "subpath-3216" to be "Succeeded or Failed"
    Feb 27 02:14:15.627: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205837ms
    Feb 27 02:14:17.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006202413s
    Feb 27 02:14:19.632: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 4.00713594s
    Feb 27 02:14:21.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 6.006551936s
    Feb 27 02:14:23.630: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 8.005777958s
    Feb 27 02:14:25.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 10.006925692s
    Feb 27 02:14:27.630: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 12.005928823s
    Feb 27 02:14:29.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 14.00661273s
    Feb 27 02:14:31.632: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 16.007795795s
    Feb 27 02:14:33.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 18.006271348s
    Feb 27 02:14:35.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=true. Elapsed: 20.006649108s
    Feb 27 02:14:37.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Running", Reason="", readiness=false. Elapsed: 22.006458763s
    Feb 27 02:14:39.631: INFO: Pod "pod-subpath-test-configmap-5x5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006957342s
    STEP: Saw pod success 02/27/23 02:14:39.631
    Feb 27 02:14:39.631: INFO: Pod "pod-subpath-test-configmap-5x5d" satisfied condition "Succeeded or Failed"
    Feb 27 02:14:39.636: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-configmap-5x5d container test-container-subpath-configmap-5x5d: <nil>
    STEP: delete the pod 02/27/23 02:14:39.647
    Feb 27 02:14:39.659: INFO: Waiting for pod pod-subpath-test-configmap-5x5d to disappear
    Feb 27 02:14:39.662: INFO: Pod pod-subpath-test-configmap-5x5d no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-5x5d 02/27/23 02:14:39.662
    Feb 27 02:14:39.662: INFO: Deleting pod "pod-subpath-test-configmap-5x5d" in namespace "subpath-3216"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3216" for this suite. 02/27/23 02:14:39.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:39.68
Feb 27 02:14:39.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:14:39.681
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:39.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:39.703
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-972755d5-f088-4cc4-8465-646d0c60181b 02/27/23 02:14:39.705
STEP: Creating a pod to test consume configMaps 02/27/23 02:14:39.709
Feb 27 02:14:39.739: INFO: Waiting up to 5m0s for pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3" in namespace "configmap-3426" to be "Succeeded or Failed"
Feb 27 02:14:39.742: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039073ms
Feb 27 02:14:41.747: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008575393s
Feb 27 02:14:43.746: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007490182s
STEP: Saw pod success 02/27/23 02:14:43.746
Feb 27 02:14:43.746: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3" satisfied condition "Succeeded or Failed"
Feb 27 02:14:43.750: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:14:43.755
Feb 27 02:14:43.766: INFO: Waiting for pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 to disappear
Feb 27 02:14:43.768: INFO: Pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:43.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3426" for this suite. 02/27/23 02:14:43.772
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:39.68
    Feb 27 02:14:39.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:14:39.681
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:39.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:39.703
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-972755d5-f088-4cc4-8465-646d0c60181b 02/27/23 02:14:39.705
    STEP: Creating a pod to test consume configMaps 02/27/23 02:14:39.709
    Feb 27 02:14:39.739: INFO: Waiting up to 5m0s for pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3" in namespace "configmap-3426" to be "Succeeded or Failed"
    Feb 27 02:14:39.742: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.039073ms
    Feb 27 02:14:41.747: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008575393s
    Feb 27 02:14:43.746: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007490182s
    STEP: Saw pod success 02/27/23 02:14:43.746
    Feb 27 02:14:43.746: INFO: Pod "pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3" satisfied condition "Succeeded or Failed"
    Feb 27 02:14:43.750: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:14:43.755
    Feb 27 02:14:43.766: INFO: Waiting for pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 to disappear
    Feb 27 02:14:43.768: INFO: Pod pod-configmaps-45f69d34-386e-4648-9fbd-277303dcb3f3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:43.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3426" for this suite. 02/27/23 02:14:43.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:43.778
Feb 27 02:14:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:14:43.779
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:43.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:43.803
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:14:43.816
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:14:44.579
STEP: Deploying the webhook pod 02/27/23 02:14:44.589
STEP: Wait for the deployment to be ready 02/27/23 02:14:44.647
Feb 27 02:14:44.658: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:14:46.671
STEP: Verifying the service has paired with the endpoint 02/27/23 02:14:46.689
Feb 27 02:14:47.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 02:14:47.7
STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 02:14:47.723
Feb 27 02:14:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:47.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9080" for this suite. 02/27/23 02:14:47.821
STEP: Destroying namespace "webhook-9080-markers" for this suite. 02/27/23 02:14:47.842
------------------------------
• [4.072 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:43.778
    Feb 27 02:14:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:14:43.779
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:43.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:43.803
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:14:43.816
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:14:44.579
    STEP: Deploying the webhook pod 02/27/23 02:14:44.589
    STEP: Wait for the deployment to be ready 02/27/23 02:14:44.647
    Feb 27 02:14:44.658: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:14:46.671
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:14:46.689
    Feb 27 02:14:47.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 02/27/23 02:14:47.7
    STEP: Creating a custom resource definition that should be denied by the webhook 02/27/23 02:14:47.723
    Feb 27 02:14:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:47.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9080" for this suite. 02/27/23 02:14:47.821
    STEP: Destroying namespace "webhook-9080-markers" for this suite. 02/27/23 02:14:47.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:47.85
Feb 27 02:14:47.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:14:47.851
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:47.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:47.89
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-2e9dd0f5-74f9-4cca-baf4-65e6bf21125a 02/27/23 02:14:47.892
STEP: Creating a pod to test consume secrets 02/27/23 02:14:47.899
Feb 27 02:14:47.937: INFO: Waiting up to 5m0s for pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19" in namespace "secrets-5168" to be "Succeeded or Failed"
Feb 27 02:14:47.945: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419822ms
Feb 27 02:14:49.950: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012075414s
Feb 27 02:14:51.949: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01179816s
STEP: Saw pod success 02/27/23 02:14:51.949
Feb 27 02:14:51.949: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19" satisfied condition "Succeeded or Failed"
Feb 27 02:14:51.952: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:14:51.962
Feb 27 02:14:51.977: INFO: Waiting for pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 to disappear
Feb 27 02:14:51.980: INFO: Pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:51.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5168" for this suite. 02/27/23 02:14:51.984
------------------------------
• [4.139 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:47.85
    Feb 27 02:14:47.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:14:47.851
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:47.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:47.89
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-2e9dd0f5-74f9-4cca-baf4-65e6bf21125a 02/27/23 02:14:47.892
    STEP: Creating a pod to test consume secrets 02/27/23 02:14:47.899
    Feb 27 02:14:47.937: INFO: Waiting up to 5m0s for pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19" in namespace "secrets-5168" to be "Succeeded or Failed"
    Feb 27 02:14:47.945: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419822ms
    Feb 27 02:14:49.950: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012075414s
    Feb 27 02:14:51.949: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01179816s
    STEP: Saw pod success 02/27/23 02:14:51.949
    Feb 27 02:14:51.949: INFO: Pod "pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19" satisfied condition "Succeeded or Failed"
    Feb 27 02:14:51.952: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:14:51.962
    Feb 27 02:14:51.977: INFO: Waiting for pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 to disappear
    Feb 27 02:14:51.980: INFO: Pod pod-secrets-d65b83c7-7931-48e1-a6aa-09210fe07c19 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:51.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5168" for this suite. 02/27/23 02:14:51.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:51.991
Feb 27 02:14:51.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:14:51.991
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:52.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:52.013
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:14:52.032
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:14:52.206
STEP: Deploying the webhook pod 02/27/23 02:14:52.212
STEP: Wait for the deployment to be ready 02/27/23 02:14:52.223
Feb 27 02:14:52.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:14:54.247
STEP: Verifying the service has paired with the endpoint 02/27/23 02:14:54.265
Feb 27 02:14:55.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Feb 27 02:14:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2868-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 02:14:55.782
STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 02:14:55.799
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:14:58.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8104" for this suite. 02/27/23 02:14:58.481
STEP: Destroying namespace "webhook-8104-markers" for this suite. 02/27/23 02:14:58.495
------------------------------
• [SLOW TEST] [6.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:51.991
    Feb 27 02:14:51.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:14:51.991
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:52.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:52.013
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:14:52.032
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:14:52.206
    STEP: Deploying the webhook pod 02/27/23 02:14:52.212
    STEP: Wait for the deployment to be ready 02/27/23 02:14:52.223
    Feb 27 02:14:52.235: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:14:54.247
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:14:54.265
    Feb 27 02:14:55.265: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Feb 27 02:14:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2868-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 02:14:55.782
    STEP: Creating a custom resource that should be mutated by the webhook 02/27/23 02:14:55.799
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:14:58.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8104" for this suite. 02/27/23 02:14:58.481
    STEP: Destroying namespace "webhook-8104-markers" for this suite. 02/27/23 02:14:58.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:58.505
Feb 27 02:14:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption 02/27/23 02:14:58.506
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:58.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:58.536
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:14:58.538
Feb 27 02:14:58.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption-2 02/27/23 02:14:58.539
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:58.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:58.582
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 02/27/23 02:14:58.591
STEP: Waiting for the pdb to be processed 02/27/23 02:15:00.62
STEP: Waiting for the pdb to be processed 02/27/23 02:15:00.629
STEP: listing a collection of PDBs across all namespaces 02/27/23 02:15:02.639
STEP: listing a collection of PDBs in namespace disruption-963 02/27/23 02:15:02.642
STEP: deleting a collection of PDBs 02/27/23 02:15:02.645
STEP: Waiting for the PDB collection to be deleted 02/27/23 02:15:02.658
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Feb 27 02:15:02.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:15:02.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-4455" for this suite. 02/27/23 02:15:02.669
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-963" for this suite. 02/27/23 02:15:02.679
------------------------------
• [4.181 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:58.505
    Feb 27 02:14:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption 02/27/23 02:14:58.506
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:58.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:58.536
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:14:58.538
    Feb 27 02:14:58.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption-2 02/27/23 02:14:58.539
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:14:58.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:14:58.582
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 02/27/23 02:14:58.591
    STEP: Waiting for the pdb to be processed 02/27/23 02:15:00.62
    STEP: Waiting for the pdb to be processed 02/27/23 02:15:00.629
    STEP: listing a collection of PDBs across all namespaces 02/27/23 02:15:02.639
    STEP: listing a collection of PDBs in namespace disruption-963 02/27/23 02:15:02.642
    STEP: deleting a collection of PDBs 02/27/23 02:15:02.645
    STEP: Waiting for the PDB collection to be deleted 02/27/23 02:15:02.658
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:15:02.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:15:02.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-4455" for this suite. 02/27/23 02:15:02.669
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-963" for this suite. 02/27/23 02:15:02.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:15:02.687
Feb 27 02:15:02.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename cronjob 02/27/23 02:15:02.688
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:15:02.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:15:02.708
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 02/27/23 02:15:02.711
STEP: Ensuring a job is scheduled 02/27/23 02:15:02.718
STEP: Ensuring exactly one is scheduled 02/27/23 02:16:00.722
STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 02:16:00.725
STEP: Ensuring no more jobs are scheduled 02/27/23 02:16:00.728
STEP: Removing cronjob 02/27/23 02:21:00.734
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1927" for this suite. 02/27/23 02:21:00.743
------------------------------
• [SLOW TEST] [358.063 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:15:02.687
    Feb 27 02:15:02.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename cronjob 02/27/23 02:15:02.688
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:15:02.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:15:02.708
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 02/27/23 02:15:02.711
    STEP: Ensuring a job is scheduled 02/27/23 02:15:02.718
    STEP: Ensuring exactly one is scheduled 02/27/23 02:16:00.722
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 02/27/23 02:16:00.725
    STEP: Ensuring no more jobs are scheduled 02/27/23 02:16:00.728
    STEP: Removing cronjob 02/27/23 02:21:00.734
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1927" for this suite. 02/27/23 02:21:00.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:00.75
Feb 27 02:21:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:00.751
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:00.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:00.78
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 02/27/23 02:21:00.782
Feb 27 02:21:00.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 create -f -'
Feb 27 02:21:01.560: INFO: stderr: ""
Feb 27 02:21:01.560: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:21:01.56
Feb 27 02:21:01.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:21:01.623: INFO: stderr: ""
Feb 27 02:21:01.623: INFO: stdout: "update-demo-nautilus-js7gj update-demo-nautilus-nxcc9 "
Feb 27 02:21:01.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:21:01.692: INFO: stderr: ""
Feb 27 02:21:01.692: INFO: stdout: ""
Feb 27 02:21:01.692: INFO: update-demo-nautilus-js7gj is created but not running
Feb 27 02:21:06.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:21:06.771: INFO: stderr: ""
Feb 27 02:21:06.771: INFO: stdout: "update-demo-nautilus-js7gj update-demo-nautilus-nxcc9 "
Feb 27 02:21:06.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:21:06.838: INFO: stderr: ""
Feb 27 02:21:06.838: INFO: stdout: "true"
Feb 27 02:21:06.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:21:06.902: INFO: stderr: ""
Feb 27 02:21:06.902: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:21:06.902: INFO: validating pod update-demo-nautilus-js7gj
Feb 27 02:21:06.908: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:21:06.908: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:21:06.908: INFO: update-demo-nautilus-js7gj is verified up and running
Feb 27 02:21:06.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-nxcc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:21:06.967: INFO: stderr: ""
Feb 27 02:21:06.967: INFO: stdout: "true"
Feb 27 02:21:06.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-nxcc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:21:07.025: INFO: stderr: ""
Feb 27 02:21:07.025: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:21:07.025: INFO: validating pod update-demo-nautilus-nxcc9
Feb 27 02:21:07.030: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:21:07.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:21:07.030: INFO: update-demo-nautilus-nxcc9 is verified up and running
STEP: using delete to clean up resources 02/27/23 02:21:07.03
Feb 27 02:21:07.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 delete --grace-period=0 --force -f -'
Feb 27 02:21:07.104: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 02:21:07.104: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 02:21:07.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get rc,svc -l name=update-demo --no-headers'
Feb 27 02:21:07.171: INFO: stderr: "No resources found in kubectl-830 namespace.\n"
Feb 27 02:21:07.171: INFO: stdout: ""
Feb 27 02:21:07.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 02:21:07.235: INFO: stderr: ""
Feb 27 02:21:07.235: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-830" for this suite. 02/27/23 02:21:07.24
------------------------------
• [SLOW TEST] [6.498 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:00.75
    Feb 27 02:21:00.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:00.751
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:00.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:00.78
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 02/27/23 02:21:00.782
    Feb 27 02:21:00.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 create -f -'
    Feb 27 02:21:01.560: INFO: stderr: ""
    Feb 27 02:21:01.560: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:21:01.56
    Feb 27 02:21:01.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:21:01.623: INFO: stderr: ""
    Feb 27 02:21:01.623: INFO: stdout: "update-demo-nautilus-js7gj update-demo-nautilus-nxcc9 "
    Feb 27 02:21:01.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:21:01.692: INFO: stderr: ""
    Feb 27 02:21:01.692: INFO: stdout: ""
    Feb 27 02:21:01.692: INFO: update-demo-nautilus-js7gj is created but not running
    Feb 27 02:21:06.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:21:06.771: INFO: stderr: ""
    Feb 27 02:21:06.771: INFO: stdout: "update-demo-nautilus-js7gj update-demo-nautilus-nxcc9 "
    Feb 27 02:21:06.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:21:06.838: INFO: stderr: ""
    Feb 27 02:21:06.838: INFO: stdout: "true"
    Feb 27 02:21:06.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-js7gj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:21:06.902: INFO: stderr: ""
    Feb 27 02:21:06.902: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:21:06.902: INFO: validating pod update-demo-nautilus-js7gj
    Feb 27 02:21:06.908: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:21:06.908: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:21:06.908: INFO: update-demo-nautilus-js7gj is verified up and running
    Feb 27 02:21:06.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-nxcc9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:21:06.967: INFO: stderr: ""
    Feb 27 02:21:06.967: INFO: stdout: "true"
    Feb 27 02:21:06.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods update-demo-nautilus-nxcc9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:21:07.025: INFO: stderr: ""
    Feb 27 02:21:07.025: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:21:07.025: INFO: validating pod update-demo-nautilus-nxcc9
    Feb 27 02:21:07.030: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:21:07.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:21:07.030: INFO: update-demo-nautilus-nxcc9 is verified up and running
    STEP: using delete to clean up resources 02/27/23 02:21:07.03
    Feb 27 02:21:07.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 delete --grace-period=0 --force -f -'
    Feb 27 02:21:07.104: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 02:21:07.104: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 02:21:07.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get rc,svc -l name=update-demo --no-headers'
    Feb 27 02:21:07.171: INFO: stderr: "No resources found in kubectl-830 namespace.\n"
    Feb 27 02:21:07.171: INFO: stdout: ""
    Feb 27 02:21:07.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-830 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 02:21:07.235: INFO: stderr: ""
    Feb 27 02:21:07.235: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-830" for this suite. 02/27/23 02:21:07.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:07.249
Feb 27 02:21:07.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:21:07.25
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:07.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:07.276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:21:07.297
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:21:07.718
STEP: Deploying the webhook pod 02/27/23 02:21:07.726
STEP: Wait for the deployment to be ready 02/27/23 02:21:07.738
Feb 27 02:21:07.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:21:09.769
STEP: Verifying the service has paired with the endpoint 02/27/23 02:21:09.79
Feb 27 02:21:10.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 02:21:10.795
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:10.795
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 02:21:10.813
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 02:21:11.823
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:11.823
STEP: Having no error when timeout is longer than webhook latency 02/27/23 02:21:12.861
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:12.861
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 02:21:17.893
STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:17.893
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:22.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8513" for this suite. 02/27/23 02:21:22.997
STEP: Destroying namespace "webhook-8513-markers" for this suite. 02/27/23 02:21:23.022
------------------------------
• [SLOW TEST] [15.785 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:07.249
    Feb 27 02:21:07.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:21:07.25
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:07.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:07.276
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:21:07.297
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:21:07.718
    STEP: Deploying the webhook pod 02/27/23 02:21:07.726
    STEP: Wait for the deployment to be ready 02/27/23 02:21:07.738
    Feb 27 02:21:07.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:21:09.769
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:21:09.79
    Feb 27 02:21:10.790: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 02/27/23 02:21:10.795
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:10.795
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 02/27/23 02:21:10.813
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 02/27/23 02:21:11.823
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:11.823
    STEP: Having no error when timeout is longer than webhook latency 02/27/23 02:21:12.861
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:12.861
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 02/27/23 02:21:17.893
    STEP: Registering slow webhook via the AdmissionRegistration API 02/27/23 02:21:17.893
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:22.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8513" for this suite. 02/27/23 02:21:22.997
    STEP: Destroying namespace "webhook-8513-markers" for this suite. 02/27/23 02:21:23.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:23.034
Feb 27 02:21:23.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:23.036
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:23.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:23.069
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 02/27/23 02:21:23.071
Feb 27 02:21:23.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7018 create -f -'
Feb 27 02:21:23.800: INFO: stderr: ""
Feb 27 02:21:23.800: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 02:21:23.8
Feb 27 02:21:24.807: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:21:24.807: INFO: Found 0 / 1
Feb 27 02:21:25.805: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:21:25.805: INFO: Found 1 / 1
Feb 27 02:21:25.805: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 02/27/23 02:21:25.805
Feb 27 02:21:25.809: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:21:25.809: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 02:21:25.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7018 patch pod agnhost-primary-2lwnj -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 27 02:21:25.881: INFO: stderr: ""
Feb 27 02:21:25.881: INFO: stdout: "pod/agnhost-primary-2lwnj patched\n"
STEP: checking annotations 02/27/23 02:21:25.881
Feb 27 02:21:25.885: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:21:25.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7018" for this suite. 02/27/23 02:21:25.891
------------------------------
• [2.886 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:23.034
    Feb 27 02:21:23.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:23.036
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:23.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:23.069
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 02/27/23 02:21:23.071
    Feb 27 02:21:23.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7018 create -f -'
    Feb 27 02:21:23.800: INFO: stderr: ""
    Feb 27 02:21:23.800: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 02:21:23.8
    Feb 27 02:21:24.807: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:21:24.807: INFO: Found 0 / 1
    Feb 27 02:21:25.805: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:21:25.805: INFO: Found 1 / 1
    Feb 27 02:21:25.805: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 02/27/23 02:21:25.805
    Feb 27 02:21:25.809: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:21:25.809: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 02:21:25.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-7018 patch pod agnhost-primary-2lwnj -p {"metadata":{"annotations":{"x":"y"}}}'
    Feb 27 02:21:25.881: INFO: stderr: ""
    Feb 27 02:21:25.881: INFO: stdout: "pod/agnhost-primary-2lwnj patched\n"
    STEP: checking annotations 02/27/23 02:21:25.881
    Feb 27 02:21:25.885: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:21:25.885: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:25.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7018" for this suite. 02/27/23 02:21:25.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:25.921
Feb 27 02:21:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:25.921
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:25.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:25.943
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 02/27/23 02:21:25.946
Feb 27 02:21:25.946: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-5477 proxy --unix-socket=/tmp/kubectl-proxy-unix4167707281/test'
STEP: retrieving proxy /api/ output 02/27/23 02:21:26.002
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:26.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5477" for this suite. 02/27/23 02:21:26.008
------------------------------
• [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:25.921
    Feb 27 02:21:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:21:25.921
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:25.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:25.943
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 02/27/23 02:21:25.946
    Feb 27 02:21:25.946: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-5477 proxy --unix-socket=/tmp/kubectl-proxy-unix4167707281/test'
    STEP: retrieving proxy /api/ output 02/27/23 02:21:26.002
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:26.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5477" for this suite. 02/27/23 02:21:26.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:26.017
Feb 27 02:21:26.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 02:21:26.018
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:26.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:26.041
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 02/27/23 02:21:26.043
STEP: When the matched label of one of its pods change 02/27/23 02:21:26.051
Feb 27 02:21:26.054: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 27 02:21:31.060: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 02:21:31.07
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2945" for this suite. 02/27/23 02:21:32.08
------------------------------
• [SLOW TEST] [6.069 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:26.017
    Feb 27 02:21:26.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 02:21:26.018
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:26.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:26.041
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 02/27/23 02:21:26.043
    STEP: When the matched label of one of its pods change 02/27/23 02:21:26.051
    Feb 27 02:21:26.054: INFO: Pod name pod-release: Found 0 pods out of 1
    Feb 27 02:21:31.060: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 02:21:31.07
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2945" for this suite. 02/27/23 02:21:32.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:32.087
Feb 27 02:21:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:21:32.088
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:32.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:32.112
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-3742 02/27/23 02:21:32.114
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[] 02/27/23 02:21:32.128
Feb 27 02:21:32.137: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Feb 27 02:21:33.150: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3742 02/27/23 02:21:33.15
Feb 27 02:21:33.194: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3742" to be "running and ready"
Feb 27 02:21:33.201: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.158741ms
Feb 27 02:21:33.201: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:21:35.206: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01186591s
Feb 27 02:21:35.206: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 02:21:35.206: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod1:[100]] 02/27/23 02:21:35.209
Feb 27 02:21:35.218: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3742 02/27/23 02:21:35.218
Feb 27 02:21:35.226: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3742" to be "running and ready"
Feb 27 02:21:35.230: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281391ms
Feb 27 02:21:35.230: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:21:37.234: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129389s
Feb 27 02:21:37.234: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 02:21:37.234: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 02:21:37.241
Feb 27 02:21:37.255: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 02/27/23 02:21:37.255
Feb 27 02:21:37.255: INFO: Creating new exec pod
Feb 27 02:21:37.263: INFO: Waiting up to 5m0s for pod "execpodpdljn" in namespace "services-3742" to be "running"
Feb 27 02:21:37.265: INFO: Pod "execpodpdljn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123905ms
Feb 27 02:21:39.269: INFO: Pod "execpodpdljn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006702787s
Feb 27 02:21:39.269: INFO: Pod "execpodpdljn" satisfied condition "running"
Feb 27 02:21:40.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Feb 27 02:21:40.430: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Feb 27 02:21:40.430: INFO: stdout: ""
Feb 27 02:21:40.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 10.107.76.36 80'
Feb 27 02:21:40.588: INFO: stderr: "+ nc -v -z -w 2 10.107.76.36 80\nConnection to 10.107.76.36 80 port [tcp/http] succeeded!\n"
Feb 27 02:21:40.588: INFO: stdout: ""
Feb 27 02:21:40.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Feb 27 02:21:40.717: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Feb 27 02:21:40.717: INFO: stdout: ""
Feb 27 02:21:40.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 10.107.76.36 81'
Feb 27 02:21:40.875: INFO: stderr: "+ nc -v -z -w 2 10.107.76.36 81\nConnection to 10.107.76.36 81 port [tcp/*] succeeded!\n"
Feb 27 02:21:40.876: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3742 02/27/23 02:21:40.876
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod2:[101]] 02/27/23 02:21:40.9
Feb 27 02:21:40.921: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3742 02/27/23 02:21:40.921
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[] 02/27/23 02:21:40.95
Feb 27 02:21:41.965: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:41.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3742" for this suite. 02/27/23 02:21:42
------------------------------
• [SLOW TEST] [9.923 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:32.087
    Feb 27 02:21:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:21:32.088
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:32.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:32.112
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-3742 02/27/23 02:21:32.114
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[] 02/27/23 02:21:32.128
    Feb 27 02:21:32.137: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Feb 27 02:21:33.150: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3742 02/27/23 02:21:33.15
    Feb 27 02:21:33.194: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3742" to be "running and ready"
    Feb 27 02:21:33.201: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.158741ms
    Feb 27 02:21:33.201: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:21:35.206: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01186591s
    Feb 27 02:21:35.206: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 02:21:35.206: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod1:[100]] 02/27/23 02:21:35.209
    Feb 27 02:21:35.218: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3742 02/27/23 02:21:35.218
    Feb 27 02:21:35.226: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3742" to be "running and ready"
    Feb 27 02:21:35.230: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281391ms
    Feb 27 02:21:35.230: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:21:37.234: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008129389s
    Feb 27 02:21:37.234: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 02:21:37.234: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod1:[100] pod2:[101]] 02/27/23 02:21:37.241
    Feb 27 02:21:37.255: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 02/27/23 02:21:37.255
    Feb 27 02:21:37.255: INFO: Creating new exec pod
    Feb 27 02:21:37.263: INFO: Waiting up to 5m0s for pod "execpodpdljn" in namespace "services-3742" to be "running"
    Feb 27 02:21:37.265: INFO: Pod "execpodpdljn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123905ms
    Feb 27 02:21:39.269: INFO: Pod "execpodpdljn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006702787s
    Feb 27 02:21:39.269: INFO: Pod "execpodpdljn" satisfied condition "running"
    Feb 27 02:21:40.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Feb 27 02:21:40.430: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Feb 27 02:21:40.430: INFO: stdout: ""
    Feb 27 02:21:40.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 10.107.76.36 80'
    Feb 27 02:21:40.588: INFO: stderr: "+ nc -v -z -w 2 10.107.76.36 80\nConnection to 10.107.76.36 80 port [tcp/http] succeeded!\n"
    Feb 27 02:21:40.588: INFO: stdout: ""
    Feb 27 02:21:40.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Feb 27 02:21:40.717: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Feb 27 02:21:40.717: INFO: stdout: ""
    Feb 27 02:21:40.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-3742 exec execpodpdljn -- /bin/sh -x -c nc -v -z -w 2 10.107.76.36 81'
    Feb 27 02:21:40.875: INFO: stderr: "+ nc -v -z -w 2 10.107.76.36 81\nConnection to 10.107.76.36 81 port [tcp/*] succeeded!\n"
    Feb 27 02:21:40.876: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3742 02/27/23 02:21:40.876
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[pod2:[101]] 02/27/23 02:21:40.9
    Feb 27 02:21:40.921: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3742 02/27/23 02:21:40.921
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3742 to expose endpoints map[] 02/27/23 02:21:40.95
    Feb 27 02:21:41.965: INFO: successfully validated that service multi-endpoint-test in namespace services-3742 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:41.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3742" for this suite. 02/27/23 02:21:42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:42.011
Feb 27 02:21:42.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:21:42.012
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:42.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:42.041
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 02/27/23 02:21:42.047
STEP: waiting for available Endpoint 02/27/23 02:21:42.052
STEP: listing all Endpoints 02/27/23 02:21:42.053
STEP: updating the Endpoint 02/27/23 02:21:42.062
STEP: fetching the Endpoint 02/27/23 02:21:42.071
STEP: patching the Endpoint 02/27/23 02:21:42.077
STEP: fetching the Endpoint 02/27/23 02:21:42.088
STEP: deleting the Endpoint by Collection 02/27/23 02:21:42.091
STEP: waiting for Endpoint deletion 02/27/23 02:21:42.101
STEP: fetching the Endpoint 02/27/23 02:21:42.103
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:42.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5366" for this suite. 02/27/23 02:21:42.111
------------------------------
• [0.109 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:42.011
    Feb 27 02:21:42.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:21:42.012
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:42.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:42.041
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 02/27/23 02:21:42.047
    STEP: waiting for available Endpoint 02/27/23 02:21:42.052
    STEP: listing all Endpoints 02/27/23 02:21:42.053
    STEP: updating the Endpoint 02/27/23 02:21:42.062
    STEP: fetching the Endpoint 02/27/23 02:21:42.071
    STEP: patching the Endpoint 02/27/23 02:21:42.077
    STEP: fetching the Endpoint 02/27/23 02:21:42.088
    STEP: deleting the Endpoint by Collection 02/27/23 02:21:42.091
    STEP: waiting for Endpoint deletion 02/27/23 02:21:42.101
    STEP: fetching the Endpoint 02/27/23 02:21:42.103
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:42.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5366" for this suite. 02/27/23 02:21:42.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:42.12
Feb 27 02:21:42.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:21:42.121
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:42.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:42.146
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:21:42.172
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:21:42.496
STEP: Deploying the webhook pod 02/27/23 02:21:42.507
STEP: Wait for the deployment to be ready 02/27/23 02:21:42.523
Feb 27 02:21:42.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:21:44.616
STEP: Verifying the service has paired with the endpoint 02/27/23 02:21:44.703
Feb 27 02:21:45.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 02:21:45.708
STEP: create a configmap that should be updated by the webhook 02/27/23 02:21:45.724
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:45.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3779" for this suite. 02/27/23 02:21:45.811
STEP: Destroying namespace "webhook-3779-markers" for this suite. 02/27/23 02:21:45.822
------------------------------
• [3.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:42.12
    Feb 27 02:21:42.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:21:42.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:42.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:42.146
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:21:42.172
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:21:42.496
    STEP: Deploying the webhook pod 02/27/23 02:21:42.507
    STEP: Wait for the deployment to be ready 02/27/23 02:21:42.523
    Feb 27 02:21:42.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:21:44.616
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:21:44.703
    Feb 27 02:21:45.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 02/27/23 02:21:45.708
    STEP: create a configmap that should be updated by the webhook 02/27/23 02:21:45.724
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:45.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3779" for this suite. 02/27/23 02:21:45.811
    STEP: Destroying namespace "webhook-3779-markers" for this suite. 02/27/23 02:21:45.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:45.845
Feb 27 02:21:45.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:21:45.846
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:45.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:45.872
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-a63b0b80-47ac-4be4-9a0d-acc076b9bf67 02/27/23 02:21:45.874
STEP: Creating secret with name secret-projected-all-test-volume-ff3bae8e-6758-44db-a71f-cc629b4d85f6 02/27/23 02:21:45.879
STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 02:21:45.884
Feb 27 02:21:45.918: INFO: Waiting up to 5m0s for pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80" in namespace "projected-193" to be "Succeeded or Failed"
Feb 27 02:21:45.920: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288128ms
Feb 27 02:21:47.925: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006947032s
Feb 27 02:21:49.926: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008601197s
STEP: Saw pod success 02/27/23 02:21:49.926
Feb 27 02:21:49.927: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80" satisfied condition "Succeeded or Failed"
Feb 27 02:21:49.931: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 container projected-all-volume-test: <nil>
STEP: delete the pod 02/27/23 02:21:49.948
Feb 27 02:21:49.968: INFO: Waiting for pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 to disappear
Feb 27 02:21:49.971: INFO: Pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Feb 27 02:21:49.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-193" for this suite. 02/27/23 02:21:49.975
------------------------------
• [4.137 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:45.845
    Feb 27 02:21:45.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:21:45.846
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:45.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:45.872
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-a63b0b80-47ac-4be4-9a0d-acc076b9bf67 02/27/23 02:21:45.874
    STEP: Creating secret with name secret-projected-all-test-volume-ff3bae8e-6758-44db-a71f-cc629b4d85f6 02/27/23 02:21:45.879
    STEP: Creating a pod to test Check all projections for projected volume plugin 02/27/23 02:21:45.884
    Feb 27 02:21:45.918: INFO: Waiting up to 5m0s for pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80" in namespace "projected-193" to be "Succeeded or Failed"
    Feb 27 02:21:45.920: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288128ms
    Feb 27 02:21:47.925: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006947032s
    Feb 27 02:21:49.926: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008601197s
    STEP: Saw pod success 02/27/23 02:21:49.926
    Feb 27 02:21:49.927: INFO: Pod "projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80" satisfied condition "Succeeded or Failed"
    Feb 27 02:21:49.931: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 container projected-all-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:21:49.948
    Feb 27 02:21:49.968: INFO: Waiting for pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 to disappear
    Feb 27 02:21:49.971: INFO: Pod projected-volume-6fc7471e-d1bc-411d-a3b5-e589b49d2d80 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:21:49.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-193" for this suite. 02/27/23 02:21:49.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:21:49.983
Feb 27 02:21:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:21:49.983
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:50.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:50.011
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 02/27/23 02:21:50.017
STEP: Creating a ResourceQuota 02/27/23 02:21:55.02
STEP: Ensuring resource quota status is calculated 02/27/23 02:21:55.026
STEP: Creating a ReplicationController 02/27/23 02:21:57.03
STEP: Ensuring resource quota status captures replication controller creation 02/27/23 02:21:57.047
STEP: Deleting a ReplicationController 02/27/23 02:21:59.052
STEP: Ensuring resource quota status released usage 02/27/23 02:21:59.06
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:22:01.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7785" for this suite. 02/27/23 02:22:01.069
------------------------------
• [SLOW TEST] [11.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:21:49.983
    Feb 27 02:21:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:21:49.983
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:21:50.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:21:50.011
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 02/27/23 02:21:50.017
    STEP: Creating a ResourceQuota 02/27/23 02:21:55.02
    STEP: Ensuring resource quota status is calculated 02/27/23 02:21:55.026
    STEP: Creating a ReplicationController 02/27/23 02:21:57.03
    STEP: Ensuring resource quota status captures replication controller creation 02/27/23 02:21:57.047
    STEP: Deleting a ReplicationController 02/27/23 02:21:59.052
    STEP: Ensuring resource quota status released usage 02/27/23 02:21:59.06
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:22:01.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7785" for this suite. 02/27/23 02:22:01.069
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:22:01.075
Feb 27 02:22:01.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:22:01.075
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:01.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:01.109
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 02:22:01.115
Feb 27 02:22:01.149: INFO: Waiting up to 5m0s for pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d" in namespace "emptydir-7743" to be "Succeeded or Failed"
Feb 27 02:22:01.154: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.746059ms
Feb 27 02:22:03.161: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011277124s
Feb 27 02:22:05.158: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008861531s
STEP: Saw pod success 02/27/23 02:22:05.158
Feb 27 02:22:05.158: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d" satisfied condition "Succeeded or Failed"
Feb 27 02:22:05.161: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d container test-container: <nil>
STEP: delete the pod 02/27/23 02:22:05.169
Feb 27 02:22:05.181: INFO: Waiting for pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d to disappear
Feb 27 02:22:05.185: INFO: Pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:22:05.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7743" for this suite. 02/27/23 02:22:05.189
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:22:01.075
    Feb 27 02:22:01.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:22:01.075
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:01.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:01.109
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 02/27/23 02:22:01.115
    Feb 27 02:22:01.149: INFO: Waiting up to 5m0s for pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d" in namespace "emptydir-7743" to be "Succeeded or Failed"
    Feb 27 02:22:01.154: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.746059ms
    Feb 27 02:22:03.161: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011277124s
    Feb 27 02:22:05.158: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008861531s
    STEP: Saw pod success 02/27/23 02:22:05.158
    Feb 27 02:22:05.158: INFO: Pod "pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d" satisfied condition "Succeeded or Failed"
    Feb 27 02:22:05.161: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d container test-container: <nil>
    STEP: delete the pod 02/27/23 02:22:05.169
    Feb 27 02:22:05.181: INFO: Waiting for pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d to disappear
    Feb 27 02:22:05.185: INFO: Pod pod-2d10fe32-c8d0-417c-b260-42bb30a87b7d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:22:05.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7743" for this suite. 02/27/23 02:22:05.189
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:22:05.196
Feb 27 02:22:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:22:05.197
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:05.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:05.221
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:22:05.224
Feb 27 02:22:05.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26" in namespace "downward-api-7323" to be "Succeeded or Failed"
Feb 27 02:22:05.247: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322905ms
Feb 27 02:22:07.252: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014734743s
Feb 27 02:22:09.250: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012501978s
STEP: Saw pod success 02/27/23 02:22:09.25
Feb 27 02:22:09.250: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26" satisfied condition "Succeeded or Failed"
Feb 27 02:22:09.252: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 container client-container: <nil>
STEP: delete the pod 02/27/23 02:22:09.257
Feb 27 02:22:09.268: INFO: Waiting for pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 to disappear
Feb 27 02:22:09.271: INFO: Pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:22:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7323" for this suite. 02/27/23 02:22:09.276
------------------------------
• [4.086 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:22:05.196
    Feb 27 02:22:05.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:22:05.197
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:05.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:05.221
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:22:05.224
    Feb 27 02:22:05.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26" in namespace "downward-api-7323" to be "Succeeded or Failed"
    Feb 27 02:22:05.247: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Pending", Reason="", readiness=false. Elapsed: 9.322905ms
    Feb 27 02:22:07.252: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014734743s
    Feb 27 02:22:09.250: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012501978s
    STEP: Saw pod success 02/27/23 02:22:09.25
    Feb 27 02:22:09.250: INFO: Pod "downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26" satisfied condition "Succeeded or Failed"
    Feb 27 02:22:09.252: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:22:09.257
    Feb 27 02:22:09.268: INFO: Waiting for pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 to disappear
    Feb 27 02:22:09.271: INFO: Pod downwardapi-volume-0fb3fe90-a846-46cf-b9a4-8fbab5173e26 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:22:09.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7323" for this suite. 02/27/23 02:22:09.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:22:09.283
Feb 27 02:22:09.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:22:09.284
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:09.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:09.313
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2314 02/27/23 02:22:09.316
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 02/27/23 02:22:09.323
Feb 27 02:22:09.342: INFO: Found 0 stateful pods, waiting for 3
Feb 27 02:22:19.347: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:22:19.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:22:19.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-4 02/27/23 02:22:19.358
Feb 27 02:22:19.390: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 02:22:19.39
STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 02:22:29.411
STEP: Performing a canary update 02/27/23 02:22:29.411
Feb 27 02:22:29.433: INFO: Updating stateful set ss2
Feb 27 02:22:29.530: INFO: Waiting for Pod statefulset-2314/ss2-2 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 02:22:39.539
Feb 27 02:22:39.580: INFO: Found 1 stateful pods, waiting for 3
Feb 27 02:22:49.586: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:22:49.586: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:22:49.586: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 02/27/23 02:22:49.594
Feb 27 02:22:49.616: INFO: Updating stateful set ss2
Feb 27 02:22:49.624: INFO: Waiting for Pod statefulset-2314/ss2-1 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
Feb 27 02:22:59.655: INFO: Updating stateful set ss2
Feb 27 02:22:59.664: INFO: Waiting for StatefulSet statefulset-2314/ss2 to complete update
Feb 27 02:22:59.665: INFO: Waiting for Pod statefulset-2314/ss2-0 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:23:09.675: INFO: Deleting all statefulset in ns statefulset-2314
Feb 27 02:23:09.678: INFO: Scaling statefulset ss2 to 0
Feb 27 02:23:19.695: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:23:19.698: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2314" for this suite. 02/27/23 02:23:19.718
------------------------------
• [SLOW TEST] [70.447 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:22:09.283
    Feb 27 02:22:09.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:22:09.284
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:22:09.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:22:09.313
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2314 02/27/23 02:22:09.316
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 02/27/23 02:22:09.323
    Feb 27 02:22:09.342: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 02:22:19.347: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:22:19.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:22:19.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-4 02/27/23 02:22:19.358
    Feb 27 02:22:19.390: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 02:22:19.39
    STEP: Not applying an update when the partition is greater than the number of replicas 02/27/23 02:22:29.411
    STEP: Performing a canary update 02/27/23 02:22:29.411
    Feb 27 02:22:29.433: INFO: Updating stateful set ss2
    Feb 27 02:22:29.530: INFO: Waiting for Pod statefulset-2314/ss2-2 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
    STEP: Restoring Pods to the correct revision when they are deleted 02/27/23 02:22:39.539
    Feb 27 02:22:39.580: INFO: Found 1 stateful pods, waiting for 3
    Feb 27 02:22:49.586: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:22:49.586: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:22:49.586: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 02/27/23 02:22:49.594
    Feb 27 02:22:49.616: INFO: Updating stateful set ss2
    Feb 27 02:22:49.624: INFO: Waiting for Pod statefulset-2314/ss2-1 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
    Feb 27 02:22:59.655: INFO: Updating stateful set ss2
    Feb 27 02:22:59.664: INFO: Waiting for StatefulSet statefulset-2314/ss2 to complete update
    Feb 27 02:22:59.665: INFO: Waiting for Pod statefulset-2314/ss2-0 to have revision ss2-5c57c56cff update revision ss2-7df77bccb6
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:23:09.675: INFO: Deleting all statefulset in ns statefulset-2314
    Feb 27 02:23:09.678: INFO: Scaling statefulset ss2 to 0
    Feb 27 02:23:19.695: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:23:19.698: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:19.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2314" for this suite. 02/27/23 02:23:19.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:19.731
Feb 27 02:23:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:23:19.732
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:19.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:19.751
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 02/27/23 02:23:19.753
Feb 27 02:23:19.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 create -f -'
Feb 27 02:23:20.480: INFO: stderr: ""
Feb 27 02:23:20.480: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 02/27/23 02:23:20.48
Feb 27 02:23:20.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 diff -f -'
Feb 27 02:23:20.646: INFO: rc: 1
Feb 27 02:23:20.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 delete -f -'
Feb 27 02:23:20.711: INFO: stderr: ""
Feb 27 02:23:20.711: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:20.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3121" for this suite. 02/27/23 02:23:20.718
------------------------------
• [0.992 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:19.731
    Feb 27 02:23:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:23:19.732
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:19.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:19.751
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 02/27/23 02:23:19.753
    Feb 27 02:23:19.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 create -f -'
    Feb 27 02:23:20.480: INFO: stderr: ""
    Feb 27 02:23:20.480: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 02/27/23 02:23:20.48
    Feb 27 02:23:20.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 diff -f -'
    Feb 27 02:23:20.646: INFO: rc: 1
    Feb 27 02:23:20.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3121 delete -f -'
    Feb 27 02:23:20.711: INFO: stderr: ""
    Feb 27 02:23:20.711: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:20.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3121" for this suite. 02/27/23 02:23:20.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:20.724
Feb 27 02:23:20.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:23:20.725
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:20.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:20.747
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 02:23:20.75
Feb 27 02:23:20.787: INFO: Waiting up to 5m0s for pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b" in namespace "emptydir-4191" to be "Succeeded or Failed"
Feb 27 02:23:20.793: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272571ms
Feb 27 02:23:22.799: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01212023s
Feb 27 02:23:24.796: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009666414s
STEP: Saw pod success 02/27/23 02:23:24.796
Feb 27 02:23:24.796: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b" satisfied condition "Succeeded or Failed"
Feb 27 02:23:24.800: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b container test-container: <nil>
STEP: delete the pod 02/27/23 02:23:24.805
Feb 27 02:23:24.815: INFO: Waiting for pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b to disappear
Feb 27 02:23:24.818: INFO: Pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:24.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4191" for this suite. 02/27/23 02:23:24.825
------------------------------
• [4.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:20.724
    Feb 27 02:23:20.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:23:20.725
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:20.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:20.747
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 02:23:20.75
    Feb 27 02:23:20.787: INFO: Waiting up to 5m0s for pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b" in namespace "emptydir-4191" to be "Succeeded or Failed"
    Feb 27 02:23:20.793: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.272571ms
    Feb 27 02:23:22.799: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01212023s
    Feb 27 02:23:24.796: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009666414s
    STEP: Saw pod success 02/27/23 02:23:24.796
    Feb 27 02:23:24.796: INFO: Pod "pod-3db6af0b-fa60-470c-8d04-db94ad05653b" satisfied condition "Succeeded or Failed"
    Feb 27 02:23:24.800: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b container test-container: <nil>
    STEP: delete the pod 02/27/23 02:23:24.805
    Feb 27 02:23:24.815: INFO: Waiting for pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b to disappear
    Feb 27 02:23:24.818: INFO: Pod pod-3db6af0b-fa60-470c-8d04-db94ad05653b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:24.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4191" for this suite. 02/27/23 02:23:24.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:24.831
Feb 27 02:23:24.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename podtemplate 02/27/23 02:23:24.832
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:24.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:24.854
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 02/27/23 02:23:24.856
Feb 27 02:23:24.866: INFO: created test-podtemplate-1
Feb 27 02:23:24.870: INFO: created test-podtemplate-2
Feb 27 02:23:24.875: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 02/27/23 02:23:24.875
STEP: delete collection of pod templates 02/27/23 02:23:24.877
Feb 27 02:23:24.877: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 02/27/23 02:23:24.893
Feb 27 02:23:24.894: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7905" for this suite. 02/27/23 02:23:24.902
------------------------------
• [0.078 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:24.831
    Feb 27 02:23:24.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename podtemplate 02/27/23 02:23:24.832
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:24.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:24.854
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 02/27/23 02:23:24.856
    Feb 27 02:23:24.866: INFO: created test-podtemplate-1
    Feb 27 02:23:24.870: INFO: created test-podtemplate-2
    Feb 27 02:23:24.875: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 02/27/23 02:23:24.875
    STEP: delete collection of pod templates 02/27/23 02:23:24.877
    Feb 27 02:23:24.877: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 02/27/23 02:23:24.893
    Feb 27 02:23:24.894: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7905" for this suite. 02/27/23 02:23:24.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:24.909
Feb 27 02:23:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename events 02/27/23 02:23:24.91
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:24.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:24.936
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 02/27/23 02:23:24.939
STEP: listing all events in all namespaces 02/27/23 02:23:24.943
STEP: patching the test event 02/27/23 02:23:24.948
STEP: fetching the test event 02/27/23 02:23:24.954
STEP: updating the test event 02/27/23 02:23:24.957
STEP: getting the test event 02/27/23 02:23:24.968
STEP: deleting the test event 02/27/23 02:23:24.971
STEP: listing all events in all namespaces 02/27/23 02:23:24.977
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8662" for this suite. 02/27/23 02:23:24.987
------------------------------
• [0.084 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:24.909
    Feb 27 02:23:24.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename events 02/27/23 02:23:24.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:24.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:24.936
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 02/27/23 02:23:24.939
    STEP: listing all events in all namespaces 02/27/23 02:23:24.943
    STEP: patching the test event 02/27/23 02:23:24.948
    STEP: fetching the test event 02/27/23 02:23:24.954
    STEP: updating the test event 02/27/23 02:23:24.957
    STEP: getting the test event 02/27/23 02:23:24.968
    STEP: deleting the test event 02/27/23 02:23:24.971
    STEP: listing all events in all namespaces 02/27/23 02:23:24.977
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:24.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8662" for this suite. 02/27/23 02:23:24.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:24.995
Feb 27 02:23:24.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename proxy 02/27/23 02:23:24.996
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:25.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:25.02
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Feb 27 02:23:25.022: INFO: Creating pod...
Feb 27 02:23:25.037: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6681" to be "running"
Feb 27 02:23:25.040: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.619326ms
Feb 27 02:23:27.045: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007770152s
Feb 27 02:23:27.045: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 02:23:27.045: INFO: Creating service...
Feb 27 02:23:27.059: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=DELETE
Feb 27 02:23:27.065: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 02:23:27.065: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=OPTIONS
Feb 27 02:23:27.072: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 02:23:27.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=PATCH
Feb 27 02:23:27.079: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 02:23:27.079: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=POST
Feb 27 02:23:27.083: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 02:23:27.083: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=PUT
Feb 27 02:23:27.086: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 02:23:27.086: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=DELETE
Feb 27 02:23:27.093: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 02:23:27.093: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=OPTIONS
Feb 27 02:23:27.102: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 02:23:27.102: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=PATCH
Feb 27 02:23:27.111: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 02:23:27.111: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=POST
Feb 27 02:23:27.116: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 02:23:27.117: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=PUT
Feb 27 02:23:27.123: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 02:23:27.123: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=GET
Feb 27 02:23:27.130: INFO: http.Client request:GET StatusCode:301
Feb 27 02:23:27.130: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=GET
Feb 27 02:23:27.135: INFO: http.Client request:GET StatusCode:301
Feb 27 02:23:27.135: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=HEAD
Feb 27 02:23:27.138: INFO: http.Client request:HEAD StatusCode:301
Feb 27 02:23:27.138: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=HEAD
Feb 27 02:23:27.143: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:27.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6681" for this suite. 02/27/23 02:23:27.147
------------------------------
• [2.164 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:24.995
    Feb 27 02:23:24.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename proxy 02/27/23 02:23:24.996
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:25.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:25.02
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Feb 27 02:23:25.022: INFO: Creating pod...
    Feb 27 02:23:25.037: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6681" to be "running"
    Feb 27 02:23:25.040: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.619326ms
    Feb 27 02:23:27.045: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007770152s
    Feb 27 02:23:27.045: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 02:23:27.045: INFO: Creating service...
    Feb 27 02:23:27.059: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=DELETE
    Feb 27 02:23:27.065: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 02:23:27.065: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=OPTIONS
    Feb 27 02:23:27.072: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 02:23:27.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=PATCH
    Feb 27 02:23:27.079: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 02:23:27.079: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=POST
    Feb 27 02:23:27.083: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 02:23:27.083: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=PUT
    Feb 27 02:23:27.086: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 02:23:27.086: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=DELETE
    Feb 27 02:23:27.093: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 02:23:27.093: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Feb 27 02:23:27.102: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 02:23:27.102: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=PATCH
    Feb 27 02:23:27.111: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 02:23:27.111: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=POST
    Feb 27 02:23:27.116: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 02:23:27.117: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=PUT
    Feb 27 02:23:27.123: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 02:23:27.123: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=GET
    Feb 27 02:23:27.130: INFO: http.Client request:GET StatusCode:301
    Feb 27 02:23:27.130: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=GET
    Feb 27 02:23:27.135: INFO: http.Client request:GET StatusCode:301
    Feb 27 02:23:27.135: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/pods/agnhost/proxy?method=HEAD
    Feb 27 02:23:27.138: INFO: http.Client request:HEAD StatusCode:301
    Feb 27 02:23:27.138: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6681/services/e2e-proxy-test-service/proxy?method=HEAD
    Feb 27 02:23:27.143: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:27.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6681" for this suite. 02/27/23 02:23:27.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:27.16
Feb 27 02:23:27.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:23:27.161
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:27.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:27.188
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9901 02/27/23 02:23:27.19
STEP: creating service affinity-nodeport in namespace services-9901 02/27/23 02:23:27.19
STEP: creating replication controller affinity-nodeport in namespace services-9901 02/27/23 02:23:27.219
I0227 02:23:27.235154      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9901, replica count: 3
I0227 02:23:30.286148      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:23:30.296: INFO: Creating new exec pod
Feb 27 02:23:30.335: INFO: Waiting up to 5m0s for pod "execpod-affinity2c2xm" in namespace "services-9901" to be "running"
Feb 27 02:23:30.340: INFO: Pod "execpod-affinity2c2xm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789258ms
Feb 27 02:23:32.344: INFO: Pod "execpod-affinity2c2xm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008938682s
Feb 27 02:23:32.344: INFO: Pod "execpod-affinity2c2xm" satisfied condition "running"
Feb 27 02:23:33.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Feb 27 02:23:33.487: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Feb 27 02:23:33.487: INFO: stdout: ""
Feb 27 02:23:33.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.99.196.118 80'
Feb 27 02:23:33.620: INFO: stderr: "+ nc -v -z -w 2 10.99.196.118 80\nConnection to 10.99.196.118 80 port [tcp/http] succeeded!\n"
Feb 27 02:23:33.620: INFO: stdout: ""
Feb 27 02:23:33.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 31114'
Feb 27 02:23:33.760: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 31114\nConnection to 10.0.10.3 31114 port [tcp/*] succeeded!\n"
Feb 27 02:23:33.760: INFO: stdout: ""
Feb 27 02:23:33.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 31114'
Feb 27 02:23:33.904: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 31114\nConnection to 10.0.10.22 31114 port [tcp/*] succeeded!\n"
Feb 27 02:23:33.904: INFO: stdout: ""
Feb 27 02:23:33.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:31114/ ; done'
Feb 27 02:23:34.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n"
Feb 27 02:23:34.120: INFO: stdout: "\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps"
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
Feb 27 02:23:34.120: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9901, will wait for the garbage collector to delete the pods 02/27/23 02:23:34.133
Feb 27 02:23:34.198: INFO: Deleting ReplicationController affinity-nodeport took: 8.594959ms
Feb 27 02:23:34.299: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.099311ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:36.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9901" for this suite. 02/27/23 02:23:36.438
------------------------------
• [SLOW TEST] [9.290 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:27.16
    Feb 27 02:23:27.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:23:27.161
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:27.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:27.188
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9901 02/27/23 02:23:27.19
    STEP: creating service affinity-nodeport in namespace services-9901 02/27/23 02:23:27.19
    STEP: creating replication controller affinity-nodeport in namespace services-9901 02/27/23 02:23:27.219
    I0227 02:23:27.235154      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9901, replica count: 3
    I0227 02:23:30.286148      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:23:30.296: INFO: Creating new exec pod
    Feb 27 02:23:30.335: INFO: Waiting up to 5m0s for pod "execpod-affinity2c2xm" in namespace "services-9901" to be "running"
    Feb 27 02:23:30.340: INFO: Pod "execpod-affinity2c2xm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789258ms
    Feb 27 02:23:32.344: INFO: Pod "execpod-affinity2c2xm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008938682s
    Feb 27 02:23:32.344: INFO: Pod "execpod-affinity2c2xm" satisfied condition "running"
    Feb 27 02:23:33.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Feb 27 02:23:33.487: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Feb 27 02:23:33.487: INFO: stdout: ""
    Feb 27 02:23:33.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.99.196.118 80'
    Feb 27 02:23:33.620: INFO: stderr: "+ nc -v -z -w 2 10.99.196.118 80\nConnection to 10.99.196.118 80 port [tcp/http] succeeded!\n"
    Feb 27 02:23:33.620: INFO: stdout: ""
    Feb 27 02:23:33.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 31114'
    Feb 27 02:23:33.760: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 31114\nConnection to 10.0.10.3 31114 port [tcp/*] succeeded!\n"
    Feb 27 02:23:33.760: INFO: stdout: ""
    Feb 27 02:23:33.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 31114'
    Feb 27 02:23:33.904: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 31114\nConnection to 10.0.10.22 31114 port [tcp/*] succeeded!\n"
    Feb 27 02:23:33.904: INFO: stdout: ""
    Feb 27 02:23:33.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9901 exec execpod-affinity2c2xm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.10.15:31114/ ; done'
    Feb 27 02:23:34.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.10.15:31114/\n"
    Feb 27 02:23:34.120: INFO: stdout: "\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps\naffinity-nodeport-89cps"
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Received response from host: affinity-nodeport-89cps
    Feb 27 02:23:34.120: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9901, will wait for the garbage collector to delete the pods 02/27/23 02:23:34.133
    Feb 27 02:23:34.198: INFO: Deleting ReplicationController affinity-nodeport took: 8.594959ms
    Feb 27 02:23:34.299: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.099311ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:36.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9901" for this suite. 02/27/23 02:23:36.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:36.45
Feb 27 02:23:36.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:23:36.451
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:36.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:36.483
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 02:23:36.486
Feb 27 02:23:36.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 02:23:43.175
Feb 27 02:23:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:23:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:52.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5345" for this suite. 02/27/23 02:23:52.238
------------------------------
• [SLOW TEST] [15.794 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:36.45
    Feb 27 02:23:36.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:23:36.451
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:36.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:36.483
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 02/27/23 02:23:36.486
    Feb 27 02:23:36.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 02/27/23 02:23:43.175
    Feb 27 02:23:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:23:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:52.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5345" for this suite. 02/27/23 02:23:52.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:52.244
Feb 27 02:23:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:23:52.245
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:52.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:52.262
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Feb 27 02:23:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:52.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9461" for this suite. 02/27/23 02:23:52.817
------------------------------
• [0.586 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:52.244
    Feb 27 02:23:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:23:52.245
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:52.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:52.262
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Feb 27 02:23:52.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:52.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9461" for this suite. 02/27/23 02:23:52.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:52.834
Feb 27 02:23:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:23:52.835
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:52.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:52.857
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-1f9029db-d1cb-48e0-8169-f95a461cac3f 02/27/23 02:23:52.864
STEP: Creating the pod 02/27/23 02:23:52.869
Feb 27 02:23:52.900: INFO: Waiting up to 5m0s for pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c" in namespace "configmap-5638" to be "running and ready"
Feb 27 02:23:52.904: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271801ms
Feb 27 02:23:52.904: INFO: The phase of Pod pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:23:54.908: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007781091s
Feb 27 02:23:54.908: INFO: The phase of Pod pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c is Running (Ready = true)
Feb 27 02:23:54.908: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-1f9029db-d1cb-48e0-8169-f95a461cac3f 02/27/23 02:23:54.916
STEP: waiting to observe update in volume 02/27/23 02:23:54.92
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:23:56.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5638" for this suite. 02/27/23 02:23:56.94
------------------------------
• [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:52.834
    Feb 27 02:23:52.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:23:52.835
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:52.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:52.857
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-1f9029db-d1cb-48e0-8169-f95a461cac3f 02/27/23 02:23:52.864
    STEP: Creating the pod 02/27/23 02:23:52.869
    Feb 27 02:23:52.900: INFO: Waiting up to 5m0s for pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c" in namespace "configmap-5638" to be "running and ready"
    Feb 27 02:23:52.904: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271801ms
    Feb 27 02:23:52.904: INFO: The phase of Pod pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:23:54.908: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007781091s
    Feb 27 02:23:54.908: INFO: The phase of Pod pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c is Running (Ready = true)
    Feb 27 02:23:54.908: INFO: Pod "pod-configmaps-0059c0da-1c97-439b-b9ed-cb71a65fb74c" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-1f9029db-d1cb-48e0-8169-f95a461cac3f 02/27/23 02:23:54.916
    STEP: waiting to observe update in volume 02/27/23 02:23:54.92
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:23:56.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5638" for this suite. 02/27/23 02:23:56.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:23:56.947
Feb 27 02:23:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:23:56.948
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:56.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:56.971
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4269 02/27/23 02:23:56.973
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-4269 02/27/23 02:23:56.978
Feb 27 02:23:56.987: INFO: Found 0 stateful pods, waiting for 1
Feb 27 02:24:06.992: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 02/27/23 02:24:06.998
STEP: updating a scale subresource 02/27/23 02:24:07
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 02:24:07.009
STEP: Patch a scale subresource 02/27/23 02:24:07.013
STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 02:24:07.02
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:24:07.023: INFO: Deleting all statefulset in ns statefulset-4269
Feb 27 02:24:07.026: INFO: Scaling statefulset ss to 0
Feb 27 02:24:17.047: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:24:17.051: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:24:17.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4269" for this suite. 02/27/23 02:24:17.07
------------------------------
• [SLOW TEST] [20.130 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:23:56.947
    Feb 27 02:23:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:23:56.948
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:23:56.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:23:56.971
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4269 02/27/23 02:23:56.973
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-4269 02/27/23 02:23:56.978
    Feb 27 02:23:56.987: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 02:24:06.992: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 02/27/23 02:24:06.998
    STEP: updating a scale subresource 02/27/23 02:24:07
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 02:24:07.009
    STEP: Patch a scale subresource 02/27/23 02:24:07.013
    STEP: verifying the statefulset Spec.Replicas was modified 02/27/23 02:24:07.02
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:24:07.023: INFO: Deleting all statefulset in ns statefulset-4269
    Feb 27 02:24:07.026: INFO: Scaling statefulset ss to 0
    Feb 27 02:24:17.047: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:24:17.051: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:24:17.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4269" for this suite. 02/27/23 02:24:17.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:24:17.077
Feb 27 02:24:17.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:24:17.078
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:24:17.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:24:17.099
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-6rmhc" 02/27/23 02:24:17.105
Feb 27 02:24:17.112: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard cpu limit of 500m
Feb 27 02:24:17.112: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.112
STEP: Confirm /status for "e2e-rq-status-6rmhc" resourceQuota via watch 02/27/23 02:24:17.12
Feb 27 02:24:17.121: INFO: observed resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList(nil)
Feb 27 02:24:17.122: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 27 02:24:17.122: INFO: ResourceQuota "e2e-rq-status-6rmhc" /status was updated
STEP: Patching hard spec values for cpu & memory 02/27/23 02:24:17.124
Feb 27 02:24:17.130: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard cpu limit of 1
Feb 27 02:24:17.130: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.13
STEP: Confirm /status for "e2e-rq-status-6rmhc" resourceQuota via watch 02/27/23 02:24:17.136
Feb 27 02:24:17.137: INFO: observed resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Feb 27 02:24:17.137: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Feb 27 02:24:17.137: INFO: ResourceQuota "e2e-rq-status-6rmhc" /status was patched
STEP: Get "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.137
Feb 27 02:24:17.140: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard cpu of 1
Feb 27 02:24:17.140: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-6rmhc" /status before checking Spec is unchanged 02/27/23 02:24:17.142
Feb 27 02:24:17.149: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard cpu of 2
Feb 27 02:24:17.149: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard memory of 2Gi
Feb 27 02:24:17.150: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Feb 27 02:24:22.157: INFO: ResourceQuota "e2e-rq-status-6rmhc" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:24:22.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9943" for this suite. 02/27/23 02:24:22.161
------------------------------
• [SLOW TEST] [5.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:24:17.077
    Feb 27 02:24:17.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:24:17.078
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:24:17.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:24:17.099
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-6rmhc" 02/27/23 02:24:17.105
    Feb 27 02:24:17.112: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard cpu limit of 500m
    Feb 27 02:24:17.112: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.112
    STEP: Confirm /status for "e2e-rq-status-6rmhc" resourceQuota via watch 02/27/23 02:24:17.12
    Feb 27 02:24:17.121: INFO: observed resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList(nil)
    Feb 27 02:24:17.122: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 27 02:24:17.122: INFO: ResourceQuota "e2e-rq-status-6rmhc" /status was updated
    STEP: Patching hard spec values for cpu & memory 02/27/23 02:24:17.124
    Feb 27 02:24:17.130: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard cpu limit of 1
    Feb 27 02:24:17.130: INFO: Resource quota "e2e-rq-status-6rmhc" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.13
    STEP: Confirm /status for "e2e-rq-status-6rmhc" resourceQuota via watch 02/27/23 02:24:17.136
    Feb 27 02:24:17.137: INFO: observed resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Feb 27 02:24:17.137: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Feb 27 02:24:17.137: INFO: ResourceQuota "e2e-rq-status-6rmhc" /status was patched
    STEP: Get "e2e-rq-status-6rmhc" /status 02/27/23 02:24:17.137
    Feb 27 02:24:17.140: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard cpu of 1
    Feb 27 02:24:17.140: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-6rmhc" /status before checking Spec is unchanged 02/27/23 02:24:17.142
    Feb 27 02:24:17.149: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard cpu of 2
    Feb 27 02:24:17.149: INFO: Resourcequota "e2e-rq-status-6rmhc" reports status: hard memory of 2Gi
    Feb 27 02:24:17.150: INFO: Found resourceQuota "e2e-rq-status-6rmhc" in namespace "resourcequota-9943" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Feb 27 02:24:22.157: INFO: ResourceQuota "e2e-rq-status-6rmhc" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:24:22.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9943" for this suite. 02/27/23 02:24:22.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:24:22.17
Feb 27 02:24:22.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:24:22.171
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:24:22.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:24:22.197
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 02:24:22.218: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 02:25:22.295: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:25:22.298
Feb 27 02:25:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 02:25:22.299
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:25:22.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:25:22.324
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Feb 27 02:25:22.341: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Feb 27 02:25:22.343: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Feb 27 02:25:22.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:25:22.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5364" for this suite. 02/27/23 02:25:22.434
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8881" for this suite. 02/27/23 02:25:22.442
------------------------------
• [SLOW TEST] [60.277 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:24:22.17
    Feb 27 02:24:22.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:24:22.171
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:24:22.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:24:22.197
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 02:24:22.218: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 02:25:22.295: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:25:22.298
    Feb 27 02:25:22.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 02:25:22.299
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:25:22.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:25:22.324
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Feb 27 02:25:22.341: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Feb 27 02:25:22.343: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:25:22.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:25:22.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5364" for this suite. 02/27/23 02:25:22.434
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8881" for this suite. 02/27/23 02:25:22.442
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:25:22.448
Feb 27 02:25:22.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 02:25:22.448
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:25:22.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:25:22.47
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 02/27/23 02:25:22.473
STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:22.727
Feb 27 02:25:22.805: INFO: Pod name wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c: Found 1 pods out of 5
Feb 27 02:25:27.816: INFO: Pod name wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 02:25:27.816
Feb 27 02:25:27.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:27.819: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250472ms
Feb 27 02:25:29.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009116312s
Feb 27 02:25:31.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009193832s
Feb 27 02:25:33.823: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007549171s
Feb 27 02:25:35.824: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008788355s
Feb 27 02:25:37.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009175101s
Feb 27 02:25:37.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx" satisfied condition "running"
Feb 27 02:25:37.825: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:37.829: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq": Phase="Running", Reason="", readiness=true. Elapsed: 3.610799ms
Feb 27 02:25:37.829: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq" satisfied condition "running"
Feb 27 02:25:37.829: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:37.832: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr": Phase="Running", Reason="", readiness=true. Elapsed: 3.167804ms
Feb 27 02:25:37.832: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr" satisfied condition "running"
Feb 27 02:25:37.832: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:37.835: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd": Phase="Running", Reason="", readiness=true. Elapsed: 2.988368ms
Feb 27 02:25:37.835: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd" satisfied condition "running"
Feb 27 02:25:37.835: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:37.838: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n": Phase="Running", Reason="", readiness=true. Elapsed: 2.794169ms
Feb 27 02:25:37.838: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:25:37.838
Feb 27 02:25:37.901: INFO: Deleting ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c took: 8.595582ms
Feb 27 02:25:38.002: INFO: Terminating ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c pods took: 101.136901ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:41.51
Feb 27 02:25:41.527: INFO: Pod name wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059: Found 0 pods out of 5
Feb 27 02:25:46.536: INFO: Pod name wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 02:25:46.536
Feb 27 02:25:46.536: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:46.540: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485997ms
Feb 27 02:25:48.547: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010425019s
Feb 27 02:25:50.546: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009658796s
Feb 27 02:25:52.544: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008191404s
Feb 27 02:25:54.545: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Running", Reason="", readiness=true. Elapsed: 8.008640312s
Feb 27 02:25:54.545: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d" satisfied condition "running"
Feb 27 02:25:54.545: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:54.548: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97": Phase="Running", Reason="", readiness=true. Elapsed: 3.300506ms
Feb 27 02:25:54.548: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97" satisfied condition "running"
Feb 27 02:25:54.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:54.559: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb": Phase="Running", Reason="", readiness=true. Elapsed: 10.95347ms
Feb 27 02:25:54.559: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb" satisfied condition "running"
Feb 27 02:25:54.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:54.562: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.93996ms
Feb 27 02:25:56.569: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010008782s
Feb 27 02:25:56.569: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b" satisfied condition "running"
Feb 27 02:25:56.569: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:25:56.575: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h": Phase="Running", Reason="", readiness=true. Elapsed: 5.842546ms
Feb 27 02:25:56.575: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:25:56.575
Feb 27 02:25:56.635: INFO: Deleting ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 took: 6.767162ms
Feb 27 02:25:56.736: INFO: Terminating ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 pods took: 101.023991ms
STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:59.442
Feb 27 02:25:59.459: INFO: Pod name wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8: Found 0 pods out of 5
Feb 27 02:26:04.468: INFO: Pod name wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8: Found 5 pods out of 5
STEP: Ensuring each pod is running 02/27/23 02:26:04.468
Feb 27 02:26:04.468: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:26:04.493: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.933479ms
Feb 27 02:26:06.497: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029207749s
Feb 27 02:26:08.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029969992s
Feb 27 02:26:10.522: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054302464s
Feb 27 02:26:12.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030336949s
Feb 27 02:26:14.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Running", Reason="", readiness=true. Elapsed: 10.029674125s
Feb 27 02:26:14.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6" satisfied condition "running"
Feb 27 02:26:14.498: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:26:14.502: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl": Phase="Running", Reason="", readiness=true. Elapsed: 3.733584ms
Feb 27 02:26:14.502: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl" satisfied condition "running"
Feb 27 02:26:14.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:26:14.504: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.783119ms
Feb 27 02:26:14.504: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp" satisfied condition "running"
Feb 27 02:26:14.504: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:26:14.507: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.747275ms
Feb 27 02:26:14.507: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6" satisfied condition "running"
Feb 27 02:26:14.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9" in namespace "emptydir-wrapper-5631" to be "running"
Feb 27 02:26:14.512: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.522425ms
Feb 27 02:26:14.512: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:26:14.512
Feb 27 02:26:14.574: INFO: Deleting ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 took: 9.69297ms
Feb 27 02:26:14.674: INFO: Terminating ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 pods took: 100.178879ms
STEP: Cleaning up the configMaps 02/27/23 02:26:17.575
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:17.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5631" for this suite. 02/27/23 02:26:17.892
------------------------------
• [SLOW TEST] [55.452 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:25:22.448
    Feb 27 02:25:22.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 02:25:22.448
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:25:22.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:25:22.47
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 02/27/23 02:25:22.473
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:22.727
    Feb 27 02:25:22.805: INFO: Pod name wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c: Found 1 pods out of 5
    Feb 27 02:25:27.816: INFO: Pod name wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 02:25:27.816
    Feb 27 02:25:27.816: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:27.819: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250472ms
    Feb 27 02:25:29.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009116312s
    Feb 27 02:25:31.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009193832s
    Feb 27 02:25:33.823: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007549171s
    Feb 27 02:25:35.824: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008788355s
    Feb 27 02:25:37.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009175101s
    Feb 27 02:25:37.825: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-l9xhx" satisfied condition "running"
    Feb 27 02:25:37.825: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:37.829: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq": Phase="Running", Reason="", readiness=true. Elapsed: 3.610799ms
    Feb 27 02:25:37.829: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-p6ttq" satisfied condition "running"
    Feb 27 02:25:37.829: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:37.832: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr": Phase="Running", Reason="", readiness=true. Elapsed: 3.167804ms
    Feb 27 02:25:37.832: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-pmkfr" satisfied condition "running"
    Feb 27 02:25:37.832: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:37.835: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd": Phase="Running", Reason="", readiness=true. Elapsed: 2.988368ms
    Feb 27 02:25:37.835: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-st5rd" satisfied condition "running"
    Feb 27 02:25:37.835: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:37.838: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n": Phase="Running", Reason="", readiness=true. Elapsed: 2.794169ms
    Feb 27 02:25:37.838: INFO: Pod "wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c-wlr7n" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:25:37.838
    Feb 27 02:25:37.901: INFO: Deleting ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c took: 8.595582ms
    Feb 27 02:25:38.002: INFO: Terminating ReplicationController wrapped-volume-race-8a0f9092-6eae-49a7-a8f6-bb40e7fe568c pods took: 101.136901ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:41.51
    Feb 27 02:25:41.527: INFO: Pod name wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059: Found 0 pods out of 5
    Feb 27 02:25:46.536: INFO: Pod name wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 02:25:46.536
    Feb 27 02:25:46.536: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:46.540: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.485997ms
    Feb 27 02:25:48.547: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010425019s
    Feb 27 02:25:50.546: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009658796s
    Feb 27 02:25:52.544: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008191404s
    Feb 27 02:25:54.545: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d": Phase="Running", Reason="", readiness=true. Elapsed: 8.008640312s
    Feb 27 02:25:54.545: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-d6q4d" satisfied condition "running"
    Feb 27 02:25:54.545: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:54.548: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97": Phase="Running", Reason="", readiness=true. Elapsed: 3.300506ms
    Feb 27 02:25:54.548: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-jzf97" satisfied condition "running"
    Feb 27 02:25:54.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:54.559: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb": Phase="Running", Reason="", readiness=true. Elapsed: 10.95347ms
    Feb 27 02:25:54.559: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-lzvrb" satisfied condition "running"
    Feb 27 02:25:54.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:54.562: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.93996ms
    Feb 27 02:25:56.569: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010008782s
    Feb 27 02:25:56.569: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-q696b" satisfied condition "running"
    Feb 27 02:25:56.569: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:25:56.575: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h": Phase="Running", Reason="", readiness=true. Elapsed: 5.842546ms
    Feb 27 02:25:56.575: INFO: Pod "wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059-qs44h" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:25:56.575
    Feb 27 02:25:56.635: INFO: Deleting ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 took: 6.767162ms
    Feb 27 02:25:56.736: INFO: Terminating ReplicationController wrapped-volume-race-7ac81a11-c2ed-4059-acd0-614f90cb3059 pods took: 101.023991ms
    STEP: Creating RC which spawns configmap-volume pods 02/27/23 02:25:59.442
    Feb 27 02:25:59.459: INFO: Pod name wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8: Found 0 pods out of 5
    Feb 27 02:26:04.468: INFO: Pod name wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 02/27/23 02:26:04.468
    Feb 27 02:26:04.468: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:26:04.493: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.933479ms
    Feb 27 02:26:06.497: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029207749s
    Feb 27 02:26:08.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029969992s
    Feb 27 02:26:10.522: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054302464s
    Feb 27 02:26:12.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.030336949s
    Feb 27 02:26:14.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6": Phase="Running", Reason="", readiness=true. Elapsed: 10.029674125s
    Feb 27 02:26:14.498: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-5v5f6" satisfied condition "running"
    Feb 27 02:26:14.498: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:26:14.502: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl": Phase="Running", Reason="", readiness=true. Elapsed: 3.733584ms
    Feb 27 02:26:14.502: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-sfqvl" satisfied condition "running"
    Feb 27 02:26:14.502: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:26:14.504: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp": Phase="Running", Reason="", readiness=true. Elapsed: 2.783119ms
    Feb 27 02:26:14.504: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-vvnhp" satisfied condition "running"
    Feb 27 02:26:14.504: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:26:14.507: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.747275ms
    Feb 27 02:26:14.507: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-x88h6" satisfied condition "running"
    Feb 27 02:26:14.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9" in namespace "emptydir-wrapper-5631" to be "running"
    Feb 27 02:26:14.512: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.522425ms
    Feb 27 02:26:14.512: INFO: Pod "wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8-zswl9" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 in namespace emptydir-wrapper-5631, will wait for the garbage collector to delete the pods 02/27/23 02:26:14.512
    Feb 27 02:26:14.574: INFO: Deleting ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 took: 9.69297ms
    Feb 27 02:26:14.674: INFO: Terminating ReplicationController wrapped-volume-race-c1fdd136-2a13-4d9e-afaf-b414466d9ab8 pods took: 100.178879ms
    STEP: Cleaning up the configMaps 02/27/23 02:26:17.575
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:17.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5631" for this suite. 02/27/23 02:26:17.892
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:17.9
Feb 27 02:26:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 02:26:17.901
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:17.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:17.922
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Feb 27 02:26:17.977: INFO: Waiting up to 5m0s for pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8" in namespace "emptydir-wrapper-5724" to be "running and ready"
Feb 27 02:26:17.982: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.345376ms
Feb 27 02:26:17.982: INFO: The phase of Pod pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:26:19.987: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009740457s
Feb 27 02:26:19.987: INFO: The phase of Pod pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8 is Running (Ready = true)
Feb 27 02:26:19.987: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8" satisfied condition "running and ready"
STEP: Cleaning up the secret 02/27/23 02:26:19.99
STEP: Cleaning up the configmap 02/27/23 02:26:19.995
STEP: Cleaning up the pod 02/27/23 02:26:19.999
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:20.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5724" for this suite. 02/27/23 02:26:20.014
------------------------------
• [2.121 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:17.9
    Feb 27 02:26:17.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir-wrapper 02/27/23 02:26:17.901
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:17.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:17.922
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Feb 27 02:26:17.977: INFO: Waiting up to 5m0s for pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8" in namespace "emptydir-wrapper-5724" to be "running and ready"
    Feb 27 02:26:17.982: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.345376ms
    Feb 27 02:26:17.982: INFO: The phase of Pod pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:26:19.987: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009740457s
    Feb 27 02:26:19.987: INFO: The phase of Pod pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8 is Running (Ready = true)
    Feb 27 02:26:19.987: INFO: Pod "pod-secrets-c336ab15-f545-4cdf-bd76-082c758cffc8" satisfied condition "running and ready"
    STEP: Cleaning up the secret 02/27/23 02:26:19.99
    STEP: Cleaning up the configmap 02/27/23 02:26:19.995
    STEP: Cleaning up the pod 02/27/23 02:26:19.999
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:20.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5724" for this suite. 02/27/23 02:26:20.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:20.021
Feb 27 02:26:20.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 02:26:20.022
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:20.041
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 02/27/23 02:26:20.043
STEP: getting /apis/storage.k8s.io 02/27/23 02:26:20.044
STEP: getting /apis/storage.k8s.io/v1 02/27/23 02:26:20.045
STEP: creating 02/27/23 02:26:20.046
STEP: watching 02/27/23 02:26:20.066
Feb 27 02:26:20.066: INFO: starting watch
STEP: getting 02/27/23 02:26:20.073
STEP: listing in namespace 02/27/23 02:26:20.076
STEP: listing across namespaces 02/27/23 02:26:20.079
STEP: patching 02/27/23 02:26:20.081
STEP: updating 02/27/23 02:26:20.088
Feb 27 02:26:20.092: INFO: waiting for watch events with expected annotations in namespace
Feb 27 02:26:20.092: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 02/27/23 02:26:20.093
STEP: deleting a collection 02/27/23 02:26:20.106
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6490" for this suite. 02/27/23 02:26:20.125
------------------------------
• [0.110 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:20.021
    Feb 27 02:26:20.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename csistoragecapacity 02/27/23 02:26:20.022
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:20.041
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 02/27/23 02:26:20.043
    STEP: getting /apis/storage.k8s.io 02/27/23 02:26:20.044
    STEP: getting /apis/storage.k8s.io/v1 02/27/23 02:26:20.045
    STEP: creating 02/27/23 02:26:20.046
    STEP: watching 02/27/23 02:26:20.066
    Feb 27 02:26:20.066: INFO: starting watch
    STEP: getting 02/27/23 02:26:20.073
    STEP: listing in namespace 02/27/23 02:26:20.076
    STEP: listing across namespaces 02/27/23 02:26:20.079
    STEP: patching 02/27/23 02:26:20.081
    STEP: updating 02/27/23 02:26:20.088
    Feb 27 02:26:20.092: INFO: waiting for watch events with expected annotations in namespace
    Feb 27 02:26:20.092: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 02/27/23 02:26:20.093
    STEP: deleting a collection 02/27/23 02:26:20.106
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:20.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6490" for this suite. 02/27/23 02:26:20.125
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:20.131
Feb 27 02:26:20.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 02:26:20.132
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:20.151
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 02/27/23 02:26:20.153
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.167
STEP: Creating a pod in the namespace 02/27/23 02:26:20.17
STEP: Waiting for the pod to have running status 02/27/23 02:26:20.178
Feb 27 02:26:20.178: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3643" to be "running"
Feb 27 02:26:20.181: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068529ms
Feb 27 02:26:22.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007330586s
Feb 27 02:26:22.186: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 02/27/23 02:26:22.186
STEP: Waiting for the namespace to be removed. 02/27/23 02:26:22.192
STEP: Recreating the namespace 02/27/23 02:26:33.197
STEP: Verifying there are no pods in the namespace 02/27/23 02:26:33.218
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-853" for this suite. 02/27/23 02:26:33.227
STEP: Destroying namespace "nsdeletetest-3643" for this suite. 02/27/23 02:26:33.232
Feb 27 02:26:33.234: INFO: Namespace nsdeletetest-3643 was already deleted
STEP: Destroying namespace "nsdeletetest-2909" for this suite. 02/27/23 02:26:33.234
------------------------------
• [SLOW TEST] [13.112 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:20.131
    Feb 27 02:26:20.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 02:26:20.132
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:20.151
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 02/27/23 02:26:20.153
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:20.167
    STEP: Creating a pod in the namespace 02/27/23 02:26:20.17
    STEP: Waiting for the pod to have running status 02/27/23 02:26:20.178
    Feb 27 02:26:20.178: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-3643" to be "running"
    Feb 27 02:26:20.181: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068529ms
    Feb 27 02:26:22.186: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007330586s
    Feb 27 02:26:22.186: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 02/27/23 02:26:22.186
    STEP: Waiting for the namespace to be removed. 02/27/23 02:26:22.192
    STEP: Recreating the namespace 02/27/23 02:26:33.197
    STEP: Verifying there are no pods in the namespace 02/27/23 02:26:33.218
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:33.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-853" for this suite. 02/27/23 02:26:33.227
    STEP: Destroying namespace "nsdeletetest-3643" for this suite. 02/27/23 02:26:33.232
    Feb 27 02:26:33.234: INFO: Namespace nsdeletetest-3643 was already deleted
    STEP: Destroying namespace "nsdeletetest-2909" for this suite. 02/27/23 02:26:33.234
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:33.244
Feb 27 02:26:33.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:26:33.244
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:33.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:33.269
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-6e3b10f4-d16b-4f5a-bbd8-d6f444557b3c 02/27/23 02:26:33.271
STEP: Creating a pod to test consume secrets 02/27/23 02:26:33.278
Feb 27 02:26:33.312: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93" in namespace "projected-2605" to be "Succeeded or Failed"
Feb 27 02:26:33.315: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825926ms
Feb 27 02:26:35.320: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00773822s
Feb 27 02:26:37.319: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006299646s
STEP: Saw pod success 02/27/23 02:26:37.319
Feb 27 02:26:37.319: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93" satisfied condition "Succeeded or Failed"
Feb 27 02:26:37.321: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:26:37.331
Feb 27 02:26:37.340: INFO: Waiting for pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 to disappear
Feb 27 02:26:37.342: INFO: Pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:37.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2605" for this suite. 02/27/23 02:26:37.346
------------------------------
• [4.109 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:33.244
    Feb 27 02:26:33.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:26:33.244
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:33.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:33.269
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-6e3b10f4-d16b-4f5a-bbd8-d6f444557b3c 02/27/23 02:26:33.271
    STEP: Creating a pod to test consume secrets 02/27/23 02:26:33.278
    Feb 27 02:26:33.312: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93" in namespace "projected-2605" to be "Succeeded or Failed"
    Feb 27 02:26:33.315: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.825926ms
    Feb 27 02:26:35.320: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00773822s
    Feb 27 02:26:37.319: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006299646s
    STEP: Saw pod success 02/27/23 02:26:37.319
    Feb 27 02:26:37.319: INFO: Pod "pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93" satisfied condition "Succeeded or Failed"
    Feb 27 02:26:37.321: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:26:37.331
    Feb 27 02:26:37.340: INFO: Waiting for pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 to disappear
    Feb 27 02:26:37.342: INFO: Pod pod-projected-secrets-8a54b774-eec8-4844-81e9-f738ee9efa93 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:37.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2605" for this suite. 02/27/23 02:26:37.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:37.353
Feb 27 02:26:37.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:26:37.353
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:37.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:37.387
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-8ca58f7e-e2c2-4e44-b456-33ae37be277b 02/27/23 02:26:37.463
STEP: Creating a pod to test consume secrets 02/27/23 02:26:37.486
Feb 27 02:26:37.508: INFO: Waiting up to 5m0s for pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190" in namespace "secrets-3465" to be "Succeeded or Failed"
Feb 27 02:26:37.511: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492625ms
Feb 27 02:26:39.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008635374s
Feb 27 02:26:41.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008656312s
STEP: Saw pod success 02/27/23 02:26:41.517
Feb 27 02:26:41.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190" satisfied condition "Succeeded or Failed"
Feb 27 02:26:41.521: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:26:41.528
Feb 27 02:26:41.540: INFO: Waiting for pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 to disappear
Feb 27 02:26:41.543: INFO: Pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:41.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3465" for this suite. 02/27/23 02:26:41.548
STEP: Destroying namespace "secret-namespace-7491" for this suite. 02/27/23 02:26:41.554
------------------------------
• [4.207 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:37.353
    Feb 27 02:26:37.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:26:37.353
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:37.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:37.387
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-8ca58f7e-e2c2-4e44-b456-33ae37be277b 02/27/23 02:26:37.463
    STEP: Creating a pod to test consume secrets 02/27/23 02:26:37.486
    Feb 27 02:26:37.508: INFO: Waiting up to 5m0s for pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190" in namespace "secrets-3465" to be "Succeeded or Failed"
    Feb 27 02:26:37.511: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.492625ms
    Feb 27 02:26:39.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008635374s
    Feb 27 02:26:41.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008656312s
    STEP: Saw pod success 02/27/23 02:26:41.517
    Feb 27 02:26:41.517: INFO: Pod "pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190" satisfied condition "Succeeded or Failed"
    Feb 27 02:26:41.521: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:26:41.528
    Feb 27 02:26:41.540: INFO: Waiting for pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 to disappear
    Feb 27 02:26:41.543: INFO: Pod pod-secrets-9b5f8bfb-ab88-4484-9433-fd2b7262a190 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:41.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3465" for this suite. 02/27/23 02:26:41.548
    STEP: Destroying namespace "secret-namespace-7491" for this suite. 02/27/23 02:26:41.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:41.56
Feb 27 02:26:41.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename ingressclass 02/27/23 02:26:41.561
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:41.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:41.589
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 02/27/23 02:26:41.592
STEP: getting /apis/networking.k8s.io 02/27/23 02:26:41.594
STEP: getting /apis/networking.k8s.iov1 02/27/23 02:26:41.596
STEP: creating 02/27/23 02:26:41.596
STEP: getting 02/27/23 02:26:41.609
STEP: listing 02/27/23 02:26:41.611
STEP: watching 02/27/23 02:26:41.614
Feb 27 02:26:41.614: INFO: starting watch
STEP: patching 02/27/23 02:26:41.615
STEP: updating 02/27/23 02:26:41.619
Feb 27 02:26:41.623: INFO: waiting for watch events with expected annotations
Feb 27 02:26:41.623: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 02:26:41.623
STEP: deleting a collection 02/27/23 02:26:41.634
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-6223" for this suite. 02/27/23 02:26:41.651
------------------------------
• [0.095 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:41.56
    Feb 27 02:26:41.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename ingressclass 02/27/23 02:26:41.561
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:41.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:41.589
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 02/27/23 02:26:41.592
    STEP: getting /apis/networking.k8s.io 02/27/23 02:26:41.594
    STEP: getting /apis/networking.k8s.iov1 02/27/23 02:26:41.596
    STEP: creating 02/27/23 02:26:41.596
    STEP: getting 02/27/23 02:26:41.609
    STEP: listing 02/27/23 02:26:41.611
    STEP: watching 02/27/23 02:26:41.614
    Feb 27 02:26:41.614: INFO: starting watch
    STEP: patching 02/27/23 02:26:41.615
    STEP: updating 02/27/23 02:26:41.619
    Feb 27 02:26:41.623: INFO: waiting for watch events with expected annotations
    Feb 27 02:26:41.623: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 02:26:41.623
    STEP: deleting a collection 02/27/23 02:26:41.634
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-6223" for this suite. 02/27/23 02:26:41.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:41.656
Feb 27 02:26:41.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:26:41.657
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:41.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:41.679
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 02/27/23 02:26:41.682
STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 02:26:41.687
STEP: delete the deployment 02/27/23 02:26:42.195
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 02:26:42.204
STEP: Gathering metrics 02/27/23 02:26:42.724
Feb 27 02:26:42.753: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 02:26:42.755: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.529694ms
Feb 27 02:26:42.755: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 02:26:42.755: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 02:26:42.845: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8545" for this suite. 02/27/23 02:26:42.851
------------------------------
• [1.216 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:41.656
    Feb 27 02:26:41.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:26:41.657
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:41.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:41.679
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 02/27/23 02:26:41.682
    STEP: Wait for the Deployment to create new ReplicaSet 02/27/23 02:26:41.687
    STEP: delete the deployment 02/27/23 02:26:42.195
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 02/27/23 02:26:42.204
    STEP: Gathering metrics 02/27/23 02:26:42.724
    Feb 27 02:26:42.753: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 02:26:42.755: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.529694ms
    Feb 27 02:26:42.755: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 02:26:42.755: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 02:26:42.845: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8545" for this suite. 02/27/23 02:26:42.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:42.873
Feb 27 02:26:42.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename watch 02/27/23 02:26:42.874
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:42.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:42.894
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 02/27/23 02:26:42.897
STEP: modifying the configmap once 02/27/23 02:26:42.902
STEP: modifying the configmap a second time 02/27/23 02:26:42.91
STEP: deleting the configmap 02/27/23 02:26:42.916
STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 02:26:42.924
STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 02:26:42.925
Feb 27 02:26:42.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4180  bf3d980f-a311-4ae6-8069-b8f17b293ed7 40817 0 2023-02-27 02:26:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 02:26:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Feb 27 02:26:42.925: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4180  bf3d980f-a311-4ae6-8069-b8f17b293ed7 40818 0 2023-02-27 02:26:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 02:26:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4180" for this suite. 02/27/23 02:26:42.931
------------------------------
• [0.070 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:42.873
    Feb 27 02:26:42.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename watch 02/27/23 02:26:42.874
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:42.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:42.894
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 02/27/23 02:26:42.897
    STEP: modifying the configmap once 02/27/23 02:26:42.902
    STEP: modifying the configmap a second time 02/27/23 02:26:42.91
    STEP: deleting the configmap 02/27/23 02:26:42.916
    STEP: creating a watch on configmaps from the resource version returned by the first update 02/27/23 02:26:42.924
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 02/27/23 02:26:42.925
    Feb 27 02:26:42.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4180  bf3d980f-a311-4ae6-8069-b8f17b293ed7 40817 0 2023-02-27 02:26:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 02:26:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Feb 27 02:26:42.925: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4180  bf3d980f-a311-4ae6-8069-b8f17b293ed7 40818 0 2023-02-27 02:26:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-02-27 02:26:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:42.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4180" for this suite. 02/27/23 02:26:42.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:42.943
Feb 27 02:26:42.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:26:42.944
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:42.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:42.968
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 02/27/23 02:26:42.97
Feb 27 02:26:42.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 create -f -'
Feb 27 02:26:43.473: INFO: stderr: ""
Feb 27 02:26:43.473: INFO: stdout: "pod/pause created\n"
Feb 27 02:26:43.473: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 27 02:26:43.473: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6912" to be "running and ready"
Feb 27 02:26:43.479: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110386ms
Feb 27 02:26:43.479: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
Feb 27 02:26:45.483: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010664845s
Feb 27 02:26:45.483: INFO: Pod "pause" satisfied condition "running and ready"
Feb 27 02:26:45.483: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 02:26:45.483
Feb 27 02:26:45.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 label pods pause testing-label=testing-label-value'
Feb 27 02:26:45.555: INFO: stderr: ""
Feb 27 02:26:45.555: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 02:26:45.555
Feb 27 02:26:45.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pod pause -L testing-label'
Feb 27 02:26:45.616: INFO: stderr: ""
Feb 27 02:26:45.616: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 02/27/23 02:26:45.616
Feb 27 02:26:45.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 label pods pause testing-label-'
Feb 27 02:26:45.687: INFO: stderr: ""
Feb 27 02:26:45.687: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 02/27/23 02:26:45.687
Feb 27 02:26:45.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pod pause -L testing-label'
Feb 27 02:26:45.774: INFO: stderr: ""
Feb 27 02:26:45.774: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 02/27/23 02:26:45.774
Feb 27 02:26:45.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 delete --grace-period=0 --force -f -'
Feb 27 02:26:45.848: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 02:26:45.848: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 27 02:26:45.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get rc,svc -l name=pause --no-headers'
Feb 27 02:26:45.918: INFO: stderr: "No resources found in kubectl-6912 namespace.\n"
Feb 27 02:26:45.918: INFO: stdout: ""
Feb 27 02:26:45.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 02:26:45.992: INFO: stderr: ""
Feb 27 02:26:45.992: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:45.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6912" for this suite. 02/27/23 02:26:45.998
------------------------------
• [3.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:42.943
    Feb 27 02:26:42.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:26:42.944
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:42.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:42.968
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 02/27/23 02:26:42.97
    Feb 27 02:26:42.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 create -f -'
    Feb 27 02:26:43.473: INFO: stderr: ""
    Feb 27 02:26:43.473: INFO: stdout: "pod/pause created\n"
    Feb 27 02:26:43.473: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Feb 27 02:26:43.473: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6912" to be "running and ready"
    Feb 27 02:26:43.479: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110386ms
    Feb 27 02:26:43.479: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
    Feb 27 02:26:45.483: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010664845s
    Feb 27 02:26:45.483: INFO: Pod "pause" satisfied condition "running and ready"
    Feb 27 02:26:45.483: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 02/27/23 02:26:45.483
    Feb 27 02:26:45.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 label pods pause testing-label=testing-label-value'
    Feb 27 02:26:45.555: INFO: stderr: ""
    Feb 27 02:26:45.555: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 02/27/23 02:26:45.555
    Feb 27 02:26:45.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pod pause -L testing-label'
    Feb 27 02:26:45.616: INFO: stderr: ""
    Feb 27 02:26:45.616: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 02/27/23 02:26:45.616
    Feb 27 02:26:45.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 label pods pause testing-label-'
    Feb 27 02:26:45.687: INFO: stderr: ""
    Feb 27 02:26:45.687: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 02/27/23 02:26:45.687
    Feb 27 02:26:45.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pod pause -L testing-label'
    Feb 27 02:26:45.774: INFO: stderr: ""
    Feb 27 02:26:45.774: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 02/27/23 02:26:45.774
    Feb 27 02:26:45.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 delete --grace-period=0 --force -f -'
    Feb 27 02:26:45.848: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 02:26:45.848: INFO: stdout: "pod \"pause\" force deleted\n"
    Feb 27 02:26:45.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get rc,svc -l name=pause --no-headers'
    Feb 27 02:26:45.918: INFO: stderr: "No resources found in kubectl-6912 namespace.\n"
    Feb 27 02:26:45.918: INFO: stdout: ""
    Feb 27 02:26:45.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-6912 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 02:26:45.992: INFO: stderr: ""
    Feb 27 02:26:45.992: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:45.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6912" for this suite. 02/27/23 02:26:45.998
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:46.005
Feb 27 02:26:46.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:26:46.006
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:46.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:46.023
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:26:46.026
Feb 27 02:26:46.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339" in namespace "projected-2169" to be "Succeeded or Failed"
Feb 27 02:26:46.041: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508136ms
Feb 27 02:26:48.050: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014673382s
Feb 27 02:26:50.047: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011088987s
STEP: Saw pod success 02/27/23 02:26:50.047
Feb 27 02:26:50.047: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339" satisfied condition "Succeeded or Failed"
Feb 27 02:26:50.050: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 container client-container: <nil>
STEP: delete the pod 02/27/23 02:26:50.058
Feb 27 02:26:50.070: INFO: Waiting for pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 to disappear
Feb 27 02:26:50.072: INFO: Pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2169" for this suite. 02/27/23 02:26:50.077
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:46.005
    Feb 27 02:26:46.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:26:46.006
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:46.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:46.023
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:26:46.026
    Feb 27 02:26:46.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339" in namespace "projected-2169" to be "Succeeded or Failed"
    Feb 27 02:26:46.041: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Pending", Reason="", readiness=false. Elapsed: 5.508136ms
    Feb 27 02:26:48.050: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014673382s
    Feb 27 02:26:50.047: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011088987s
    STEP: Saw pod success 02/27/23 02:26:50.047
    Feb 27 02:26:50.047: INFO: Pod "downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339" satisfied condition "Succeeded or Failed"
    Feb 27 02:26:50.050: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:26:50.058
    Feb 27 02:26:50.070: INFO: Waiting for pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 to disappear
    Feb 27 02:26:50.072: INFO: Pod downwardapi-volume-8d5e080e-3c49-4597-8a3a-f6e98d8e6339 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2169" for this suite. 02/27/23 02:26:50.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:50.083
Feb 27 02:26:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 02:26:50.086
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:50.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:50.117
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 02:26:50.137
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:26:50.144
Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:50.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:26:50.152: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:26:51.156: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:51.157: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:51.157: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:51.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:26:51.161: INFO: Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 02:26:52.204: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 02:26:52.212
Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:26:52.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 02:26:52.238: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 02:26:52.238
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:26:53.249
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2859, will wait for the garbage collector to delete the pods 02/27/23 02:26:53.249
Feb 27 02:26:53.310: INFO: Deleting DaemonSet.extensions daemon-set took: 6.924823ms
Feb 27 02:26:53.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.08864ms
Feb 27 02:26:56.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:26:56.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 02:26:56.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41145"},"items":null}

Feb 27 02:26:56.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41145"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2859" for this suite. 02/27/23 02:26:56.146
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:50.083
    Feb 27 02:26:50.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 02:26:50.086
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:50.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:50.117
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 02/27/23 02:26:50.137
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:26:50.144
    Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:50.149: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:50.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:26:50.152: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:26:51.156: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:51.157: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:51.157: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:51.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:26:51.161: INFO: Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.179: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 02:26:52.204: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 02/27/23 02:26:52.212
    Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.234: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:26:52.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 02:26:52.238: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 02/27/23 02:26:52.238
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:26:53.249
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2859, will wait for the garbage collector to delete the pods 02/27/23 02:26:53.249
    Feb 27 02:26:53.310: INFO: Deleting DaemonSet.extensions daemon-set took: 6.924823ms
    Feb 27 02:26:53.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.08864ms
    Feb 27 02:26:56.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:26:56.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 02:26:56.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41145"},"items":null}

    Feb 27 02:26:56.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41145"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:56.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2859" for this suite. 02/27/23 02:26:56.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:56.151
Feb 27 02:26:56.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:26:56.152
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:56.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:56.177
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Feb 27 02:26:56.179: INFO: Creating simple deployment test-new-deployment
Feb 27 02:26:56.204: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 02/27/23 02:26:58.217
STEP: updating a scale subresource 02/27/23 02:26:58.219
STEP: verifying the deployment Spec.Replicas was modified 02/27/23 02:26:58.226
STEP: Patch a scale subresource 02/27/23 02:26:58.231
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:26:58.275: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3994  c8ee75a1-db5e-40dd-8994-54308d48327f 41202 3 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059fd708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-648db5888" has successfully progressed.,LastUpdateTime:2023-02-27 02:26:57 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:26:58 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 02:26:58.280: INFO: New ReplicaSet "test-new-deployment-648db5888" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-648db5888  deployment-3994  de5c6826-1cdc-4d34-ac5f-d5ba932a6130 41200 3 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c8ee75a1-db5e-40dd-8994-54308d48327f 0xc0031ab327 0xc0031ab328}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ee75a1-db5e-40dd-8994-54308d48327f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 648db5888,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031ab3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:26:58.285: INFO: Pod "test-new-deployment-648db5888-h68c5" is available:
&Pod{ObjectMeta:{test-new-deployment-648db5888-h68c5 test-new-deployment-648db5888- deployment-3994  77554f9c-d10d-4371-bb14-e9ba978ff337 41186 0 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.171"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.171"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-648db5888 de5c6826-1cdc-4d34-ac5f-d5ba932a6130 0xc0057f4927 0xc0057f4928}] [] [{kube-controller-manager Update v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de5c6826-1cdc-4d34-ac5f-d5ba932a6130\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:26:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njpqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njpqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.171,StartTime:2023-02-27 02:26:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:26:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://f1abcc111fa15136d78490b4e362a09508fb13b7acacef3edfa460ae44345340,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:26:58.285: INFO: Pod "test-new-deployment-648db5888-vqkzm" is not available:
&Pod{ObjectMeta:{test-new-deployment-648db5888-vqkzm test-new-deployment-648db5888- deployment-3994  832f7bc9-22c5-4449-8853-e73e5c416829 41204 0 2023-02-27 02:26:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [{apps/v1 ReplicaSet test-new-deployment-648db5888 de5c6826-1cdc-4d34-ac5f-d5ba932a6130 0xc0057f4b20 0xc0057f4b21}] [] [{kube-controller-manager Update v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de5c6826-1cdc-4d34-ac5f-d5ba932a6130\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcqp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcqp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:26:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:26:58.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3994" for this suite. 02/27/23 02:26:58.293
------------------------------
• [2.150 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:56.151
    Feb 27 02:26:56.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:26:56.152
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:56.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:56.177
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Feb 27 02:26:56.179: INFO: Creating simple deployment test-new-deployment
    Feb 27 02:26:56.204: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 02/27/23 02:26:58.217
    STEP: updating a scale subresource 02/27/23 02:26:58.219
    STEP: verifying the deployment Spec.Replicas was modified 02/27/23 02:26:58.226
    STEP: Patch a scale subresource 02/27/23 02:26:58.231
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:26:58.275: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3994  c8ee75a1-db5e-40dd-8994-54308d48327f 41202 3 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059fd708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-648db5888" has successfully progressed.,LastUpdateTime:2023-02-27 02:26:57 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:26:58 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 02:26:58.280: INFO: New ReplicaSet "test-new-deployment-648db5888" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-648db5888  deployment-3994  de5c6826-1cdc-4d34-ac5f-d5ba932a6130 41200 3 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c8ee75a1-db5e-40dd-8994-54308d48327f 0xc0031ab327 0xc0031ab328}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8ee75a1-db5e-40dd-8994-54308d48327f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 648db5888,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031ab3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:26:58.285: INFO: Pod "test-new-deployment-648db5888-h68c5" is available:
    &Pod{ObjectMeta:{test-new-deployment-648db5888-h68c5 test-new-deployment-648db5888- deployment-3994  77554f9c-d10d-4371-bb14-e9ba978ff337 41186 0 2023-02-27 02:26:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.171"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.171"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-new-deployment-648db5888 de5c6826-1cdc-4d34-ac5f-d5ba932a6130 0xc0057f4927 0xc0057f4928}] [] [{kube-controller-manager Update v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de5c6826-1cdc-4d34-ac5f-d5ba932a6130\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:26:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:26:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njpqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njpqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.171,StartTime:2023-02-27 02:26:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:26:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://f1abcc111fa15136d78490b4e362a09508fb13b7acacef3edfa460ae44345340,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:26:58.285: INFO: Pod "test-new-deployment-648db5888-vqkzm" is not available:
    &Pod{ObjectMeta:{test-new-deployment-648db5888-vqkzm test-new-deployment-648db5888- deployment-3994  832f7bc9-22c5-4449-8853-e73e5c416829 41204 0 2023-02-27 02:26:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [{apps/v1 ReplicaSet test-new-deployment-648db5888 de5c6826-1cdc-4d34-ac5f-d5ba932a6130 0xc0057f4b20 0xc0057f4b21}] [] [{kube-controller-manager Update v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de5c6826-1cdc-4d34-ac5f-d5ba932a6130\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:26:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcqp9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcqp9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:26:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:26:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:26:58.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3994" for this suite. 02/27/23 02:26:58.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:26:58.303
Feb 27 02:26:58.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:26:58.303
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:58.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:58.344
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 02:26:58.347
Feb 27 02:26:58.385: INFO: Waiting up to 5m0s for pod "pod-e029241d-691d-4dd6-935c-19989e115180" in namespace "emptydir-9965" to be "Succeeded or Failed"
Feb 27 02:26:58.388: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046801ms
Feb 27 02:27:00.392: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007023741s
Feb 27 02:27:02.393: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00873651s
STEP: Saw pod success 02/27/23 02:27:02.393
Feb 27 02:27:02.393: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180" satisfied condition "Succeeded or Failed"
Feb 27 02:27:02.396: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-e029241d-691d-4dd6-935c-19989e115180 container test-container: <nil>
STEP: delete the pod 02/27/23 02:27:02.401
Feb 27 02:27:02.416: INFO: Waiting for pod pod-e029241d-691d-4dd6-935c-19989e115180 to disappear
Feb 27 02:27:02.419: INFO: Pod pod-e029241d-691d-4dd6-935c-19989e115180 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:27:02.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9965" for this suite. 02/27/23 02:27:02.424
------------------------------
• [4.128 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:26:58.303
    Feb 27 02:26:58.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:26:58.303
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:26:58.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:26:58.344
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 02:26:58.347
    Feb 27 02:26:58.385: INFO: Waiting up to 5m0s for pod "pod-e029241d-691d-4dd6-935c-19989e115180" in namespace "emptydir-9965" to be "Succeeded or Failed"
    Feb 27 02:26:58.388: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046801ms
    Feb 27 02:27:00.392: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007023741s
    Feb 27 02:27:02.393: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00873651s
    STEP: Saw pod success 02/27/23 02:27:02.393
    Feb 27 02:27:02.393: INFO: Pod "pod-e029241d-691d-4dd6-935c-19989e115180" satisfied condition "Succeeded or Failed"
    Feb 27 02:27:02.396: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-e029241d-691d-4dd6-935c-19989e115180 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:27:02.401
    Feb 27 02:27:02.416: INFO: Waiting for pod pod-e029241d-691d-4dd6-935c-19989e115180 to disappear
    Feb 27 02:27:02.419: INFO: Pod pod-e029241d-691d-4dd6-935c-19989e115180 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:27:02.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9965" for this suite. 02/27/23 02:27:02.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:27:02.432
Feb 27 02:27:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:27:02.432
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:27:02.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:27:02.452
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Feb 27 02:27:02.471: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7964 to be scheduled
Feb 27 02:27:02.477: INFO: 1 pods are not scheduled: [runtimeclass-7964/test-runtimeclass-runtimeclass-7964-preconfigured-handler-gjzzt(45711538-e0e5-48e5-a27d-ba37c235ea8d)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 02:27:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7964" for this suite. 02/27/23 02:27:04.503
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:27:02.432
    Feb 27 02:27:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:27:02.432
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:27:02.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:27:02.452
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Feb 27 02:27:02.471: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7964 to be scheduled
    Feb 27 02:27:02.477: INFO: 1 pods are not scheduled: [runtimeclass-7964/test-runtimeclass-runtimeclass-7964-preconfigured-handler-gjzzt(45711538-e0e5-48e5-a27d-ba37c235ea8d)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:27:04.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7964" for this suite. 02/27/23 02:27:04.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:27:04.509
Feb 27 02:27:04.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 02:27:04.51
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:27:04.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:27:04.533
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Feb 27 02:27:04.534: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 02:28:04.601: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Feb 27 02:28:04.605: INFO: Starting informer...
STEP: Starting pods... 02/27/23 02:28:04.605
Feb 27 02:28:04.849: INFO: Pod1 is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
Feb 27 02:28:05.063: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5997" to be "running"
Feb 27 02:28:05.066: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.413889ms
Feb 27 02:28:07.071: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008045831s
Feb 27 02:28:07.071: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Feb 27 02:28:07.071: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5997" to be "running"
Feb 27 02:28:07.074: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.747066ms
Feb 27 02:28:07.074: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Feb 27 02:28:07.074: INFO: Pod2 is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
STEP: Trying to apply a taint on the Node 02/27/23 02:28:07.074
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 02:28:07.121
STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 02:28:07.174
Feb 27 02:28:13.183: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 27 02:28:33.216: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 02:28:33.23
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:28:33.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-5997" for this suite. 02/27/23 02:28:33.244
------------------------------
• [SLOW TEST] [88.741 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:27:04.509
    Feb 27 02:27:04.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename taint-multiple-pods 02/27/23 02:27:04.51
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:27:04.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:27:04.533
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Feb 27 02:27:04.534: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 02:28:04.601: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Feb 27 02:28:04.605: INFO: Starting informer...
    STEP: Starting pods... 02/27/23 02:28:04.605
    Feb 27 02:28:04.849: INFO: Pod1 is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
    Feb 27 02:28:05.063: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5997" to be "running"
    Feb 27 02:28:05.066: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.413889ms
    Feb 27 02:28:07.071: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008045831s
    Feb 27 02:28:07.071: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Feb 27 02:28:07.071: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5997" to be "running"
    Feb 27 02:28:07.074: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.747066ms
    Feb 27 02:28:07.074: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Feb 27 02:28:07.074: INFO: Pod2 is running on worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins. Tainting Node
    STEP: Trying to apply a taint on the Node 02/27/23 02:28:07.074
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 02:28:07.121
    STEP: Waiting for Pod1 and Pod2 to be deleted 02/27/23 02:28:07.174
    Feb 27 02:28:13.183: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Feb 27 02:28:33.216: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 02/27/23 02:28:33.23
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:28:33.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-5997" for this suite. 02/27/23 02:28:33.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:28:33.25
Feb 27 02:28:33.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename limitrange 02/27/23 02:28:33.251
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:33.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:33.275
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 02/27/23 02:28:33.277
STEP: Setting up watch 02/27/23 02:28:33.277
STEP: Submitting a LimitRange 02/27/23 02:28:33.385
STEP: Verifying LimitRange creation was observed 02/27/23 02:28:33.394
STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 02:28:33.417
Feb 27 02:28:33.420: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 02:28:33.420: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 02/27/23 02:28:33.42
STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 02:28:33.455
Feb 27 02:28:33.462: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Feb 27 02:28:33.462: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 02/27/23 02:28:33.462
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 02:28:33.482
Feb 27 02:28:33.484: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Feb 27 02:28:33.484: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 02/27/23 02:28:33.484
STEP: Failing to create a Pod with more than max resources 02/27/23 02:28:33.488
STEP: Updating a LimitRange 02/27/23 02:28:33.493
STEP: Verifying LimitRange updating is effective 02/27/23 02:28:33.498
STEP: Creating a Pod with less than former min resources 02/27/23 02:28:35.503
STEP: Failing to create a Pod with more than max resources 02/27/23 02:28:35.514
STEP: Deleting a LimitRange 02/27/23 02:28:35.52
STEP: Verifying the LimitRange was deleted 02/27/23 02:28:35.529
Feb 27 02:28:40.533: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 02/27/23 02:28:40.533
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 27 02:28:40.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-237" for this suite. 02/27/23 02:28:40.578
------------------------------
• [SLOW TEST] [7.337 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:28:33.25
    Feb 27 02:28:33.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename limitrange 02/27/23 02:28:33.251
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:33.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:33.275
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 02/27/23 02:28:33.277
    STEP: Setting up watch 02/27/23 02:28:33.277
    STEP: Submitting a LimitRange 02/27/23 02:28:33.385
    STEP: Verifying LimitRange creation was observed 02/27/23 02:28:33.394
    STEP: Fetching the LimitRange to ensure it has proper values 02/27/23 02:28:33.417
    Feb 27 02:28:33.420: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 02:28:33.420: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 02/27/23 02:28:33.42
    STEP: Ensuring Pod has resource requirements applied from LimitRange 02/27/23 02:28:33.455
    Feb 27 02:28:33.462: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Feb 27 02:28:33.462: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 02/27/23 02:28:33.462
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 02/27/23 02:28:33.482
    Feb 27 02:28:33.484: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Feb 27 02:28:33.484: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 02/27/23 02:28:33.484
    STEP: Failing to create a Pod with more than max resources 02/27/23 02:28:33.488
    STEP: Updating a LimitRange 02/27/23 02:28:33.493
    STEP: Verifying LimitRange updating is effective 02/27/23 02:28:33.498
    STEP: Creating a Pod with less than former min resources 02/27/23 02:28:35.503
    STEP: Failing to create a Pod with more than max resources 02/27/23 02:28:35.514
    STEP: Deleting a LimitRange 02/27/23 02:28:35.52
    STEP: Verifying the LimitRange was deleted 02/27/23 02:28:35.529
    Feb 27 02:28:40.533: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 02/27/23 02:28:40.533
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:28:40.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-237" for this suite. 02/27/23 02:28:40.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:28:40.588
Feb 27 02:28:40.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svc-latency 02/27/23 02:28:40.589
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:40.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:40.609
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Feb 27 02:28:40.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3786 02/27/23 02:28:40.612
I0227 02:28:40.619446      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3786, replica count: 1
I0227 02:28:41.670177      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0227 02:28:42.670265      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:28:42.798: INFO: Created: latency-svc-jb2x6
Feb 27 02:28:42.811: INFO: Got endpoints: latency-svc-jb2x6 [40.954834ms]
Feb 27 02:28:42.838: INFO: Created: latency-svc-dl5tf
Feb 27 02:28:42.849: INFO: Got endpoints: latency-svc-dl5tf [37.081325ms]
Feb 27 02:28:42.866: INFO: Created: latency-svc-7blvt
Feb 27 02:28:42.874: INFO: Created: latency-svc-nhcws
Feb 27 02:28:42.886: INFO: Got endpoints: latency-svc-nhcws [74.26164ms]
Feb 27 02:28:42.886: INFO: Got endpoints: latency-svc-7blvt [73.980364ms]
Feb 27 02:28:42.896: INFO: Created: latency-svc-ggzk8
Feb 27 02:28:42.913: INFO: Got endpoints: latency-svc-ggzk8 [100.873978ms]
Feb 27 02:28:42.923: INFO: Created: latency-svc-wtpht
Feb 27 02:28:42.931: INFO: Got endpoints: latency-svc-wtpht [119.200348ms]
Feb 27 02:28:42.936: INFO: Created: latency-svc-d954l
Feb 27 02:28:42.949: INFO: Got endpoints: latency-svc-d954l [136.814252ms]
Feb 27 02:28:42.961: INFO: Created: latency-svc-dlrkl
Feb 27 02:28:42.970: INFO: Got endpoints: latency-svc-dlrkl [157.974544ms]
Feb 27 02:28:42.984: INFO: Created: latency-svc-wggpk
Feb 27 02:28:42.991: INFO: Got endpoints: latency-svc-wggpk [178.725398ms]
Feb 27 02:28:43.004: INFO: Created: latency-svc-zkgqs
Feb 27 02:28:43.027: INFO: Got endpoints: latency-svc-zkgqs [215.588959ms]
Feb 27 02:28:43.043: INFO: Created: latency-svc-dftnz
Feb 27 02:28:43.058: INFO: Got endpoints: latency-svc-dftnz [245.697883ms]
Feb 27 02:28:43.066: INFO: Created: latency-svc-pmscp
Feb 27 02:28:43.072: INFO: Got endpoints: latency-svc-pmscp [259.908049ms]
Feb 27 02:28:43.080: INFO: Created: latency-svc-jmbps
Feb 27 02:28:43.114: INFO: Got endpoints: latency-svc-jmbps [302.374109ms]
Feb 27 02:28:43.122: INFO: Created: latency-svc-5shtq
Feb 27 02:28:43.126: INFO: Got endpoints: latency-svc-5shtq [314.498981ms]
Feb 27 02:28:43.141: INFO: Created: latency-svc-8txmj
Feb 27 02:28:43.154: INFO: Got endpoints: latency-svc-8txmj [342.144227ms]
Feb 27 02:28:43.169: INFO: Created: latency-svc-rght6
Feb 27 02:28:43.173: INFO: Got endpoints: latency-svc-rght6 [361.460496ms]
Feb 27 02:28:43.482: INFO: Created: latency-svc-bql82
Feb 27 02:28:43.483: INFO: Created: latency-svc-c6xhr
Feb 27 02:28:43.484: INFO: Created: latency-svc-hjr6z
Feb 27 02:28:43.484: INFO: Created: latency-svc-fq9rl
Feb 27 02:28:43.484: INFO: Created: latency-svc-jvgcr
Feb 27 02:28:43.484: INFO: Created: latency-svc-nv2dl
Feb 27 02:28:43.486: INFO: Created: latency-svc-8svm5
Feb 27 02:28:43.486: INFO: Created: latency-svc-p8fc6
Feb 27 02:28:43.486: INFO: Created: latency-svc-bl9mg
Feb 27 02:28:43.486: INFO: Created: latency-svc-g28zp
Feb 27 02:28:43.486: INFO: Created: latency-svc-tbbng
Feb 27 02:28:43.489: INFO: Created: latency-svc-668p7
Feb 27 02:28:43.489: INFO: Created: latency-svc-pgknf
Feb 27 02:28:43.491: INFO: Created: latency-svc-l7gf2
Feb 27 02:28:43.491: INFO: Created: latency-svc-s9rj5
Feb 27 02:28:43.500: INFO: Got endpoints: latency-svc-bql82 [614.381874ms]
Feb 27 02:28:43.511: INFO: Got endpoints: latency-svc-668p7 [337.650873ms]
Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-g28zp [358.078964ms]
Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-p8fc6 [626.318069ms]
Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-8svm5 [440.374233ms]
Feb 27 02:28:43.516: INFO: Got endpoints: latency-svc-tbbng [585.060431ms]
Feb 27 02:28:43.527: INFO: Got endpoints: latency-svc-c6xhr [678.180637ms]
Feb 27 02:28:43.529: INFO: Got endpoints: latency-svc-l7gf2 [580.924161ms]
Feb 27 02:28:43.537: INFO: Got endpoints: latency-svc-pgknf [509.481706ms]
Feb 27 02:28:43.537: INFO: Got endpoints: latency-svc-bl9mg [546.859488ms]
Feb 27 02:28:43.540: INFO: Got endpoints: latency-svc-s9rj5 [425.739642ms]
Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-hjr6z [596.761344ms]
Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-fq9rl [654.126164ms]
Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-jvgcr [509.212878ms]
Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-nv2dl [440.484728ms]
Feb 27 02:28:43.581: INFO: Created: latency-svc-zg2vk
Feb 27 02:28:43.596: INFO: Got endpoints: latency-svc-zg2vk [96.161603ms]
Feb 27 02:28:43.607: INFO: Created: latency-svc-r85fq
Feb 27 02:28:43.614: INFO: Created: latency-svc-kq8nr
Feb 27 02:28:43.616: INFO: Got endpoints: latency-svc-r85fq [104.969161ms]
Feb 27 02:28:43.632: INFO: Got endpoints: latency-svc-kq8nr [119.895932ms]
Feb 27 02:28:43.640: INFO: Created: latency-svc-smr7q
Feb 27 02:28:43.646: INFO: Got endpoints: latency-svc-smr7q [133.810935ms]
Feb 27 02:28:43.659: INFO: Created: latency-svc-gwshw
Feb 27 02:28:43.674: INFO: Got endpoints: latency-svc-gwshw [161.793989ms]
Feb 27 02:28:43.693: INFO: Created: latency-svc-h6l9m
Feb 27 02:28:43.709: INFO: Got endpoints: latency-svc-h6l9m [193.105056ms]
Feb 27 02:28:43.711: INFO: Created: latency-svc-hfm5m
Feb 27 02:28:43.746: INFO: Got endpoints: latency-svc-hfm5m [219.364414ms]
Feb 27 02:28:43.754: INFO: Created: latency-svc-kpslj
Feb 27 02:28:43.762: INFO: Got endpoints: latency-svc-kpslj [232.882245ms]
Feb 27 02:28:43.769: INFO: Created: latency-svc-8k4xw
Feb 27 02:28:43.780: INFO: Got endpoints: latency-svc-8k4xw [243.093507ms]
Feb 27 02:28:43.789: INFO: Created: latency-svc-fhbsh
Feb 27 02:28:43.801: INFO: Got endpoints: latency-svc-fhbsh [263.890202ms]
Feb 27 02:28:43.811: INFO: Created: latency-svc-52d2c
Feb 27 02:28:43.820: INFO: Got endpoints: latency-svc-52d2c [279.646193ms]
Feb 27 02:28:43.832: INFO: Created: latency-svc-xb69b
Feb 27 02:28:43.846: INFO: Got endpoints: latency-svc-xb69b [279.758581ms]
Feb 27 02:28:43.854: INFO: Created: latency-svc-ccn5f
Feb 27 02:28:43.876: INFO: Got endpoints: latency-svc-ccn5f [309.048264ms]
Feb 27 02:28:43.885: INFO: Created: latency-svc-nwrqs
Feb 27 02:28:43.892: INFO: Got endpoints: latency-svc-nwrqs [325.274676ms]
Feb 27 02:28:43.916: INFO: Created: latency-svc-bbzwx
Feb 27 02:28:43.930: INFO: Got endpoints: latency-svc-bbzwx [362.991584ms]
Feb 27 02:28:43.937: INFO: Created: latency-svc-vncxl
Feb 27 02:28:43.947: INFO: Got endpoints: latency-svc-vncxl [350.181345ms]
Feb 27 02:28:43.962: INFO: Created: latency-svc-t6hcw
Feb 27 02:28:43.973: INFO: Got endpoints: latency-svc-t6hcw [357.026962ms]
Feb 27 02:28:43.980: INFO: Created: latency-svc-dcdx2
Feb 27 02:28:44.003: INFO: Got endpoints: latency-svc-dcdx2 [370.439412ms]
Feb 27 02:28:44.022: INFO: Created: latency-svc-dtvhk
Feb 27 02:28:44.022: INFO: Got endpoints: latency-svc-dtvhk [375.996799ms]
Feb 27 02:28:44.045: INFO: Created: latency-svc-bpc2f
Feb 27 02:28:44.056: INFO: Got endpoints: latency-svc-bpc2f [381.585523ms]
Feb 27 02:28:44.062: INFO: Created: latency-svc-r62zt
Feb 27 02:28:44.092: INFO: Got endpoints: latency-svc-r62zt [382.693458ms]
Feb 27 02:28:44.100: INFO: Created: latency-svc-8wkd4
Feb 27 02:28:44.116: INFO: Got endpoints: latency-svc-8wkd4 [369.263833ms]
Feb 27 02:28:44.134: INFO: Created: latency-svc-dlbdg
Feb 27 02:28:44.137: INFO: Created: latency-svc-kz78d
Feb 27 02:28:44.149: INFO: Got endpoints: latency-svc-dlbdg [386.7517ms]
Feb 27 02:28:44.157: INFO: Got endpoints: latency-svc-kz78d [377.432054ms]
Feb 27 02:28:44.167: INFO: Created: latency-svc-nrtl7
Feb 27 02:28:44.177: INFO: Got endpoints: latency-svc-nrtl7 [375.786312ms]
Feb 27 02:28:44.191: INFO: Created: latency-svc-hhjpd
Feb 27 02:28:44.197: INFO: Created: latency-svc-j4cvq
Feb 27 02:28:44.202: INFO: Got endpoints: latency-svc-hhjpd [381.975093ms]
Feb 27 02:28:44.226: INFO: Got endpoints: latency-svc-j4cvq [379.376609ms]
Feb 27 02:28:44.240: INFO: Created: latency-svc-j976f
Feb 27 02:28:44.257: INFO: Got endpoints: latency-svc-j976f [380.720464ms]
Feb 27 02:28:44.274: INFO: Created: latency-svc-dpjnk
Feb 27 02:28:44.299: INFO: Created: latency-svc-9l7md
Feb 27 02:28:44.314: INFO: Got endpoints: latency-svc-dpjnk [422.225521ms]
Feb 27 02:28:44.363: INFO: Got endpoints: latency-svc-9l7md [433.45272ms]
Feb 27 02:28:44.363: INFO: Created: latency-svc-wr9th
Feb 27 02:28:44.431: INFO: Got endpoints: latency-svc-wr9th [484.581531ms]
Feb 27 02:28:44.484: INFO: Created: latency-svc-9s7rg
Feb 27 02:28:44.498: INFO: Created: latency-svc-87xjr
Feb 27 02:28:44.518: INFO: Got endpoints: latency-svc-9s7rg [515.508592ms]
Feb 27 02:28:44.518: INFO: Got endpoints: latency-svc-87xjr [545.169273ms]
Feb 27 02:28:44.543: INFO: Created: latency-svc-pkpjz
Feb 27 02:28:44.565: INFO: Created: latency-svc-m9gpv
Feb 27 02:28:44.569: INFO: Got endpoints: latency-svc-pkpjz [547.285009ms]
Feb 27 02:28:44.599: INFO: Created: latency-svc-r6z4k
Feb 27 02:28:44.606: INFO: Got endpoints: latency-svc-m9gpv [550.51856ms]
Feb 27 02:28:44.624: INFO: Created: latency-svc-jg54g
Feb 27 02:28:44.660: INFO: Created: latency-svc-bmxnd
Feb 27 02:28:44.664: INFO: Got endpoints: latency-svc-r6z4k [572.138359ms]
Feb 27 02:28:44.687: INFO: Created: latency-svc-9tx7l
Feb 27 02:28:44.717: INFO: Got endpoints: latency-svc-jg54g [601.630967ms]
Feb 27 02:28:44.718: INFO: Created: latency-svc-j9qbs
Feb 27 02:28:44.740: INFO: Created: latency-svc-58jjt
Feb 27 02:28:44.751: INFO: Created: latency-svc-l5z9q
Feb 27 02:28:44.759: INFO: Got endpoints: latency-svc-bmxnd [609.433799ms]
Feb 27 02:28:44.794: INFO: Created: latency-svc-qkbvb
Feb 27 02:28:44.809: INFO: Got endpoints: latency-svc-9tx7l [651.939245ms]
Feb 27 02:28:44.825: INFO: Created: latency-svc-dd9rx
Feb 27 02:28:44.845: INFO: Created: latency-svc-h8kqk
Feb 27 02:28:44.881: INFO: Got endpoints: latency-svc-j9qbs [704.1561ms]
Feb 27 02:28:44.884: INFO: Created: latency-svc-4q6bk
Feb 27 02:28:44.913: INFO: Got endpoints: latency-svc-58jjt [711.042156ms]
Feb 27 02:28:44.914: INFO: Created: latency-svc-kwnjg
Feb 27 02:28:44.924: INFO: Created: latency-svc-t9lpl
Feb 27 02:28:44.943: INFO: Created: latency-svc-skdbr
Feb 27 02:28:44.961: INFO: Got endpoints: latency-svc-l5z9q [735.172146ms]
Feb 27 02:28:44.980: INFO: Created: latency-svc-m5dp5
Feb 27 02:28:44.991: INFO: Created: latency-svc-q4vwz
Feb 27 02:28:45.006: INFO: Got endpoints: latency-svc-qkbvb [749.757675ms]
Feb 27 02:28:45.027: INFO: Created: latency-svc-z4hs4
Feb 27 02:28:45.061: INFO: Got endpoints: latency-svc-dd9rx [697.3839ms]
Feb 27 02:28:45.075: INFO: Created: latency-svc-hngzb
Feb 27 02:28:45.077: INFO: Created: latency-svc-2bqj5
Feb 27 02:28:45.108: INFO: Created: latency-svc-5w5s6
Feb 27 02:28:45.116: INFO: Got endpoints: latency-svc-h8kqk [801.868822ms]
Feb 27 02:28:45.130: INFO: Created: latency-svc-5m5f8
Feb 27 02:28:45.158: INFO: Created: latency-svc-vgmts
Feb 27 02:28:45.172: INFO: Got endpoints: latency-svc-4q6bk [740.834966ms]
Feb 27 02:28:45.207: INFO: Got endpoints: latency-svc-kwnjg [689.017961ms]
Feb 27 02:28:45.232: INFO: Created: latency-svc-9k4vl
Feb 27 02:28:45.249: INFO: Created: latency-svc-v8887
Feb 27 02:28:45.261: INFO: Got endpoints: latency-svc-t9lpl [743.062999ms]
Feb 27 02:28:45.269: INFO: Created: latency-svc-2rr7d
Feb 27 02:28:45.298: INFO: Created: latency-svc-fsmtq
Feb 27 02:28:45.311: INFO: Got endpoints: latency-svc-skdbr [741.339156ms]
Feb 27 02:28:45.335: INFO: Created: latency-svc-8qtc4
Feb 27 02:28:45.349: INFO: Created: latency-svc-8ksk9
Feb 27 02:28:45.366: INFO: Got endpoints: latency-svc-m5dp5 [701.881264ms]
Feb 27 02:28:45.378: INFO: Created: latency-svc-lgvfd
Feb 27 02:28:45.422: INFO: Created: latency-svc-lp4sp
Feb 27 02:28:45.426: INFO: Got endpoints: latency-svc-q4vwz [819.569371ms]
Feb 27 02:28:45.479: INFO: Got endpoints: latency-svc-z4hs4 [761.995322ms]
Feb 27 02:28:45.479: INFO: Created: latency-svc-6mn2h
Feb 27 02:28:45.497: INFO: Created: latency-svc-qjldv
Feb 27 02:28:45.507: INFO: Got endpoints: latency-svc-hngzb [747.882084ms]
Feb 27 02:28:45.529: INFO: Created: latency-svc-zdqsz
Feb 27 02:28:45.561: INFO: Got endpoints: latency-svc-2bqj5 [751.712531ms]
Feb 27 02:28:45.605: INFO: Created: latency-svc-2sx5r
Feb 27 02:28:45.623: INFO: Got endpoints: latency-svc-5w5s6 [741.641329ms]
Feb 27 02:28:45.645: INFO: Created: latency-svc-9lrtd
Feb 27 02:28:45.661: INFO: Got endpoints: latency-svc-5m5f8 [747.767864ms]
Feb 27 02:28:45.698: INFO: Created: latency-svc-xx67g
Feb 27 02:28:45.710: INFO: Got endpoints: latency-svc-vgmts [748.578786ms]
Feb 27 02:28:45.730: INFO: Created: latency-svc-sl68q
Feb 27 02:28:45.757: INFO: Got endpoints: latency-svc-9k4vl [750.831189ms]
Feb 27 02:28:45.779: INFO: Created: latency-svc-gtw78
Feb 27 02:28:45.813: INFO: Got endpoints: latency-svc-v8887 [751.912069ms]
Feb 27 02:28:45.861: INFO: Got endpoints: latency-svc-2rr7d [745.1195ms]
Feb 27 02:28:45.862: INFO: Created: latency-svc-t7gp6
Feb 27 02:28:45.890: INFO: Created: latency-svc-h8z5z
Feb 27 02:28:45.913: INFO: Got endpoints: latency-svc-fsmtq [741.013ms]
Feb 27 02:28:45.953: INFO: Created: latency-svc-wfd75
Feb 27 02:28:45.961: INFO: Got endpoints: latency-svc-8qtc4 [754.120886ms]
Feb 27 02:28:46.010: INFO: Got endpoints: latency-svc-8ksk9 [749.063314ms]
Feb 27 02:28:46.036: INFO: Created: latency-svc-ng8xq
Feb 27 02:28:46.054: INFO: Created: latency-svc-t8dwx
Feb 27 02:28:46.058: INFO: Got endpoints: latency-svc-lgvfd [746.817018ms]
Feb 27 02:28:46.079: INFO: Created: latency-svc-qcfzs
Feb 27 02:28:46.117: INFO: Got endpoints: latency-svc-lp4sp [751.167465ms]
Feb 27 02:28:46.138: INFO: Created: latency-svc-xcg8b
Feb 27 02:28:46.158: INFO: Got endpoints: latency-svc-6mn2h [732.194895ms]
Feb 27 02:28:46.178: INFO: Created: latency-svc-ppqzj
Feb 27 02:28:46.207: INFO: Got endpoints: latency-svc-qjldv [727.908553ms]
Feb 27 02:28:46.243: INFO: Created: latency-svc-7mhcx
Feb 27 02:28:46.262: INFO: Got endpoints: latency-svc-zdqsz [755.127476ms]
Feb 27 02:28:46.294: INFO: Created: latency-svc-gwdrb
Feb 27 02:28:46.307: INFO: Got endpoints: latency-svc-2sx5r [745.844803ms]
Feb 27 02:28:46.361: INFO: Created: latency-svc-kcvmf
Feb 27 02:28:46.375: INFO: Got endpoints: latency-svc-9lrtd [751.546704ms]
Feb 27 02:28:46.407: INFO: Got endpoints: latency-svc-xx67g [746.153446ms]
Feb 27 02:28:46.409: INFO: Created: latency-svc-8zvjt
Feb 27 02:28:46.443: INFO: Created: latency-svc-rsgv4
Feb 27 02:28:46.459: INFO: Got endpoints: latency-svc-sl68q [749.268688ms]
Feb 27 02:28:46.575: INFO: Got endpoints: latency-svc-gtw78 [817.639858ms]
Feb 27 02:28:46.579: INFO: Got endpoints: latency-svc-t7gp6 [766.086545ms]
Feb 27 02:28:46.579: INFO: Created: latency-svc-dpgvl
Feb 27 02:28:46.616: INFO: Created: latency-svc-k5qk2
Feb 27 02:28:46.624: INFO: Got endpoints: latency-svc-h8z5z [762.368556ms]
Feb 27 02:28:46.657: INFO: Created: latency-svc-75654
Feb 27 02:28:46.662: INFO: Got endpoints: latency-svc-wfd75 [748.322399ms]
Feb 27 02:28:46.685: INFO: Created: latency-svc-j25z2
Feb 27 02:28:46.707: INFO: Created: latency-svc-hmbbx
Feb 27 02:28:46.708: INFO: Got endpoints: latency-svc-ng8xq [746.712164ms]
Feb 27 02:28:46.763: INFO: Got endpoints: latency-svc-t8dwx [752.190872ms]
Feb 27 02:28:46.793: INFO: Created: latency-svc-7mq78
Feb 27 02:28:46.796: INFO: Created: latency-svc-6bd7x
Feb 27 02:28:46.814: INFO: Got endpoints: latency-svc-qcfzs [756.100421ms]
Feb 27 02:28:46.829: INFO: Created: latency-svc-9zw8q
Feb 27 02:28:46.858: INFO: Got endpoints: latency-svc-xcg8b [740.634676ms]
Feb 27 02:28:46.916: INFO: Got endpoints: latency-svc-ppqzj [757.560843ms]
Feb 27 02:28:46.930: INFO: Created: latency-svc-wv4x8
Feb 27 02:28:46.939: INFO: Created: latency-svc-lc6fr
Feb 27 02:28:46.962: INFO: Got endpoints: latency-svc-7mhcx [754.81166ms]
Feb 27 02:28:46.984: INFO: Created: latency-svc-5swww
Feb 27 02:28:47.016: INFO: Got endpoints: latency-svc-gwdrb [754.705133ms]
Feb 27 02:28:47.045: INFO: Created: latency-svc-jbt8b
Feb 27 02:28:47.057: INFO: Got endpoints: latency-svc-kcvmf [749.82078ms]
Feb 27 02:28:47.102: INFO: Created: latency-svc-87jll
Feb 27 02:28:47.111: INFO: Got endpoints: latency-svc-8zvjt [735.877853ms]
Feb 27 02:28:47.153: INFO: Created: latency-svc-w2669
Feb 27 02:28:47.157: INFO: Got endpoints: latency-svc-rsgv4 [750.67015ms]
Feb 27 02:28:47.182: INFO: Created: latency-svc-dpqbt
Feb 27 02:28:47.218: INFO: Got endpoints: latency-svc-dpgvl [758.850114ms]
Feb 27 02:28:47.234: INFO: Created: latency-svc-7xscp
Feb 27 02:28:47.256: INFO: Got endpoints: latency-svc-k5qk2 [681.462776ms]
Feb 27 02:28:47.294: INFO: Created: latency-svc-x98g6
Feb 27 02:28:47.352: INFO: Got endpoints: latency-svc-75654 [773.316849ms]
Feb 27 02:28:47.409: INFO: Got endpoints: latency-svc-j25z2 [784.952586ms]
Feb 27 02:28:47.419: INFO: Got endpoints: latency-svc-hmbbx [757.369022ms]
Feb 27 02:28:47.433: INFO: Created: latency-svc-6tffl
Feb 27 02:28:47.451: INFO: Created: latency-svc-mn6p8
Feb 27 02:28:47.467: INFO: Got endpoints: latency-svc-7mq78 [758.697207ms]
Feb 27 02:28:47.469: INFO: Created: latency-svc-449zf
Feb 27 02:28:47.499: INFO: Created: latency-svc-m7jvt
Feb 27 02:28:47.508: INFO: Got endpoints: latency-svc-6bd7x [745.230062ms]
Feb 27 02:28:47.544: INFO: Created: latency-svc-rb6lp
Feb 27 02:28:47.559: INFO: Got endpoints: latency-svc-9zw8q [744.917302ms]
Feb 27 02:28:47.591: INFO: Created: latency-svc-sch9w
Feb 27 02:28:47.614: INFO: Got endpoints: latency-svc-wv4x8 [755.888061ms]
Feb 27 02:28:47.648: INFO: Created: latency-svc-t727k
Feb 27 02:28:47.667: INFO: Got endpoints: latency-svc-lc6fr [750.880131ms]
Feb 27 02:28:47.686: INFO: Created: latency-svc-cphql
Feb 27 02:28:47.705: INFO: Got endpoints: latency-svc-5swww [743.390783ms]
Feb 27 02:28:47.761: INFO: Got endpoints: latency-svc-jbt8b [744.09072ms]
Feb 27 02:28:47.761: INFO: Created: latency-svc-jjnfb
Feb 27 02:28:47.789: INFO: Created: latency-svc-5b8dg
Feb 27 02:28:47.813: INFO: Got endpoints: latency-svc-87jll [755.962078ms]
Feb 27 02:28:47.839: INFO: Created: latency-svc-x8kng
Feb 27 02:28:47.857: INFO: Got endpoints: latency-svc-w2669 [746.782446ms]
Feb 27 02:28:47.897: INFO: Created: latency-svc-twqqs
Feb 27 02:28:47.909: INFO: Got endpoints: latency-svc-dpqbt [751.239816ms]
Feb 27 02:28:47.961: INFO: Got endpoints: latency-svc-7xscp [743.02718ms]
Feb 27 02:28:47.975: INFO: Created: latency-svc-47295
Feb 27 02:28:47.984: INFO: Created: latency-svc-m2jtd
Feb 27 02:28:48.008: INFO: Got endpoints: latency-svc-x98g6 [751.728022ms]
Feb 27 02:28:48.026: INFO: Created: latency-svc-tssrn
Feb 27 02:28:48.072: INFO: Got endpoints: latency-svc-6tffl [719.508781ms]
Feb 27 02:28:48.106: INFO: Created: latency-svc-4f6qz
Feb 27 02:28:48.115: INFO: Got endpoints: latency-svc-mn6p8 [706.576899ms]
Feb 27 02:28:48.137: INFO: Created: latency-svc-t5jrr
Feb 27 02:28:48.156: INFO: Got endpoints: latency-svc-449zf [736.788345ms]
Feb 27 02:28:48.181: INFO: Created: latency-svc-9rq6c
Feb 27 02:28:48.206: INFO: Got endpoints: latency-svc-m7jvt [739.430646ms]
Feb 27 02:28:48.227: INFO: Created: latency-svc-kcw6q
Feb 27 02:28:48.269: INFO: Got endpoints: latency-svc-rb6lp [760.948214ms]
Feb 27 02:28:48.300: INFO: Created: latency-svc-t2ck7
Feb 27 02:28:48.313: INFO: Got endpoints: latency-svc-sch9w [754.577335ms]
Feb 27 02:28:48.350: INFO: Created: latency-svc-bghk4
Feb 27 02:28:48.360: INFO: Got endpoints: latency-svc-t727k [746.140667ms]
Feb 27 02:28:48.428: INFO: Got endpoints: latency-svc-cphql [761.137333ms]
Feb 27 02:28:48.429: INFO: Created: latency-svc-7v9w7
Feb 27 02:28:48.460: INFO: Got endpoints: latency-svc-jjnfb [754.207007ms]
Feb 27 02:28:48.466: INFO: Created: latency-svc-mhvbd
Feb 27 02:28:48.482: INFO: Created: latency-svc-2nqrt
Feb 27 02:28:48.513: INFO: Got endpoints: latency-svc-5b8dg [752.065756ms]
Feb 27 02:28:48.546: INFO: Created: latency-svc-kthng
Feb 27 02:28:48.558: INFO: Got endpoints: latency-svc-x8kng [745.238287ms]
Feb 27 02:28:48.586: INFO: Created: latency-svc-kqcmz
Feb 27 02:28:48.606: INFO: Got endpoints: latency-svc-twqqs [748.331415ms]
Feb 27 02:28:48.627: INFO: Created: latency-svc-5qx98
Feb 27 02:28:48.656: INFO: Got endpoints: latency-svc-47295 [746.890551ms]
Feb 27 02:28:48.683: INFO: Created: latency-svc-d9pkt
Feb 27 02:28:48.709: INFO: Got endpoints: latency-svc-m2jtd [748.608548ms]
Feb 27 02:28:48.729: INFO: Created: latency-svc-hwmmz
Feb 27 02:28:48.755: INFO: Got endpoints: latency-svc-tssrn [747.009791ms]
Feb 27 02:28:48.821: INFO: Created: latency-svc-k6llf
Feb 27 02:28:48.825: INFO: Got endpoints: latency-svc-4f6qz [753.635566ms]
Feb 27 02:28:48.855: INFO: Created: latency-svc-vwb46
Feb 27 02:28:48.860: INFO: Got endpoints: latency-svc-t5jrr [744.043763ms]
Feb 27 02:28:48.875: INFO: Created: latency-svc-d4s8m
Feb 27 02:28:48.905: INFO: Got endpoints: latency-svc-9rq6c [749.003957ms]
Feb 27 02:28:48.925: INFO: Created: latency-svc-76wp2
Feb 27 02:28:48.962: INFO: Got endpoints: latency-svc-kcw6q [756.035337ms]
Feb 27 02:28:48.983: INFO: Created: latency-svc-xp4mj
Feb 27 02:28:49.016: INFO: Got endpoints: latency-svc-t2ck7 [747.654807ms]
Feb 27 02:28:49.031: INFO: Created: latency-svc-kvvrz
Feb 27 02:28:49.068: INFO: Got endpoints: latency-svc-bghk4 [754.75423ms]
Feb 27 02:28:49.103: INFO: Created: latency-svc-hfvzk
Feb 27 02:28:49.109: INFO: Got endpoints: latency-svc-7v9w7 [748.630254ms]
Feb 27 02:28:49.132: INFO: Created: latency-svc-5sw7l
Feb 27 02:28:49.161: INFO: Got endpoints: latency-svc-mhvbd [733.042261ms]
Feb 27 02:28:49.192: INFO: Created: latency-svc-kfddt
Feb 27 02:28:49.205: INFO: Got endpoints: latency-svc-2nqrt [745.109815ms]
Feb 27 02:28:49.241: INFO: Created: latency-svc-7l8zs
Feb 27 02:28:49.261: INFO: Got endpoints: latency-svc-kthng [748.120768ms]
Feb 27 02:28:49.295: INFO: Created: latency-svc-qvshb
Feb 27 02:28:49.312: INFO: Got endpoints: latency-svc-kqcmz [753.951901ms]
Feb 27 02:28:49.343: INFO: Created: latency-svc-znfb9
Feb 27 02:28:49.363: INFO: Got endpoints: latency-svc-5qx98 [756.864726ms]
Feb 27 02:28:49.390: INFO: Created: latency-svc-wx8fm
Feb 27 02:28:49.405: INFO: Got endpoints: latency-svc-d9pkt [749.746576ms]
Feb 27 02:28:49.434: INFO: Created: latency-svc-tv7m6
Feb 27 02:28:49.467: INFO: Got endpoints: latency-svc-hwmmz [757.727489ms]
Feb 27 02:28:49.499: INFO: Created: latency-svc-vskrd
Feb 27 02:28:49.505: INFO: Got endpoints: latency-svc-k6llf [750.050548ms]
Feb 27 02:28:49.551: INFO: Created: latency-svc-8qpt2
Feb 27 02:28:49.562: INFO: Got endpoints: latency-svc-vwb46 [736.814669ms]
Feb 27 02:28:49.592: INFO: Created: latency-svc-fcbkw
Feb 27 02:28:49.608: INFO: Got endpoints: latency-svc-d4s8m [747.920918ms]
Feb 27 02:28:49.622: INFO: Created: latency-svc-pt4s4
Feb 27 02:28:49.657: INFO: Got endpoints: latency-svc-76wp2 [752.526164ms]
Feb 27 02:28:49.683: INFO: Created: latency-svc-s5snc
Feb 27 02:28:49.708: INFO: Got endpoints: latency-svc-xp4mj [745.69224ms]
Feb 27 02:28:49.746: INFO: Created: latency-svc-gl7h4
Feb 27 02:28:49.761: INFO: Got endpoints: latency-svc-kvvrz [744.7754ms]
Feb 27 02:28:49.791: INFO: Created: latency-svc-n4cpz
Feb 27 02:28:49.810: INFO: Got endpoints: latency-svc-hfvzk [742.062065ms]
Feb 27 02:28:49.827: INFO: Created: latency-svc-9lk24
Feb 27 02:28:49.871: INFO: Got endpoints: latency-svc-5sw7l [762.81618ms]
Feb 27 02:28:49.961: INFO: Got endpoints: latency-svc-kfddt [800.00926ms]
Feb 27 02:28:49.975: INFO: Got endpoints: latency-svc-7l8zs [769.957268ms]
Feb 27 02:28:49.993: INFO: Created: latency-svc-tkchd
Feb 27 02:28:50.002: INFO: Created: latency-svc-zktj9
Feb 27 02:28:50.012: INFO: Got endpoints: latency-svc-qvshb [751.388875ms]
Feb 27 02:28:50.027: INFO: Created: latency-svc-6fws7
Feb 27 02:28:50.066: INFO: Got endpoints: latency-svc-znfb9 [754.287052ms]
Feb 27 02:28:50.068: INFO: Created: latency-svc-cl958
Feb 27 02:28:50.106: INFO: Created: latency-svc-tsvlx
Feb 27 02:28:50.108: INFO: Got endpoints: latency-svc-wx8fm [745.589715ms]
Feb 27 02:28:50.130: INFO: Created: latency-svc-l8jmq
Feb 27 02:28:50.156: INFO: Got endpoints: latency-svc-tv7m6 [750.759508ms]
Feb 27 02:28:50.180: INFO: Created: latency-svc-vmcj8
Feb 27 02:28:50.210: INFO: Got endpoints: latency-svc-vskrd [742.471822ms]
Feb 27 02:28:50.248: INFO: Created: latency-svc-n9kfw
Feb 27 02:28:50.261: INFO: Got endpoints: latency-svc-8qpt2 [755.749047ms]
Feb 27 02:28:50.295: INFO: Created: latency-svc-q466k
Feb 27 02:28:50.312: INFO: Got endpoints: latency-svc-fcbkw [749.486474ms]
Feb 27 02:28:50.331: INFO: Created: latency-svc-wnkzh
Feb 27 02:28:50.360: INFO: Got endpoints: latency-svc-pt4s4 [752.085866ms]
Feb 27 02:28:50.377: INFO: Created: latency-svc-mrnwd
Feb 27 02:28:50.407: INFO: Got endpoints: latency-svc-s5snc [750.095669ms]
Feb 27 02:28:50.451: INFO: Created: latency-svc-rnwrv
Feb 27 02:28:50.456: INFO: Got endpoints: latency-svc-gl7h4 [748.093887ms]
Feb 27 02:28:50.477: INFO: Created: latency-svc-zns4v
Feb 27 02:28:50.514: INFO: Got endpoints: latency-svc-n4cpz [752.331889ms]
Feb 27 02:28:50.534: INFO: Created: latency-svc-k5q58
Feb 27 02:28:50.557: INFO: Got endpoints: latency-svc-9lk24 [746.888714ms]
Feb 27 02:28:50.574: INFO: Created: latency-svc-2ks8x
Feb 27 02:28:50.614: INFO: Got endpoints: latency-svc-tkchd [742.107813ms]
Feb 27 02:28:50.645: INFO: Created: latency-svc-jf9q8
Feb 27 02:28:50.660: INFO: Got endpoints: latency-svc-zktj9 [699.053416ms]
Feb 27 02:28:50.707: INFO: Got endpoints: latency-svc-6fws7 [731.805413ms]
Feb 27 02:28:50.758: INFO: Got endpoints: latency-svc-cl958 [745.421681ms]
Feb 27 02:28:50.811: INFO: Got endpoints: latency-svc-tsvlx [744.60289ms]
Feb 27 02:28:50.857: INFO: Got endpoints: latency-svc-l8jmq [748.244603ms]
Feb 27 02:28:50.914: INFO: Got endpoints: latency-svc-vmcj8 [757.975634ms]
Feb 27 02:28:50.963: INFO: Got endpoints: latency-svc-n9kfw [753.320096ms]
Feb 27 02:28:51.013: INFO: Got endpoints: latency-svc-q466k [752.285938ms]
Feb 27 02:28:51.062: INFO: Got endpoints: latency-svc-wnkzh [750.220355ms]
Feb 27 02:28:51.114: INFO: Got endpoints: latency-svc-mrnwd [754.695977ms]
Feb 27 02:28:51.159: INFO: Got endpoints: latency-svc-rnwrv [751.360927ms]
Feb 27 02:28:51.213: INFO: Got endpoints: latency-svc-zns4v [757.048519ms]
Feb 27 02:28:51.257: INFO: Got endpoints: latency-svc-k5q58 [742.872657ms]
Feb 27 02:28:51.306: INFO: Got endpoints: latency-svc-2ks8x [748.954319ms]
Feb 27 02:28:51.357: INFO: Got endpoints: latency-svc-jf9q8 [743.822393ms]
Feb 27 02:28:51.357: INFO: Latencies: [37.081325ms 73.980364ms 74.26164ms 96.161603ms 100.873978ms 104.969161ms 119.200348ms 119.895932ms 133.810935ms 136.814252ms 157.974544ms 161.793989ms 178.725398ms 193.105056ms 215.588959ms 219.364414ms 232.882245ms 243.093507ms 245.697883ms 259.908049ms 263.890202ms 279.646193ms 279.758581ms 302.374109ms 309.048264ms 314.498981ms 325.274676ms 337.650873ms 342.144227ms 350.181345ms 357.026962ms 358.078964ms 361.460496ms 362.991584ms 369.263833ms 370.439412ms 375.786312ms 375.996799ms 377.432054ms 379.376609ms 380.720464ms 381.585523ms 381.975093ms 382.693458ms 386.7517ms 422.225521ms 425.739642ms 433.45272ms 440.374233ms 440.484728ms 484.581531ms 509.212878ms 509.481706ms 515.508592ms 545.169273ms 546.859488ms 547.285009ms 550.51856ms 572.138359ms 580.924161ms 585.060431ms 596.761344ms 601.630967ms 609.433799ms 614.381874ms 626.318069ms 651.939245ms 654.126164ms 678.180637ms 681.462776ms 689.017961ms 697.3839ms 699.053416ms 701.881264ms 704.1561ms 706.576899ms 711.042156ms 719.508781ms 727.908553ms 731.805413ms 732.194895ms 733.042261ms 735.172146ms 735.877853ms 736.788345ms 736.814669ms 739.430646ms 740.634676ms 740.834966ms 741.013ms 741.339156ms 741.641329ms 742.062065ms 742.107813ms 742.471822ms 742.872657ms 743.02718ms 743.062999ms 743.390783ms 743.822393ms 744.043763ms 744.09072ms 744.60289ms 744.7754ms 744.917302ms 745.109815ms 745.1195ms 745.230062ms 745.238287ms 745.421681ms 745.589715ms 745.69224ms 745.844803ms 746.140667ms 746.153446ms 746.712164ms 746.782446ms 746.817018ms 746.888714ms 746.890551ms 747.009791ms 747.654807ms 747.767864ms 747.882084ms 747.920918ms 748.093887ms 748.120768ms 748.244603ms 748.322399ms 748.331415ms 748.578786ms 748.608548ms 748.630254ms 748.954319ms 749.003957ms 749.063314ms 749.268688ms 749.486474ms 749.746576ms 749.757675ms 749.82078ms 750.050548ms 750.095669ms 750.220355ms 750.67015ms 750.759508ms 750.831189ms 750.880131ms 751.167465ms 751.239816ms 751.360927ms 751.388875ms 751.546704ms 751.712531ms 751.728022ms 751.912069ms 752.065756ms 752.085866ms 752.190872ms 752.285938ms 752.331889ms 752.526164ms 753.320096ms 753.635566ms 753.951901ms 754.120886ms 754.207007ms 754.287052ms 754.577335ms 754.695977ms 754.705133ms 754.75423ms 754.81166ms 755.127476ms 755.749047ms 755.888061ms 755.962078ms 756.035337ms 756.100421ms 756.864726ms 757.048519ms 757.369022ms 757.560843ms 757.727489ms 757.975634ms 758.697207ms 758.850114ms 760.948214ms 761.137333ms 761.995322ms 762.368556ms 762.81618ms 766.086545ms 769.957268ms 773.316849ms 784.952586ms 800.00926ms 801.868822ms 817.639858ms 819.569371ms]
Feb 27 02:28:51.358: INFO: 50 %ile: 744.043763ms
Feb 27 02:28:51.358: INFO: 90 %ile: 757.048519ms
Feb 27 02:28:51.358: INFO: 99 %ile: 817.639858ms
Feb 27 02:28:51.358: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Feb 27 02:28:51.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-3786" for this suite. 02/27/23 02:28:51.366
------------------------------
• [SLOW TEST] [10.789 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:28:40.588
    Feb 27 02:28:40.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svc-latency 02/27/23 02:28:40.589
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:40.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:40.609
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Feb 27 02:28:40.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3786 02/27/23 02:28:40.612
    I0227 02:28:40.619446      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3786, replica count: 1
    I0227 02:28:41.670177      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0227 02:28:42.670265      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:28:42.798: INFO: Created: latency-svc-jb2x6
    Feb 27 02:28:42.811: INFO: Got endpoints: latency-svc-jb2x6 [40.954834ms]
    Feb 27 02:28:42.838: INFO: Created: latency-svc-dl5tf
    Feb 27 02:28:42.849: INFO: Got endpoints: latency-svc-dl5tf [37.081325ms]
    Feb 27 02:28:42.866: INFO: Created: latency-svc-7blvt
    Feb 27 02:28:42.874: INFO: Created: latency-svc-nhcws
    Feb 27 02:28:42.886: INFO: Got endpoints: latency-svc-nhcws [74.26164ms]
    Feb 27 02:28:42.886: INFO: Got endpoints: latency-svc-7blvt [73.980364ms]
    Feb 27 02:28:42.896: INFO: Created: latency-svc-ggzk8
    Feb 27 02:28:42.913: INFO: Got endpoints: latency-svc-ggzk8 [100.873978ms]
    Feb 27 02:28:42.923: INFO: Created: latency-svc-wtpht
    Feb 27 02:28:42.931: INFO: Got endpoints: latency-svc-wtpht [119.200348ms]
    Feb 27 02:28:42.936: INFO: Created: latency-svc-d954l
    Feb 27 02:28:42.949: INFO: Got endpoints: latency-svc-d954l [136.814252ms]
    Feb 27 02:28:42.961: INFO: Created: latency-svc-dlrkl
    Feb 27 02:28:42.970: INFO: Got endpoints: latency-svc-dlrkl [157.974544ms]
    Feb 27 02:28:42.984: INFO: Created: latency-svc-wggpk
    Feb 27 02:28:42.991: INFO: Got endpoints: latency-svc-wggpk [178.725398ms]
    Feb 27 02:28:43.004: INFO: Created: latency-svc-zkgqs
    Feb 27 02:28:43.027: INFO: Got endpoints: latency-svc-zkgqs [215.588959ms]
    Feb 27 02:28:43.043: INFO: Created: latency-svc-dftnz
    Feb 27 02:28:43.058: INFO: Got endpoints: latency-svc-dftnz [245.697883ms]
    Feb 27 02:28:43.066: INFO: Created: latency-svc-pmscp
    Feb 27 02:28:43.072: INFO: Got endpoints: latency-svc-pmscp [259.908049ms]
    Feb 27 02:28:43.080: INFO: Created: latency-svc-jmbps
    Feb 27 02:28:43.114: INFO: Got endpoints: latency-svc-jmbps [302.374109ms]
    Feb 27 02:28:43.122: INFO: Created: latency-svc-5shtq
    Feb 27 02:28:43.126: INFO: Got endpoints: latency-svc-5shtq [314.498981ms]
    Feb 27 02:28:43.141: INFO: Created: latency-svc-8txmj
    Feb 27 02:28:43.154: INFO: Got endpoints: latency-svc-8txmj [342.144227ms]
    Feb 27 02:28:43.169: INFO: Created: latency-svc-rght6
    Feb 27 02:28:43.173: INFO: Got endpoints: latency-svc-rght6 [361.460496ms]
    Feb 27 02:28:43.482: INFO: Created: latency-svc-bql82
    Feb 27 02:28:43.483: INFO: Created: latency-svc-c6xhr
    Feb 27 02:28:43.484: INFO: Created: latency-svc-hjr6z
    Feb 27 02:28:43.484: INFO: Created: latency-svc-fq9rl
    Feb 27 02:28:43.484: INFO: Created: latency-svc-jvgcr
    Feb 27 02:28:43.484: INFO: Created: latency-svc-nv2dl
    Feb 27 02:28:43.486: INFO: Created: latency-svc-8svm5
    Feb 27 02:28:43.486: INFO: Created: latency-svc-p8fc6
    Feb 27 02:28:43.486: INFO: Created: latency-svc-bl9mg
    Feb 27 02:28:43.486: INFO: Created: latency-svc-g28zp
    Feb 27 02:28:43.486: INFO: Created: latency-svc-tbbng
    Feb 27 02:28:43.489: INFO: Created: latency-svc-668p7
    Feb 27 02:28:43.489: INFO: Created: latency-svc-pgknf
    Feb 27 02:28:43.491: INFO: Created: latency-svc-l7gf2
    Feb 27 02:28:43.491: INFO: Created: latency-svc-s9rj5
    Feb 27 02:28:43.500: INFO: Got endpoints: latency-svc-bql82 [614.381874ms]
    Feb 27 02:28:43.511: INFO: Got endpoints: latency-svc-668p7 [337.650873ms]
    Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-g28zp [358.078964ms]
    Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-p8fc6 [626.318069ms]
    Feb 27 02:28:43.512: INFO: Got endpoints: latency-svc-8svm5 [440.374233ms]
    Feb 27 02:28:43.516: INFO: Got endpoints: latency-svc-tbbng [585.060431ms]
    Feb 27 02:28:43.527: INFO: Got endpoints: latency-svc-c6xhr [678.180637ms]
    Feb 27 02:28:43.529: INFO: Got endpoints: latency-svc-l7gf2 [580.924161ms]
    Feb 27 02:28:43.537: INFO: Got endpoints: latency-svc-pgknf [509.481706ms]
    Feb 27 02:28:43.537: INFO: Got endpoints: latency-svc-bl9mg [546.859488ms]
    Feb 27 02:28:43.540: INFO: Got endpoints: latency-svc-s9rj5 [425.739642ms]
    Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-hjr6z [596.761344ms]
    Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-fq9rl [654.126164ms]
    Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-jvgcr [509.212878ms]
    Feb 27 02:28:43.567: INFO: Got endpoints: latency-svc-nv2dl [440.484728ms]
    Feb 27 02:28:43.581: INFO: Created: latency-svc-zg2vk
    Feb 27 02:28:43.596: INFO: Got endpoints: latency-svc-zg2vk [96.161603ms]
    Feb 27 02:28:43.607: INFO: Created: latency-svc-r85fq
    Feb 27 02:28:43.614: INFO: Created: latency-svc-kq8nr
    Feb 27 02:28:43.616: INFO: Got endpoints: latency-svc-r85fq [104.969161ms]
    Feb 27 02:28:43.632: INFO: Got endpoints: latency-svc-kq8nr [119.895932ms]
    Feb 27 02:28:43.640: INFO: Created: latency-svc-smr7q
    Feb 27 02:28:43.646: INFO: Got endpoints: latency-svc-smr7q [133.810935ms]
    Feb 27 02:28:43.659: INFO: Created: latency-svc-gwshw
    Feb 27 02:28:43.674: INFO: Got endpoints: latency-svc-gwshw [161.793989ms]
    Feb 27 02:28:43.693: INFO: Created: latency-svc-h6l9m
    Feb 27 02:28:43.709: INFO: Got endpoints: latency-svc-h6l9m [193.105056ms]
    Feb 27 02:28:43.711: INFO: Created: latency-svc-hfm5m
    Feb 27 02:28:43.746: INFO: Got endpoints: latency-svc-hfm5m [219.364414ms]
    Feb 27 02:28:43.754: INFO: Created: latency-svc-kpslj
    Feb 27 02:28:43.762: INFO: Got endpoints: latency-svc-kpslj [232.882245ms]
    Feb 27 02:28:43.769: INFO: Created: latency-svc-8k4xw
    Feb 27 02:28:43.780: INFO: Got endpoints: latency-svc-8k4xw [243.093507ms]
    Feb 27 02:28:43.789: INFO: Created: latency-svc-fhbsh
    Feb 27 02:28:43.801: INFO: Got endpoints: latency-svc-fhbsh [263.890202ms]
    Feb 27 02:28:43.811: INFO: Created: latency-svc-52d2c
    Feb 27 02:28:43.820: INFO: Got endpoints: latency-svc-52d2c [279.646193ms]
    Feb 27 02:28:43.832: INFO: Created: latency-svc-xb69b
    Feb 27 02:28:43.846: INFO: Got endpoints: latency-svc-xb69b [279.758581ms]
    Feb 27 02:28:43.854: INFO: Created: latency-svc-ccn5f
    Feb 27 02:28:43.876: INFO: Got endpoints: latency-svc-ccn5f [309.048264ms]
    Feb 27 02:28:43.885: INFO: Created: latency-svc-nwrqs
    Feb 27 02:28:43.892: INFO: Got endpoints: latency-svc-nwrqs [325.274676ms]
    Feb 27 02:28:43.916: INFO: Created: latency-svc-bbzwx
    Feb 27 02:28:43.930: INFO: Got endpoints: latency-svc-bbzwx [362.991584ms]
    Feb 27 02:28:43.937: INFO: Created: latency-svc-vncxl
    Feb 27 02:28:43.947: INFO: Got endpoints: latency-svc-vncxl [350.181345ms]
    Feb 27 02:28:43.962: INFO: Created: latency-svc-t6hcw
    Feb 27 02:28:43.973: INFO: Got endpoints: latency-svc-t6hcw [357.026962ms]
    Feb 27 02:28:43.980: INFO: Created: latency-svc-dcdx2
    Feb 27 02:28:44.003: INFO: Got endpoints: latency-svc-dcdx2 [370.439412ms]
    Feb 27 02:28:44.022: INFO: Created: latency-svc-dtvhk
    Feb 27 02:28:44.022: INFO: Got endpoints: latency-svc-dtvhk [375.996799ms]
    Feb 27 02:28:44.045: INFO: Created: latency-svc-bpc2f
    Feb 27 02:28:44.056: INFO: Got endpoints: latency-svc-bpc2f [381.585523ms]
    Feb 27 02:28:44.062: INFO: Created: latency-svc-r62zt
    Feb 27 02:28:44.092: INFO: Got endpoints: latency-svc-r62zt [382.693458ms]
    Feb 27 02:28:44.100: INFO: Created: latency-svc-8wkd4
    Feb 27 02:28:44.116: INFO: Got endpoints: latency-svc-8wkd4 [369.263833ms]
    Feb 27 02:28:44.134: INFO: Created: latency-svc-dlbdg
    Feb 27 02:28:44.137: INFO: Created: latency-svc-kz78d
    Feb 27 02:28:44.149: INFO: Got endpoints: latency-svc-dlbdg [386.7517ms]
    Feb 27 02:28:44.157: INFO: Got endpoints: latency-svc-kz78d [377.432054ms]
    Feb 27 02:28:44.167: INFO: Created: latency-svc-nrtl7
    Feb 27 02:28:44.177: INFO: Got endpoints: latency-svc-nrtl7 [375.786312ms]
    Feb 27 02:28:44.191: INFO: Created: latency-svc-hhjpd
    Feb 27 02:28:44.197: INFO: Created: latency-svc-j4cvq
    Feb 27 02:28:44.202: INFO: Got endpoints: latency-svc-hhjpd [381.975093ms]
    Feb 27 02:28:44.226: INFO: Got endpoints: latency-svc-j4cvq [379.376609ms]
    Feb 27 02:28:44.240: INFO: Created: latency-svc-j976f
    Feb 27 02:28:44.257: INFO: Got endpoints: latency-svc-j976f [380.720464ms]
    Feb 27 02:28:44.274: INFO: Created: latency-svc-dpjnk
    Feb 27 02:28:44.299: INFO: Created: latency-svc-9l7md
    Feb 27 02:28:44.314: INFO: Got endpoints: latency-svc-dpjnk [422.225521ms]
    Feb 27 02:28:44.363: INFO: Got endpoints: latency-svc-9l7md [433.45272ms]
    Feb 27 02:28:44.363: INFO: Created: latency-svc-wr9th
    Feb 27 02:28:44.431: INFO: Got endpoints: latency-svc-wr9th [484.581531ms]
    Feb 27 02:28:44.484: INFO: Created: latency-svc-9s7rg
    Feb 27 02:28:44.498: INFO: Created: latency-svc-87xjr
    Feb 27 02:28:44.518: INFO: Got endpoints: latency-svc-9s7rg [515.508592ms]
    Feb 27 02:28:44.518: INFO: Got endpoints: latency-svc-87xjr [545.169273ms]
    Feb 27 02:28:44.543: INFO: Created: latency-svc-pkpjz
    Feb 27 02:28:44.565: INFO: Created: latency-svc-m9gpv
    Feb 27 02:28:44.569: INFO: Got endpoints: latency-svc-pkpjz [547.285009ms]
    Feb 27 02:28:44.599: INFO: Created: latency-svc-r6z4k
    Feb 27 02:28:44.606: INFO: Got endpoints: latency-svc-m9gpv [550.51856ms]
    Feb 27 02:28:44.624: INFO: Created: latency-svc-jg54g
    Feb 27 02:28:44.660: INFO: Created: latency-svc-bmxnd
    Feb 27 02:28:44.664: INFO: Got endpoints: latency-svc-r6z4k [572.138359ms]
    Feb 27 02:28:44.687: INFO: Created: latency-svc-9tx7l
    Feb 27 02:28:44.717: INFO: Got endpoints: latency-svc-jg54g [601.630967ms]
    Feb 27 02:28:44.718: INFO: Created: latency-svc-j9qbs
    Feb 27 02:28:44.740: INFO: Created: latency-svc-58jjt
    Feb 27 02:28:44.751: INFO: Created: latency-svc-l5z9q
    Feb 27 02:28:44.759: INFO: Got endpoints: latency-svc-bmxnd [609.433799ms]
    Feb 27 02:28:44.794: INFO: Created: latency-svc-qkbvb
    Feb 27 02:28:44.809: INFO: Got endpoints: latency-svc-9tx7l [651.939245ms]
    Feb 27 02:28:44.825: INFO: Created: latency-svc-dd9rx
    Feb 27 02:28:44.845: INFO: Created: latency-svc-h8kqk
    Feb 27 02:28:44.881: INFO: Got endpoints: latency-svc-j9qbs [704.1561ms]
    Feb 27 02:28:44.884: INFO: Created: latency-svc-4q6bk
    Feb 27 02:28:44.913: INFO: Got endpoints: latency-svc-58jjt [711.042156ms]
    Feb 27 02:28:44.914: INFO: Created: latency-svc-kwnjg
    Feb 27 02:28:44.924: INFO: Created: latency-svc-t9lpl
    Feb 27 02:28:44.943: INFO: Created: latency-svc-skdbr
    Feb 27 02:28:44.961: INFO: Got endpoints: latency-svc-l5z9q [735.172146ms]
    Feb 27 02:28:44.980: INFO: Created: latency-svc-m5dp5
    Feb 27 02:28:44.991: INFO: Created: latency-svc-q4vwz
    Feb 27 02:28:45.006: INFO: Got endpoints: latency-svc-qkbvb [749.757675ms]
    Feb 27 02:28:45.027: INFO: Created: latency-svc-z4hs4
    Feb 27 02:28:45.061: INFO: Got endpoints: latency-svc-dd9rx [697.3839ms]
    Feb 27 02:28:45.075: INFO: Created: latency-svc-hngzb
    Feb 27 02:28:45.077: INFO: Created: latency-svc-2bqj5
    Feb 27 02:28:45.108: INFO: Created: latency-svc-5w5s6
    Feb 27 02:28:45.116: INFO: Got endpoints: latency-svc-h8kqk [801.868822ms]
    Feb 27 02:28:45.130: INFO: Created: latency-svc-5m5f8
    Feb 27 02:28:45.158: INFO: Created: latency-svc-vgmts
    Feb 27 02:28:45.172: INFO: Got endpoints: latency-svc-4q6bk [740.834966ms]
    Feb 27 02:28:45.207: INFO: Got endpoints: latency-svc-kwnjg [689.017961ms]
    Feb 27 02:28:45.232: INFO: Created: latency-svc-9k4vl
    Feb 27 02:28:45.249: INFO: Created: latency-svc-v8887
    Feb 27 02:28:45.261: INFO: Got endpoints: latency-svc-t9lpl [743.062999ms]
    Feb 27 02:28:45.269: INFO: Created: latency-svc-2rr7d
    Feb 27 02:28:45.298: INFO: Created: latency-svc-fsmtq
    Feb 27 02:28:45.311: INFO: Got endpoints: latency-svc-skdbr [741.339156ms]
    Feb 27 02:28:45.335: INFO: Created: latency-svc-8qtc4
    Feb 27 02:28:45.349: INFO: Created: latency-svc-8ksk9
    Feb 27 02:28:45.366: INFO: Got endpoints: latency-svc-m5dp5 [701.881264ms]
    Feb 27 02:28:45.378: INFO: Created: latency-svc-lgvfd
    Feb 27 02:28:45.422: INFO: Created: latency-svc-lp4sp
    Feb 27 02:28:45.426: INFO: Got endpoints: latency-svc-q4vwz [819.569371ms]
    Feb 27 02:28:45.479: INFO: Got endpoints: latency-svc-z4hs4 [761.995322ms]
    Feb 27 02:28:45.479: INFO: Created: latency-svc-6mn2h
    Feb 27 02:28:45.497: INFO: Created: latency-svc-qjldv
    Feb 27 02:28:45.507: INFO: Got endpoints: latency-svc-hngzb [747.882084ms]
    Feb 27 02:28:45.529: INFO: Created: latency-svc-zdqsz
    Feb 27 02:28:45.561: INFO: Got endpoints: latency-svc-2bqj5 [751.712531ms]
    Feb 27 02:28:45.605: INFO: Created: latency-svc-2sx5r
    Feb 27 02:28:45.623: INFO: Got endpoints: latency-svc-5w5s6 [741.641329ms]
    Feb 27 02:28:45.645: INFO: Created: latency-svc-9lrtd
    Feb 27 02:28:45.661: INFO: Got endpoints: latency-svc-5m5f8 [747.767864ms]
    Feb 27 02:28:45.698: INFO: Created: latency-svc-xx67g
    Feb 27 02:28:45.710: INFO: Got endpoints: latency-svc-vgmts [748.578786ms]
    Feb 27 02:28:45.730: INFO: Created: latency-svc-sl68q
    Feb 27 02:28:45.757: INFO: Got endpoints: latency-svc-9k4vl [750.831189ms]
    Feb 27 02:28:45.779: INFO: Created: latency-svc-gtw78
    Feb 27 02:28:45.813: INFO: Got endpoints: latency-svc-v8887 [751.912069ms]
    Feb 27 02:28:45.861: INFO: Got endpoints: latency-svc-2rr7d [745.1195ms]
    Feb 27 02:28:45.862: INFO: Created: latency-svc-t7gp6
    Feb 27 02:28:45.890: INFO: Created: latency-svc-h8z5z
    Feb 27 02:28:45.913: INFO: Got endpoints: latency-svc-fsmtq [741.013ms]
    Feb 27 02:28:45.953: INFO: Created: latency-svc-wfd75
    Feb 27 02:28:45.961: INFO: Got endpoints: latency-svc-8qtc4 [754.120886ms]
    Feb 27 02:28:46.010: INFO: Got endpoints: latency-svc-8ksk9 [749.063314ms]
    Feb 27 02:28:46.036: INFO: Created: latency-svc-ng8xq
    Feb 27 02:28:46.054: INFO: Created: latency-svc-t8dwx
    Feb 27 02:28:46.058: INFO: Got endpoints: latency-svc-lgvfd [746.817018ms]
    Feb 27 02:28:46.079: INFO: Created: latency-svc-qcfzs
    Feb 27 02:28:46.117: INFO: Got endpoints: latency-svc-lp4sp [751.167465ms]
    Feb 27 02:28:46.138: INFO: Created: latency-svc-xcg8b
    Feb 27 02:28:46.158: INFO: Got endpoints: latency-svc-6mn2h [732.194895ms]
    Feb 27 02:28:46.178: INFO: Created: latency-svc-ppqzj
    Feb 27 02:28:46.207: INFO: Got endpoints: latency-svc-qjldv [727.908553ms]
    Feb 27 02:28:46.243: INFO: Created: latency-svc-7mhcx
    Feb 27 02:28:46.262: INFO: Got endpoints: latency-svc-zdqsz [755.127476ms]
    Feb 27 02:28:46.294: INFO: Created: latency-svc-gwdrb
    Feb 27 02:28:46.307: INFO: Got endpoints: latency-svc-2sx5r [745.844803ms]
    Feb 27 02:28:46.361: INFO: Created: latency-svc-kcvmf
    Feb 27 02:28:46.375: INFO: Got endpoints: latency-svc-9lrtd [751.546704ms]
    Feb 27 02:28:46.407: INFO: Got endpoints: latency-svc-xx67g [746.153446ms]
    Feb 27 02:28:46.409: INFO: Created: latency-svc-8zvjt
    Feb 27 02:28:46.443: INFO: Created: latency-svc-rsgv4
    Feb 27 02:28:46.459: INFO: Got endpoints: latency-svc-sl68q [749.268688ms]
    Feb 27 02:28:46.575: INFO: Got endpoints: latency-svc-gtw78 [817.639858ms]
    Feb 27 02:28:46.579: INFO: Got endpoints: latency-svc-t7gp6 [766.086545ms]
    Feb 27 02:28:46.579: INFO: Created: latency-svc-dpgvl
    Feb 27 02:28:46.616: INFO: Created: latency-svc-k5qk2
    Feb 27 02:28:46.624: INFO: Got endpoints: latency-svc-h8z5z [762.368556ms]
    Feb 27 02:28:46.657: INFO: Created: latency-svc-75654
    Feb 27 02:28:46.662: INFO: Got endpoints: latency-svc-wfd75 [748.322399ms]
    Feb 27 02:28:46.685: INFO: Created: latency-svc-j25z2
    Feb 27 02:28:46.707: INFO: Created: latency-svc-hmbbx
    Feb 27 02:28:46.708: INFO: Got endpoints: latency-svc-ng8xq [746.712164ms]
    Feb 27 02:28:46.763: INFO: Got endpoints: latency-svc-t8dwx [752.190872ms]
    Feb 27 02:28:46.793: INFO: Created: latency-svc-7mq78
    Feb 27 02:28:46.796: INFO: Created: latency-svc-6bd7x
    Feb 27 02:28:46.814: INFO: Got endpoints: latency-svc-qcfzs [756.100421ms]
    Feb 27 02:28:46.829: INFO: Created: latency-svc-9zw8q
    Feb 27 02:28:46.858: INFO: Got endpoints: latency-svc-xcg8b [740.634676ms]
    Feb 27 02:28:46.916: INFO: Got endpoints: latency-svc-ppqzj [757.560843ms]
    Feb 27 02:28:46.930: INFO: Created: latency-svc-wv4x8
    Feb 27 02:28:46.939: INFO: Created: latency-svc-lc6fr
    Feb 27 02:28:46.962: INFO: Got endpoints: latency-svc-7mhcx [754.81166ms]
    Feb 27 02:28:46.984: INFO: Created: latency-svc-5swww
    Feb 27 02:28:47.016: INFO: Got endpoints: latency-svc-gwdrb [754.705133ms]
    Feb 27 02:28:47.045: INFO: Created: latency-svc-jbt8b
    Feb 27 02:28:47.057: INFO: Got endpoints: latency-svc-kcvmf [749.82078ms]
    Feb 27 02:28:47.102: INFO: Created: latency-svc-87jll
    Feb 27 02:28:47.111: INFO: Got endpoints: latency-svc-8zvjt [735.877853ms]
    Feb 27 02:28:47.153: INFO: Created: latency-svc-w2669
    Feb 27 02:28:47.157: INFO: Got endpoints: latency-svc-rsgv4 [750.67015ms]
    Feb 27 02:28:47.182: INFO: Created: latency-svc-dpqbt
    Feb 27 02:28:47.218: INFO: Got endpoints: latency-svc-dpgvl [758.850114ms]
    Feb 27 02:28:47.234: INFO: Created: latency-svc-7xscp
    Feb 27 02:28:47.256: INFO: Got endpoints: latency-svc-k5qk2 [681.462776ms]
    Feb 27 02:28:47.294: INFO: Created: latency-svc-x98g6
    Feb 27 02:28:47.352: INFO: Got endpoints: latency-svc-75654 [773.316849ms]
    Feb 27 02:28:47.409: INFO: Got endpoints: latency-svc-j25z2 [784.952586ms]
    Feb 27 02:28:47.419: INFO: Got endpoints: latency-svc-hmbbx [757.369022ms]
    Feb 27 02:28:47.433: INFO: Created: latency-svc-6tffl
    Feb 27 02:28:47.451: INFO: Created: latency-svc-mn6p8
    Feb 27 02:28:47.467: INFO: Got endpoints: latency-svc-7mq78 [758.697207ms]
    Feb 27 02:28:47.469: INFO: Created: latency-svc-449zf
    Feb 27 02:28:47.499: INFO: Created: latency-svc-m7jvt
    Feb 27 02:28:47.508: INFO: Got endpoints: latency-svc-6bd7x [745.230062ms]
    Feb 27 02:28:47.544: INFO: Created: latency-svc-rb6lp
    Feb 27 02:28:47.559: INFO: Got endpoints: latency-svc-9zw8q [744.917302ms]
    Feb 27 02:28:47.591: INFO: Created: latency-svc-sch9w
    Feb 27 02:28:47.614: INFO: Got endpoints: latency-svc-wv4x8 [755.888061ms]
    Feb 27 02:28:47.648: INFO: Created: latency-svc-t727k
    Feb 27 02:28:47.667: INFO: Got endpoints: latency-svc-lc6fr [750.880131ms]
    Feb 27 02:28:47.686: INFO: Created: latency-svc-cphql
    Feb 27 02:28:47.705: INFO: Got endpoints: latency-svc-5swww [743.390783ms]
    Feb 27 02:28:47.761: INFO: Got endpoints: latency-svc-jbt8b [744.09072ms]
    Feb 27 02:28:47.761: INFO: Created: latency-svc-jjnfb
    Feb 27 02:28:47.789: INFO: Created: latency-svc-5b8dg
    Feb 27 02:28:47.813: INFO: Got endpoints: latency-svc-87jll [755.962078ms]
    Feb 27 02:28:47.839: INFO: Created: latency-svc-x8kng
    Feb 27 02:28:47.857: INFO: Got endpoints: latency-svc-w2669 [746.782446ms]
    Feb 27 02:28:47.897: INFO: Created: latency-svc-twqqs
    Feb 27 02:28:47.909: INFO: Got endpoints: latency-svc-dpqbt [751.239816ms]
    Feb 27 02:28:47.961: INFO: Got endpoints: latency-svc-7xscp [743.02718ms]
    Feb 27 02:28:47.975: INFO: Created: latency-svc-47295
    Feb 27 02:28:47.984: INFO: Created: latency-svc-m2jtd
    Feb 27 02:28:48.008: INFO: Got endpoints: latency-svc-x98g6 [751.728022ms]
    Feb 27 02:28:48.026: INFO: Created: latency-svc-tssrn
    Feb 27 02:28:48.072: INFO: Got endpoints: latency-svc-6tffl [719.508781ms]
    Feb 27 02:28:48.106: INFO: Created: latency-svc-4f6qz
    Feb 27 02:28:48.115: INFO: Got endpoints: latency-svc-mn6p8 [706.576899ms]
    Feb 27 02:28:48.137: INFO: Created: latency-svc-t5jrr
    Feb 27 02:28:48.156: INFO: Got endpoints: latency-svc-449zf [736.788345ms]
    Feb 27 02:28:48.181: INFO: Created: latency-svc-9rq6c
    Feb 27 02:28:48.206: INFO: Got endpoints: latency-svc-m7jvt [739.430646ms]
    Feb 27 02:28:48.227: INFO: Created: latency-svc-kcw6q
    Feb 27 02:28:48.269: INFO: Got endpoints: latency-svc-rb6lp [760.948214ms]
    Feb 27 02:28:48.300: INFO: Created: latency-svc-t2ck7
    Feb 27 02:28:48.313: INFO: Got endpoints: latency-svc-sch9w [754.577335ms]
    Feb 27 02:28:48.350: INFO: Created: latency-svc-bghk4
    Feb 27 02:28:48.360: INFO: Got endpoints: latency-svc-t727k [746.140667ms]
    Feb 27 02:28:48.428: INFO: Got endpoints: latency-svc-cphql [761.137333ms]
    Feb 27 02:28:48.429: INFO: Created: latency-svc-7v9w7
    Feb 27 02:28:48.460: INFO: Got endpoints: latency-svc-jjnfb [754.207007ms]
    Feb 27 02:28:48.466: INFO: Created: latency-svc-mhvbd
    Feb 27 02:28:48.482: INFO: Created: latency-svc-2nqrt
    Feb 27 02:28:48.513: INFO: Got endpoints: latency-svc-5b8dg [752.065756ms]
    Feb 27 02:28:48.546: INFO: Created: latency-svc-kthng
    Feb 27 02:28:48.558: INFO: Got endpoints: latency-svc-x8kng [745.238287ms]
    Feb 27 02:28:48.586: INFO: Created: latency-svc-kqcmz
    Feb 27 02:28:48.606: INFO: Got endpoints: latency-svc-twqqs [748.331415ms]
    Feb 27 02:28:48.627: INFO: Created: latency-svc-5qx98
    Feb 27 02:28:48.656: INFO: Got endpoints: latency-svc-47295 [746.890551ms]
    Feb 27 02:28:48.683: INFO: Created: latency-svc-d9pkt
    Feb 27 02:28:48.709: INFO: Got endpoints: latency-svc-m2jtd [748.608548ms]
    Feb 27 02:28:48.729: INFO: Created: latency-svc-hwmmz
    Feb 27 02:28:48.755: INFO: Got endpoints: latency-svc-tssrn [747.009791ms]
    Feb 27 02:28:48.821: INFO: Created: latency-svc-k6llf
    Feb 27 02:28:48.825: INFO: Got endpoints: latency-svc-4f6qz [753.635566ms]
    Feb 27 02:28:48.855: INFO: Created: latency-svc-vwb46
    Feb 27 02:28:48.860: INFO: Got endpoints: latency-svc-t5jrr [744.043763ms]
    Feb 27 02:28:48.875: INFO: Created: latency-svc-d4s8m
    Feb 27 02:28:48.905: INFO: Got endpoints: latency-svc-9rq6c [749.003957ms]
    Feb 27 02:28:48.925: INFO: Created: latency-svc-76wp2
    Feb 27 02:28:48.962: INFO: Got endpoints: latency-svc-kcw6q [756.035337ms]
    Feb 27 02:28:48.983: INFO: Created: latency-svc-xp4mj
    Feb 27 02:28:49.016: INFO: Got endpoints: latency-svc-t2ck7 [747.654807ms]
    Feb 27 02:28:49.031: INFO: Created: latency-svc-kvvrz
    Feb 27 02:28:49.068: INFO: Got endpoints: latency-svc-bghk4 [754.75423ms]
    Feb 27 02:28:49.103: INFO: Created: latency-svc-hfvzk
    Feb 27 02:28:49.109: INFO: Got endpoints: latency-svc-7v9w7 [748.630254ms]
    Feb 27 02:28:49.132: INFO: Created: latency-svc-5sw7l
    Feb 27 02:28:49.161: INFO: Got endpoints: latency-svc-mhvbd [733.042261ms]
    Feb 27 02:28:49.192: INFO: Created: latency-svc-kfddt
    Feb 27 02:28:49.205: INFO: Got endpoints: latency-svc-2nqrt [745.109815ms]
    Feb 27 02:28:49.241: INFO: Created: latency-svc-7l8zs
    Feb 27 02:28:49.261: INFO: Got endpoints: latency-svc-kthng [748.120768ms]
    Feb 27 02:28:49.295: INFO: Created: latency-svc-qvshb
    Feb 27 02:28:49.312: INFO: Got endpoints: latency-svc-kqcmz [753.951901ms]
    Feb 27 02:28:49.343: INFO: Created: latency-svc-znfb9
    Feb 27 02:28:49.363: INFO: Got endpoints: latency-svc-5qx98 [756.864726ms]
    Feb 27 02:28:49.390: INFO: Created: latency-svc-wx8fm
    Feb 27 02:28:49.405: INFO: Got endpoints: latency-svc-d9pkt [749.746576ms]
    Feb 27 02:28:49.434: INFO: Created: latency-svc-tv7m6
    Feb 27 02:28:49.467: INFO: Got endpoints: latency-svc-hwmmz [757.727489ms]
    Feb 27 02:28:49.499: INFO: Created: latency-svc-vskrd
    Feb 27 02:28:49.505: INFO: Got endpoints: latency-svc-k6llf [750.050548ms]
    Feb 27 02:28:49.551: INFO: Created: latency-svc-8qpt2
    Feb 27 02:28:49.562: INFO: Got endpoints: latency-svc-vwb46 [736.814669ms]
    Feb 27 02:28:49.592: INFO: Created: latency-svc-fcbkw
    Feb 27 02:28:49.608: INFO: Got endpoints: latency-svc-d4s8m [747.920918ms]
    Feb 27 02:28:49.622: INFO: Created: latency-svc-pt4s4
    Feb 27 02:28:49.657: INFO: Got endpoints: latency-svc-76wp2 [752.526164ms]
    Feb 27 02:28:49.683: INFO: Created: latency-svc-s5snc
    Feb 27 02:28:49.708: INFO: Got endpoints: latency-svc-xp4mj [745.69224ms]
    Feb 27 02:28:49.746: INFO: Created: latency-svc-gl7h4
    Feb 27 02:28:49.761: INFO: Got endpoints: latency-svc-kvvrz [744.7754ms]
    Feb 27 02:28:49.791: INFO: Created: latency-svc-n4cpz
    Feb 27 02:28:49.810: INFO: Got endpoints: latency-svc-hfvzk [742.062065ms]
    Feb 27 02:28:49.827: INFO: Created: latency-svc-9lk24
    Feb 27 02:28:49.871: INFO: Got endpoints: latency-svc-5sw7l [762.81618ms]
    Feb 27 02:28:49.961: INFO: Got endpoints: latency-svc-kfddt [800.00926ms]
    Feb 27 02:28:49.975: INFO: Got endpoints: latency-svc-7l8zs [769.957268ms]
    Feb 27 02:28:49.993: INFO: Created: latency-svc-tkchd
    Feb 27 02:28:50.002: INFO: Created: latency-svc-zktj9
    Feb 27 02:28:50.012: INFO: Got endpoints: latency-svc-qvshb [751.388875ms]
    Feb 27 02:28:50.027: INFO: Created: latency-svc-6fws7
    Feb 27 02:28:50.066: INFO: Got endpoints: latency-svc-znfb9 [754.287052ms]
    Feb 27 02:28:50.068: INFO: Created: latency-svc-cl958
    Feb 27 02:28:50.106: INFO: Created: latency-svc-tsvlx
    Feb 27 02:28:50.108: INFO: Got endpoints: latency-svc-wx8fm [745.589715ms]
    Feb 27 02:28:50.130: INFO: Created: latency-svc-l8jmq
    Feb 27 02:28:50.156: INFO: Got endpoints: latency-svc-tv7m6 [750.759508ms]
    Feb 27 02:28:50.180: INFO: Created: latency-svc-vmcj8
    Feb 27 02:28:50.210: INFO: Got endpoints: latency-svc-vskrd [742.471822ms]
    Feb 27 02:28:50.248: INFO: Created: latency-svc-n9kfw
    Feb 27 02:28:50.261: INFO: Got endpoints: latency-svc-8qpt2 [755.749047ms]
    Feb 27 02:28:50.295: INFO: Created: latency-svc-q466k
    Feb 27 02:28:50.312: INFO: Got endpoints: latency-svc-fcbkw [749.486474ms]
    Feb 27 02:28:50.331: INFO: Created: latency-svc-wnkzh
    Feb 27 02:28:50.360: INFO: Got endpoints: latency-svc-pt4s4 [752.085866ms]
    Feb 27 02:28:50.377: INFO: Created: latency-svc-mrnwd
    Feb 27 02:28:50.407: INFO: Got endpoints: latency-svc-s5snc [750.095669ms]
    Feb 27 02:28:50.451: INFO: Created: latency-svc-rnwrv
    Feb 27 02:28:50.456: INFO: Got endpoints: latency-svc-gl7h4 [748.093887ms]
    Feb 27 02:28:50.477: INFO: Created: latency-svc-zns4v
    Feb 27 02:28:50.514: INFO: Got endpoints: latency-svc-n4cpz [752.331889ms]
    Feb 27 02:28:50.534: INFO: Created: latency-svc-k5q58
    Feb 27 02:28:50.557: INFO: Got endpoints: latency-svc-9lk24 [746.888714ms]
    Feb 27 02:28:50.574: INFO: Created: latency-svc-2ks8x
    Feb 27 02:28:50.614: INFO: Got endpoints: latency-svc-tkchd [742.107813ms]
    Feb 27 02:28:50.645: INFO: Created: latency-svc-jf9q8
    Feb 27 02:28:50.660: INFO: Got endpoints: latency-svc-zktj9 [699.053416ms]
    Feb 27 02:28:50.707: INFO: Got endpoints: latency-svc-6fws7 [731.805413ms]
    Feb 27 02:28:50.758: INFO: Got endpoints: latency-svc-cl958 [745.421681ms]
    Feb 27 02:28:50.811: INFO: Got endpoints: latency-svc-tsvlx [744.60289ms]
    Feb 27 02:28:50.857: INFO: Got endpoints: latency-svc-l8jmq [748.244603ms]
    Feb 27 02:28:50.914: INFO: Got endpoints: latency-svc-vmcj8 [757.975634ms]
    Feb 27 02:28:50.963: INFO: Got endpoints: latency-svc-n9kfw [753.320096ms]
    Feb 27 02:28:51.013: INFO: Got endpoints: latency-svc-q466k [752.285938ms]
    Feb 27 02:28:51.062: INFO: Got endpoints: latency-svc-wnkzh [750.220355ms]
    Feb 27 02:28:51.114: INFO: Got endpoints: latency-svc-mrnwd [754.695977ms]
    Feb 27 02:28:51.159: INFO: Got endpoints: latency-svc-rnwrv [751.360927ms]
    Feb 27 02:28:51.213: INFO: Got endpoints: latency-svc-zns4v [757.048519ms]
    Feb 27 02:28:51.257: INFO: Got endpoints: latency-svc-k5q58 [742.872657ms]
    Feb 27 02:28:51.306: INFO: Got endpoints: latency-svc-2ks8x [748.954319ms]
    Feb 27 02:28:51.357: INFO: Got endpoints: latency-svc-jf9q8 [743.822393ms]
    Feb 27 02:28:51.357: INFO: Latencies: [37.081325ms 73.980364ms 74.26164ms 96.161603ms 100.873978ms 104.969161ms 119.200348ms 119.895932ms 133.810935ms 136.814252ms 157.974544ms 161.793989ms 178.725398ms 193.105056ms 215.588959ms 219.364414ms 232.882245ms 243.093507ms 245.697883ms 259.908049ms 263.890202ms 279.646193ms 279.758581ms 302.374109ms 309.048264ms 314.498981ms 325.274676ms 337.650873ms 342.144227ms 350.181345ms 357.026962ms 358.078964ms 361.460496ms 362.991584ms 369.263833ms 370.439412ms 375.786312ms 375.996799ms 377.432054ms 379.376609ms 380.720464ms 381.585523ms 381.975093ms 382.693458ms 386.7517ms 422.225521ms 425.739642ms 433.45272ms 440.374233ms 440.484728ms 484.581531ms 509.212878ms 509.481706ms 515.508592ms 545.169273ms 546.859488ms 547.285009ms 550.51856ms 572.138359ms 580.924161ms 585.060431ms 596.761344ms 601.630967ms 609.433799ms 614.381874ms 626.318069ms 651.939245ms 654.126164ms 678.180637ms 681.462776ms 689.017961ms 697.3839ms 699.053416ms 701.881264ms 704.1561ms 706.576899ms 711.042156ms 719.508781ms 727.908553ms 731.805413ms 732.194895ms 733.042261ms 735.172146ms 735.877853ms 736.788345ms 736.814669ms 739.430646ms 740.634676ms 740.834966ms 741.013ms 741.339156ms 741.641329ms 742.062065ms 742.107813ms 742.471822ms 742.872657ms 743.02718ms 743.062999ms 743.390783ms 743.822393ms 744.043763ms 744.09072ms 744.60289ms 744.7754ms 744.917302ms 745.109815ms 745.1195ms 745.230062ms 745.238287ms 745.421681ms 745.589715ms 745.69224ms 745.844803ms 746.140667ms 746.153446ms 746.712164ms 746.782446ms 746.817018ms 746.888714ms 746.890551ms 747.009791ms 747.654807ms 747.767864ms 747.882084ms 747.920918ms 748.093887ms 748.120768ms 748.244603ms 748.322399ms 748.331415ms 748.578786ms 748.608548ms 748.630254ms 748.954319ms 749.003957ms 749.063314ms 749.268688ms 749.486474ms 749.746576ms 749.757675ms 749.82078ms 750.050548ms 750.095669ms 750.220355ms 750.67015ms 750.759508ms 750.831189ms 750.880131ms 751.167465ms 751.239816ms 751.360927ms 751.388875ms 751.546704ms 751.712531ms 751.728022ms 751.912069ms 752.065756ms 752.085866ms 752.190872ms 752.285938ms 752.331889ms 752.526164ms 753.320096ms 753.635566ms 753.951901ms 754.120886ms 754.207007ms 754.287052ms 754.577335ms 754.695977ms 754.705133ms 754.75423ms 754.81166ms 755.127476ms 755.749047ms 755.888061ms 755.962078ms 756.035337ms 756.100421ms 756.864726ms 757.048519ms 757.369022ms 757.560843ms 757.727489ms 757.975634ms 758.697207ms 758.850114ms 760.948214ms 761.137333ms 761.995322ms 762.368556ms 762.81618ms 766.086545ms 769.957268ms 773.316849ms 784.952586ms 800.00926ms 801.868822ms 817.639858ms 819.569371ms]
    Feb 27 02:28:51.358: INFO: 50 %ile: 744.043763ms
    Feb 27 02:28:51.358: INFO: 90 %ile: 757.048519ms
    Feb 27 02:28:51.358: INFO: 99 %ile: 817.639858ms
    Feb 27 02:28:51.358: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:28:51.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-3786" for this suite. 02/27/23 02:28:51.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:28:51.378
Feb 27 02:28:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename proxy 02/27/23 02:28:51.378
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:51.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:51.412
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 02/27/23 02:28:51.434
STEP: creating replication controller proxy-service-9frmj in namespace proxy-8311 02/27/23 02:28:51.434
I0227 02:28:51.453666      23 runners.go:193] Created replication controller with name: proxy-service-9frmj, namespace: proxy-8311, replica count: 1
I0227 02:28:52.505561      23 runners.go:193] proxy-service-9frmj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0227 02:28:53.506003      23 runners.go:193] proxy-service-9frmj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:28:53.510: INFO: setup took 2.095386277s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 02:28:53.51
Feb 27 02:28:53.528: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 18.179081ms)
Feb 27 02:28:53.530: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 19.804989ms)
Feb 27 02:28:53.530: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 19.896163ms)
Feb 27 02:28:53.531: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 20.990269ms)
Feb 27 02:28:53.531: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 20.918539ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 21.850809ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 21.737301ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 21.824525ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 21.775276ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 21.759424ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 22.090219ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 22.16119ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 22.263709ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 22.123503ms)
Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 22.229548ms)
Feb 27 02:28:53.533: INFO: (0) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 22.473483ms)
Feb 27 02:28:53.539: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.360671ms)
Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.451432ms)
Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.502076ms)
Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.575484ms)
Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.461711ms)
Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.537785ms)
Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.931645ms)
Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.086056ms)
Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.221972ms)
Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.233798ms)
Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.794899ms)
Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 9.940493ms)
Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.027352ms)
Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.949229ms)
Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.006065ms)
Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.205934ms)
Feb 27 02:28:53.551: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.34564ms)
Feb 27 02:28:53.552: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.144092ms)
Feb 27 02:28:53.552: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.247488ms)
Feb 27 02:28:53.553: INFO: (2) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 8.177408ms)
Feb 27 02:28:53.554: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.397057ms)
Feb 27 02:28:53.554: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.841271ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.664488ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.442064ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.726585ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 10.680774ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 10.770074ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.152552ms)
Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.900804ms)
Feb 27 02:28:53.556: INFO: (2) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.728682ms)
Feb 27 02:28:53.557: INFO: (2) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.387846ms)
Feb 27 02:28:53.557: INFO: (2) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.700181ms)
Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 5.516604ms)
Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.384628ms)
Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.132082ms)
Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.535105ms)
Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 6.379436ms)
Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 6.41694ms)
Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.481865ms)
Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.521081ms)
Feb 27 02:28:53.565: INFO: (3) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 8.141048ms)
Feb 27 02:28:53.566: INFO: (3) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 8.795211ms)
Feb 27 02:28:53.567: INFO: (3) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 9.521615ms)
Feb 27 02:28:53.567: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.628988ms)
Feb 27 02:28:53.568: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.328866ms)
Feb 27 02:28:53.568: INFO: (3) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.886ms)
Feb 27 02:28:53.569: INFO: (3) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.511175ms)
Feb 27 02:28:53.569: INFO: (3) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.691797ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.77299ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.688161ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 11.210955ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.948569ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.404267ms)
Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 12.187285ms)
Feb 27 02:28:53.582: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 12.378329ms)
Feb 27 02:28:53.582: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.237841ms)
Feb 27 02:28:53.583: INFO: (4) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 13.934709ms)
Feb 27 02:28:53.583: INFO: (4) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.634801ms)
Feb 27 02:28:53.584: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 14.375705ms)
Feb 27 02:28:53.584: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 14.33924ms)
Feb 27 02:28:53.585: INFO: (4) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 15.595664ms)
Feb 27 02:28:53.585: INFO: (4) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 15.449567ms)
Feb 27 02:28:53.586: INFO: (4) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 16.363701ms)
Feb 27 02:28:53.586: INFO: (4) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 16.264848ms)
Feb 27 02:28:53.592: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.363894ms)
Feb 27 02:28:53.592: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 6.571547ms)
Feb 27 02:28:53.593: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 6.679525ms)
Feb 27 02:28:53.593: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.762113ms)
Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.759842ms)
Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.014226ms)
Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 9.386527ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.938375ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.986011ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.075934ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.161066ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.246426ms)
Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.378408ms)
Feb 27 02:28:53.597: INFO: (5) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.735802ms)
Feb 27 02:28:53.597: INFO: (5) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.367164ms)
Feb 27 02:28:53.598: INFO: (5) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.752496ms)
Feb 27 02:28:53.604: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.736388ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.004316ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.02508ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.189446ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.39407ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.339023ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.514519ms)
Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.499598ms)
Feb 27 02:28:53.606: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 8.008211ms)
Feb 27 02:28:53.606: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.23414ms)
Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.049937ms)
Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.155861ms)
Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.248382ms)
Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.739906ms)
Feb 27 02:28:53.609: INFO: (6) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.062316ms)
Feb 27 02:28:53.609: INFO: (6) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.175041ms)
Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.079466ms)
Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.309221ms)
Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.466144ms)
Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.389369ms)
Feb 27 02:28:53.617: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.474813ms)
Feb 27 02:28:53.617: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.627172ms)
Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.013078ms)
Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.217771ms)
Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 10.216785ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 10.546328ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.683354ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.695625ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.856252ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.057307ms)
Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.118596ms)
Feb 27 02:28:53.621: INFO: (7) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.079326ms)
Feb 27 02:28:53.627: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 5.388491ms)
Feb 27 02:28:53.627: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 5.479629ms)
Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.980324ms)
Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.948281ms)
Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.633111ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.838571ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 8.139244ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.112097ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.292257ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.621717ms)
Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.357648ms)
Feb 27 02:28:53.631: INFO: (8) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.869914ms)
Feb 27 02:28:53.632: INFO: (8) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.05353ms)
Feb 27 02:28:53.632: INFO: (8) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.559731ms)
Feb 27 02:28:53.634: INFO: (8) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.942994ms)
Feb 27 02:28:53.634: INFO: (8) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.910256ms)
Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.580629ms)
Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 6.418805ms)
Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.473401ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.998135ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.116143ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.036206ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.386209ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.428423ms)
Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.363878ms)
Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 9.284222ms)
Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.391978ms)
Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 9.6061ms)
Feb 27 02:28:53.644: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.594348ms)
Feb 27 02:28:53.644: INFO: (9) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 9.617169ms)
Feb 27 02:28:53.645: INFO: (9) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.183595ms)
Feb 27 02:28:53.646: INFO: (9) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.013886ms)
Feb 27 02:28:53.654: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.116952ms)
Feb 27 02:28:53.654: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.209259ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.451307ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 8.587407ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.556916ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.575822ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.674799ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.778424ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.701355ms)
Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.858474ms)
Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.864496ms)
Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.832254ms)
Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.056064ms)
Feb 27 02:28:53.658: INFO: (10) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.53193ms)
Feb 27 02:28:53.658: INFO: (10) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.728409ms)
Feb 27 02:28:53.659: INFO: (10) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.563342ms)
Feb 27 02:28:53.665: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.668683ms)
Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.936274ms)
Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.154759ms)
Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.766541ms)
Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.917978ms)
Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.998188ms)
Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.072722ms)
Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.21584ms)
Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.948756ms)
Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 12.445405ms)
Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 12.428775ms)
Feb 27 02:28:53.672: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.827317ms)
Feb 27 02:28:53.672: INFO: (11) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.773142ms)
Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 14.405622ms)
Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 14.451475ms)
Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 14.47722ms)
Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.504401ms)
Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.52791ms)
Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.615827ms)
Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 6.672184ms)
Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.079072ms)
Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.27867ms)
Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.326723ms)
Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.766581ms)
Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.527699ms)
Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.656518ms)
Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.777941ms)
Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.066365ms)
Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.619301ms)
Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 12.711645ms)
Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.893246ms)
Feb 27 02:28:53.688: INFO: (12) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 14.758844ms)
Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.780588ms)
Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.089449ms)
Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.18885ms)
Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.088838ms)
Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.163539ms)
Feb 27 02:28:53.699: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.555762ms)
Feb 27 02:28:53.699: INFO: (13) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.521381ms)
Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 11.14794ms)
Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 11.204425ms)
Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.548338ms)
Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 11.586432ms)
Feb 27 02:28:53.713: INFO: (13) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 25.128006ms)
Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 25.19754ms)
Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 25.258324ms)
Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 25.395543ms)
Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 25.688612ms)
Feb 27 02:28:53.720: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 6.138114ms)
Feb 27 02:28:53.721: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.119322ms)
Feb 27 02:28:53.721: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.149257ms)
Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.235834ms)
Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.283783ms)
Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.292199ms)
Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.271478ms)
Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.371819ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.53845ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.715482ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.787801ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 9.932399ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.003356ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.315133ms)
Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.356992ms)
Feb 27 02:28:53.726: INFO: (14) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.822162ms)
Feb 27 02:28:53.733: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.982103ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.728288ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.911349ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.006788ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.969217ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.201836ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.221503ms)
Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.260789ms)
Feb 27 02:28:53.736: INFO: (15) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.850885ms)
Feb 27 02:28:53.737: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 10.74734ms)
Feb 27 02:28:53.737: INFO: (15) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.392788ms)
Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.673491ms)
Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.158517ms)
Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.352749ms)
Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.297576ms)
Feb 27 02:28:53.739: INFO: (15) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 12.554504ms)
Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.543194ms)
Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.598811ms)
Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.580143ms)
Feb 27 02:28:53.753: INFO: (16) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 14.704143ms)
Feb 27 02:28:53.753: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 14.788804ms)
Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 16.450291ms)
Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 16.504287ms)
Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 16.484917ms)
Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 16.507272ms)
Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 17.035874ms)
Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 17.055179ms)
Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 16.975091ms)
Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 17.529593ms)
Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 17.67633ms)
Feb 27 02:28:53.757: INFO: (16) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 17.966082ms)
Feb 27 02:28:53.757: INFO: (16) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 18.080757ms)
Feb 27 02:28:53.762: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 5.290091ms)
Feb 27 02:28:53.763: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 5.627308ms)
Feb 27 02:28:53.763: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 5.938663ms)
Feb 27 02:28:53.764: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.500681ms)
Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 9.577389ms)
Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.529392ms)
Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.680657ms)
Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.543592ms)
Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.822932ms)
Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.471809ms)
Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.506502ms)
Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.244813ms)
Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.291875ms)
Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.356316ms)
Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.667913ms)
Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.825369ms)
Feb 27 02:28:53.776: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.92278ms)
Feb 27 02:28:53.777: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.283326ms)
Feb 27 02:28:53.777: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.794269ms)
Feb 27 02:28:53.778: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.096119ms)
Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.914943ms)
Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.706859ms)
Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.302142ms)
Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.493737ms)
Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.904591ms)
Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 11.230329ms)
Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.709654ms)
Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.959399ms)
Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.339511ms)
Feb 27 02:28:53.783: INFO: (18) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 13.087347ms)
Feb 27 02:28:53.784: INFO: (18) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.822965ms)
Feb 27 02:28:53.787: INFO: (18) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 17.134112ms)
Feb 27 02:28:53.794: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.065681ms)
Feb 27 02:28:53.794: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.418982ms)
Feb 27 02:28:53.795: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.376902ms)
Feb 27 02:28:53.797: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.580614ms)
Feb 27 02:28:53.798: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.967486ms)
Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 12.076792ms)
Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.051829ms)
Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 12.367575ms)
Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 12.352744ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.874475ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.795044ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 12.763513ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 12.740809ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 13.265485ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 13.372598ms)
Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.498781ms)
STEP: deleting ReplicationController proxy-service-9frmj in namespace proxy-8311, will wait for the garbage collector to delete the pods 02/27/23 02:28:53.8
Feb 27 02:28:53.862: INFO: Deleting ReplicationController proxy-service-9frmj took: 8.550712ms
Feb 27 02:28:53.963: INFO: Terminating ReplicationController proxy-service-9frmj pods took: 100.570597ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 02:28:56.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8311" for this suite. 02/27/23 02:28:56.37
------------------------------
• [SLOW TEST] [5.012 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:28:51.378
    Feb 27 02:28:51.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename proxy 02/27/23 02:28:51.378
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:51.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:51.412
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 02/27/23 02:28:51.434
    STEP: creating replication controller proxy-service-9frmj in namespace proxy-8311 02/27/23 02:28:51.434
    I0227 02:28:51.453666      23 runners.go:193] Created replication controller with name: proxy-service-9frmj, namespace: proxy-8311, replica count: 1
    I0227 02:28:52.505561      23 runners.go:193] proxy-service-9frmj Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0227 02:28:53.506003      23 runners.go:193] proxy-service-9frmj Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:28:53.510: INFO: setup took 2.095386277s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 02/27/23 02:28:53.51
    Feb 27 02:28:53.528: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 18.179081ms)
    Feb 27 02:28:53.530: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 19.804989ms)
    Feb 27 02:28:53.530: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 19.896163ms)
    Feb 27 02:28:53.531: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 20.990269ms)
    Feb 27 02:28:53.531: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 20.918539ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 21.850809ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 21.737301ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 21.824525ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 21.775276ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 21.759424ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 22.090219ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 22.16119ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 22.263709ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 22.123503ms)
    Feb 27 02:28:53.532: INFO: (0) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 22.229548ms)
    Feb 27 02:28:53.533: INFO: (0) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 22.473483ms)
    Feb 27 02:28:53.539: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.360671ms)
    Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.451432ms)
    Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.502076ms)
    Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.575484ms)
    Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.461711ms)
    Feb 27 02:28:53.540: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.537785ms)
    Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.931645ms)
    Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.086056ms)
    Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.221972ms)
    Feb 27 02:28:53.541: INFO: (1) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.233798ms)
    Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.794899ms)
    Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 9.940493ms)
    Feb 27 02:28:53.543: INFO: (1) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.027352ms)
    Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.949229ms)
    Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.006065ms)
    Feb 27 02:28:53.544: INFO: (1) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.205934ms)
    Feb 27 02:28:53.551: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.34564ms)
    Feb 27 02:28:53.552: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.144092ms)
    Feb 27 02:28:53.552: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.247488ms)
    Feb 27 02:28:53.553: INFO: (2) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 8.177408ms)
    Feb 27 02:28:53.554: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.397057ms)
    Feb 27 02:28:53.554: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.841271ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.664488ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.442064ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.726585ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 10.680774ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 10.770074ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.152552ms)
    Feb 27 02:28:53.555: INFO: (2) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.900804ms)
    Feb 27 02:28:53.556: INFO: (2) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.728682ms)
    Feb 27 02:28:53.557: INFO: (2) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.387846ms)
    Feb 27 02:28:53.557: INFO: (2) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.700181ms)
    Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 5.516604ms)
    Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.384628ms)
    Feb 27 02:28:53.563: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.132082ms)
    Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.535105ms)
    Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 6.379436ms)
    Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 6.41694ms)
    Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.481865ms)
    Feb 27 02:28:53.564: INFO: (3) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.521081ms)
    Feb 27 02:28:53.565: INFO: (3) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 8.141048ms)
    Feb 27 02:28:53.566: INFO: (3) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 8.795211ms)
    Feb 27 02:28:53.567: INFO: (3) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 9.521615ms)
    Feb 27 02:28:53.567: INFO: (3) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.628988ms)
    Feb 27 02:28:53.568: INFO: (3) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.328866ms)
    Feb 27 02:28:53.568: INFO: (3) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.886ms)
    Feb 27 02:28:53.569: INFO: (3) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.511175ms)
    Feb 27 02:28:53.569: INFO: (3) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.691797ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.77299ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.688161ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 11.210955ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.948569ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.404267ms)
    Feb 27 02:28:53.581: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 12.187285ms)
    Feb 27 02:28:53.582: INFO: (4) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 12.378329ms)
    Feb 27 02:28:53.582: INFO: (4) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.237841ms)
    Feb 27 02:28:53.583: INFO: (4) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 13.934709ms)
    Feb 27 02:28:53.583: INFO: (4) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.634801ms)
    Feb 27 02:28:53.584: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 14.375705ms)
    Feb 27 02:28:53.584: INFO: (4) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 14.33924ms)
    Feb 27 02:28:53.585: INFO: (4) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 15.595664ms)
    Feb 27 02:28:53.585: INFO: (4) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 15.449567ms)
    Feb 27 02:28:53.586: INFO: (4) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 16.363701ms)
    Feb 27 02:28:53.586: INFO: (4) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 16.264848ms)
    Feb 27 02:28:53.592: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.363894ms)
    Feb 27 02:28:53.592: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 6.571547ms)
    Feb 27 02:28:53.593: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 6.679525ms)
    Feb 27 02:28:53.593: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.762113ms)
    Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.759842ms)
    Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.014226ms)
    Feb 27 02:28:53.595: INFO: (5) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 9.386527ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.938375ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.986011ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.075934ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.161066ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.246426ms)
    Feb 27 02:28:53.596: INFO: (5) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.378408ms)
    Feb 27 02:28:53.597: INFO: (5) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.735802ms)
    Feb 27 02:28:53.597: INFO: (5) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.367164ms)
    Feb 27 02:28:53.598: INFO: (5) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.752496ms)
    Feb 27 02:28:53.604: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.736388ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.004316ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.02508ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.189446ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.39407ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.339023ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.514519ms)
    Feb 27 02:28:53.605: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.499598ms)
    Feb 27 02:28:53.606: INFO: (6) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 8.008211ms)
    Feb 27 02:28:53.606: INFO: (6) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.23414ms)
    Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.049937ms)
    Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.155861ms)
    Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.248382ms)
    Feb 27 02:28:53.608: INFO: (6) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.739906ms)
    Feb 27 02:28:53.609: INFO: (6) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.062316ms)
    Feb 27 02:28:53.609: INFO: (6) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.175041ms)
    Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.079466ms)
    Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.309221ms)
    Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.466144ms)
    Feb 27 02:28:53.616: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.389369ms)
    Feb 27 02:28:53.617: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.474813ms)
    Feb 27 02:28:53.617: INFO: (7) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.627172ms)
    Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.013078ms)
    Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.217771ms)
    Feb 27 02:28:53.619: INFO: (7) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 10.216785ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 10.546328ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.683354ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.695625ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.856252ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.057307ms)
    Feb 27 02:28:53.620: INFO: (7) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.118596ms)
    Feb 27 02:28:53.621: INFO: (7) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.079326ms)
    Feb 27 02:28:53.627: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 5.388491ms)
    Feb 27 02:28:53.627: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 5.479629ms)
    Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.980324ms)
    Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.948281ms)
    Feb 27 02:28:53.629: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.633111ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.838571ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 8.139244ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.112097ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.292257ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.621717ms)
    Feb 27 02:28:53.630: INFO: (8) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.357648ms)
    Feb 27 02:28:53.631: INFO: (8) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.869914ms)
    Feb 27 02:28:53.632: INFO: (8) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.05353ms)
    Feb 27 02:28:53.632: INFO: (8) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.559731ms)
    Feb 27 02:28:53.634: INFO: (8) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.942994ms)
    Feb 27 02:28:53.634: INFO: (8) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.910256ms)
    Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.580629ms)
    Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 6.418805ms)
    Feb 27 02:28:53.640: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.473401ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.998135ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.116143ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.036206ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.386209ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 7.428423ms)
    Feb 27 02:28:53.641: INFO: (9) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.363878ms)
    Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 9.284222ms)
    Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.391978ms)
    Feb 27 02:28:53.643: INFO: (9) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 9.6061ms)
    Feb 27 02:28:53.644: INFO: (9) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.594348ms)
    Feb 27 02:28:53.644: INFO: (9) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 9.617169ms)
    Feb 27 02:28:53.645: INFO: (9) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.183595ms)
    Feb 27 02:28:53.646: INFO: (9) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.013886ms)
    Feb 27 02:28:53.654: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.116952ms)
    Feb 27 02:28:53.654: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.209259ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.451307ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 8.587407ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.556916ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.575822ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.674799ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.778424ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.701355ms)
    Feb 27 02:28:53.655: INFO: (10) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.858474ms)
    Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.864496ms)
    Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 10.832254ms)
    Feb 27 02:28:53.657: INFO: (10) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.056064ms)
    Feb 27 02:28:53.658: INFO: (10) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.53193ms)
    Feb 27 02:28:53.658: INFO: (10) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.728409ms)
    Feb 27 02:28:53.659: INFO: (10) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.563342ms)
    Feb 27 02:28:53.665: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.668683ms)
    Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.936274ms)
    Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.154759ms)
    Feb 27 02:28:53.668: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.766541ms)
    Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.917978ms)
    Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.998188ms)
    Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.072722ms)
    Feb 27 02:28:53.669: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 10.21584ms)
    Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 11.948756ms)
    Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 12.445405ms)
    Feb 27 02:28:53.671: INFO: (11) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 12.428775ms)
    Feb 27 02:28:53.672: INFO: (11) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.827317ms)
    Feb 27 02:28:53.672: INFO: (11) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.773142ms)
    Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 14.405622ms)
    Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 14.451475ms)
    Feb 27 02:28:53.673: INFO: (11) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 14.47722ms)
    Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 6.504401ms)
    Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 6.52791ms)
    Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.615827ms)
    Feb 27 02:28:53.680: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 6.672184ms)
    Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.079072ms)
    Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 7.27867ms)
    Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 7.326723ms)
    Feb 27 02:28:53.681: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.766581ms)
    Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.527699ms)
    Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.656518ms)
    Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.777941ms)
    Feb 27 02:28:53.683: INFO: (12) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.066365ms)
    Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.619301ms)
    Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 12.711645ms)
    Feb 27 02:28:53.686: INFO: (12) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.893246ms)
    Feb 27 02:28:53.688: INFO: (12) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 14.758844ms)
    Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 7.780588ms)
    Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 8.089449ms)
    Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.18885ms)
    Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 8.088838ms)
    Feb 27 02:28:53.696: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 8.163539ms)
    Feb 27 02:28:53.699: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.555762ms)
    Feb 27 02:28:53.699: INFO: (13) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.521381ms)
    Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 11.14794ms)
    Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 11.204425ms)
    Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.548338ms)
    Feb 27 02:28:53.700: INFO: (13) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 11.586432ms)
    Feb 27 02:28:53.713: INFO: (13) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 25.128006ms)
    Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 25.19754ms)
    Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 25.258324ms)
    Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 25.395543ms)
    Feb 27 02:28:53.714: INFO: (13) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 25.688612ms)
    Feb 27 02:28:53.720: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 6.138114ms)
    Feb 27 02:28:53.721: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.119322ms)
    Feb 27 02:28:53.721: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.149257ms)
    Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.235834ms)
    Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.283783ms)
    Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.292199ms)
    Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.271478ms)
    Feb 27 02:28:53.723: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 9.371819ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 9.53845ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.715482ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.787801ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 9.932399ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 10.003356ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 10.315133ms)
    Feb 27 02:28:53.724: INFO: (14) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.356992ms)
    Feb 27 02:28:53.726: INFO: (14) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.822162ms)
    Feb 27 02:28:53.733: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 6.982103ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.728288ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.911349ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 8.006788ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.969217ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 8.201836ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 8.221503ms)
    Feb 27 02:28:53.734: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.260789ms)
    Feb 27 02:28:53.736: INFO: (15) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 9.850885ms)
    Feb 27 02:28:53.737: INFO: (15) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 10.74734ms)
    Feb 27 02:28:53.737: INFO: (15) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.392788ms)
    Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 11.673491ms)
    Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 12.158517ms)
    Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.352749ms)
    Feb 27 02:28:53.738: INFO: (15) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.297576ms)
    Feb 27 02:28:53.739: INFO: (15) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 12.554504ms)
    Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.543194ms)
    Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.598811ms)
    Feb 27 02:28:53.751: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 12.580143ms)
    Feb 27 02:28:53.753: INFO: (16) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 14.704143ms)
    Feb 27 02:28:53.753: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 14.788804ms)
    Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 16.450291ms)
    Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 16.504287ms)
    Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 16.484917ms)
    Feb 27 02:28:53.755: INFO: (16) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 16.507272ms)
    Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 17.035874ms)
    Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 17.055179ms)
    Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 16.975091ms)
    Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 17.529593ms)
    Feb 27 02:28:53.756: INFO: (16) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 17.67633ms)
    Feb 27 02:28:53.757: INFO: (16) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 17.966082ms)
    Feb 27 02:28:53.757: INFO: (16) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 18.080757ms)
    Feb 27 02:28:53.762: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 5.290091ms)
    Feb 27 02:28:53.763: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 5.627308ms)
    Feb 27 02:28:53.763: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 5.938663ms)
    Feb 27 02:28:53.764: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.500681ms)
    Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 9.577389ms)
    Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 9.529392ms)
    Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 9.680657ms)
    Feb 27 02:28:53.767: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 9.543592ms)
    Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.822932ms)
    Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 10.471809ms)
    Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 10.506502ms)
    Feb 27 02:28:53.768: INFO: (17) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 11.244813ms)
    Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.291875ms)
    Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 11.356316ms)
    Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 11.667913ms)
    Feb 27 02:28:53.769: INFO: (17) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 11.825369ms)
    Feb 27 02:28:53.776: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 6.92278ms)
    Feb 27 02:28:53.777: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 7.283326ms)
    Feb 27 02:28:53.777: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 7.794269ms)
    Feb 27 02:28:53.778: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.096119ms)
    Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 9.914943ms)
    Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.706859ms)
    Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 10.302142ms)
    Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 10.493737ms)
    Feb 27 02:28:53.780: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 9.904591ms)
    Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 11.230329ms)
    Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 11.709654ms)
    Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 10.959399ms)
    Feb 27 02:28:53.781: INFO: (18) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 11.339511ms)
    Feb 27 02:28:53.783: INFO: (18) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 13.087347ms)
    Feb 27 02:28:53.784: INFO: (18) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.822965ms)
    Feb 27 02:28:53.787: INFO: (18) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 17.134112ms)
    Feb 27 02:28:53.794: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:160/proxy/: foo (200; 7.065681ms)
    Feb 27 02:28:53.794: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:1080/proxy/rewriteme">test<... (200; 7.418982ms)
    Feb 27 02:28:53.795: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:162/proxy/: bar (200; 8.376902ms)
    Feb 27 02:28:53.797: INFO: (19) /api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/http:proxy-service-9frmj-96kqn:1080/proxy/rewriteme">... (200; 10.580614ms)
    Feb 27 02:28:53.798: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:443/proxy/tlsrewritem... (200; 10.967486ms)
    Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/: <a href="/api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn/proxy/rewriteme">test</a> (200; 12.076792ms)
    Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:160/proxy/: foo (200; 12.051829ms)
    Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/pods/proxy-service-9frmj-96kqn:162/proxy/: bar (200; 12.367575ms)
    Feb 27 02:28:53.799: INFO: (19) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname1/proxy/: foo (200; 12.352744ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname2/proxy/: tls qux (200; 12.874475ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/https:proxy-service-9frmj:tlsportname1/proxy/: tls baz (200; 12.795044ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:460/proxy/: tls baz (200; 12.763513ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/pods/https:proxy-service-9frmj-96kqn:462/proxy/: tls qux (200; 12.740809ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/http:proxy-service-9frmj:portname2/proxy/: bar (200; 13.265485ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname1/proxy/: foo (200; 13.372598ms)
    Feb 27 02:28:53.800: INFO: (19) /api/v1/namespaces/proxy-8311/services/proxy-service-9frmj:portname2/proxy/: bar (200; 13.498781ms)
    STEP: deleting ReplicationController proxy-service-9frmj in namespace proxy-8311, will wait for the garbage collector to delete the pods 02/27/23 02:28:53.8
    Feb 27 02:28:53.862: INFO: Deleting ReplicationController proxy-service-9frmj took: 8.550712ms
    Feb 27 02:28:53.963: INFO: Terminating ReplicationController proxy-service-9frmj pods took: 100.570597ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:28:56.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8311" for this suite. 02/27/23 02:28:56.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:28:56.39
Feb 27 02:28:56.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:28:56.391
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:56.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:56.431
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b1b8552-33f3-4eab-b21c-f95f155dcda2 02/27/23 02:28:56.444
STEP: Creating the pod 02/27/23 02:28:56.47
Feb 27 02:28:56.511: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f" in namespace "projected-3496" to be "running and ready"
Feb 27 02:28:56.514: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602973ms
Feb 27 02:28:56.514: INFO: The phase of Pod pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:28:58.518: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081719s
Feb 27 02:28:58.518: INFO: The phase of Pod pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f is Running (Ready = true)
Feb 27 02:28:58.518: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-6b1b8552-33f3-4eab-b21c-f95f155dcda2 02/27/23 02:28:58.544
STEP: waiting to observe update in volume 02/27/23 02:28:58.553
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:29:00.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3496" for this suite. 02/27/23 02:29:00.609
------------------------------
• [4.240 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:28:56.39
    Feb 27 02:28:56.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:28:56.391
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:28:56.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:28:56.431
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b1b8552-33f3-4eab-b21c-f95f155dcda2 02/27/23 02:28:56.444
    STEP: Creating the pod 02/27/23 02:28:56.47
    Feb 27 02:28:56.511: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f" in namespace "projected-3496" to be "running and ready"
    Feb 27 02:28:56.514: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.602973ms
    Feb 27 02:28:56.514: INFO: The phase of Pod pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:28:58.518: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f": Phase="Running", Reason="", readiness=true. Elapsed: 2.007081719s
    Feb 27 02:28:58.518: INFO: The phase of Pod pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f is Running (Ready = true)
    Feb 27 02:28:58.518: INFO: Pod "pod-projected-configmaps-559d3b85-7ce0-411a-b09c-4ccd6ab7296f" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-6b1b8552-33f3-4eab-b21c-f95f155dcda2 02/27/23 02:28:58.544
    STEP: waiting to observe update in volume 02/27/23 02:28:58.553
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:29:00.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3496" for this suite. 02/27/23 02:29:00.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:29:00.631
Feb 27 02:29:00.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 02:29:00.632
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:29:00.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:29:00.731
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f in namespace container-probe-6537 02/27/23 02:29:00.734
Feb 27 02:29:00.749: INFO: Waiting up to 5m0s for pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f" in namespace "container-probe-6537" to be "not pending"
Feb 27 02:29:00.764: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.579122ms
Feb 27 02:29:02.769: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020463109s
Feb 27 02:29:02.769: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f" satisfied condition "not pending"
Feb 27 02:29:02.769: INFO: Started pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f in namespace container-probe-6537
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:29:02.769
Feb 27 02:29:02.773: INFO: Initial restart count of pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f is 0
STEP: deleting the pod 02/27/23 02:33:03.405
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:03.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6537" for this suite. 02/27/23 02:33:03.426
------------------------------
• [SLOW TEST] [242.799 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:29:00.631
    Feb 27 02:29:00.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 02:29:00.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:29:00.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:29:00.731
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f in namespace container-probe-6537 02/27/23 02:29:00.734
    Feb 27 02:29:00.749: INFO: Waiting up to 5m0s for pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f" in namespace "container-probe-6537" to be "not pending"
    Feb 27 02:29:00.764: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.579122ms
    Feb 27 02:29:02.769: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020463109s
    Feb 27 02:29:02.769: INFO: Pod "test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f" satisfied condition "not pending"
    Feb 27 02:29:02.769: INFO: Started pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f in namespace container-probe-6537
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:29:02.769
    Feb 27 02:29:02.773: INFO: Initial restart count of pod test-webserver-7b4013cf-de9f-4534-974b-571fbe8e916f is 0
    STEP: deleting the pod 02/27/23 02:33:03.405
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:03.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6537" for this suite. 02/27/23 02:33:03.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:03.431
Feb 27 02:33:03.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename subpath 02/27/23 02:33:03.432
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:03.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:03.45
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 02:33:03.452
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-l94w 02/27/23 02:33:03.461
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:33:03.461
Feb 27 02:33:03.492: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-l94w" in namespace "subpath-6912" to be "Succeeded or Failed"
Feb 27 02:33:03.495: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535923ms
Feb 27 02:33:05.502: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 2.009250819s
Feb 27 02:33:07.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 4.007939455s
Feb 27 02:33:09.502: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 6.009274298s
Feb 27 02:33:11.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 8.007200445s
Feb 27 02:33:13.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 10.007312838s
Feb 27 02:33:15.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 12.006410873s
Feb 27 02:33:17.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 14.00904313s
Feb 27 02:33:19.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 16.007076755s
Feb 27 02:33:21.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 18.008117651s
Feb 27 02:33:23.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 20.00830528s
Feb 27 02:33:25.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=false. Elapsed: 22.007118877s
Feb 27 02:33:27.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006371379s
STEP: Saw pod success 02/27/23 02:33:27.499
Feb 27 02:33:27.499: INFO: Pod "pod-subpath-test-projected-l94w" satisfied condition "Succeeded or Failed"
Feb 27 02:33:27.503: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-projected-l94w container test-container-subpath-projected-l94w: <nil>
STEP: delete the pod 02/27/23 02:33:27.515
Feb 27 02:33:27.527: INFO: Waiting for pod pod-subpath-test-projected-l94w to disappear
Feb 27 02:33:27.531: INFO: Pod pod-subpath-test-projected-l94w no longer exists
STEP: Deleting pod pod-subpath-test-projected-l94w 02/27/23 02:33:27.531
Feb 27 02:33:27.531: INFO: Deleting pod "pod-subpath-test-projected-l94w" in namespace "subpath-6912"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:27.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6912" for this suite. 02/27/23 02:33:27.54
------------------------------
• [SLOW TEST] [24.114 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:03.431
    Feb 27 02:33:03.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename subpath 02/27/23 02:33:03.432
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:03.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:03.45
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 02:33:03.452
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-l94w 02/27/23 02:33:03.461
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:33:03.461
    Feb 27 02:33:03.492: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-l94w" in namespace "subpath-6912" to be "Succeeded or Failed"
    Feb 27 02:33:03.495: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535923ms
    Feb 27 02:33:05.502: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 2.009250819s
    Feb 27 02:33:07.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 4.007939455s
    Feb 27 02:33:09.502: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 6.009274298s
    Feb 27 02:33:11.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 8.007200445s
    Feb 27 02:33:13.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 10.007312838s
    Feb 27 02:33:15.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 12.006410873s
    Feb 27 02:33:17.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 14.00904313s
    Feb 27 02:33:19.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 16.007076755s
    Feb 27 02:33:21.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 18.008117651s
    Feb 27 02:33:23.501: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=true. Elapsed: 20.00830528s
    Feb 27 02:33:25.500: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Running", Reason="", readiness=false. Elapsed: 22.007118877s
    Feb 27 02:33:27.499: INFO: Pod "pod-subpath-test-projected-l94w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006371379s
    STEP: Saw pod success 02/27/23 02:33:27.499
    Feb 27 02:33:27.499: INFO: Pod "pod-subpath-test-projected-l94w" satisfied condition "Succeeded or Failed"
    Feb 27 02:33:27.503: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-projected-l94w container test-container-subpath-projected-l94w: <nil>
    STEP: delete the pod 02/27/23 02:33:27.515
    Feb 27 02:33:27.527: INFO: Waiting for pod pod-subpath-test-projected-l94w to disappear
    Feb 27 02:33:27.531: INFO: Pod pod-subpath-test-projected-l94w no longer exists
    STEP: Deleting pod pod-subpath-test-projected-l94w 02/27/23 02:33:27.531
    Feb 27 02:33:27.531: INFO: Deleting pod "pod-subpath-test-projected-l94w" in namespace "subpath-6912"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:27.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6912" for this suite. 02/27/23 02:33:27.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:27.546
Feb 27 02:33:27.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:33:27.547
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:27.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:27.579
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-2286381a-0c70-401e-99a1-5de538748815 02/27/23 02:33:27.586
STEP: Creating configMap with name cm-test-opt-upd-3edfe08f-ddf7-440f-bf20-58d574038da7 02/27/23 02:33:27.59
STEP: Creating the pod 02/27/23 02:33:27.595
Feb 27 02:33:27.627: INFO: Waiting up to 5m0s for pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf" in namespace "configmap-9246" to be "running and ready"
Feb 27 02:33:27.630: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.697662ms
Feb 27 02:33:27.630: INFO: The phase of Pod pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:33:29.634: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080732s
Feb 27 02:33:29.634: INFO: The phase of Pod pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf is Running (Ready = true)
Feb 27 02:33:29.634: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2286381a-0c70-401e-99a1-5de538748815 02/27/23 02:33:29.657
STEP: Updating configmap cm-test-opt-upd-3edfe08f-ddf7-440f-bf20-58d574038da7 02/27/23 02:33:29.664
STEP: Creating configMap with name cm-test-opt-create-2875db7f-df70-49ed-8f53-8397e1f71dae 02/27/23 02:33:29.668
STEP: waiting to observe update in volume 02/27/23 02:33:29.672
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:31.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9246" for this suite. 02/27/23 02:33:31.697
------------------------------
• [4.158 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:27.546
    Feb 27 02:33:27.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:33:27.547
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:27.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:27.579
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-2286381a-0c70-401e-99a1-5de538748815 02/27/23 02:33:27.586
    STEP: Creating configMap with name cm-test-opt-upd-3edfe08f-ddf7-440f-bf20-58d574038da7 02/27/23 02:33:27.59
    STEP: Creating the pod 02/27/23 02:33:27.595
    Feb 27 02:33:27.627: INFO: Waiting up to 5m0s for pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf" in namespace "configmap-9246" to be "running and ready"
    Feb 27 02:33:27.630: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.697662ms
    Feb 27 02:33:27.630: INFO: The phase of Pod pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:33:29.634: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007080732s
    Feb 27 02:33:29.634: INFO: The phase of Pod pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf is Running (Ready = true)
    Feb 27 02:33:29.634: INFO: Pod "pod-configmaps-4dd17b79-2db4-4dad-875c-acac3ac2efaf" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2286381a-0c70-401e-99a1-5de538748815 02/27/23 02:33:29.657
    STEP: Updating configmap cm-test-opt-upd-3edfe08f-ddf7-440f-bf20-58d574038da7 02/27/23 02:33:29.664
    STEP: Creating configMap with name cm-test-opt-create-2875db7f-df70-49ed-8f53-8397e1f71dae 02/27/23 02:33:29.668
    STEP: waiting to observe update in volume 02/27/23 02:33:29.672
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:31.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9246" for this suite. 02/27/23 02:33:31.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:31.705
Feb 27 02:33:31.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption 02/27/23 02:33:31.705
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:31.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:31.725
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 02:33:31.728
STEP: Waiting for the pdb to be processed 02/27/23 02:33:31.733
STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 02:33:33.745
STEP: Waiting for all pods to be running 02/27/23 02:33:33.745
Feb 27 02:33:33.748: INFO: pods: 0 < 3
Feb 27 02:33:35.752: INFO: running pods: 2 < 3
STEP: locating a running pod 02/27/23 02:33:37.753
STEP: Updating the pdb to allow a pod to be evicted 02/27/23 02:33:37.766
STEP: Waiting for the pdb to be processed 02/27/23 02:33:37.774
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 02:33:39.781
STEP: Waiting for all pods to be running 02/27/23 02:33:39.781
STEP: Waiting for the pdb to observed all healthy pods 02/27/23 02:33:39.785
STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 02:33:39.811
STEP: Waiting for the pdb to be processed 02/27/23 02:33:39.824
STEP: Waiting for all pods to be running 02/27/23 02:33:41.838
STEP: locating a running pod 02/27/23 02:33:41.842
STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 02:33:41.853
STEP: Waiting for the pdb to be deleted 02/27/23 02:33:41.858
STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 02:33:41.86
STEP: Waiting for all pods to be running 02/27/23 02:33:41.86
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:41.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1381" for this suite. 02/27/23 02:33:41.885
------------------------------
• [SLOW TEST] [10.202 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:31.705
    Feb 27 02:33:31.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption 02/27/23 02:33:31.705
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:31.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:31.725
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 02/27/23 02:33:31.728
    STEP: Waiting for the pdb to be processed 02/27/23 02:33:31.733
    STEP: First trying to evict a pod which shouldn't be evictable 02/27/23 02:33:33.745
    STEP: Waiting for all pods to be running 02/27/23 02:33:33.745
    Feb 27 02:33:33.748: INFO: pods: 0 < 3
    Feb 27 02:33:35.752: INFO: running pods: 2 < 3
    STEP: locating a running pod 02/27/23 02:33:37.753
    STEP: Updating the pdb to allow a pod to be evicted 02/27/23 02:33:37.766
    STEP: Waiting for the pdb to be processed 02/27/23 02:33:37.774
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 02:33:39.781
    STEP: Waiting for all pods to be running 02/27/23 02:33:39.781
    STEP: Waiting for the pdb to observed all healthy pods 02/27/23 02:33:39.785
    STEP: Patching the pdb to disallow a pod to be evicted 02/27/23 02:33:39.811
    STEP: Waiting for the pdb to be processed 02/27/23 02:33:39.824
    STEP: Waiting for all pods to be running 02/27/23 02:33:41.838
    STEP: locating a running pod 02/27/23 02:33:41.842
    STEP: Deleting the pdb to allow a pod to be evicted 02/27/23 02:33:41.853
    STEP: Waiting for the pdb to be deleted 02/27/23 02:33:41.858
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 02/27/23 02:33:41.86
    STEP: Waiting for all pods to be running 02/27/23 02:33:41.86
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:41.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1381" for this suite. 02/27/23 02:33:41.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:41.907
Feb 27 02:33:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 02:33:41.907
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:41.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:41.947
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Feb 27 02:33:41.981: INFO: Waiting up to 5m0s for pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29" in namespace "pods-1216" to be "running and ready"
Feb 27 02:33:41.985: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928836ms
Feb 27 02:33:41.985: INFO: The phase of Pod server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:33:43.990: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29": Phase="Running", Reason="", readiness=true. Elapsed: 2.008628381s
Feb 27 02:33:43.990: INFO: The phase of Pod server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29 is Running (Ready = true)
Feb 27 02:33:43.990: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29" satisfied condition "running and ready"
Feb 27 02:33:44.024: INFO: Waiting up to 5m0s for pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08" in namespace "pods-1216" to be "Succeeded or Failed"
Feb 27 02:33:44.027: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.776618ms
Feb 27 02:33:46.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Running", Reason="", readiness=false. Elapsed: 2.008470229s
Feb 27 02:33:48.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008461682s
STEP: Saw pod success 02/27/23 02:33:48.032
Feb 27 02:33:48.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08" satisfied condition "Succeeded or Failed"
Feb 27 02:33:48.038: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 container env3cont: <nil>
STEP: delete the pod 02/27/23 02:33:48.044
Feb 27 02:33:48.061: INFO: Waiting for pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 to disappear
Feb 27 02:33:48.065: INFO: Pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:48.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1216" for this suite. 02/27/23 02:33:48.07
------------------------------
• [SLOW TEST] [6.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:41.907
    Feb 27 02:33:41.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 02:33:41.907
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:41.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:41.947
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Feb 27 02:33:41.981: INFO: Waiting up to 5m0s for pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29" in namespace "pods-1216" to be "running and ready"
    Feb 27 02:33:41.985: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928836ms
    Feb 27 02:33:41.985: INFO: The phase of Pod server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:33:43.990: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29": Phase="Running", Reason="", readiness=true. Elapsed: 2.008628381s
    Feb 27 02:33:43.990: INFO: The phase of Pod server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29 is Running (Ready = true)
    Feb 27 02:33:43.990: INFO: Pod "server-envvars-7ad71882-1fe1-4f5d-87c5-1f816e8f2a29" satisfied condition "running and ready"
    Feb 27 02:33:44.024: INFO: Waiting up to 5m0s for pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08" in namespace "pods-1216" to be "Succeeded or Failed"
    Feb 27 02:33:44.027: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.776618ms
    Feb 27 02:33:46.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Running", Reason="", readiness=false. Elapsed: 2.008470229s
    Feb 27 02:33:48.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008461682s
    STEP: Saw pod success 02/27/23 02:33:48.032
    Feb 27 02:33:48.032: INFO: Pod "client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08" satisfied condition "Succeeded or Failed"
    Feb 27 02:33:48.038: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 container env3cont: <nil>
    STEP: delete the pod 02/27/23 02:33:48.044
    Feb 27 02:33:48.061: INFO: Waiting for pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 to disappear
    Feb 27 02:33:48.065: INFO: Pod client-envvars-ad769d38-dbeb-43b1-828b-62802a193b08 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:48.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1216" for this suite. 02/27/23 02:33:48.07
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:48.078
Feb 27 02:33:48.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:33:48.079
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:48.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:48.111
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:33:48.13
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:33:48.625
STEP: Deploying the webhook pod 02/27/23 02:33:48.634
STEP: Wait for the deployment to be ready 02/27/23 02:33:48.645
Feb 27 02:33:48.651: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 02:33:50.662
STEP: Verifying the service has paired with the endpoint 02/27/23 02:33:50.69
Feb 27 02:33:51.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 02:33:51.694
STEP: create a pod 02/27/23 02:33:51.71
Feb 27 02:33:51.741: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9221" to be "running"
Feb 27 02:33:51.744: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.429933ms
Feb 27 02:33:53.750: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008929045s
Feb 27 02:33:53.750: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 02:33:53.75
Feb 27 02:33:53.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=webhook-9221 attach --namespace=webhook-9221 to-be-attached-pod -i -c=container1'
Feb 27 02:33:53.842: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9221" for this suite. 02/27/23 02:33:53.934
STEP: Destroying namespace "webhook-9221-markers" for this suite. 02/27/23 02:33:53.945
------------------------------
• [SLOW TEST] [5.878 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:48.078
    Feb 27 02:33:48.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:33:48.079
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:48.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:48.111
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:33:48.13
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:33:48.625
    STEP: Deploying the webhook pod 02/27/23 02:33:48.634
    STEP: Wait for the deployment to be ready 02/27/23 02:33:48.645
    Feb 27 02:33:48.651: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 02:33:50.662
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:33:50.69
    Feb 27 02:33:51.690: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 02/27/23 02:33:51.694
    STEP: create a pod 02/27/23 02:33:51.71
    Feb 27 02:33:51.741: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9221" to be "running"
    Feb 27 02:33:51.744: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.429933ms
    Feb 27 02:33:53.750: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008929045s
    Feb 27 02:33:53.750: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 02/27/23 02:33:53.75
    Feb 27 02:33:53.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=webhook-9221 attach --namespace=webhook-9221 to-be-attached-pod -i -c=container1'
    Feb 27 02:33:53.842: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:53.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9221" for this suite. 02/27/23 02:33:53.934
    STEP: Destroying namespace "webhook-9221-markers" for this suite. 02/27/23 02:33:53.945
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:53.956
Feb 27 02:33:53.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-pred 02/27/23 02:33:53.957
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:53.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:53.985
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 02:33:53.997: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 02:33:54.010: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 02:33:54.014: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
Feb 27 02:33:54.028: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:33:54.028: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:33:54.028: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container registry ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container sidecar ready: true, restart count 0
Feb 27 02:33:54.028: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Feb 27 02:33:54.028: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container controller ready: true, restart count 0
Feb 27 02:33:54.028: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:33:54.028: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:33:54.028: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:33:54.028: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:33:54.028: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:33:54.028: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:33:54.028: INFO: isp-logger-74bf5ff65d-b59dr from monitoring started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container isp-logger ready: true, restart count 0
Feb 27 02:33:54.028: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:33:54.028: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container e2e ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:33:54.028: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:33:54.028: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
Feb 27 02:33:54.044: INFO: default-http-backend-67df9bcb5b-fspzf from ingress-nginx started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 27 02:33:54.045: INFO: nginx-ingress-controller-67d95699d-qglqv from ingress-nginx started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 02:33:54.045: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:33:54.045: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container calicoctl ready: true, restart count 0
Feb 27 02:33:54.045: INFO: ccd-license-consumer-69f48d6d8f-bfqz9 from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Feb 27 02:33:54.045: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:33:54.045: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:33:54.045: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:33:54.045: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:33:54.045: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 02:33:54.045: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 02:33:54.045: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:33:54.045: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 02:28:08 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Feb 27 02:33:54.045: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:33:54.045: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 02:33:54.045: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.045: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:33:54.045: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
Feb 27 02:33:54.058: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:33:54.058: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:33:54.058: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:33:54.058: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:33:54.058: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:33:54.058: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:33:54.058: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:33:54.058: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:33:54.058: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 02:33:54.058: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:33:54.058: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:33:54.058: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:33:54.058: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:33:54.058: INFO: to-be-attached-pod from webhook-9221 started at 2023-02-27 02:33:51 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.058: INFO: 	Container container1 ready: true, restart count 0
Feb 27 02:33:54.058: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
Feb 27 02:33:54.074: INFO: rs-gwzbw from disruption-1381 started at 2023-02-27 02:33:41 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container donothing ready: false, restart count 0
Feb 27 02:33:54.074: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 02:33:54.074: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:33:54.074: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:33:54.074: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:33:54.074: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:33:54.074: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:33:54.074: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:33:54.074: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:33:54.074: INFO: network-resources-injector-545655c748-mzkpv from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 02:33:54.074: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:33:54.074: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Feb 27 02:33:54.074: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Feb 27 02:33:54.074: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Feb 27 02:33:54.074: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:33:54.074: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:33:54.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:33:54.074: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.115
STEP: verifying the node has the label node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.138
STEP: verifying the node has the label node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.156
STEP: verifying the node has the label node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.173
Feb 27 02:33:54.202: INFO: Pod rs-gwzbw requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod default-http-backend-67df9bcb5b-fspzf requesting resource cpu=10m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod nginx-ingress-controller-67d95699d-nkkpt requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod nginx-ingress-controller-67d95699d-qglqv requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod calico-node-42t9d requesting resource cpu=250m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod calico-node-5k85w requesting resource cpu=250m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod calico-node-7chz2 requesting resource cpu=250m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod calico-node-q6vpb requesting resource cpu=250m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod calicoctl-64848f7f7c-ssjb9 requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod ccd-license-consumer-69f48d6d8f-bfqz9 requesting resource cpu=10m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-46cv8 requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-bds62 requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-ptfx8 requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-r96td requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-app-sys-info-handler-674c6dfbf5-jt2mw requesting resource cpu=50m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-data-document-database-pg-0 requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-lcm-container-registry-registry-54ddd9cdb9-2l27b requesting resource cpu=400m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-lm-combined-server-license-server-client-76bf797c48-g6zcr requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-controller-57c598db58dsg requesting resource cpu=50m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-4xqtd requesting resource cpu=50m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-7zm7w requesting resource cpu=50m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-hkwmb requesting resource cpu=50m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-xjcj4 requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-g6mkh requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-m4dgt requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-nlglc requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-sqs2s requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-proxy-8w2tp requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-proxy-jcbgm requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-proxy-pvz7k requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kube-proxy-wdnsg requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kucero-8svhq requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kucero-lgwqj requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kucero-w7c79 requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod kucero-zsvpx requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod metrics-server-697d576bc4-2hwrw requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod network-resources-injector-545655c748-8ttr4 requesting resource cpu=250m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod network-resources-injector-545655c748-mzkpv requesting resource cpu=250m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-local-dns-99jdq requesting resource cpu=25m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-local-dns-jghdz requesting resource cpu=25m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-local-dns-jmd95 requesting resource cpu=25m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-local-dns-s8t4t requesting resource cpu=25m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-alertmanager-547b74fff-2zrcr requesting resource cpu=110m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-kube-state-metrics-8488b76fc5-pcnjq requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-4mg5c requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-pthcx requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-tg4hb requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-z4dl2 requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-pm-server-utils-56888b6858-kjswh requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-agent-6fb8955b7b-x7hlx requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-alert-server-77cc8f97f-2pcrj requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vmstorage-0 requesting resource cpu=500m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod isp-logger-74bf5ff65d-b59dr requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-7h7nd requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-gngw2 requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-jgb7v requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-sdthd requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy-e2e-job-ce280b850091407c requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.202: INFO: Pod to-be-attached-pod requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 02:33:54.202
Feb 27 02:33:54.202: INFO: Creating a pod which consumes cpu=2607m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.248: INFO: Creating a pod which consumes cpu=2166m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.258: INFO: Creating a pod which consumes cpu=2992m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.281: INFO: Creating a pod which consumes cpu=2572m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
Feb 27 02:33:54.291: INFO: Waiting up to 5m0s for pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404" in namespace "sched-pred-2079" to be "running"
Feb 27 02:33:54.303: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404": Phase="Pending", Reason="", readiness=false. Elapsed: 12.562883ms
Feb 27 02:33:56.308: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404": Phase="Running", Reason="", readiness=true. Elapsed: 2.017652338s
Feb 27 02:33:56.308: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404" satisfied condition "running"
Feb 27 02:33:56.308: INFO: Waiting up to 5m0s for pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e" in namespace "sched-pred-2079" to be "running"
Feb 27 02:33:56.312: INFO: Pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e": Phase="Running", Reason="", readiness=true. Elapsed: 3.83382ms
Feb 27 02:33:56.312: INFO: Pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e" satisfied condition "running"
Feb 27 02:33:56.312: INFO: Waiting up to 5m0s for pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6" in namespace "sched-pred-2079" to be "running"
Feb 27 02:33:56.315: INFO: Pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.946811ms
Feb 27 02:33:56.315: INFO: Pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6" satisfied condition "running"
Feb 27 02:33:56.315: INFO: Waiting up to 5m0s for pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096" in namespace "sched-pred-2079" to be "running"
Feb 27 02:33:56.318: INFO: Pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096": Phase="Running", Reason="", readiness=true. Elapsed: 2.656779ms
Feb 27 02:33:56.318: INFO: Pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 02:33:56.318
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe3303abe1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404 to worker-pool1-58bsk712-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe545ad5ce], Reason = [AddedInterface], Message = [Add eth0 [192.168.226.97/32] from k8s-pod-network] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe5bb7e451], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe5cf45a38], Reason = [Created], Message = [Created container filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe627b6fa1], Reason = [Started], Message = [Started container filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe343f6faa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e to worker-pool1-losn7d81-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.321
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe57743534], Reason = [AddedInterface], Message = [Add eth0 [192.168.128.49/32] from k8s-pod-network] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe5d6e43a4], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe5ea44962], Reason = [Created], Message = [Created container filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe6489f395], Reason = [Started], Message = [Started container filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe34d61d66], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6 to worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe55be0757], Reason = [AddedInterface], Message = [Add eth0 [192.168.214.186/32] from k8s-pod-network] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe5cb3141d], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe5dc05951], Reason = [Created], Message = [Created container filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe6385d28e], Reason = [Started], Message = [Started container filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe35fcdc99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096 to worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5818261c], Reason = [AddedInterface], Message = [Add eth0 [192.168.21.183/32] from k8s-pod-network] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5ebf2bb3], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5fb3bd11], Reason = [Created], Message = [Created container filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe64b3a268], Reason = [Started], Message = [Started container filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096] 02/27/23 02:33:56.322
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17478dbeae955e2c], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod..] 02/27/23 02:33:56.337
STEP: removing the label node off the node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.336
STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.352
STEP: removing the label node off the node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.36
STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.377
STEP: removing the label node off the node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.38
STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.396
STEP: removing the label node off the node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.398
STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.419
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:57.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2079" for this suite. 02/27/23 02:33:57.426
------------------------------
• [3.483 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:53.956
    Feb 27 02:33:53.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-pred 02/27/23 02:33:53.957
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:53.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:53.985
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 02:33:53.997: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 02:33:54.010: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 02:33:54.014: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
    Feb 27 02:33:54.028: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container registry ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container sidecar ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container controller ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:33:54.028: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:33:54.028: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: isp-logger-74bf5ff65d-b59dr from monitoring started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container isp-logger ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:33:54.028: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
    Feb 27 02:33:54.044: INFO: default-http-backend-67df9bcb5b-fspzf from ingress-nginx started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container default-http-backend ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: nginx-ingress-controller-67d95699d-qglqv from ingress-nginx started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container calicoctl ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: ccd-license-consumer-69f48d6d8f-bfqz9 from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:33:54.045: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:33:54.045: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 02:28:08 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.045: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:33:54.045: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
    Feb 27 02:33:54.058: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:33:54.058: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: to-be-attached-pod from webhook-9221 started at 2023-02-27 02:33:51 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.058: INFO: 	Container container1 ready: true, restart count 0
    Feb 27 02:33:54.058: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
    Feb 27 02:33:54.074: INFO: rs-gwzbw from disruption-1381 started at 2023-02-27 02:33:41 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container donothing ready: false, restart count 0
    Feb 27 02:33:54.074: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:33:54.074: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: network-resources-injector-545655c748-mzkpv from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:33:54.074: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:33:54.074: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:33:54.074: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.115
    STEP: verifying the node has the label node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.138
    STEP: verifying the node has the label node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.156
    STEP: verifying the node has the label node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 02:33:54.173
    Feb 27 02:33:54.202: INFO: Pod rs-gwzbw requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod default-http-backend-67df9bcb5b-fspzf requesting resource cpu=10m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod nginx-ingress-controller-67d95699d-nkkpt requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod nginx-ingress-controller-67d95699d-qglqv requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod calico-node-42t9d requesting resource cpu=250m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod calico-node-5k85w requesting resource cpu=250m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod calico-node-7chz2 requesting resource cpu=250m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod calico-node-q6vpb requesting resource cpu=250m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod calicoctl-64848f7f7c-ssjb9 requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod ccd-license-consumer-69f48d6d8f-bfqz9 requesting resource cpu=10m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-46cv8 requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-bds62 requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-ptfx8 requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod csi-cinder-nodeplugin-r96td requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-app-sys-info-handler-674c6dfbf5-jt2mw requesting resource cpu=50m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-data-document-database-pg-0 requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-lcm-container-registry-registry-54ddd9cdb9-2l27b requesting resource cpu=400m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-lm-combined-server-license-server-client-76bf797c48-g6zcr requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-controller-57c598db58dsg requesting resource cpu=50m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-4xqtd requesting resource cpu=50m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-7zm7w requesting resource cpu=50m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-hkwmb requesting resource cpu=50m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-tm-external-connectivity-frontend-speaker-xjcj4 requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-g6mkh requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-m4dgt requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-nlglc requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-multus-ds-amd64-sqs2s requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-proxy-8w2tp requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-proxy-jcbgm requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-proxy-pvz7k requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kube-proxy-wdnsg requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kucero-8svhq requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kucero-lgwqj requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kucero-w7c79 requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod kucero-zsvpx requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod metrics-server-697d576bc4-2hwrw requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod network-resources-injector-545655c748-8ttr4 requesting resource cpu=250m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod network-resources-injector-545655c748-mzkpv requesting resource cpu=250m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-local-dns-99jdq requesting resource cpu=25m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-local-dns-jghdz requesting resource cpu=25m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-local-dns-jmd95 requesting resource cpu=25m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-local-dns-s8t4t requesting resource cpu=25m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-alertmanager-547b74fff-2zrcr requesting resource cpu=110m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-kube-state-metrics-8488b76fc5-pcnjq requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-4mg5c requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-pthcx requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-tg4hb requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-node-exporter-z4dl2 requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-pm-server-utils-56888b6858-kjswh requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-agent-6fb8955b7b-x7hlx requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-alert-server-77cc8f97f-2pcrj requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l requesting resource cpu=50m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod eric-victoria-metrics-cluster-vmstorage-0 requesting resource cpu=500m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod isp-logger-74bf5ff65d-b59dr requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-7h7nd requesting resource cpu=100m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-gngw2 requesting resource cpu=100m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-jgb7v requesting resource cpu=100m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod node-cert-exporter-sdthd requesting resource cpu=100m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy-e2e-job-ce280b850091407c requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc requesting resource cpu=0m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 requesting resource cpu=0m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 requesting resource cpu=0m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.202: INFO: Pod to-be-attached-pod requesting resource cpu=0m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    STEP: Starting Pods to consume most of the cluster CPU. 02/27/23 02:33:54.202
    Feb 27 02:33:54.202: INFO: Creating a pod which consumes cpu=2607m on Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.248: INFO: Creating a pod which consumes cpu=2166m on Node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.258: INFO: Creating a pod which consumes cpu=2992m on Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.281: INFO: Creating a pod which consumes cpu=2572m on Node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins
    Feb 27 02:33:54.291: INFO: Waiting up to 5m0s for pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404" in namespace "sched-pred-2079" to be "running"
    Feb 27 02:33:54.303: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404": Phase="Pending", Reason="", readiness=false. Elapsed: 12.562883ms
    Feb 27 02:33:56.308: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404": Phase="Running", Reason="", readiness=true. Elapsed: 2.017652338s
    Feb 27 02:33:56.308: INFO: Pod "filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404" satisfied condition "running"
    Feb 27 02:33:56.308: INFO: Waiting up to 5m0s for pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e" in namespace "sched-pred-2079" to be "running"
    Feb 27 02:33:56.312: INFO: Pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e": Phase="Running", Reason="", readiness=true. Elapsed: 3.83382ms
    Feb 27 02:33:56.312: INFO: Pod "filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e" satisfied condition "running"
    Feb 27 02:33:56.312: INFO: Waiting up to 5m0s for pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6" in namespace "sched-pred-2079" to be "running"
    Feb 27 02:33:56.315: INFO: Pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6": Phase="Running", Reason="", readiness=true. Elapsed: 2.946811ms
    Feb 27 02:33:56.315: INFO: Pod "filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6" satisfied condition "running"
    Feb 27 02:33:56.315: INFO: Waiting up to 5m0s for pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096" in namespace "sched-pred-2079" to be "running"
    Feb 27 02:33:56.318: INFO: Pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096": Phase="Running", Reason="", readiness=true. Elapsed: 2.656779ms
    Feb 27 02:33:56.318: INFO: Pod "filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 02/27/23 02:33:56.318
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe3303abe1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404 to worker-pool1-58bsk712-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe545ad5ce], Reason = [AddedInterface], Message = [Add eth0 [192.168.226.97/32] from k8s-pod-network] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe5bb7e451], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe5cf45a38], Reason = [Created], Message = [Created container filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404.17478dbe627b6fa1], Reason = [Started], Message = [Started container filler-pod-150d8bf7-5af8-419f-8d61-2b659c693404] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe343f6faa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e to worker-pool1-losn7d81-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.321
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe57743534], Reason = [AddedInterface], Message = [Add eth0 [192.168.128.49/32] from k8s-pod-network] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe5d6e43a4], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe5ea44962], Reason = [Created], Message = [Created container filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e.17478dbe6489f395], Reason = [Started], Message = [Started container filler-pod-3adf74d4-3220-4d0c-9937-00ed592ffc7e] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe34d61d66], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6 to worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe55be0757], Reason = [AddedInterface], Message = [Add eth0 [192.168.214.186/32] from k8s-pod-network] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe5cb3141d], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe5dc05951], Reason = [Created], Message = [Created container filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6.17478dbe6385d28e], Reason = [Started], Message = [Started container filler-pod-97d23b46-867a-4e4a-8b06-755675cc56d6] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe35fcdc99], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2079/filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096 to worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5818261c], Reason = [AddedInterface], Message = [Add eth0 [192.168.21.183/32] from k8s-pod-network] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5ebf2bb3], Reason = [Pulled], Message = [Container image "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9" already present on machine] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe5fb3bd11], Reason = [Created], Message = [Created container filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096.17478dbe64b3a268], Reason = [Started], Message = [Started container filler-pod-b277809f-127f-4cd2-b2c9-9e509c8db096] 02/27/23 02:33:56.322
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17478dbeae955e2c], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod..] 02/27/23 02:33:56.337
    STEP: removing the label node off the node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.336
    STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.352
    STEP: removing the label node off the node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.36
    STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.377
    STEP: removing the label node off the node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.38
    STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.396
    STEP: removing the label node off the node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins 02/27/23 02:33:57.398
    STEP: verifying the node doesn't have the label node 02/27/23 02:33:57.419
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:57.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2079" for this suite. 02/27/23 02:33:57.426
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:57.44
Feb 27 02:33:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:33:57.441
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:57.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:57.461
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Feb 27 02:33:57.464: INFO: Creating deployment "test-recreate-deployment"
Feb 27 02:33:57.469: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 27 02:33:57.475: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 27 02:33:59.482: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 27 02:33:59.484: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 27 02:33:59.493: INFO: Updating deployment test-recreate-deployment
Feb 27 02:33:59.493: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:33:59.709: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8474  368aae91-4111-4780-b200-5cdd97820107 45882 2 2023-02-27 02:33:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f21d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:33:59 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-564556c788" is progressing.,LastUpdateTime:2023-02-27 02:33:59 +0000 UTC,LastTransitionTime:2023-02-27 02:33:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 27 02:33:59.712: INFO: New ReplicaSet "test-recreate-deployment-564556c788" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-564556c788  deployment-8474  c8891dd8-2f12-4b8b-a9d7-41b8e18586f7 45880 1 2023-02-27 02:33:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 368aae91-4111-4780-b200-5cdd97820107 0xc0072f2567 0xc0072f2568}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368aae91-4111-4780-b200-5cdd97820107\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 564556c788,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f2608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:33:59.712: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 27 02:33:59.712: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-86b6c96685  deployment-8474  6a6bd97d-2ab2-464f-9363-7d3b98d4db63 45869 2 2023-02-27 02:33:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:86b6c96685] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 368aae91-4111-4780-b200-5cdd97820107 0xc0072f2677 0xc0072f2678}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368aae91-4111-4780-b200-5cdd97820107\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 86b6c96685,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:86b6c96685] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f2728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:33:59.715: INFO: Pod "test-recreate-deployment-564556c788-kvfxd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-564556c788-kvfxd test-recreate-deployment-564556c788- deployment-8474  30435f5c-9be7-49e7-8066-f464748f2bed 45881 0 2023-02-27 02:33:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[] [{apps/v1 ReplicaSet test-recreate-deployment-564556c788 c8891dd8-2f12-4b8b-a9d7-41b8e18586f7 0xc0072f2d07 0xc0072f2d08}] [] [{kube-controller-manager Update v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8891dd8-2f12-4b8b-a9d7-41b8e18586f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr8bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr8bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:33:59.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8474" for this suite. 02/27/23 02:33:59.755
------------------------------
• [2.323 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:57.44
    Feb 27 02:33:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:33:57.441
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:57.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:57.461
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Feb 27 02:33:57.464: INFO: Creating deployment "test-recreate-deployment"
    Feb 27 02:33:57.469: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Feb 27 02:33:57.475: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Feb 27 02:33:59.482: INFO: Waiting deployment "test-recreate-deployment" to complete
    Feb 27 02:33:59.484: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Feb 27 02:33:59.493: INFO: Updating deployment test-recreate-deployment
    Feb 27 02:33:59.493: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:33:59.709: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8474  368aae91-4111-4780-b200-5cdd97820107 45882 2 2023-02-27 02:33:57 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f21d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:33:59 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-564556c788" is progressing.,LastUpdateTime:2023-02-27 02:33:59 +0000 UTC,LastTransitionTime:2023-02-27 02:33:57 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Feb 27 02:33:59.712: INFO: New ReplicaSet "test-recreate-deployment-564556c788" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-564556c788  deployment-8474  c8891dd8-2f12-4b8b-a9d7-41b8e18586f7 45880 1 2023-02-27 02:33:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 368aae91-4111-4780-b200-5cdd97820107 0xc0072f2567 0xc0072f2568}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368aae91-4111-4780-b200-5cdd97820107\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 564556c788,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f2608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:33:59.712: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Feb 27 02:33:59.712: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-86b6c96685  deployment-8474  6a6bd97d-2ab2-464f-9363-7d3b98d4db63 45869 2 2023-02-27 02:33:57 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:86b6c96685] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 368aae91-4111-4780-b200-5cdd97820107 0xc0072f2677 0xc0072f2678}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"368aae91-4111-4780-b200-5cdd97820107\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 86b6c96685,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:86b6c96685] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0072f2728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:33:59.715: INFO: Pod "test-recreate-deployment-564556c788-kvfxd" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-564556c788-kvfxd test-recreate-deployment-564556c788- deployment-8474  30435f5c-9be7-49e7-8066-f464748f2bed 45881 0 2023-02-27 02:33:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:564556c788] map[] [{apps/v1 ReplicaSet test-recreate-deployment-564556c788 c8891dd8-2f12-4b8b-a9d7-41b8e18586f7 0xc0072f2d07 0xc0072f2d08}] [] [{kube-controller-manager Update v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8891dd8-2f12-4b8b-a9d7-41b8e18586f7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:33:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr8bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr8bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:33:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:33:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:33:59.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8474" for this suite. 02/27/23 02:33:59.755
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:33:59.763
Feb 27 02:33:59.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:33:59.764
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:59.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:59.784
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-1b7b9b9d-d386-44d7-89b2-84a087cb3d61 02/27/23 02:33:59.787
STEP: Creating a pod to test consume configMaps 02/27/23 02:33:59.792
Feb 27 02:33:59.805: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0" in namespace "projected-2537" to be "Succeeded or Failed"
Feb 27 02:33:59.808: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838084ms
Feb 27 02:34:01.813: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008316327s
Feb 27 02:34:03.812: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006734198s
STEP: Saw pod success 02/27/23 02:34:03.812
Feb 27 02:34:03.812: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0" satisfied condition "Succeeded or Failed"
Feb 27 02:34:03.816: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:34:03.822
Feb 27 02:34:03.834: INFO: Waiting for pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 to disappear
Feb 27 02:34:03.837: INFO: Pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:03.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2537" for this suite. 02/27/23 02:34:03.842
------------------------------
• [4.084 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:33:59.763
    Feb 27 02:33:59.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:33:59.764
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:33:59.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:33:59.784
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-1b7b9b9d-d386-44d7-89b2-84a087cb3d61 02/27/23 02:33:59.787
    STEP: Creating a pod to test consume configMaps 02/27/23 02:33:59.792
    Feb 27 02:33:59.805: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0" in namespace "projected-2537" to be "Succeeded or Failed"
    Feb 27 02:33:59.808: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.838084ms
    Feb 27 02:34:01.813: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008316327s
    Feb 27 02:34:03.812: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006734198s
    STEP: Saw pod success 02/27/23 02:34:03.812
    Feb 27 02:34:03.812: INFO: Pod "pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0" satisfied condition "Succeeded or Failed"
    Feb 27 02:34:03.816: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:34:03.822
    Feb 27 02:34:03.834: INFO: Waiting for pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 to disappear
    Feb 27 02:34:03.837: INFO: Pod pod-projected-configmaps-fb3c0fa0-605e-4963-8122-0b6c83f94fd0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:03.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2537" for this suite. 02/27/23 02:34:03.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:03.847
Feb 27 02:34:03.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename endpointslice 02/27/23 02:34:03.848
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:03.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:03.871
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Feb 27 02:34:03.879: INFO: Endpoints addresses: [10.0.10.12 10.0.10.13 10.0.10.25] , ports: [6443]
Feb 27 02:34:03.879: INFO: EndpointSlices addresses: [10.0.10.12 10.0.10.13 10.0.10.25] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4003" for this suite. 02/27/23 02:34:03.883
------------------------------
• [0.040 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:03.847
    Feb 27 02:34:03.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename endpointslice 02/27/23 02:34:03.848
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:03.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:03.871
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Feb 27 02:34:03.879: INFO: Endpoints addresses: [10.0.10.12 10.0.10.13 10.0.10.25] , ports: [6443]
    Feb 27 02:34:03.879: INFO: EndpointSlices addresses: [10.0.10.12 10.0.10.13 10.0.10.25] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4003" for this suite. 02/27/23 02:34:03.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:03.888
Feb 27 02:34:03.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:34:03.889
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:03.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:03.909
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 02/27/23 02:34:03.911
STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:34:03.919
STEP: Creating a ResourceQuota with not terminating scope 02/27/23 02:34:05.925
STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:34:05.932
STEP: Creating a long running pod 02/27/23 02:34:07.938
STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 02:34:07.98
STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 02:34:09.984
STEP: Deleting the pod 02/27/23 02:34:11.989
STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:12.008
STEP: Creating a terminating pod 02/27/23 02:34:14.012
STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 02:34:14.048
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 02:34:16.052
STEP: Deleting the pod 02/27/23 02:34:18.059
STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:18.072
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1322" for this suite. 02/27/23 02:34:20.079
------------------------------
• [SLOW TEST] [16.196 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:03.888
    Feb 27 02:34:03.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:34:03.889
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:03.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:03.909
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 02/27/23 02:34:03.911
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:34:03.919
    STEP: Creating a ResourceQuota with not terminating scope 02/27/23 02:34:05.925
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:34:05.932
    STEP: Creating a long running pod 02/27/23 02:34:07.938
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 02/27/23 02:34:07.98
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 02/27/23 02:34:09.984
    STEP: Deleting the pod 02/27/23 02:34:11.989
    STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:12.008
    STEP: Creating a terminating pod 02/27/23 02:34:14.012
    STEP: Ensuring resource quota with terminating scope captures the pod usage 02/27/23 02:34:14.048
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 02/27/23 02:34:16.052
    STEP: Deleting the pod 02/27/23 02:34:18.059
    STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:18.072
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1322" for this suite. 02/27/23 02:34:20.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:20.086
Feb 27 02:34:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:34:20.087
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:20.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:20.112
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4637 02/27/23 02:34:20.115
STEP: changing the ExternalName service to type=ClusterIP 02/27/23 02:34:20.119
STEP: creating replication controller externalname-service in namespace services-4637 02/27/23 02:34:20.14
I0227 02:34:20.149138      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4637, replica count: 2
I0227 02:34:23.200502      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:34:23.200: INFO: Creating new exec pod
Feb 27 02:34:23.236: INFO: Waiting up to 5m0s for pod "execpodh5hnt" in namespace "services-4637" to be "running"
Feb 27 02:34:23.240: INFO: Pod "execpodh5hnt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135617ms
Feb 27 02:34:25.244: INFO: Pod "execpodh5hnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.007380646s
Feb 27 02:34:25.244: INFO: Pod "execpodh5hnt" satisfied condition "running"
Feb 27 02:34:26.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-4637 exec execpodh5hnt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Feb 27 02:34:26.390: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 27 02:34:26.390: INFO: stdout: ""
Feb 27 02:34:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-4637 exec execpodh5hnt -- /bin/sh -x -c nc -v -z -w 2 10.98.230.177 80'
Feb 27 02:34:26.521: INFO: stderr: "+ nc -v -z -w 2 10.98.230.177 80\nConnection to 10.98.230.177 80 port [tcp/http] succeeded!\n"
Feb 27 02:34:26.521: INFO: stdout: ""
Feb 27 02:34:26.521: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:26.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4637" for this suite. 02/27/23 02:34:26.574
------------------------------
• [SLOW TEST] [6.505 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:20.086
    Feb 27 02:34:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:34:20.087
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:20.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:20.112
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4637 02/27/23 02:34:20.115
    STEP: changing the ExternalName service to type=ClusterIP 02/27/23 02:34:20.119
    STEP: creating replication controller externalname-service in namespace services-4637 02/27/23 02:34:20.14
    I0227 02:34:20.149138      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4637, replica count: 2
    I0227 02:34:23.200502      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:34:23.200: INFO: Creating new exec pod
    Feb 27 02:34:23.236: INFO: Waiting up to 5m0s for pod "execpodh5hnt" in namespace "services-4637" to be "running"
    Feb 27 02:34:23.240: INFO: Pod "execpodh5hnt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135617ms
    Feb 27 02:34:25.244: INFO: Pod "execpodh5hnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.007380646s
    Feb 27 02:34:25.244: INFO: Pod "execpodh5hnt" satisfied condition "running"
    Feb 27 02:34:26.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-4637 exec execpodh5hnt -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Feb 27 02:34:26.390: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Feb 27 02:34:26.390: INFO: stdout: ""
    Feb 27 02:34:26.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-4637 exec execpodh5hnt -- /bin/sh -x -c nc -v -z -w 2 10.98.230.177 80'
    Feb 27 02:34:26.521: INFO: stderr: "+ nc -v -z -w 2 10.98.230.177 80\nConnection to 10.98.230.177 80 port [tcp/http] succeeded!\n"
    Feb 27 02:34:26.521: INFO: stdout: ""
    Feb 27 02:34:26.521: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:26.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4637" for this suite. 02/27/23 02:34:26.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:26.591
Feb 27 02:34:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 02:34:26.592
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:26.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:26.622
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 02:34:26.673
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:34:26.679
Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:26.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:34:26.693: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:34:27.698: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:27.698: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:27.699: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:27.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:34:27.701: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:34:28.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 02:34:28.704: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 02/27/23 02:34:28.707
Feb 27 02:34:28.710: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 02/27/23 02:34:28.711
Feb 27 02:34:28.720: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 02/27/23 02:34:28.72
Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: ADDED
Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.722: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.722: INFO: Found daemon set daemon-set in namespace daemonsets-6915 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 02:34:28.722: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 02/27/23 02:34:28.722
STEP: watching for the daemon set status to be patched 02/27/23 02:34:28.728
Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: ADDED
Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.731: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.731: INFO: Observed daemon set daemon-set in namespace daemonsets-6915 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 02:34:28.731: INFO: Observed &DaemonSet event: MODIFIED
Feb 27 02:34:28.731: INFO: Found daemon set daemon-set in namespace daemonsets-6915 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Feb 27 02:34:28.731: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:34:28.735
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6915, will wait for the garbage collector to delete the pods 02/27/23 02:34:28.735
Feb 27 02:34:28.794: INFO: Deleting DaemonSet.extensions daemon-set took: 6.20608ms
Feb 27 02:34:28.895: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.569976ms
Feb 27 02:34:31.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:34:31.501: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 02:34:31.505: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46384"},"items":null}

Feb 27 02:34:31.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46385"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:31.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6915" for this suite. 02/27/23 02:34:31.535
------------------------------
• [4.950 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:26.591
    Feb 27 02:34:26.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 02:34:26.592
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:26.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:26.622
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 02:34:26.673
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:34:26.679
    Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:26.686: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:26.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:34:26.693: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:34:27.698: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:27.698: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:27.699: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:27.701: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:34:27.701: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:28.700: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:34:28.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 02:34:28.704: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 02/27/23 02:34:28.707
    Feb 27 02:34:28.710: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 02/27/23 02:34:28.711
    Feb 27 02:34:28.720: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 02/27/23 02:34:28.72
    Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: ADDED
    Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.721: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.722: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.722: INFO: Found daemon set daemon-set in namespace daemonsets-6915 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 02:34:28.722: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 02/27/23 02:34:28.722
    STEP: watching for the daemon set status to be patched 02/27/23 02:34:28.728
    Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: ADDED
    Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.730: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.731: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.731: INFO: Observed daemon set daemon-set in namespace daemonsets-6915 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 02:34:28.731: INFO: Observed &DaemonSet event: MODIFIED
    Feb 27 02:34:28.731: INFO: Found daemon set daemon-set in namespace daemonsets-6915 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Feb 27 02:34:28.731: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:34:28.735
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6915, will wait for the garbage collector to delete the pods 02/27/23 02:34:28.735
    Feb 27 02:34:28.794: INFO: Deleting DaemonSet.extensions daemon-set took: 6.20608ms
    Feb 27 02:34:28.895: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.569976ms
    Feb 27 02:34:31.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:34:31.501: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 02:34:31.505: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46384"},"items":null}

    Feb 27 02:34:31.511: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46385"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:31.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6915" for this suite. 02/27/23 02:34:31.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:31.542
Feb 27 02:34:31.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:34:31.543
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:31.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:31.567
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Feb 27 02:34:31.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:37.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-319" for this suite. 02/27/23 02:34:37.932
------------------------------
• [SLOW TEST] [6.399 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:31.542
    Feb 27 02:34:31.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:34:31.543
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:31.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:31.567
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Feb 27 02:34:31.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:37.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-319" for this suite. 02/27/23 02:34:37.932
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:37.941
Feb 27 02:34:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:34:37.942
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:37.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:37.96
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Feb 27 02:34:38.001: INFO: Waiting up to 5m0s for pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830" in namespace "kubelet-test-7794" to be "running and ready"
Feb 27 02:34:38.006: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784132ms
Feb 27 02:34:38.006: INFO: The phase of Pod busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:34:40.009: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830": Phase="Running", Reason="", readiness=true. Elapsed: 2.008198974s
Feb 27 02:34:40.009: INFO: The phase of Pod busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830 is Running (Ready = true)
Feb 27 02:34:40.009: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7794" for this suite. 02/27/23 02:34:40.022
------------------------------
• [2.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:37.941
    Feb 27 02:34:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:34:37.942
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:37.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:37.96
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Feb 27 02:34:38.001: INFO: Waiting up to 5m0s for pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830" in namespace "kubelet-test-7794" to be "running and ready"
    Feb 27 02:34:38.006: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784132ms
    Feb 27 02:34:38.006: INFO: The phase of Pod busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:34:40.009: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830": Phase="Running", Reason="", readiness=true. Elapsed: 2.008198974s
    Feb 27 02:34:40.009: INFO: The phase of Pod busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830 is Running (Ready = true)
    Feb 27 02:34:40.009: INFO: Pod "busybox-scheduling-56c40ce0-6502-406a-b9c0-59cac0a1f830" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:40.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7794" for this suite. 02/27/23 02:34:40.022
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:40.029
Feb 27 02:34:40.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename disruption 02/27/23 02:34:40.03
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:40.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:40.05
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 02/27/23 02:34:40.053
STEP: Waiting for the pdb to be processed 02/27/23 02:34:40.059
STEP: updating the pdb 02/27/23 02:34:42.067
STEP: Waiting for the pdb to be processed 02/27/23 02:34:42.083
STEP: patching the pdb 02/27/23 02:34:44.093
STEP: Waiting for the pdb to be processed 02/27/23 02:34:44.102
STEP: Waiting for the pdb to be deleted 02/27/23 02:34:46.123
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-229" for this suite. 02/27/23 02:34:46.13
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:40.029
    Feb 27 02:34:40.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename disruption 02/27/23 02:34:40.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:40.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:40.05
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 02/27/23 02:34:40.053
    STEP: Waiting for the pdb to be processed 02/27/23 02:34:40.059
    STEP: updating the pdb 02/27/23 02:34:42.067
    STEP: Waiting for the pdb to be processed 02/27/23 02:34:42.083
    STEP: patching the pdb 02/27/23 02:34:44.093
    STEP: Waiting for the pdb to be processed 02/27/23 02:34:44.102
    STEP: Waiting for the pdb to be deleted 02/27/23 02:34:46.123
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-229" for this suite. 02/27/23 02:34:46.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:46.138
Feb 27 02:34:46.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:34:46.138
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:46.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:46.16
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 02/27/23 02:34:46.163
STEP: Creating a ResourceQuota 02/27/23 02:34:51.168
STEP: Ensuring resource quota status is calculated 02/27/23 02:34:51.176
STEP: Creating a Pod that fits quota 02/27/23 02:34:53.182
STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 02:34:53.219
STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 02:34:55.222
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 02:34:55.226
STEP: Ensuring a pod cannot update its resource requirements 02/27/23 02:34:55.23
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 02:34:55.234
STEP: Deleting the pod 02/27/23 02:34:57.24
STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:57.251
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:34:59.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1049" for this suite. 02/27/23 02:34:59.26
------------------------------
• [SLOW TEST] [13.133 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:46.138
    Feb 27 02:34:46.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:34:46.138
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:46.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:46.16
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 02/27/23 02:34:46.163
    STEP: Creating a ResourceQuota 02/27/23 02:34:51.168
    STEP: Ensuring resource quota status is calculated 02/27/23 02:34:51.176
    STEP: Creating a Pod that fits quota 02/27/23 02:34:53.182
    STEP: Ensuring ResourceQuota status captures the pod usage 02/27/23 02:34:53.219
    STEP: Not allowing a pod to be created that exceeds remaining quota 02/27/23 02:34:55.222
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 02/27/23 02:34:55.226
    STEP: Ensuring a pod cannot update its resource requirements 02/27/23 02:34:55.23
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 02/27/23 02:34:55.234
    STEP: Deleting the pod 02/27/23 02:34:57.24
    STEP: Ensuring resource quota status released the pod usage 02/27/23 02:34:57.251
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:34:59.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1049" for this suite. 02/27/23 02:34:59.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:34:59.272
Feb 27 02:34:59.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:34:59.273
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:59.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:59.299
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-5dd6997d-f2e2-4675-86df-83acdee81585 02/27/23 02:34:59.302
STEP: Creating a pod to test consume configMaps 02/27/23 02:34:59.306
Feb 27 02:34:59.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad" in namespace "configmap-9820" to be "Succeeded or Failed"
Feb 27 02:34:59.317: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209533ms
Feb 27 02:35:01.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008453262s
Feb 27 02:35:03.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00838326s
STEP: Saw pod success 02/27/23 02:35:03.323
Feb 27 02:35:03.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad" satisfied condition "Succeeded or Failed"
Feb 27 02:35:03.327: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:35:03.334
Feb 27 02:35:03.351: INFO: Waiting for pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad to disappear
Feb 27 02:35:03.354: INFO: Pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:03.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9820" for this suite. 02/27/23 02:35:03.36
------------------------------
• [4.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:34:59.272
    Feb 27 02:34:59.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:34:59.273
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:34:59.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:34:59.299
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-5dd6997d-f2e2-4675-86df-83acdee81585 02/27/23 02:34:59.302
    STEP: Creating a pod to test consume configMaps 02/27/23 02:34:59.306
    Feb 27 02:34:59.314: INFO: Waiting up to 5m0s for pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad" in namespace "configmap-9820" to be "Succeeded or Failed"
    Feb 27 02:34:59.317: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209533ms
    Feb 27 02:35:01.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008453262s
    Feb 27 02:35:03.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00838326s
    STEP: Saw pod success 02/27/23 02:35:03.323
    Feb 27 02:35:03.323: INFO: Pod "pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad" satisfied condition "Succeeded or Failed"
    Feb 27 02:35:03.327: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:35:03.334
    Feb 27 02:35:03.351: INFO: Waiting for pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad to disappear
    Feb 27 02:35:03.354: INFO: Pod pod-configmaps-99a91d5a-8703-4942-8550-436bb40160ad no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:03.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9820" for this suite. 02/27/23 02:35:03.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:03.368
Feb 27 02:35:03.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:35:03.369
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:03.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:03.393
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9026 02/27/23 02:35:03.396
STEP: creating replication controller nodeport-test in namespace services-9026 02/27/23 02:35:03.417
I0227 02:35:03.425336      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9026, replica count: 2
I0227 02:35:06.475905      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 27 02:35:06.475: INFO: Creating new exec pod
Feb 27 02:35:06.510: INFO: Waiting up to 5m0s for pod "execpodb6ncm" in namespace "services-9026" to be "running"
Feb 27 02:35:06.512: INFO: Pod "execpodb6ncm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501056ms
Feb 27 02:35:08.516: INFO: Pod "execpodb6ncm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006158702s
Feb 27 02:35:08.516: INFO: Pod "execpodb6ncm" satisfied condition "running"
Feb 27 02:35:09.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Feb 27 02:35:09.659: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 27 02:35:09.659: INFO: stdout: ""
Feb 27 02:35:09.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.110.229.193 80'
Feb 27 02:35:09.817: INFO: stderr: "+ nc -v -z -w 2 10.110.229.193 80\nConnection to 10.110.229.193 80 port [tcp/http] succeeded!\n"
Feb 27 02:35:09.817: INFO: stdout: ""
Feb 27 02:35:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 31899'
Feb 27 02:35:09.957: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 31899\nConnection to 10.0.10.22 31899 port [tcp/*] succeeded!\n"
Feb 27 02:35:09.957: INFO: stdout: ""
Feb 27 02:35:09.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 31899'
Feb 27 02:35:10.096: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 31899\nConnection to 10.0.10.3 31899 port [tcp/*] succeeded!\n"
Feb 27 02:35:10.096: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:10.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9026" for this suite. 02/27/23 02:35:10.101
------------------------------
• [SLOW TEST] [6.739 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:03.368
    Feb 27 02:35:03.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:35:03.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:03.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:03.393
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9026 02/27/23 02:35:03.396
    STEP: creating replication controller nodeport-test in namespace services-9026 02/27/23 02:35:03.417
    I0227 02:35:03.425336      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9026, replica count: 2
    I0227 02:35:06.475905      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Feb 27 02:35:06.475: INFO: Creating new exec pod
    Feb 27 02:35:06.510: INFO: Waiting up to 5m0s for pod "execpodb6ncm" in namespace "services-9026" to be "running"
    Feb 27 02:35:06.512: INFO: Pod "execpodb6ncm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501056ms
    Feb 27 02:35:08.516: INFO: Pod "execpodb6ncm": Phase="Running", Reason="", readiness=true. Elapsed: 2.006158702s
    Feb 27 02:35:08.516: INFO: Pod "execpodb6ncm" satisfied condition "running"
    Feb 27 02:35:09.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Feb 27 02:35:09.659: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Feb 27 02:35:09.659: INFO: stdout: ""
    Feb 27 02:35:09.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.110.229.193 80'
    Feb 27 02:35:09.817: INFO: stderr: "+ nc -v -z -w 2 10.110.229.193 80\nConnection to 10.110.229.193 80 port [tcp/http] succeeded!\n"
    Feb 27 02:35:09.817: INFO: stdout: ""
    Feb 27 02:35:09.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.22 31899'
    Feb 27 02:35:09.957: INFO: stderr: "+ nc -v -z -w 2 10.0.10.22 31899\nConnection to 10.0.10.22 31899 port [tcp/*] succeeded!\n"
    Feb 27 02:35:09.957: INFO: stdout: ""
    Feb 27 02:35:09.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=services-9026 exec execpodb6ncm -- /bin/sh -x -c nc -v -z -w 2 10.0.10.3 31899'
    Feb 27 02:35:10.096: INFO: stderr: "+ nc -v -z -w 2 10.0.10.3 31899\nConnection to 10.0.10.3 31899 port [tcp/*] succeeded!\n"
    Feb 27 02:35:10.096: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:10.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9026" for this suite. 02/27/23 02:35:10.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:10.107
Feb 27 02:35:10.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename conformance-tests 02/27/23 02:35:10.108
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:10.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:10.128
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 02/27/23 02:35:10.13
Feb 27 02:35:10.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:10.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-8682" for this suite. 02/27/23 02:35:10.145
------------------------------
• [0.044 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:10.107
    Feb 27 02:35:10.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename conformance-tests 02/27/23 02:35:10.108
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:10.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:10.128
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 02/27/23 02:35:10.13
    Feb 27 02:35:10.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:10.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-8682" for this suite. 02/27/23 02:35:10.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:10.152
Feb 27 02:35:10.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 02:35:10.153
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:10.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:10.214
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 02:35:10.228
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 02/27/23 02:35:10.228
STEP: creating a pod to probe DNS 02/27/23 02:35:10.228
STEP: submitting the pod to kubernetes 02/27/23 02:35:10.228
Feb 27 02:35:10.240: INFO: Waiting up to 15m0s for pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b" in namespace "dns-3567" to be "running"
Feb 27 02:35:10.243: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726758ms
Feb 27 02:35:12.247: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007676479s
Feb 27 02:35:12.248: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:35:12.248
STEP: looking for the results for each expected name from probers 02/27/23 02:35:12.251
Feb 27 02:35:12.269: INFO: DNS probes using dns-3567/dns-test-37ed9e61-6352-4406-89f4-726bda871e7b succeeded

STEP: deleting the pod 02/27/23 02:35:12.269
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:12.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3567" for this suite. 02/27/23 02:35:12.286
------------------------------
• [2.139 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:10.152
    Feb 27 02:35:10.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 02:35:10.153
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:10.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:10.214
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 02:35:10.228
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     02/27/23 02:35:10.228
    STEP: creating a pod to probe DNS 02/27/23 02:35:10.228
    STEP: submitting the pod to kubernetes 02/27/23 02:35:10.228
    Feb 27 02:35:10.240: INFO: Waiting up to 15m0s for pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b" in namespace "dns-3567" to be "running"
    Feb 27 02:35:10.243: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726758ms
    Feb 27 02:35:12.247: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007676479s
    Feb 27 02:35:12.248: INFO: Pod "dns-test-37ed9e61-6352-4406-89f4-726bda871e7b" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:35:12.248
    STEP: looking for the results for each expected name from probers 02/27/23 02:35:12.251
    Feb 27 02:35:12.269: INFO: DNS probes using dns-3567/dns-test-37ed9e61-6352-4406-89f4-726bda871e7b succeeded

    STEP: deleting the pod 02/27/23 02:35:12.269
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:12.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3567" for this suite. 02/27/23 02:35:12.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:12.292
Feb 27 02:35:12.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:35:12.293
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:12.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:12.318
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:35:12.34
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:35:12.642
STEP: Deploying the webhook pod 02/27/23 02:35:12.653
STEP: Wait for the deployment to be ready 02/27/23 02:35:12.665
Feb 27 02:35:12.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:35:14.684
STEP: Verifying the service has paired with the endpoint 02/27/23 02:35:14.699
Feb 27 02:35:15.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 02/27/23 02:35:15.705
Feb 27 02:35:15.725: INFO: Waiting for webhook configuration to be ready...
STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 02:35:15.837
STEP: Creating a configMap that should not be mutated 02/27/23 02:35:15.846
STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 02:35:15.856
STEP: Creating a configMap that should be mutated 02/27/23 02:35:15.863
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:15.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-53" for this suite. 02/27/23 02:35:15.95
STEP: Destroying namespace "webhook-53-markers" for this suite. 02/27/23 02:35:15.962
------------------------------
• [3.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:12.292
    Feb 27 02:35:12.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:35:12.293
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:12.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:12.318
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:35:12.34
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:35:12.642
    STEP: Deploying the webhook pod 02/27/23 02:35:12.653
    STEP: Wait for the deployment to be ready 02/27/23 02:35:12.665
    Feb 27 02:35:12.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:35:14.684
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:35:14.699
    Feb 27 02:35:15.700: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 02/27/23 02:35:15.705
    Feb 27 02:35:15.725: INFO: Waiting for webhook configuration to be ready...
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 02/27/23 02:35:15.837
    STEP: Creating a configMap that should not be mutated 02/27/23 02:35:15.846
    STEP: Patching a mutating webhook configuration's rules to include the create operation 02/27/23 02:35:15.856
    STEP: Creating a configMap that should be mutated 02/27/23 02:35:15.863
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:15.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-53" for this suite. 02/27/23 02:35:15.95
    STEP: Destroying namespace "webhook-53-markers" for this suite. 02/27/23 02:35:15.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:15.978
Feb 27 02:35:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:35:15.978
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:16.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:16.011
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-739 02/27/23 02:35:16.013
STEP: creating a selector 02/27/23 02:35:16.013
STEP: Creating the service pods in kubernetes 02/27/23 02:35:16.013
Feb 27 02:35:16.013: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 02:35:16.114: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-739" to be "running and ready"
Feb 27 02:35:16.135: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.15101ms
Feb 27 02:35:16.135: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:35:18.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025777689s
Feb 27 02:35:18.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:35:20.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025751161s
Feb 27 02:35:20.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:35:22.141: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026484887s
Feb 27 02:35:22.141: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:35:24.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025380042s
Feb 27 02:35:24.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:35:26.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026356283s
Feb 27 02:35:26.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:35:28.142: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.027447696s
Feb 27 02:35:28.142: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 02:35:28.142: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 02:35:28.145: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-739" to be "running and ready"
Feb 27 02:35:28.149: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.883193ms
Feb 27 02:35:28.149: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 02:35:28.149: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 02:35:28.154: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-739" to be "running and ready"
Feb 27 02:35:28.156: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.291078ms
Feb 27 02:35:28.156: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 02:35:28.156: INFO: Pod "netserver-2" satisfied condition "running and ready"
Feb 27 02:35:28.159: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-739" to be "running and ready"
Feb 27 02:35:28.161: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.945622ms
Feb 27 02:35:28.161: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Feb 27 02:35:28.161: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 02:35:28.163
Feb 27 02:35:28.195: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-739" to be "running"
Feb 27 02:35:28.198: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625457ms
Feb 27 02:35:30.203: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007859255s
Feb 27 02:35:30.203: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 02:35:30.207: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 27 02:35:30.207: INFO: Breadth first check of 192.168.226.101 on host 10.0.10.15...
Feb 27 02:35:30.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.226.101&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:35:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:35:30.215: INFO: ExecWithOptions: Clientset creation
Feb 27 02:35:30.215: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.226.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:35:30.279: INFO: Waiting for responses: map[]
Feb 27 02:35:30.279: INFO: reached 192.168.226.101 after 0/1 tries
Feb 27 02:35:30.279: INFO: Breadth first check of 192.168.128.51 on host 10.0.10.22...
Feb 27 02:35:30.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.128.51&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:35:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:35:30.285: INFO: ExecWithOptions: Clientset creation
Feb 27 02:35:30.285: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.128.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:35:30.331: INFO: Waiting for responses: map[]
Feb 27 02:35:30.331: INFO: reached 192.168.128.51 after 0/1 tries
Feb 27 02:35:30.331: INFO: Breadth first check of 192.168.214.143 on host 10.0.10.3...
Feb 27 02:35:30.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.214.143&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:35:30.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:35:30.335: INFO: ExecWithOptions: Clientset creation
Feb 27 02:35:30.335: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.214.143%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:35:30.419: INFO: Waiting for responses: map[]
Feb 27 02:35:30.419: INFO: reached 192.168.214.143 after 0/1 tries
Feb 27 02:35:30.419: INFO: Breadth first check of 192.168.21.185 on host 10.0.10.5...
Feb 27 02:35:30.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.21.185&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:35:30.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:35:30.423: INFO: ExecWithOptions: Clientset creation
Feb 27 02:35:30.424: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.21.185%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:35:30.485: INFO: Waiting for responses: map[]
Feb 27 02:35:30.485: INFO: reached 192.168.21.185 after 0/1 tries
Feb 27 02:35:30.485: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-739" for this suite. 02/27/23 02:35:30.489
------------------------------
• [SLOW TEST] [14.518 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:15.978
    Feb 27 02:35:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:35:15.978
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:16.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:16.011
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-739 02/27/23 02:35:16.013
    STEP: creating a selector 02/27/23 02:35:16.013
    STEP: Creating the service pods in kubernetes 02/27/23 02:35:16.013
    Feb 27 02:35:16.013: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 02:35:16.114: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-739" to be "running and ready"
    Feb 27 02:35:16.135: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.15101ms
    Feb 27 02:35:16.135: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:35:18.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.025777689s
    Feb 27 02:35:18.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:35:20.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.025751161s
    Feb 27 02:35:20.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:35:22.141: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.026484887s
    Feb 27 02:35:22.141: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:35:24.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025380042s
    Feb 27 02:35:24.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:35:26.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.026356283s
    Feb 27 02:35:26.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:35:28.142: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.027447696s
    Feb 27 02:35:28.142: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 02:35:28.142: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 02:35:28.145: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-739" to be "running and ready"
    Feb 27 02:35:28.149: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.883193ms
    Feb 27 02:35:28.149: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 02:35:28.149: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 02:35:28.154: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-739" to be "running and ready"
    Feb 27 02:35:28.156: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.291078ms
    Feb 27 02:35:28.156: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 02:35:28.156: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Feb 27 02:35:28.159: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-739" to be "running and ready"
    Feb 27 02:35:28.161: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.945622ms
    Feb 27 02:35:28.161: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Feb 27 02:35:28.161: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 02:35:28.163
    Feb 27 02:35:28.195: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-739" to be "running"
    Feb 27 02:35:28.198: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625457ms
    Feb 27 02:35:30.203: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007859255s
    Feb 27 02:35:30.203: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 02:35:30.207: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Feb 27 02:35:30.207: INFO: Breadth first check of 192.168.226.101 on host 10.0.10.15...
    Feb 27 02:35:30.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.226.101&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:35:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:35:30.215: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:35:30.215: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.226.101%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:35:30.279: INFO: Waiting for responses: map[]
    Feb 27 02:35:30.279: INFO: reached 192.168.226.101 after 0/1 tries
    Feb 27 02:35:30.279: INFO: Breadth first check of 192.168.128.51 on host 10.0.10.22...
    Feb 27 02:35:30.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.128.51&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:35:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:35:30.285: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:35:30.285: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.128.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:35:30.331: INFO: Waiting for responses: map[]
    Feb 27 02:35:30.331: INFO: reached 192.168.128.51 after 0/1 tries
    Feb 27 02:35:30.331: INFO: Breadth first check of 192.168.214.143 on host 10.0.10.3...
    Feb 27 02:35:30.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.214.143&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:35:30.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:35:30.335: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:35:30.335: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.214.143%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:35:30.419: INFO: Waiting for responses: map[]
    Feb 27 02:35:30.419: INFO: reached 192.168.214.143 after 0/1 tries
    Feb 27 02:35:30.419: INFO: Breadth first check of 192.168.21.185 on host 10.0.10.5...
    Feb 27 02:35:30.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.144:9080/dial?request=hostname&protocol=http&host=192.168.21.185&port=8083&tries=1'] Namespace:pod-network-test-739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:35:30.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:35:30.423: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:35:30.424: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.144%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.21.185%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:35:30.485: INFO: Waiting for responses: map[]
    Feb 27 02:35:30.485: INFO: reached 192.168.21.185 after 0/1 tries
    Feb 27 02:35:30.485: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-739" for this suite. 02/27/23 02:35:30.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:30.496
Feb 27 02:35:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:35:30.497
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:30.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:30.52
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 02/27/23 02:35:30.522
STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:35:30.527
STEP: Creating a ResourceQuota with not best effort scope 02/27/23 02:35:32.531
STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:35:32.536
STEP: Creating a best-effort pod 02/27/23 02:35:34.541
STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 02:35:34.577
STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 02:35:36.581
STEP: Deleting the pod 02/27/23 02:35:38.585
STEP: Ensuring resource quota status released the pod usage 02/27/23 02:35:38.623
STEP: Creating a not best-effort pod 02/27/23 02:35:40.63
STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 02:35:40.666
STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 02:35:42.671
STEP: Deleting the pod 02/27/23 02:35:44.675
STEP: Ensuring resource quota status released the pod usage 02/27/23 02:35:44.685
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:35:46.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8569" for this suite. 02/27/23 02:35:46.693
------------------------------
• [SLOW TEST] [16.203 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:30.496
    Feb 27 02:35:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:35:30.497
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:30.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:30.52
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 02/27/23 02:35:30.522
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:35:30.527
    STEP: Creating a ResourceQuota with not best effort scope 02/27/23 02:35:32.531
    STEP: Ensuring ResourceQuota status is calculated 02/27/23 02:35:32.536
    STEP: Creating a best-effort pod 02/27/23 02:35:34.541
    STEP: Ensuring resource quota with best effort scope captures the pod usage 02/27/23 02:35:34.577
    STEP: Ensuring resource quota with not best effort ignored the pod usage 02/27/23 02:35:36.581
    STEP: Deleting the pod 02/27/23 02:35:38.585
    STEP: Ensuring resource quota status released the pod usage 02/27/23 02:35:38.623
    STEP: Creating a not best-effort pod 02/27/23 02:35:40.63
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 02/27/23 02:35:40.666
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 02/27/23 02:35:42.671
    STEP: Deleting the pod 02/27/23 02:35:44.675
    STEP: Ensuring resource quota status released the pod usage 02/27/23 02:35:44.685
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:35:46.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8569" for this suite. 02/27/23 02:35:46.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:35:46.699
Feb 27 02:35:46.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:35:46.7
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:46.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:46.721
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6032 02/27/23 02:35:46.724
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6032 02/27/23 02:35:46.734
Feb 27 02:35:46.747: INFO: Found 0 stateful pods, waiting for 1
Feb 27 02:35:56.754: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 02/27/23 02:35:56.76
STEP: Getting /status 02/27/23 02:35:56.766
Feb 27 02:35:56.771: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 02/27/23 02:35:56.771
Feb 27 02:35:56.780: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 02/27/23 02:35:56.78
Feb 27 02:35:56.781: INFO: Observed &StatefulSet event: ADDED
Feb 27 02:35:56.781: INFO: Found Statefulset ss in namespace statefulset-6032 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 02:35:56.781: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 02/27/23 02:35:56.781
Feb 27 02:35:56.782: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 02:35:56.788: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 02/27/23 02:35:56.788
Feb 27 02:35:56.789: INFO: Observed &StatefulSet event: ADDED
Feb 27 02:35:56.789: INFO: Observed Statefulset ss in namespace statefulset-6032 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 02:35:56.789: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:35:56.789: INFO: Deleting all statefulset in ns statefulset-6032
Feb 27 02:35:56.792: INFO: Scaling statefulset ss to 0
Feb 27 02:36:06.824: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:36:06.831: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:36:06.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6032" for this suite. 02/27/23 02:36:06.866
------------------------------
• [SLOW TEST] [20.178 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:35:46.699
    Feb 27 02:35:46.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:35:46.7
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:35:46.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:35:46.721
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6032 02/27/23 02:35:46.724
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6032 02/27/23 02:35:46.734
    Feb 27 02:35:46.747: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 02:35:56.754: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 02/27/23 02:35:56.76
    STEP: Getting /status 02/27/23 02:35:56.766
    Feb 27 02:35:56.771: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 02/27/23 02:35:56.771
    Feb 27 02:35:56.780: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 02/27/23 02:35:56.78
    Feb 27 02:35:56.781: INFO: Observed &StatefulSet event: ADDED
    Feb 27 02:35:56.781: INFO: Found Statefulset ss in namespace statefulset-6032 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 02:35:56.781: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 02/27/23 02:35:56.781
    Feb 27 02:35:56.782: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 02:35:56.788: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 02/27/23 02:35:56.788
    Feb 27 02:35:56.789: INFO: Observed &StatefulSet event: ADDED
    Feb 27 02:35:56.789: INFO: Observed Statefulset ss in namespace statefulset-6032 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 02:35:56.789: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:35:56.789: INFO: Deleting all statefulset in ns statefulset-6032
    Feb 27 02:35:56.792: INFO: Scaling statefulset ss to 0
    Feb 27 02:36:06.824: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:36:06.831: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:36:06.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6032" for this suite. 02/27/23 02:36:06.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:36:06.878
Feb 27 02:36:06.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:36:06.879
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:36:06.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:36:06.899
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 02:36:06.915: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 02:37:06.996: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 02/27/23 02:37:06.999
Feb 27 02:37:07.047: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 02:37:07.059: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 02:37:07.084: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 02:37:07.094: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 02:37:07.125: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 02:37:07.140: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Feb 27 02:37:07.164: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Feb 27 02:37:07.172: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 02:37:07.172
Feb 27 02:37:07.172: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:07.175: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.028725ms
Feb 27 02:37:09.179: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007234868s
Feb 27 02:37:09.179: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 02:37:09.179: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.182: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.857478ms
Feb 27 02:37:09.182: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.182: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.185: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.478199ms
Feb 27 02:37:09.185: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.185: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.189: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.183883ms
Feb 27 02:37:09.189: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.189: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.192: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.031454ms
Feb 27 02:37:09.192: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.192: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.195: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.881438ms
Feb 27 02:37:09.195: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.196: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.198: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.597357ms
Feb 27 02:37:09.198: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:37:09.198: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
Feb 27 02:37:09.201: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.509001ms
Feb 27 02:37:09.201: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 02:37:09.201
Feb 27 02:37:09.210: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Feb 27 02:37:09.220: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008298ms
Feb 27 02:37:11.225: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014626442s
Feb 27 02:37:13.226: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015596399s
Feb 27 02:37:15.224: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014362358s
Feb 27 02:37:15.224: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:15.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3612" for this suite. 02/27/23 02:37:15.323
------------------------------
• [SLOW TEST] [68.452 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:36:06.878
    Feb 27 02:36:06.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:36:06.879
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:36:06.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:36:06.899
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 02:36:06.915: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 02:37:06.996: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 02/27/23 02:37:06.999
    Feb 27 02:37:07.047: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 02:37:07.059: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 02:37:07.084: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 02:37:07.094: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 02:37:07.125: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 02:37:07.140: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Feb 27 02:37:07.164: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Feb 27 02:37:07.172: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 02:37:07.172
    Feb 27 02:37:07.172: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:07.175: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.028725ms
    Feb 27 02:37:09.179: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007234868s
    Feb 27 02:37:09.179: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 02:37:09.179: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.182: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.857478ms
    Feb 27 02:37:09.182: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.182: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.185: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.478199ms
    Feb 27 02:37:09.185: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.185: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.189: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.183883ms
    Feb 27 02:37:09.189: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.189: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.192: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.031454ms
    Feb 27 02:37:09.192: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.192: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.195: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.881438ms
    Feb 27 02:37:09.195: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.196: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.198: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.597357ms
    Feb 27 02:37:09.198: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:37:09.198: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-3612" to be "running"
    Feb 27 02:37:09.201: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.509001ms
    Feb 27 02:37:09.201: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 02/27/23 02:37:09.201
    Feb 27 02:37:09.210: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Feb 27 02:37:09.220: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008298ms
    Feb 27 02:37:11.225: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014626442s
    Feb 27 02:37:13.226: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015596399s
    Feb 27 02:37:15.224: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.014362358s
    Feb 27 02:37:15.224: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:15.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3612" for this suite. 02/27/23 02:37:15.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:15.331
Feb 27 02:37:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:37:15.332
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:15.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:15.358
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Feb 27 02:37:15.361: INFO: Creating deployment "webserver-deployment"
Feb 27 02:37:15.365: INFO: Waiting for observed generation 1
Feb 27 02:37:17.374: INFO: Waiting for all required pods to come up
Feb 27 02:37:17.381: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 02/27/23 02:37:17.381
Feb 27 02:37:17.381: INFO: Waiting up to 5m0s for pod "webserver-deployment-648db5888-wk8ww" in namespace "deployment-3814" to be "running"
Feb 27 02:37:17.386: INFO: Pod "webserver-deployment-648db5888-wk8ww": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321773ms
Feb 27 02:37:19.390: INFO: Pod "webserver-deployment-648db5888-wk8ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115268s
Feb 27 02:37:19.390: INFO: Pod "webserver-deployment-648db5888-wk8ww" satisfied condition "running"
Feb 27 02:37:19.390: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 27 02:37:19.397: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 27 02:37:19.409: INFO: Updating deployment webserver-deployment
Feb 27 02:37:19.409: INFO: Waiting for observed generation 2
Feb 27 02:37:21.415: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 27 02:37:21.417: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 27 02:37:21.422: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 02:37:21.432: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 27 02:37:21.432: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 27 02:37:21.435: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 27 02:37:21.441: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 27 02:37:21.441: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 27 02:37:21.449: INFO: Updating deployment webserver-deployment
Feb 27 02:37:21.449: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 27 02:37:21.465: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 27 02:37:23.474: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:37:23.479: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3814  5a73754a-0072-4c49-a25f-00b1a2b7655c 48711 3 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004584708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:37:21 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-27 02:37:23 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Feb 27 02:37:23.482: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3814  47d3b4d6-82e0-4347-a5b7-2462785dd3b6 48506 3 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5a73754a-0072-4c49-a25f-00b1a2b7655c 0xc004f8ab87 0xc004f8ab88}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a73754a-0072-4c49-a25f-00b1a2b7655c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8ac28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:37:23.482: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 27 02:37:23.482: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-648db5888  deployment-3814  a9d81e55-b383-4547-83d9-3ebe5f5ab57f 48709 3 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5a73754a-0072-4c49-a25f-00b1a2b7655c 0xc004f8aa77 0xc004f8aa78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a73754a-0072-4c49-a25f-00b1a2b7655c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 648db5888,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8ab28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-2p49v" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-2p49v webserver-deployment-648db5888- deployment-3814  ee804374-93b8-403f-b5f6-eb204f0a6f5a 48582 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.57"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.57"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b3e7 0xc004c2b3e8}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4cnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4cnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-2qcgl" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-2qcgl webserver-deployment-648db5888- deployment-3814  4079f28d-0c75-4fee-8dc1-492d34561623 48615 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.138"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.138"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b5c0 0xc004c2b5c1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpnhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpnhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-5hfkx" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-5hfkx webserver-deployment-648db5888- deployment-3814  cf84d10f-ff60-4b48-8973-c9973bfa3904 48708 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.59"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.59"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b790 0xc004c2b791}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-787t4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-787t4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.59,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d52fdc4fb0896050f3ed40e43c78d08931f4b99f75547b8acc7ec57ce299af4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-79vz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-79vz2 webserver-deployment-648db5888- deployment-3814  5f1633ec-0f13-4263-970f-647d5547975d 48636 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.110"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.110"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b980 0xc004c2b981}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk2qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk2qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-8thwd" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-8thwd webserver-deployment-648db5888- deployment-3814  b5b9f301-6b41-4998-95a3-a9afaa533a0a 48243 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.55"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.55"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bb50 0xc004c2bb51}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fcpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fcpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.55,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://f64ba43915d6391ab8c8a65992eba4dba0eb7c8122662173abbd9f8473c2588f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-dhgwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-dhgwp webserver-deployment-648db5888- deployment-3814  5b92e19a-dd46-48f3-9afb-6360975d6ed5 48713 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.112"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.112"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bd40 0xc004c2bd41}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzqkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzqkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-dhvsc" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-dhvsc webserver-deployment-648db5888- deployment-3814  5eeb3aff-d67c-4d17-85e0-2395ffb0d7ab 48591 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.148"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.148"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bf10 0xc004c2bf11}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9njr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9njr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-fqv8f" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-fqv8f webserver-deployment-648db5888- deployment-3814  0ee3c4e7-1d19-4da8-8f89-5027c101ae9d 48238 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.188"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.188"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8090 0xc0055a8091}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6547,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6547,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.188,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://227488df84f33eaca4dcf54cfd4f77871341b139c2b95d8528da638bea76139d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-hjllx" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-hjllx webserver-deployment-648db5888- deployment-3814  fa634942-9919-4378-97c5-e0af5a211880 48634 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.155"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8290 0xc0055a8291}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbgpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbgpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-jq254" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-jq254 webserver-deployment-648db5888- deployment-3814  deb630ed-9f9f-4632-9594-c79ccc16a850 48232 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.136"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.136"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8420 0xc0055a8421}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s95xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s95xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.136,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://9432ea5045951272fe872b6affbed87029a1bec4562fbdce44cd956a045439f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-jrzx7" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-jrzx7 webserver-deployment-648db5888- deployment-3814  b0f88b52-150b-4045-ac25-d71efac07ee5 48240 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.189"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.189"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8630 0xc0055a8631}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hmll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hmll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.189,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://3ee39d112eb266365bf3483158d1a13458da4a74eafdcc0366e6ca448fee0dcf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-k7b8w" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-k7b8w webserver-deployment-648db5888- deployment-3814  1314116b-ac0f-4ed1-ad03-1684516525a2 48222 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.105"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8850 0xc0055a8851}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8twhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8twhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.105,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://2e005845502cb50e76efde19f9dc0aca10ede872a6feb08e2298ccef3f593ceb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-lr88v" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-lr88v webserver-deployment-648db5888- deployment-3814  3f646563-77a6-405e-a9cd-1dd238c756dd 48213 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.130"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.130"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8a50 0xc0055a8a51}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-td7zc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-td7zc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.130,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://2bcd0226b8278260e676a5c8c79a00f6ef467716e18a8deac115f2e440d64d8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-lx6p6" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-lx6p6 webserver-deployment-648db5888- deployment-3814  62acc87e-581d-48cc-9f0a-407d690c4245 48525 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8c60 0xc0055a8c61}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4w9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4w9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-p9x7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-p9x7v webserver-deployment-648db5888- deployment-3814  4889bc8c-3109-42bc-b75a-d8ab067e9d63 48702 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.156"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.156"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8e20 0xc0055a8e21}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzshl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzshl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-qv8ws" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-qv8ws webserver-deployment-648db5888- deployment-3814  9e1b9891-8cf9-4dda-9bce-117a8ac74a30 48707 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.158"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.158"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8f90 0xc0055a8f91}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9vzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9vzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-rzg6c" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-rzg6c webserver-deployment-648db5888- deployment-3814  0adb1789-d877-4833-9ec9-a6411a0139a2 48651 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.139"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.139"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a9100 0xc0055a9101}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgtrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgtrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-t6kvv" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-t6kvv webserver-deployment-648db5888- deployment-3814  a18a66a4-9095-4e12-b765-2b7751a82c39 48245 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.54"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.54"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a92d0 0xc0055a92d1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86s6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86s6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.54,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://bf3839a1789098596347aa3c5fefb9dd64cab329075cb35ee023176d9121e775,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-648db5888-vzpbl" is available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-vzpbl webserver-deployment-648db5888- deployment-3814  8d15f4ff-345f-4adb-886f-59163731b3f8 48218 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.106"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.106"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a94c0 0xc0055a94c1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kh26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kh26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.106,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d6a35c8fb4f0ee9ebda84b7bcfbc94bc0bad44eafb343d4bcd8934f6189dd8d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-648db5888-w4k6r" is not available:
&Pod{ObjectMeta:{webserver-deployment-648db5888-w4k6r webserver-deployment-648db5888- deployment-3814  d20f0583-cbed-4cbe-9108-72fb6a085adc 48678 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.111"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.111"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a96b0 0xc0055a96b1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnm7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnm7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-7pblm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7pblm webserver-deployment-d9f79cb5- deployment-3814  0f4124a6-0762-4d2c-83bf-6af5e7020429 48704 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.190"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.190"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a986f 0xc0055a9880}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ph8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ph8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.190,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:19a5e3165e83024c2bb0020882b278772510e6169111504f8cd046d8cbc3789c: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-bskc6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bskc6 webserver-deployment-d9f79cb5- deployment-3814  0be7e995-95e7-4519-b0a7-62b12bd1636a 48609 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.137"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.137"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9a8f 0xc0055a9aa0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45hht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45hht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-ffchz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ffchz webserver-deployment-d9f79cb5- deployment-3814  22036342-b3af-4bff-a25b-d5f32ec956ab 48337 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.107"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.107"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9c7f 0xc0055a9c90}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2g24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2g24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-ljfz5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ljfz5 webserver-deployment-d9f79cb5- deployment-3814  0f7394bc-06f9-438a-b98e-eb80f105ef4e 48644 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.154"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.154"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9e6f 0xc0055a9e80}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsk2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsk2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-mhgpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mhgpr webserver-deployment-d9f79cb5- deployment-3814  b8844c79-066a-465e-bf4a-a05a584f2dc9 48351 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.135"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.135"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9fef 0xc004a06000}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jvtnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvtnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-pltjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pltjg webserver-deployment-d9f79cb5- deployment-3814  20a4d62a-8d90-42c3-a127-31a44826e2d9 48605 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.109"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.109"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a061df 0xc004a061f0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn7b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn7b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-sg8mz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sg8mz webserver-deployment-d9f79cb5- deployment-3814  4f50aedd-1196-4e7e-b722-c9addb3b0cab 48336 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.131"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.131"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a063cf 0xc004a063e0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4c4xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c4xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-vqmmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vqmmb webserver-deployment-d9f79cb5- deployment-3814  bcae4305-44ae-4699-a4c0-e28ea5491253 48562 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.108"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.108"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a065bf 0xc004a065d0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxhz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxhz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-vw88b" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vw88b webserver-deployment-d9f79cb5- deployment-3814  840faf41-d11d-494a-bf7f-0b9dbc3aca27 48350 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.56"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.56"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a067af 0xc004a067c0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxkgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxkgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-w6dhv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w6dhv webserver-deployment-d9f79cb5- deployment-3814  e683778d-914b-44a6-be9a-53fc70ce84a8 48580 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.58"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.128.58"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a0699f 0xc004a069b0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-28lj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-wf7x9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wf7x9 webserver-deployment-d9f79cb5- deployment-3814  a349162c-b2ce-4e5a-a616-8d0c3745984d 48566 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.191"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06b8f 0xc004a06ba0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7jlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7jlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-wqhxx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wqhxx webserver-deployment-d9f79cb5- deployment-3814  45487e01-23cd-4540-bef9-4a2110b043d4 48501 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06d7f 0xc004a06d90}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf55n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf55n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-xx857" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xx857 webserver-deployment-d9f79cb5- deployment-3814  92a263e3-589b-4058-b339-e05f513a5d8c 48675 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.157"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06eef 0xc004a06f00}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh2nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh2nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:23.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3814" for this suite. 02/27/23 02:37:23.497
------------------------------
• [SLOW TEST] [8.171 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:15.331
    Feb 27 02:37:15.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:37:15.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:15.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:15.358
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Feb 27 02:37:15.361: INFO: Creating deployment "webserver-deployment"
    Feb 27 02:37:15.365: INFO: Waiting for observed generation 1
    Feb 27 02:37:17.374: INFO: Waiting for all required pods to come up
    Feb 27 02:37:17.381: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 02/27/23 02:37:17.381
    Feb 27 02:37:17.381: INFO: Waiting up to 5m0s for pod "webserver-deployment-648db5888-wk8ww" in namespace "deployment-3814" to be "running"
    Feb 27 02:37:17.386: INFO: Pod "webserver-deployment-648db5888-wk8ww": Phase="Pending", Reason="", readiness=false. Elapsed: 4.321773ms
    Feb 27 02:37:19.390: INFO: Pod "webserver-deployment-648db5888-wk8ww": Phase="Running", Reason="", readiness=true. Elapsed: 2.009115268s
    Feb 27 02:37:19.390: INFO: Pod "webserver-deployment-648db5888-wk8ww" satisfied condition "running"
    Feb 27 02:37:19.390: INFO: Waiting for deployment "webserver-deployment" to complete
    Feb 27 02:37:19.397: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Feb 27 02:37:19.409: INFO: Updating deployment webserver-deployment
    Feb 27 02:37:19.409: INFO: Waiting for observed generation 2
    Feb 27 02:37:21.415: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Feb 27 02:37:21.417: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Feb 27 02:37:21.422: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 02:37:21.432: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Feb 27 02:37:21.432: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Feb 27 02:37:21.435: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 02:37:21.441: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Feb 27 02:37:21.441: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Feb 27 02:37:21.449: INFO: Updating deployment webserver-deployment
    Feb 27 02:37:21.449: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Feb 27 02:37:21.465: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Feb 27 02:37:23.474: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:37:23.479: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3814  5a73754a-0072-4c49-a25f-00b1a2b7655c 48711 3 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004584708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-02-27 02:37:21 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-02-27 02:37:23 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

    Feb 27 02:37:23.482: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3814  47d3b4d6-82e0-4347-a5b7-2462785dd3b6 48506 3 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5a73754a-0072-4c49-a25f-00b1a2b7655c 0xc004f8ab87 0xc004f8ab88}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a73754a-0072-4c49-a25f-00b1a2b7655c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8ac28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:37:23.482: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Feb 27 02:37:23.482: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-648db5888  deployment-3814  a9d81e55-b383-4547-83d9-3ebe5f5ab57f 48709 3 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5a73754a-0072-4c49-a25f-00b1a2b7655c 0xc004f8aa77 0xc004f8aa78}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a73754a-0072-4c49-a25f-00b1a2b7655c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 648db5888,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f8ab28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-2p49v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-2p49v webserver-deployment-648db5888- deployment-3814  ee804374-93b8-403f-b5f6-eb204f0a6f5a 48582 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.57"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.57"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b3e7 0xc004c2b3e8}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4cnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4cnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-2qcgl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-2qcgl webserver-deployment-648db5888- deployment-3814  4079f28d-0c75-4fee-8dc1-492d34561623 48615 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.138"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.138"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b5c0 0xc004c2b5c1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpnhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpnhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-5hfkx" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-5hfkx webserver-deployment-648db5888- deployment-3814  cf84d10f-ff60-4b48-8973-c9973bfa3904 48708 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.59"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.59"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b790 0xc004c2b791}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-787t4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-787t4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.59,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d52fdc4fb0896050f3ed40e43c78d08931f4b99f75547b8acc7ec57ce299af4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.487: INFO: Pod "webserver-deployment-648db5888-79vz2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-79vz2 webserver-deployment-648db5888- deployment-3814  5f1633ec-0f13-4263-970f-647d5547975d 48636 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.110"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.110"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2b980 0xc004c2b981}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk2qr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk2qr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-8thwd" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-8thwd webserver-deployment-648db5888- deployment-3814  b5b9f301-6b41-4998-95a3-a9afaa533a0a 48243 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.55"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.55"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bb50 0xc004c2bb51}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9fcpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9fcpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.55,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://f64ba43915d6391ab8c8a65992eba4dba0eb7c8122662173abbd9f8473c2588f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-dhgwp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-dhgwp webserver-deployment-648db5888- deployment-3814  5b92e19a-dd46-48f3-9afb-6360975d6ed5 48713 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.112"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.112"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bd40 0xc004c2bd41}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzqkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzqkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-dhvsc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-dhvsc webserver-deployment-648db5888- deployment-3814  5eeb3aff-d67c-4d17-85e0-2395ffb0d7ab 48591 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.148"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.148"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc004c2bf10 0xc004c2bf11}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9njr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9njr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-fqv8f" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-fqv8f webserver-deployment-648db5888- deployment-3814  0ee3c4e7-1d19-4da8-8f89-5027c101ae9d 48238 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.188"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.188"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8090 0xc0055a8091}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6547,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6547,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.188,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://227488df84f33eaca4dcf54cfd4f77871341b139c2b95d8528da638bea76139d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.188,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-hjllx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-hjllx webserver-deployment-648db5888- deployment-3814  fa634942-9919-4378-97c5-e0af5a211880 48634 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.155"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.155"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8290 0xc0055a8291}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tbgpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tbgpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.488: INFO: Pod "webserver-deployment-648db5888-jq254" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-jq254 webserver-deployment-648db5888- deployment-3814  deb630ed-9f9f-4632-9594-c79ccc16a850 48232 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.136"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.136"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8420 0xc0055a8421}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s95xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s95xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.136,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://9432ea5045951272fe872b6affbed87029a1bec4562fbdce44cd956a045439f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-jrzx7" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-jrzx7 webserver-deployment-648db5888- deployment-3814  b0f88b52-150b-4045-ac25-d71efac07ee5 48240 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.189"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.189"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8630 0xc0055a8631}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hmll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hmll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.189,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://3ee39d112eb266365bf3483158d1a13458da4a74eafdcc0366e6ca448fee0dcf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-k7b8w" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-k7b8w webserver-deployment-648db5888- deployment-3814  1314116b-ac0f-4ed1-ad03-1684516525a2 48222 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.105"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.105"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8850 0xc0055a8851}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8twhc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8twhc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.105,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://2e005845502cb50e76efde19f9dc0aca10ede872a6feb08e2298ccef3f593ceb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-lr88v" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-lr88v webserver-deployment-648db5888- deployment-3814  3f646563-77a6-405e-a9cd-1dd238c756dd 48213 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.130"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.130"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8a50 0xc0055a8a51}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-td7zc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-td7zc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.130,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://2bcd0226b8278260e676a5c8c79a00f6ef467716e18a8deac115f2e440d64d8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-lx6p6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-lx6p6 webserver-deployment-648db5888- deployment-3814  62acc87e-581d-48cc-9f0a-407d690c4245 48525 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8c60 0xc0055a8c61}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4w9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4w9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-p9x7v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-p9x7v webserver-deployment-648db5888- deployment-3814  4889bc8c-3109-42bc-b75a-d8ab067e9d63 48702 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.156"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.156"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8e20 0xc0055a8e21}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzshl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzshl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-qv8ws" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-qv8ws webserver-deployment-648db5888- deployment-3814  9e1b9891-8cf9-4dda-9bce-117a8ac74a30 48707 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.158"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.158"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a8f90 0xc0055a8f91}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9vzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9vzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-rzg6c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-rzg6c webserver-deployment-648db5888- deployment-3814  0adb1789-d877-4833-9ec9-a6411a0139a2 48651 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.139"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.139"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a9100 0xc0055a9101}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgtrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgtrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.489: INFO: Pod "webserver-deployment-648db5888-t6kvv" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-t6kvv webserver-deployment-648db5888- deployment-3814  a18a66a4-9095-4e12-b765-2b7751a82c39 48245 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.54"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.54"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a92d0 0xc0055a92d1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.128.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86s6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86s6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:192.168.128.54,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://bf3839a1789098596347aa3c5fefb9dd64cab329075cb35ee023176d9121e775,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.128.54,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-648db5888-vzpbl" is available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-vzpbl webserver-deployment-648db5888- deployment-3814  8d15f4ff-345f-4adb-886f-59163731b3f8 48218 0 2023-02-27 02:37:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.106"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.106"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a94c0 0xc0055a94c1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kh26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kh26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.106,StartTime:2023-02-27 02:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:37:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d6a35c8fb4f0ee9ebda84b7bcfbc94bc0bad44eafb343d4bcd8934f6189dd8d1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-648db5888-w4k6r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-648db5888-w4k6r webserver-deployment-648db5888- deployment-3814  d20f0583-cbed-4cbe-9108-72fb6a085adc 48678 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:648db5888] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.111"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.111"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-648db5888 a9d81e55-b383-4547-83d9-3ebe5f5ab57f 0xc0055a96b0 0xc0055a96b1}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9d81e55-b383-4547-83d9-3ebe5f5ab57f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wnm7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wnm7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-7pblm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7pblm webserver-deployment-d9f79cb5- deployment-3814  0f4124a6-0762-4d2c-83bf-6af5e7020429 48704 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.190"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.190"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a986f 0xc0055a9880}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ph8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ph8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.190,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:19a5e3165e83024c2bb0020882b278772510e6169111504f8cd046d8cbc3789c: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-bskc6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bskc6 webserver-deployment-d9f79cb5- deployment-3814  0be7e995-95e7-4519-b0a7-62b12bd1636a 48609 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.137"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.137"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9a8f 0xc0055a9aa0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45hht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45hht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-ffchz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ffchz webserver-deployment-d9f79cb5- deployment-3814  22036342-b3af-4bff-a25b-d5f32ec956ab 48337 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.107"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.107"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9c7f 0xc0055a9c90}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2g24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2g24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-ljfz5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ljfz5 webserver-deployment-d9f79cb5- deployment-3814  0f7394bc-06f9-438a-b98e-eb80f105ef4e 48644 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.154"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.154"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9e6f 0xc0055a9e80}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dsk2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dsk2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-mhgpr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mhgpr webserver-deployment-d9f79cb5- deployment-3814  b8844c79-066a-465e-bf4a-a05a584f2dc9 48351 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.135"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.135"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc0055a9fef 0xc004a06000}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jvtnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvtnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.490: INFO: Pod "webserver-deployment-d9f79cb5-pltjg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pltjg webserver-deployment-d9f79cb5- deployment-3814  20a4d62a-8d90-42c3-a127-31a44826e2d9 48605 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.109"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.109"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a061df 0xc004a061f0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xn7b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xn7b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-sg8mz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sg8mz webserver-deployment-d9f79cb5- deployment-3814  4f50aedd-1196-4e7e-b722-c9addb3b0cab 48336 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.131"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.131"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a063cf 0xc004a063e0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4c4xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c4xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-vqmmb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vqmmb webserver-deployment-d9f79cb5- deployment-3814  bcae4305-44ae-4699-a4c0-e28ea5491253 48562 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.108"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.108"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a065bf 0xc004a065d0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxhz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxhz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-vw88b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vw88b webserver-deployment-d9f79cb5- deployment-3814  840faf41-d11d-494a-bf7f-0b9dbc3aca27 48350 0 2023-02-27 02:37:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.56"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.56"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a067af 0xc004a067c0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxkgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxkgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-w6dhv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w6dhv webserver-deployment-d9f79cb5- deployment-3814  e683778d-914b-44a6-be9a-53fc70ce84a8 48580 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.58"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.128.58"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a0699f 0xc004a069b0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-28lj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-losn7d81-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.22,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-wf7x9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wf7x9 webserver-deployment-d9f79cb5- deployment-3814  a349162c-b2ce-4e5a-a616-8d0c3745984d 48566 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.191"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.191"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06b8f 0xc004a06ba0}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w7jlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w7jlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:,StartTime:2023-02-27 02:37:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-wqhxx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wqhxx webserver-deployment-d9f79cb5- deployment-3814  45487e01-23cd-4540-bef9-4a2110b043d4 48501 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06d7f 0xc004a06d90}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rf55n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rf55n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 02:37:23.491: INFO: Pod "webserver-deployment-d9f79cb5-xx857" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xx857 webserver-deployment-d9f79cb5- deployment-3814  92a263e3-589b-4058-b339-e05f513a5d8c 48675 0 2023-02-27 02:37:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.157"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.157"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 47d3b4d6-82e0-4347-a5b7-2462785dd3b6 0xc004a06eef 0xc004a06f00}] [] [{kube-controller-manager Update v1 2023-02-27 02:37:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47d3b4d6-82e0-4347-a5b7-2462785dd3b6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh2nv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh2nv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:37:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:23.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3814" for this suite. 02/27/23 02:37:23.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:23.504
Feb 27 02:37:23.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:37:23.504
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:23.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:23.527
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-485a760a-fea0-4402-aa5a-d24107a7c040 02/27/23 02:37:23.53
STEP: Creating a pod to test consume configMaps 02/27/23 02:37:23.534
Feb 27 02:37:23.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04" in namespace "projected-7022" to be "Succeeded or Failed"
Feb 27 02:37:23.566: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160441ms
Feb 27 02:37:25.570: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008173243s
Feb 27 02:37:27.572: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010003575s
Feb 27 02:37:29.570: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008525553s
Feb 27 02:37:31.574: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011945098s
STEP: Saw pod success 02/27/23 02:37:31.574
Feb 27 02:37:31.574: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04" satisfied condition "Succeeded or Failed"
Feb 27 02:37:31.578: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:37:31.592
Feb 27 02:37:31.604: INFO: Waiting for pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 to disappear
Feb 27 02:37:31.607: INFO: Pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7022" for this suite. 02/27/23 02:37:31.612
------------------------------
• [SLOW TEST] [8.114 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:23.504
    Feb 27 02:37:23.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:37:23.504
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:23.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:23.527
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-485a760a-fea0-4402-aa5a-d24107a7c040 02/27/23 02:37:23.53
    STEP: Creating a pod to test consume configMaps 02/27/23 02:37:23.534
    Feb 27 02:37:23.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04" in namespace "projected-7022" to be "Succeeded or Failed"
    Feb 27 02:37:23.566: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160441ms
    Feb 27 02:37:25.570: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008173243s
    Feb 27 02:37:27.572: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010003575s
    Feb 27 02:37:29.570: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008525553s
    Feb 27 02:37:31.574: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011945098s
    STEP: Saw pod success 02/27/23 02:37:31.574
    Feb 27 02:37:31.574: INFO: Pod "pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04" satisfied condition "Succeeded or Failed"
    Feb 27 02:37:31.578: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:37:31.592
    Feb 27 02:37:31.604: INFO: Waiting for pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 to disappear
    Feb 27 02:37:31.607: INFO: Pod pod-projected-configmaps-002f1399-4ea4-4cda-8b77-8583abbf7d04 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7022" for this suite. 02/27/23 02:37:31.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:31.618
Feb 27 02:37:31.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:37:31.619
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:31.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:31.689
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:37:31.692
Feb 27 02:37:31.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b" in namespace "projected-4477" to be "Succeeded or Failed"
Feb 27 02:37:31.733: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059356ms
Feb 27 02:37:33.737: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008431671s
Feb 27 02:37:35.737: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008148429s
Feb 27 02:37:37.738: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009224385s
STEP: Saw pod success 02/27/23 02:37:37.738
Feb 27 02:37:37.738: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b" satisfied condition "Succeeded or Failed"
Feb 27 02:37:37.740: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b container client-container: <nil>
STEP: delete the pod 02/27/23 02:37:37.747
Feb 27 02:37:37.759: INFO: Waiting for pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b to disappear
Feb 27 02:37:37.762: INFO: Pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:37.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4477" for this suite. 02/27/23 02:37:37.767
------------------------------
• [SLOW TEST] [6.155 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:31.618
    Feb 27 02:37:31.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:37:31.619
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:31.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:31.689
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:37:31.692
    Feb 27 02:37:31.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b" in namespace "projected-4477" to be "Succeeded or Failed"
    Feb 27 02:37:31.733: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059356ms
    Feb 27 02:37:33.737: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008431671s
    Feb 27 02:37:35.737: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008148429s
    Feb 27 02:37:37.738: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009224385s
    STEP: Saw pod success 02/27/23 02:37:37.738
    Feb 27 02:37:37.738: INFO: Pod "downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b" satisfied condition "Succeeded or Failed"
    Feb 27 02:37:37.740: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b container client-container: <nil>
    STEP: delete the pod 02/27/23 02:37:37.747
    Feb 27 02:37:37.759: INFO: Waiting for pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b to disappear
    Feb 27 02:37:37.762: INFO: Pod downwardapi-volume-5a8886ac-9e82-444d-9df1-dfdb6c20c62b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:37.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4477" for this suite. 02/27/23 02:37:37.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:37.773
Feb 27 02:37:37.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 02:37:37.774
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:37.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:37.794
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 02:37:37.801
Feb 27 02:37:37.831: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5905" to be "running and ready"
Feb 27 02:37:37.834: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671024ms
Feb 27 02:37:37.834: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:37:39.838: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006581352s
Feb 27 02:37:39.838: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 02:37:39.838: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 02/27/23 02:37:39.841
Feb 27 02:37:39.850: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5905" to be "running and ready"
Feb 27 02:37:39.855: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002176ms
Feb 27 02:37:39.855: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:37:41.860: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009500107s
Feb 27 02:37:41.860: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Feb 27 02:37:41.860: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 02/27/23 02:37:41.863
Feb 27 02:37:41.871: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 02:37:41.874: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 02:37:43.875: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 02:37:43.881: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 27 02:37:45.875: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 27 02:37:45.879: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 02/27/23 02:37:45.879
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:45.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5905" for this suite. 02/27/23 02:37:45.899
------------------------------
• [SLOW TEST] [8.133 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:37.773
    Feb 27 02:37:37.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 02:37:37.774
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:37.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:37.794
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 02:37:37.801
    Feb 27 02:37:37.831: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5905" to be "running and ready"
    Feb 27 02:37:37.834: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671024ms
    Feb 27 02:37:37.834: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:37:39.838: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006581352s
    Feb 27 02:37:39.838: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 02:37:39.838: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 02/27/23 02:37:39.841
    Feb 27 02:37:39.850: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5905" to be "running and ready"
    Feb 27 02:37:39.855: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.002176ms
    Feb 27 02:37:39.855: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:37:41.860: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009500107s
    Feb 27 02:37:41.860: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Feb 27 02:37:41.860: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 02/27/23 02:37:41.863
    Feb 27 02:37:41.871: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 02:37:41.874: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 02:37:43.875: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 02:37:43.881: INFO: Pod pod-with-prestop-exec-hook still exists
    Feb 27 02:37:45.875: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Feb 27 02:37:45.879: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 02/27/23 02:37:45.879
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:45.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5905" for this suite. 02/27/23 02:37:45.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:45.909
Feb 27 02:37:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 02:37:45.91
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:45.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:45.937
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 02/27/23 02:37:45.944
STEP: Verify that the required pods have come up. 02/27/23 02:37:45.948
Feb 27 02:37:45.951: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 02:37:50.955: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 02:37:50.955
STEP: Getting /status 02/27/23 02:37:50.955
Feb 27 02:37:50.959: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 02/27/23 02:37:50.959
Feb 27 02:37:50.970: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 02/27/23 02:37:50.97
Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: ADDED
Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:50.972: INFO: Found replicaset test-rs in namespace replicaset-6054 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Feb 27 02:37:50.972: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 02/27/23 02:37:50.972
Feb 27 02:37:50.972: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 02:37:51.020: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 02/27/23 02:37:51.02
Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: ADDED
Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:51.022: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:51.022: INFO: Observed replicaset test-rs in namespace replicaset-6054 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 02:37:51.022: INFO: Observed &ReplicaSet event: MODIFIED
Feb 27 02:37:51.022: INFO: Found replicaset test-rs in namespace replicaset-6054 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 02:37:51.022: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:37:51.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6054" for this suite. 02/27/23 02:37:51.031
------------------------------
• [SLOW TEST] [5.130 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:45.909
    Feb 27 02:37:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 02:37:45.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:45.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:45.937
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 02/27/23 02:37:45.944
    STEP: Verify that the required pods have come up. 02/27/23 02:37:45.948
    Feb 27 02:37:45.951: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 02:37:50.955: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 02:37:50.955
    STEP: Getting /status 02/27/23 02:37:50.955
    Feb 27 02:37:50.959: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 02/27/23 02:37:50.959
    Feb 27 02:37:50.970: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 02/27/23 02:37:50.97
    Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:50.972: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:50.972: INFO: Found replicaset test-rs in namespace replicaset-6054 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Feb 27 02:37:50.972: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 02/27/23 02:37:50.972
    Feb 27 02:37:50.972: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 02:37:51.020: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 02/27/23 02:37:51.02
    Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: ADDED
    Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:51.021: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:51.022: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:51.022: INFO: Observed replicaset test-rs in namespace replicaset-6054 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 02:37:51.022: INFO: Observed &ReplicaSet event: MODIFIED
    Feb 27 02:37:51.022: INFO: Found replicaset test-rs in namespace replicaset-6054 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 02:37:51.022: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:37:51.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6054" for this suite. 02/27/23 02:37:51.031
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:37:51.04
Feb 27 02:37:51.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:37:51.04
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:51.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:51.066
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 02/27/23 02:37:51.069
Feb 27 02:37:51.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 create -f -'
Feb 27 02:37:51.813: INFO: stderr: ""
Feb 27 02:37:51.813: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:37:51.813
Feb 27 02:37:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:37:51.877: INFO: stderr: ""
Feb 27 02:37:51.877: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
Feb 27 02:37:51.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:37:51.942: INFO: stderr: ""
Feb 27 02:37:51.942: INFO: stdout: ""
Feb 27 02:37:51.942: INFO: update-demo-nautilus-t6cm5 is created but not running
Feb 27 02:37:56.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:37:57.008: INFO: stderr: ""
Feb 27 02:37:57.008: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
Feb 27 02:37:57.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:37:57.070: INFO: stderr: ""
Feb 27 02:37:57.070: INFO: stdout: "true"
Feb 27 02:37:57.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:37:57.132: INFO: stderr: ""
Feb 27 02:37:57.132: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:37:57.132: INFO: validating pod update-demo-nautilus-t6cm5
Feb 27 02:37:57.136: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:37:57.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:37:57.136: INFO: update-demo-nautilus-t6cm5 is verified up and running
Feb 27 02:37:57.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:37:57.195: INFO: stderr: ""
Feb 27 02:37:57.195: INFO: stdout: "true"
Feb 27 02:37:57.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:37:57.261: INFO: stderr: ""
Feb 27 02:37:57.261: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:37:57.261: INFO: validating pod update-demo-nautilus-wrmbw
Feb 27 02:37:57.266: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:37:57.266: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:37:57.266: INFO: update-demo-nautilus-wrmbw is verified up and running
STEP: scaling down the replication controller 02/27/23 02:37:57.266
Feb 27 02:37:57.267: INFO: scanned /root for discovery docs: <nil>
Feb 27 02:37:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Feb 27 02:37:58.354: INFO: stderr: ""
Feb 27 02:37:58.354: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:37:58.354
Feb 27 02:37:58.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:37:58.422: INFO: stderr: ""
Feb 27 02:37:58.423: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
STEP: Replicas for name=update-demo: expected=1 actual=2 02/27/23 02:37:58.423
Feb 27 02:38:03.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:38:03.490: INFO: stderr: ""
Feb 27 02:38:03.490: INFO: stdout: "update-demo-nautilus-wrmbw "
Feb 27 02:38:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:38:03.554: INFO: stderr: ""
Feb 27 02:38:03.554: INFO: stdout: "true"
Feb 27 02:38:03.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:38:03.617: INFO: stderr: ""
Feb 27 02:38:03.617: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:38:03.617: INFO: validating pod update-demo-nautilus-wrmbw
Feb 27 02:38:03.622: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:38:03.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:38:03.622: INFO: update-demo-nautilus-wrmbw is verified up and running
STEP: scaling up the replication controller 02/27/23 02:38:03.622
Feb 27 02:38:03.623: INFO: scanned /root for discovery docs: <nil>
Feb 27 02:38:03.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Feb 27 02:38:04.708: INFO: stderr: ""
Feb 27 02:38:04.708: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:38:04.708
Feb 27 02:38:04.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:38:04.780: INFO: stderr: ""
Feb 27 02:38:04.780: INFO: stdout: "update-demo-nautilus-hzw9k update-demo-nautilus-wrmbw "
Feb 27 02:38:04.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:38:04.840: INFO: stderr: ""
Feb 27 02:38:04.840: INFO: stdout: ""
Feb 27 02:38:04.840: INFO: update-demo-nautilus-hzw9k is created but not running
Feb 27 02:38:09.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Feb 27 02:38:09.906: INFO: stderr: ""
Feb 27 02:38:09.906: INFO: stdout: "update-demo-nautilus-hzw9k update-demo-nautilus-wrmbw "
Feb 27 02:38:09.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:38:09.971: INFO: stderr: ""
Feb 27 02:38:09.971: INFO: stdout: "true"
Feb 27 02:38:09.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:38:10.034: INFO: stderr: ""
Feb 27 02:38:10.034: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:38:10.034: INFO: validating pod update-demo-nautilus-hzw9k
Feb 27 02:38:10.040: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:38:10.040: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:38:10.040: INFO: update-demo-nautilus-hzw9k is verified up and running
Feb 27 02:38:10.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Feb 27 02:38:10.111: INFO: stderr: ""
Feb 27 02:38:10.111: INFO: stdout: "true"
Feb 27 02:38:10.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Feb 27 02:38:10.180: INFO: stderr: ""
Feb 27 02:38:10.180: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
Feb 27 02:38:10.180: INFO: validating pod update-demo-nautilus-wrmbw
Feb 27 02:38:10.184: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 27 02:38:10.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 27 02:38:10.184: INFO: update-demo-nautilus-wrmbw is verified up and running
STEP: using delete to clean up resources 02/27/23 02:38:10.184
Feb 27 02:38:10.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 delete --grace-period=0 --force -f -'
Feb 27 02:38:10.246: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 27 02:38:10.246: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 27 02:38:10.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get rc,svc -l name=update-demo --no-headers'
Feb 27 02:38:10.336: INFO: stderr: "No resources found in kubectl-3172 namespace.\n"
Feb 27 02:38:10.336: INFO: stdout: ""
Feb 27 02:38:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 27 02:38:10.418: INFO: stderr: ""
Feb 27 02:38:10.418: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:38:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3172" for this suite. 02/27/23 02:38:10.424
------------------------------
• [SLOW TEST] [19.393 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:37:51.04
    Feb 27 02:37:51.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:37:51.04
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:37:51.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:37:51.066
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 02/27/23 02:37:51.069
    Feb 27 02:37:51.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 create -f -'
    Feb 27 02:37:51.813: INFO: stderr: ""
    Feb 27 02:37:51.813: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:37:51.813
    Feb 27 02:37:51.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:37:51.877: INFO: stderr: ""
    Feb 27 02:37:51.877: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
    Feb 27 02:37:51.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:37:51.942: INFO: stderr: ""
    Feb 27 02:37:51.942: INFO: stdout: ""
    Feb 27 02:37:51.942: INFO: update-demo-nautilus-t6cm5 is created but not running
    Feb 27 02:37:56.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:37:57.008: INFO: stderr: ""
    Feb 27 02:37:57.008: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
    Feb 27 02:37:57.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:37:57.070: INFO: stderr: ""
    Feb 27 02:37:57.070: INFO: stdout: "true"
    Feb 27 02:37:57.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-t6cm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:37:57.132: INFO: stderr: ""
    Feb 27 02:37:57.132: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:37:57.132: INFO: validating pod update-demo-nautilus-t6cm5
    Feb 27 02:37:57.136: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:37:57.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:37:57.136: INFO: update-demo-nautilus-t6cm5 is verified up and running
    Feb 27 02:37:57.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:37:57.195: INFO: stderr: ""
    Feb 27 02:37:57.195: INFO: stdout: "true"
    Feb 27 02:37:57.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:37:57.261: INFO: stderr: ""
    Feb 27 02:37:57.261: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:37:57.261: INFO: validating pod update-demo-nautilus-wrmbw
    Feb 27 02:37:57.266: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:37:57.266: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:37:57.266: INFO: update-demo-nautilus-wrmbw is verified up and running
    STEP: scaling down the replication controller 02/27/23 02:37:57.266
    Feb 27 02:37:57.267: INFO: scanned /root for discovery docs: <nil>
    Feb 27 02:37:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Feb 27 02:37:58.354: INFO: stderr: ""
    Feb 27 02:37:58.354: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:37:58.354
    Feb 27 02:37:58.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:37:58.422: INFO: stderr: ""
    Feb 27 02:37:58.423: INFO: stdout: "update-demo-nautilus-t6cm5 update-demo-nautilus-wrmbw "
    STEP: Replicas for name=update-demo: expected=1 actual=2 02/27/23 02:37:58.423
    Feb 27 02:38:03.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:38:03.490: INFO: stderr: ""
    Feb 27 02:38:03.490: INFO: stdout: "update-demo-nautilus-wrmbw "
    Feb 27 02:38:03.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:38:03.554: INFO: stderr: ""
    Feb 27 02:38:03.554: INFO: stdout: "true"
    Feb 27 02:38:03.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:38:03.617: INFO: stderr: ""
    Feb 27 02:38:03.617: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:38:03.617: INFO: validating pod update-demo-nautilus-wrmbw
    Feb 27 02:38:03.622: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:38:03.622: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:38:03.622: INFO: update-demo-nautilus-wrmbw is verified up and running
    STEP: scaling up the replication controller 02/27/23 02:38:03.622
    Feb 27 02:38:03.623: INFO: scanned /root for discovery docs: <nil>
    Feb 27 02:38:03.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Feb 27 02:38:04.708: INFO: stderr: ""
    Feb 27 02:38:04.708: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 02/27/23 02:38:04.708
    Feb 27 02:38:04.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:38:04.780: INFO: stderr: ""
    Feb 27 02:38:04.780: INFO: stdout: "update-demo-nautilus-hzw9k update-demo-nautilus-wrmbw "
    Feb 27 02:38:04.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:38:04.840: INFO: stderr: ""
    Feb 27 02:38:04.840: INFO: stdout: ""
    Feb 27 02:38:04.840: INFO: update-demo-nautilus-hzw9k is created but not running
    Feb 27 02:38:09.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Feb 27 02:38:09.906: INFO: stderr: ""
    Feb 27 02:38:09.906: INFO: stdout: "update-demo-nautilus-hzw9k update-demo-nautilus-wrmbw "
    Feb 27 02:38:09.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:38:09.971: INFO: stderr: ""
    Feb 27 02:38:09.971: INFO: stdout: "true"
    Feb 27 02:38:09.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-hzw9k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:38:10.034: INFO: stderr: ""
    Feb 27 02:38:10.034: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:38:10.034: INFO: validating pod update-demo-nautilus-hzw9k
    Feb 27 02:38:10.040: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:38:10.040: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:38:10.040: INFO: update-demo-nautilus-hzw9k is verified up and running
    Feb 27 02:38:10.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Feb 27 02:38:10.111: INFO: stderr: ""
    Feb 27 02:38:10.111: INFO: stdout: "true"
    Feb 27 02:38:10.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods update-demo-nautilus-wrmbw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Feb 27 02:38:10.180: INFO: stderr: ""
    Feb 27 02:38:10.180: INFO: stdout: "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/nautilus:1.7"
    Feb 27 02:38:10.180: INFO: validating pod update-demo-nautilus-wrmbw
    Feb 27 02:38:10.184: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Feb 27 02:38:10.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Feb 27 02:38:10.184: INFO: update-demo-nautilus-wrmbw is verified up and running
    STEP: using delete to clean up resources 02/27/23 02:38:10.184
    Feb 27 02:38:10.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 delete --grace-period=0 --force -f -'
    Feb 27 02:38:10.246: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Feb 27 02:38:10.246: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Feb 27 02:38:10.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get rc,svc -l name=update-demo --no-headers'
    Feb 27 02:38:10.336: INFO: stderr: "No resources found in kubectl-3172 namespace.\n"
    Feb 27 02:38:10.336: INFO: stdout: ""
    Feb 27 02:38:10.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3172 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Feb 27 02:38:10.418: INFO: stderr: ""
    Feb 27 02:38:10.418: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:38:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3172" for this suite. 02/27/23 02:38:10.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:38:10.433
Feb 27 02:38:10.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:38:10.433
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:38:10.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:38:10.463
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 02:38:10.481: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 02:39:10.562: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:39:10.566
Feb 27 02:39:10.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 02:39:10.566
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:39:10.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:39:10.593
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 02/27/23 02:39:10.596
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 02:39:10.596
Feb 27 02:39:10.624: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1789" to be "running"
Feb 27 02:39:10.626: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.585977ms
Feb 27 02:39:12.632: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008047391s
Feb 27 02:39:12.632: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 02:39:12.636
Feb 27 02:39:12.650: INFO: found a healthy node: worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Feb 27 02:39:18.722: INFO: pods created so far: [1 1 1]
Feb 27 02:39:18.722: INFO: length of pods created so far: 3
Feb 27 02:39:20.769: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Feb 27 02:39:27.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:39:27.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1789" for this suite. 02/27/23 02:39:27.884
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-726" for this suite. 02/27/23 02:39:27.891
------------------------------
• [SLOW TEST] [77.462 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:38:10.433
    Feb 27 02:38:10.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:38:10.433
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:38:10.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:38:10.463
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 02:38:10.481: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 02:39:10.562: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:39:10.566
    Feb 27 02:39:10.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption-path 02/27/23 02:39:10.566
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:39:10.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:39:10.593
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 02/27/23 02:39:10.596
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 02:39:10.596
    Feb 27 02:39:10.624: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1789" to be "running"
    Feb 27 02:39:10.626: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.585977ms
    Feb 27 02:39:12.632: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008047391s
    Feb 27 02:39:12.632: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 02:39:12.636
    Feb 27 02:39:12.650: INFO: found a healthy node: worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Feb 27 02:39:18.722: INFO: pods created so far: [1 1 1]
    Feb 27 02:39:18.722: INFO: length of pods created so far: 3
    Feb 27 02:39:20.769: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:39:27.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:39:27.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1789" for this suite. 02/27/23 02:39:27.884
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-726" for this suite. 02/27/23 02:39:27.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:39:27.895
Feb 27 02:39:27.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 02:39:27.896
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:39:27.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:39:27.915
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 02/27/23 02:39:27.917
STEP: Ensuring active pods == parallelism 02/27/23 02:39:27.925
STEP: delete a job 02/27/23 02:39:29.931
STEP: deleting Job.batch foo in namespace job-237, will wait for the garbage collector to delete the pods 02/27/23 02:39:29.931
Feb 27 02:39:29.993: INFO: Deleting Job.batch foo took: 6.969601ms
Feb 27 02:39:30.094: INFO: Terminating Job.batch foo pods took: 100.663896ms
STEP: Ensuring job was deleted 02/27/23 02:40:02.894
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:02.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-237" for this suite. 02/27/23 02:40:02.902
------------------------------
• [SLOW TEST] [35.014 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:39:27.895
    Feb 27 02:39:27.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 02:39:27.896
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:39:27.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:39:27.915
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 02/27/23 02:39:27.917
    STEP: Ensuring active pods == parallelism 02/27/23 02:39:27.925
    STEP: delete a job 02/27/23 02:39:29.931
    STEP: deleting Job.batch foo in namespace job-237, will wait for the garbage collector to delete the pods 02/27/23 02:39:29.931
    Feb 27 02:39:29.993: INFO: Deleting Job.batch foo took: 6.969601ms
    Feb 27 02:39:30.094: INFO: Terminating Job.batch foo pods took: 100.663896ms
    STEP: Ensuring job was deleted 02/27/23 02:40:02.894
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:02.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-237" for this suite. 02/27/23 02:40:02.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:02.909
Feb 27 02:40:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 02:40:02.91
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:02.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:02.932
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 02/27/23 02:40:02.934
Feb 27 02:40:02.969: INFO: Waiting up to 5m0s for pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c" in namespace "var-expansion-5924" to be "Succeeded or Failed"
Feb 27 02:40:02.976: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.618794ms
Feb 27 02:40:04.980: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Running", Reason="", readiness=false. Elapsed: 2.01072306s
Feb 27 02:40:06.981: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992101s
STEP: Saw pod success 02/27/23 02:40:06.981
Feb 27 02:40:06.981: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c" satisfied condition "Succeeded or Failed"
Feb 27 02:40:06.985: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c container dapi-container: <nil>
STEP: delete the pod 02/27/23 02:40:06.996
Feb 27 02:40:07.008: INFO: Waiting for pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c to disappear
Feb 27 02:40:07.010: INFO: Pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5924" for this suite. 02/27/23 02:40:07.013
------------------------------
• [4.113 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:02.909
    Feb 27 02:40:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 02:40:02.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:02.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:02.932
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 02/27/23 02:40:02.934
    Feb 27 02:40:02.969: INFO: Waiting up to 5m0s for pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c" in namespace "var-expansion-5924" to be "Succeeded or Failed"
    Feb 27 02:40:02.976: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.618794ms
    Feb 27 02:40:04.980: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Running", Reason="", readiness=false. Elapsed: 2.01072306s
    Feb 27 02:40:06.981: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992101s
    STEP: Saw pod success 02/27/23 02:40:06.981
    Feb 27 02:40:06.981: INFO: Pod "var-expansion-953fa517-ba80-4286-be11-e107de9c339c" satisfied condition "Succeeded or Failed"
    Feb 27 02:40:06.985: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c container dapi-container: <nil>
    STEP: delete the pod 02/27/23 02:40:06.996
    Feb 27 02:40:07.008: INFO: Waiting for pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c to disappear
    Feb 27 02:40:07.010: INFO: Pod var-expansion-953fa517-ba80-4286-be11-e107de9c339c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5924" for this suite. 02/27/23 02:40:07.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:07.022
Feb 27 02:40:07.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:40:07.023
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:07.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:07.052
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-dc1670c6-d7af-44ec-ac21-bb72fe917d12 02/27/23 02:40:07.054
STEP: Creating a pod to test consume configMaps 02/27/23 02:40:07.059
Feb 27 02:40:07.069: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212" in namespace "projected-3941" to be "Succeeded or Failed"
Feb 27 02:40:07.072: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996851ms
Feb 27 02:40:09.077: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007748371s
Feb 27 02:40:11.078: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008222705s
STEP: Saw pod success 02/27/23 02:40:11.078
Feb 27 02:40:11.078: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212" satisfied condition "Succeeded or Failed"
Feb 27 02:40:11.082: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 container projected-configmap-volume-test: <nil>
STEP: delete the pod 02/27/23 02:40:11.089
Feb 27 02:40:11.111: INFO: Waiting for pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 to disappear
Feb 27 02:40:11.114: INFO: Pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3941" for this suite. 02/27/23 02:40:11.118
------------------------------
• [4.104 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:07.022
    Feb 27 02:40:07.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:40:07.023
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:07.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:07.052
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-dc1670c6-d7af-44ec-ac21-bb72fe917d12 02/27/23 02:40:07.054
    STEP: Creating a pod to test consume configMaps 02/27/23 02:40:07.059
    Feb 27 02:40:07.069: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212" in namespace "projected-3941" to be "Succeeded or Failed"
    Feb 27 02:40:07.072: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.996851ms
    Feb 27 02:40:09.077: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007748371s
    Feb 27 02:40:11.078: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008222705s
    STEP: Saw pod success 02/27/23 02:40:11.078
    Feb 27 02:40:11.078: INFO: Pod "pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212" satisfied condition "Succeeded or Failed"
    Feb 27 02:40:11.082: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:40:11.089
    Feb 27 02:40:11.111: INFO: Waiting for pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 to disappear
    Feb 27 02:40:11.114: INFO: Pod pod-projected-configmaps-dd82fc7e-2500-4bce-a673-09afcdda5212 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3941" for this suite. 02/27/23 02:40:11.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:11.127
Feb 27 02:40:11.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:40:11.127
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:11.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:11.153
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 02/27/23 02:40:11.156
Feb 27 02:40:11.156: INFO: namespace kubectl-380
Feb 27 02:40:11.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 create -f -'
Feb 27 02:40:11.860: INFO: stderr: ""
Feb 27 02:40:11.860: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 02:40:11.86
Feb 27 02:40:12.864: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:40:12.864: INFO: Found 1 / 1
Feb 27 02:40:12.864: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 02:40:12.866: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:40:12.866: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 02:40:12.866: INFO: wait on agnhost-primary startup in kubectl-380 
Feb 27 02:40:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 logs agnhost-primary-hv5wt agnhost-primary'
Feb 27 02:40:12.942: INFO: stderr: ""
Feb 27 02:40:12.942: INFO: stdout: "Paused\n"
STEP: exposing RC 02/27/23 02:40:12.942
Feb 27 02:40:12.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Feb 27 02:40:13.023: INFO: stderr: ""
Feb 27 02:40:13.023: INFO: stdout: "service/rm2 exposed\n"
Feb 27 02:40:13.031: INFO: Service rm2 in namespace kubectl-380 found.
STEP: exposing service 02/27/23 02:40:15.042
Feb 27 02:40:15.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Feb 27 02:40:15.124: INFO: stderr: ""
Feb 27 02:40:15.124: INFO: stdout: "service/rm3 exposed\n"
Feb 27 02:40:15.132: INFO: Service rm3 in namespace kubectl-380 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:17.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-380" for this suite. 02/27/23 02:40:17.143
------------------------------
• [SLOW TEST] [6.029 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:11.127
    Feb 27 02:40:11.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:40:11.127
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:11.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:11.153
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 02/27/23 02:40:11.156
    Feb 27 02:40:11.156: INFO: namespace kubectl-380
    Feb 27 02:40:11.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 create -f -'
    Feb 27 02:40:11.860: INFO: stderr: ""
    Feb 27 02:40:11.860: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 02:40:11.86
    Feb 27 02:40:12.864: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:40:12.864: INFO: Found 1 / 1
    Feb 27 02:40:12.864: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 02:40:12.866: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:40:12.866: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 02:40:12.866: INFO: wait on agnhost-primary startup in kubectl-380 
    Feb 27 02:40:12.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 logs agnhost-primary-hv5wt agnhost-primary'
    Feb 27 02:40:12.942: INFO: stderr: ""
    Feb 27 02:40:12.942: INFO: stdout: "Paused\n"
    STEP: exposing RC 02/27/23 02:40:12.942
    Feb 27 02:40:12.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Feb 27 02:40:13.023: INFO: stderr: ""
    Feb 27 02:40:13.023: INFO: stdout: "service/rm2 exposed\n"
    Feb 27 02:40:13.031: INFO: Service rm2 in namespace kubectl-380 found.
    STEP: exposing service 02/27/23 02:40:15.042
    Feb 27 02:40:15.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-380 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Feb 27 02:40:15.124: INFO: stderr: ""
    Feb 27 02:40:15.124: INFO: stdout: "service/rm3 exposed\n"
    Feb 27 02:40:15.132: INFO: Service rm3 in namespace kubectl-380 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:17.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-380" for this suite. 02/27/23 02:40:17.143
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:17.156
Feb 27 02:40:17.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:40:17.157
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:17.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:17.177
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8306 02/27/23 02:40:17.18
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 02/27/23 02:40:17.189
STEP: Creating pod with conflicting port in namespace statefulset-8306 02/27/23 02:40:17.199
W0227 02:40:17.230305      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until pod test-pod will start running in namespace statefulset-8306 02/27/23 02:40:17.23
Feb 27 02:40:17.230: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8306" to be "running"
Feb 27 02:40:17.236: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84752ms
Feb 27 02:40:19.239: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175908s
Feb 27 02:40:19.239: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8306 02/27/23 02:40:19.239
W0227 02:40:19.244952      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8306 02/27/23 02:40:19.245
Feb 27 02:40:19.283: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Pending. Waiting for statefulset controller to delete.
Feb 27 02:40:19.300: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 02:40:19.310: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Failed. Waiting for statefulset controller to delete.
Feb 27 02:40:19.316: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8306
STEP: Removing pod with conflicting port in namespace statefulset-8306 02/27/23 02:40:19.316
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8306 and will be in running state 02/27/23 02:40:19.334
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:40:21.341: INFO: Deleting all statefulset in ns statefulset-8306
Feb 27 02:40:21.344: INFO: Scaling statefulset ss to 0
W0227 02:40:21.355767      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
Feb 27 02:40:31.365: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:40:31.368: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:31.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8306" for this suite. 02/27/23 02:40:31.395
------------------------------
• [SLOW TEST] [14.248 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:17.156
    Feb 27 02:40:17.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:40:17.157
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:17.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:17.177
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8306 02/27/23 02:40:17.18
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 02/27/23 02:40:17.189
    STEP: Creating pod with conflicting port in namespace statefulset-8306 02/27/23 02:40:17.199
    W0227 02:40:17.230305      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8306 02/27/23 02:40:17.23
    Feb 27 02:40:17.230: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8306" to be "running"
    Feb 27 02:40:17.236: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84752ms
    Feb 27 02:40:19.239: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009175908s
    Feb 27 02:40:19.239: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8306 02/27/23 02:40:19.239
    W0227 02:40:19.244952      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8306 02/27/23 02:40:19.245
    Feb 27 02:40:19.283: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Pending. Waiting for statefulset controller to delete.
    Feb 27 02:40:19.300: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 02:40:19.310: INFO: Observed stateful pod in namespace: statefulset-8306, name: ss-0, uid: 8c1febe8-dbc8-4988-98bd-a047877f1e14, status phase: Failed. Waiting for statefulset controller to delete.
    Feb 27 02:40:19.316: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8306
    STEP: Removing pod with conflicting port in namespace statefulset-8306 02/27/23 02:40:19.316
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8306 and will be in running state 02/27/23 02:40:19.334
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:40:21.341: INFO: Deleting all statefulset in ns statefulset-8306
    Feb 27 02:40:21.344: INFO: Scaling statefulset ss to 0
    W0227 02:40:21.355767      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    Feb 27 02:40:31.365: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:40:31.368: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:31.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8306" for this suite. 02/27/23 02:40:31.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:31.404
Feb 27 02:40:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:40:31.405
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:31.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:31.427
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Feb 27 02:40:31.430: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 27 02:40:31.444: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 02:40:36.451: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 02:40:36.451
Feb 27 02:40:36.451: INFO: Creating deployment "test-rolling-update-deployment"
Feb 27 02:40:36.457: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 27 02:40:36.466: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 27 02:40:38.475: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 27 02:40:38.477: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:40:38.486: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8711  67860f97-6db7-4d83-9310-6b5c6bdc5742 50922 1 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0ebe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 02:40:36 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-ddd944b89" has successfully progressed.,LastUpdateTime:2023-02-27 02:40:37 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 02:40:38.488: INFO: New ReplicaSet "test-rolling-update-deployment-ddd944b89" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-ddd944b89  deployment-8711  acccffc7-cc6f-4809-8d7d-fb10e0089bd6 50912 1 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 67860f97-6db7-4d83-9310-6b5c6bdc5742 0xc004e0f0e0 0xc004e0f0e1}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67860f97-6db7-4d83-9310-6b5c6bdc5742\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: ddd944b89,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0f178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:40:38.489: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 27 02:40:38.489: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8711  267d781d-d975-4f8e-9bb0-8b2edf59a40a 50921 2 2023-02-27 02:40:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 67860f97-6db7-4d83-9310-6b5c6bdc5742 0xc004e0efb7 0xc004e0efb8}] [] [{e2e.test Update apps/v1 2023-02-27 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67860f97-6db7-4d83-9310-6b5c6bdc5742\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e0f078 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:40:38.492: INFO: Pod "test-rolling-update-deployment-ddd944b89-r9hsx" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-ddd944b89-r9hsx test-rolling-update-deployment-ddd944b89- deployment-8711  de7cb290-45fa-479f-be88-fe95fb2e4188 50911 0 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.173"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.173"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-ddd944b89 acccffc7-cc6f-4809-8d7d-fb10e0089bd6 0xc004e0f5d0 0xc004e0f5d1}] [] [{kube-controller-manager Update v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acccffc7-cc6f-4809-8d7d-fb10e0089bd6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9mfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9mfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.173,StartTime:2023-02-27 02:40:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:40:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://fe0986b95157cdf5f5aa5e3b68b55800f03071c970f083d9eb902e333b68f743,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8711" for this suite. 02/27/23 02:40:38.5
------------------------------
• [SLOW TEST] [7.105 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:31.404
    Feb 27 02:40:31.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:40:31.405
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:31.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:31.427
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Feb 27 02:40:31.430: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Feb 27 02:40:31.444: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 02:40:36.451: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 02:40:36.451
    Feb 27 02:40:36.451: INFO: Creating deployment "test-rolling-update-deployment"
    Feb 27 02:40:36.457: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Feb 27 02:40:36.466: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Feb 27 02:40:38.475: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Feb 27 02:40:38.477: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:40:38.486: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8711  67860f97-6db7-4d83-9310-6b5c6bdc5742 50922 1 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0ebe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 02:40:36 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-ddd944b89" has successfully progressed.,LastUpdateTime:2023-02-27 02:40:37 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 02:40:38.488: INFO: New ReplicaSet "test-rolling-update-deployment-ddd944b89" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-ddd944b89  deployment-8711  acccffc7-cc6f-4809-8d7d-fb10e0089bd6 50912 1 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 67860f97-6db7-4d83-9310-6b5c6bdc5742 0xc004e0f0e0 0xc004e0f0e1}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67860f97-6db7-4d83-9310-6b5c6bdc5742\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: ddd944b89,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0f178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:40:38.489: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Feb 27 02:40:38.489: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8711  267d781d-d975-4f8e-9bb0-8b2edf59a40a 50921 2 2023-02-27 02:40:31 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 67860f97-6db7-4d83-9310-6b5c6bdc5742 0xc004e0efb7 0xc004e0efb8}] [] [{e2e.test Update apps/v1 2023-02-27 02:40:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67860f97-6db7-4d83-9310-6b5c6bdc5742\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e0f078 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:40:38.492: INFO: Pod "test-rolling-update-deployment-ddd944b89-r9hsx" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-ddd944b89-r9hsx test-rolling-update-deployment-ddd944b89- deployment-8711  de7cb290-45fa-479f-be88-fe95fb2e4188 50911 0 2023-02-27 02:40:36 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:ddd944b89] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.173"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.173"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rolling-update-deployment-ddd944b89 acccffc7-cc6f-4809-8d7d-fb10e0089bd6 0xc004e0f5d0 0xc004e0f5d1}] [] [{kube-controller-manager Update v1 2023-02-27 02:40:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acccffc7-cc6f-4809-8d7d-fb10e0089bd6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:40:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9mfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9mfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:40:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.173,StartTime:2023-02-27 02:40:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:40:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://fe0986b95157cdf5f5aa5e3b68b55800f03071c970f083d9eb902e333b68f743,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:38.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8711" for this suite. 02/27/23 02:40:38.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:38.511
Feb 27 02:40:38.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:40:38.512
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.537
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 02/27/23 02:40:38.542
Feb 27 02:40:38.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-5736 api-versions'
Feb 27 02:40:38.616: INFO: stderr: ""
Feb 27 02:40:38.616: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:38.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5736" for this suite. 02/27/23 02:40:38.62
------------------------------
• [0.115 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:38.511
    Feb 27 02:40:38.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:40:38.512
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.537
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 02/27/23 02:40:38.542
    Feb 27 02:40:38.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-5736 api-versions'
    Feb 27 02:40:38.616: INFO: stderr: ""
    Feb 27 02:40:38.616: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nk8s.cni.cncf.io/v1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:38.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5736" for this suite. 02/27/23 02:40:38.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:38.628
Feb 27 02:40:38.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 02:40:38.629
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.653
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 02/27/23 02:40:38.657
STEP: submitting the pod to kubernetes 02/27/23 02:40:38.657
STEP: verifying QOS class is set on the pod 02/27/23 02:40:38.687
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2716" for this suite. 02/27/23 02:40:38.698
------------------------------
• [0.086 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:38.628
    Feb 27 02:40:38.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 02:40:38.629
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.653
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 02/27/23 02:40:38.657
    STEP: submitting the pod to kubernetes 02/27/23 02:40:38.657
    STEP: verifying QOS class is set on the pod 02/27/23 02:40:38.687
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:38.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2716" for this suite. 02/27/23 02:40:38.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:38.714
Feb 27 02:40:38.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:40:38.715
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.736
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-281 02/27/23 02:40:38.739
STEP: creating a selector 02/27/23 02:40:38.739
STEP: Creating the service pods in kubernetes 02/27/23 02:40:38.739
Feb 27 02:40:38.739: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 02:40:38.795: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-281" to be "running and ready"
Feb 27 02:40:38.811: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.28375ms
Feb 27 02:40:38.811: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:40:40.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020365479s
Feb 27 02:40:40.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:40:42.815: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019922134s
Feb 27 02:40:42.815: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:40:44.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018927857s
Feb 27 02:40:44.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:40:46.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018364859s
Feb 27 02:40:46.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:40:48.829: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033741174s
Feb 27 02:40:48.829: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:40:50.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.021090185s
Feb 27 02:40:50.817: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 02:40:50.817: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 02:40:50.821: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-281" to be "running and ready"
Feb 27 02:40:50.825: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.819507ms
Feb 27 02:40:50.825: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 02:40:50.825: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 02:40:50.828: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-281" to be "running and ready"
Feb 27 02:40:50.831: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.061437ms
Feb 27 02:40:50.831: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 02:40:50.831: INFO: Pod "netserver-2" satisfied condition "running and ready"
Feb 27 02:40:50.834: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-281" to be "running and ready"
Feb 27 02:40:50.838: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.362757ms
Feb 27 02:40:50.838: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Feb 27 02:40:50.838: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 02:40:50.841
Feb 27 02:40:50.880: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-281" to be "running"
Feb 27 02:40:50.889: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.756722ms
Feb 27 02:40:52.893: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013307437s
Feb 27 02:40:52.894: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 02:40:52.897: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 27 02:40:52.897: INFO: Breadth first check of 192.168.226.114 on host 10.0.10.15...
Feb 27 02:40:52.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.226.114&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:40:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:40:52.900: INFO: ExecWithOptions: Clientset creation
Feb 27 02:40:52.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.226.114%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:40:52.967: INFO: Waiting for responses: map[]
Feb 27 02:40:52.967: INFO: reached 192.168.226.114 after 0/1 tries
Feb 27 02:40:52.967: INFO: Breadth first check of 192.168.128.60 on host 10.0.10.22...
Feb 27 02:40:52.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.128.60&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:40:52.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:40:52.971: INFO: ExecWithOptions: Clientset creation
Feb 27 02:40:52.971: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.128.60%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:40:53.019: INFO: Waiting for responses: map[]
Feb 27 02:40:53.019: INFO: reached 192.168.128.60 after 0/1 tries
Feb 27 02:40:53.019: INFO: Breadth first check of 192.168.214.176 on host 10.0.10.3...
Feb 27 02:40:53.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.214.176&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:40:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:40:53.023: INFO: ExecWithOptions: Clientset creation
Feb 27 02:40:53.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.214.176%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:40:53.085: INFO: Waiting for responses: map[]
Feb 27 02:40:53.085: INFO: reached 192.168.214.176 after 0/1 tries
Feb 27 02:40:53.085: INFO: Breadth first check of 192.168.21.142 on host 10.0.10.5...
Feb 27 02:40:53.089: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.21.142&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:40:53.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:40:53.090: INFO: ExecWithOptions: Clientset creation
Feb 27 02:40:53.090: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.21.142%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Feb 27 02:40:53.131: INFO: Waiting for responses: map[]
Feb 27 02:40:53.131: INFO: reached 192.168.21.142 after 0/1 tries
Feb 27 02:40:53.131: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:53.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-281" for this suite. 02/27/23 02:40:53.137
------------------------------
• [SLOW TEST] [14.429 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:38.714
    Feb 27 02:40:38.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:40:38.715
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:38.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:38.736
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-281 02/27/23 02:40:38.739
    STEP: creating a selector 02/27/23 02:40:38.739
    STEP: Creating the service pods in kubernetes 02/27/23 02:40:38.739
    Feb 27 02:40:38.739: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 02:40:38.795: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-281" to be "running and ready"
    Feb 27 02:40:38.811: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.28375ms
    Feb 27 02:40:38.811: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:40:40.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.020365479s
    Feb 27 02:40:40.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:40:42.815: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019922134s
    Feb 27 02:40:42.815: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:40:44.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018927857s
    Feb 27 02:40:44.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:40:46.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018364859s
    Feb 27 02:40:46.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:40:48.829: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033741174s
    Feb 27 02:40:48.829: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:40:50.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.021090185s
    Feb 27 02:40:50.817: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 02:40:50.817: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 02:40:50.821: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-281" to be "running and ready"
    Feb 27 02:40:50.825: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.819507ms
    Feb 27 02:40:50.825: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 02:40:50.825: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 02:40:50.828: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-281" to be "running and ready"
    Feb 27 02:40:50.831: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.061437ms
    Feb 27 02:40:50.831: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 02:40:50.831: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Feb 27 02:40:50.834: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-281" to be "running and ready"
    Feb 27 02:40:50.838: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 4.362757ms
    Feb 27 02:40:50.838: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Feb 27 02:40:50.838: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 02:40:50.841
    Feb 27 02:40:50.880: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-281" to be "running"
    Feb 27 02:40:50.889: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.756722ms
    Feb 27 02:40:52.893: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013307437s
    Feb 27 02:40:52.894: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 02:40:52.897: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Feb 27 02:40:52.897: INFO: Breadth first check of 192.168.226.114 on host 10.0.10.15...
    Feb 27 02:40:52.899: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.226.114&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:40:52.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:40:52.900: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:40:52.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.226.114%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:40:52.967: INFO: Waiting for responses: map[]
    Feb 27 02:40:52.967: INFO: reached 192.168.226.114 after 0/1 tries
    Feb 27 02:40:52.967: INFO: Breadth first check of 192.168.128.60 on host 10.0.10.22...
    Feb 27 02:40:52.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.128.60&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:40:52.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:40:52.971: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:40:52.971: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.128.60%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:40:53.019: INFO: Waiting for responses: map[]
    Feb 27 02:40:53.019: INFO: reached 192.168.128.60 after 0/1 tries
    Feb 27 02:40:53.019: INFO: Breadth first check of 192.168.214.176 on host 10.0.10.3...
    Feb 27 02:40:53.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.214.176&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:40:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:40:53.023: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:40:53.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.214.176%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:40:53.085: INFO: Waiting for responses: map[]
    Feb 27 02:40:53.085: INFO: reached 192.168.214.176 after 0/1 tries
    Feb 27 02:40:53.085: INFO: Breadth first check of 192.168.21.142 on host 10.0.10.5...
    Feb 27 02:40:53.089: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.214.177:9080/dial?request=hostname&protocol=udp&host=192.168.21.142&port=8081&tries=1'] Namespace:pod-network-test-281 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:40:53.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:40:53.090: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:40:53.090: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-281/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.214.177%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.21.142%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Feb 27 02:40:53.131: INFO: Waiting for responses: map[]
    Feb 27 02:40:53.131: INFO: reached 192.168.21.142 after 0/1 tries
    Feb 27 02:40:53.131: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:53.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-281" for this suite. 02/27/23 02:40:53.137
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:53.143
Feb 27 02:40:53.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:40:53.144
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:53.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:53.174
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-61616e27-3fd9-4d8b-b744-4ed85082babe 02/27/23 02:40:53.176
STEP: Creating a pod to test consume secrets 02/27/23 02:40:53.191
Feb 27 02:40:53.206: INFO: Waiting up to 5m0s for pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516" in namespace "secrets-2179" to be "Succeeded or Failed"
Feb 27 02:40:53.210: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201516ms
Feb 27 02:40:55.215: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008448482s
Feb 27 02:40:57.216: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009379593s
STEP: Saw pod success 02/27/23 02:40:57.216
Feb 27 02:40:57.216: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516" satisfied condition "Succeeded or Failed"
Feb 27 02:40:57.219: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:40:57.225
Feb 27 02:40:57.242: INFO: Waiting for pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 to disappear
Feb 27 02:40:57.245: INFO: Pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:40:57.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2179" for this suite. 02/27/23 02:40:57.249
------------------------------
• [4.111 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:53.143
    Feb 27 02:40:53.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:40:53.144
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:53.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:53.174
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-61616e27-3fd9-4d8b-b744-4ed85082babe 02/27/23 02:40:53.176
    STEP: Creating a pod to test consume secrets 02/27/23 02:40:53.191
    Feb 27 02:40:53.206: INFO: Waiting up to 5m0s for pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516" in namespace "secrets-2179" to be "Succeeded or Failed"
    Feb 27 02:40:53.210: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Pending", Reason="", readiness=false. Elapsed: 4.201516ms
    Feb 27 02:40:55.215: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008448482s
    Feb 27 02:40:57.216: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009379593s
    STEP: Saw pod success 02/27/23 02:40:57.216
    Feb 27 02:40:57.216: INFO: Pod "pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516" satisfied condition "Succeeded or Failed"
    Feb 27 02:40:57.219: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:40:57.225
    Feb 27 02:40:57.242: INFO: Waiting for pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 to disappear
    Feb 27 02:40:57.245: INFO: Pod pod-secrets-6c0b8b53-c0cf-4e8e-83c7-9fb11bea1516 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:40:57.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2179" for this suite. 02/27/23 02:40:57.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:40:57.255
Feb 27 02:40:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:40:57.256
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:57.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:57.282
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 02:40:57.285
Feb 27 02:40:57.337: INFO: Waiting up to 5m0s for pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0" in namespace "emptydir-3063" to be "Succeeded or Failed"
Feb 27 02:40:57.342: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234448ms
Feb 27 02:40:59.347: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009761209s
Feb 27 02:41:01.345: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008417619s
STEP: Saw pod success 02/27/23 02:41:01.345
Feb 27 02:41:01.345: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0" satisfied condition "Succeeded or Failed"
Feb 27 02:41:01.349: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 container test-container: <nil>
STEP: delete the pod 02/27/23 02:41:01.353
Feb 27 02:41:01.367: INFO: Waiting for pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 to disappear
Feb 27 02:41:01.370: INFO: Pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3063" for this suite. 02/27/23 02:41:01.376
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:40:57.255
    Feb 27 02:40:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:40:57.256
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:40:57.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:40:57.282
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 02:40:57.285
    Feb 27 02:40:57.337: INFO: Waiting up to 5m0s for pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0" in namespace "emptydir-3063" to be "Succeeded or Failed"
    Feb 27 02:40:57.342: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.234448ms
    Feb 27 02:40:59.347: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009761209s
    Feb 27 02:41:01.345: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008417619s
    STEP: Saw pod success 02/27/23 02:41:01.345
    Feb 27 02:41:01.345: INFO: Pod "pod-207a308f-2181-406b-b1f5-10d8175bd6f0" satisfied condition "Succeeded or Failed"
    Feb 27 02:41:01.349: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:41:01.353
    Feb 27 02:41:01.367: INFO: Waiting for pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 to disappear
    Feb 27 02:41:01.370: INFO: Pod pod-207a308f-2181-406b-b1f5-10d8175bd6f0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:01.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3063" for this suite. 02/27/23 02:41:01.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:01.383
Feb 27 02:41:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:41:01.384
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:01.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:01.406
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:41:01.414
Feb 27 02:41:01.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f" in namespace "projected-9701" to be "Succeeded or Failed"
Feb 27 02:41:01.435: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.416151ms
Feb 27 02:41:03.439: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01071576s
Feb 27 02:41:05.441: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0121591s
STEP: Saw pod success 02/27/23 02:41:05.441
Feb 27 02:41:05.441: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f" satisfied condition "Succeeded or Failed"
Feb 27 02:41:05.444: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f container client-container: <nil>
STEP: delete the pod 02/27/23 02:41:05.45
Feb 27 02:41:05.464: INFO: Waiting for pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f to disappear
Feb 27 02:41:05.467: INFO: Pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9701" for this suite. 02/27/23 02:41:05.471
------------------------------
• [4.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:01.383
    Feb 27 02:41:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:41:01.384
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:01.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:01.406
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:41:01.414
    Feb 27 02:41:01.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f" in namespace "projected-9701" to be "Succeeded or Failed"
    Feb 27 02:41:01.435: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.416151ms
    Feb 27 02:41:03.439: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01071576s
    Feb 27 02:41:05.441: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0121591s
    STEP: Saw pod success 02/27/23 02:41:05.441
    Feb 27 02:41:05.441: INFO: Pod "downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f" satisfied condition "Succeeded or Failed"
    Feb 27 02:41:05.444: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f container client-container: <nil>
    STEP: delete the pod 02/27/23 02:41:05.45
    Feb 27 02:41:05.464: INFO: Waiting for pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f to disappear
    Feb 27 02:41:05.467: INFO: Pod downwardapi-volume-52343194-41d6-4935-be94-f5140d60dd1f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:05.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9701" for this suite. 02/27/23 02:41:05.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:05.479
Feb 27 02:41:05.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:41:05.48
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:05.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:05.503
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 02:41:05.505
Feb 27 02:41:05.516: INFO: Waiting up to 5m0s for pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8" in namespace "emptydir-630" to be "Succeeded or Failed"
Feb 27 02:41:05.521: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658767ms
Feb 27 02:41:07.525: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009051745s
Feb 27 02:41:09.524: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008137632s
STEP: Saw pod success 02/27/23 02:41:09.524
Feb 27 02:41:09.525: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8" satisfied condition "Succeeded or Failed"
Feb 27 02:41:09.527: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 container test-container: <nil>
STEP: delete the pod 02/27/23 02:41:09.532
Feb 27 02:41:09.547: INFO: Waiting for pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 to disappear
Feb 27 02:41:09.549: INFO: Pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:09.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-630" for this suite. 02/27/23 02:41:09.553
------------------------------
• [4.080 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:05.479
    Feb 27 02:41:05.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:41:05.48
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:05.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:05.503
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 02/27/23 02:41:05.505
    Feb 27 02:41:05.516: INFO: Waiting up to 5m0s for pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8" in namespace "emptydir-630" to be "Succeeded or Failed"
    Feb 27 02:41:05.521: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658767ms
    Feb 27 02:41:07.525: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009051745s
    Feb 27 02:41:09.524: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008137632s
    STEP: Saw pod success 02/27/23 02:41:09.524
    Feb 27 02:41:09.525: INFO: Pod "pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8" satisfied condition "Succeeded or Failed"
    Feb 27 02:41:09.527: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:41:09.532
    Feb 27 02:41:09.547: INFO: Waiting for pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 to disappear
    Feb 27 02:41:09.549: INFO: Pod pod-28646396-1ce1-4502-8cd5-d6dcbfbc75c8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:09.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-630" for this suite. 02/27/23 02:41:09.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:09.561
Feb 27 02:41:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:41:09.561
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:09.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:09.587
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-26c80258-3b56-456c-b0ee-777b35eeb3c8 02/27/23 02:41:09.589
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:09.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9980" for this suite. 02/27/23 02:41:09.595
------------------------------
• [0.041 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:09.561
    Feb 27 02:41:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:41:09.561
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:09.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:09.587
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-26c80258-3b56-456c-b0ee-777b35eeb3c8 02/27/23 02:41:09.589
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:09.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9980" for this suite. 02/27/23 02:41:09.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:09.602
Feb 27 02:41:09.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:41:09.603
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:09.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:09.629
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Feb 27 02:41:09.643: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369" in namespace "kubelet-test-2386" to be "running and ready"
Feb 27 02:41:09.649: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369": Phase="Pending", Reason="", readiness=false. Elapsed: 5.95224ms
Feb 27 02:41:09.649: INFO: The phase of Pod busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:11.653: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883066s
Feb 27 02:41:11.653: INFO: The phase of Pod busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369 is Running (Ready = true)
Feb 27 02:41:11.653: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:11.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2386" for this suite. 02/27/23 02:41:11.666
------------------------------
• [2.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:09.602
    Feb 27 02:41:09.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 02:41:09.603
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:09.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:09.629
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Feb 27 02:41:09.643: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369" in namespace "kubelet-test-2386" to be "running and ready"
    Feb 27 02:41:09.649: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369": Phase="Pending", Reason="", readiness=false. Elapsed: 5.95224ms
    Feb 27 02:41:09.649: INFO: The phase of Pod busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:11.653: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883066s
    Feb 27 02:41:11.653: INFO: The phase of Pod busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369 is Running (Ready = true)
    Feb 27 02:41:11.653: INFO: Pod "busybox-readonly-fs905495a7-22f9-4666-a2ba-46a69279c369" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:11.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2386" for this suite. 02/27/23 02:41:11.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:11.674
Feb 27 02:41:11.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename init-container 02/27/23 02:41:11.674
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:11.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:11.693
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 02/27/23 02:41:11.696
Feb 27 02:41:11.696: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:15.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1188" for this suite. 02/27/23 02:41:15.972
------------------------------
• [4.304 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:11.674
    Feb 27 02:41:11.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename init-container 02/27/23 02:41:11.674
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:11.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:11.693
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 02/27/23 02:41:11.696
    Feb 27 02:41:11.696: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:15.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1188" for this suite. 02/27/23 02:41:15.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:15.978
Feb 27 02:41:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 02:41:15.979
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:15.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:16.001
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 02:41:16.003
Feb 27 02:41:16.012: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1160" to be "running and ready"
Feb 27 02:41:16.015: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550253ms
Feb 27 02:41:16.015: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:18.020: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007368341s
Feb 27 02:41:18.020: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Feb 27 02:41:18.020: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 02/27/23 02:41:18.023
STEP: Then the orphan pod is adopted 02/27/23 02:41:18.029
STEP: When the matched label of one of its pods change 02/27/23 02:41:19.047
Feb 27 02:41:19.052: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 02/27/23 02:41:19.065
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1160" for this suite. 02/27/23 02:41:20.084
------------------------------
• [4.113 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:15.978
    Feb 27 02:41:15.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 02:41:15.979
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:15.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:16.001
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 02/27/23 02:41:16.003
    Feb 27 02:41:16.012: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1160" to be "running and ready"
    Feb 27 02:41:16.015: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.550253ms
    Feb 27 02:41:16.015: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:18.020: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.007368341s
    Feb 27 02:41:18.020: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Feb 27 02:41:18.020: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 02/27/23 02:41:18.023
    STEP: Then the orphan pod is adopted 02/27/23 02:41:18.029
    STEP: When the matched label of one of its pods change 02/27/23 02:41:19.047
    Feb 27 02:41:19.052: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 02/27/23 02:41:19.065
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:20.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1160" for this suite. 02/27/23 02:41:20.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:20.091
Feb 27 02:41:20.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename hostport 02/27/23 02:41:20.092
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:20.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:20.122
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 02:41:20.129
W0227 02:41:20.139963      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Feb 27 02:41:20.140: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4900" to be "running and ready"
Feb 27 02:41:20.142: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552247ms
Feb 27 02:41:20.142: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:22.146: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607354s
Feb 27 02:41:22.146: INFO: The phase of Pod pod1 is Running (Ready = true)
Feb 27 02:41:22.146: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.3 on the node which pod1 resides and expect scheduled 02/27/23 02:41:22.146
W0227 02:41:22.155411      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Feb 27 02:41:22.155: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4900" to be "running and ready"
Feb 27 02:41:22.158: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733098ms
Feb 27 02:41:22.158: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:24.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007219506s
Feb 27 02:41:24.162: INFO: The phase of Pod pod2 is Running (Ready = true)
Feb 27 02:41:24.162: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.3 but use UDP protocol on the node which pod2 resides 02/27/23 02:41:24.162
W0227 02:41:24.176712      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Feb 27 02:41:24.176: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4900" to be "running and ready"
Feb 27 02:41:24.180: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535432ms
Feb 27 02:41:24.180: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:26.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.007919065s
Feb 27 02:41:26.184: INFO: The phase of Pod pod3 is Running (Ready = false)
Feb 27 02:41:28.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.007262146s
Feb 27 02:41:28.184: INFO: The phase of Pod pod3 is Running (Ready = true)
Feb 27 02:41:28.184: INFO: Pod "pod3" satisfied condition "running and ready"
W0227 02:41:28.193995      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Feb 27 02:41:28.194: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4900" to be "running and ready"
Feb 27 02:41:28.199: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.290839ms
Feb 27 02:41:28.199: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:41:30.203: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910095s
Feb 27 02:41:30.204: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Feb 27 02:41:30.204: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 02:41:30.207
Feb 27 02:41:30.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.3 http://127.0.0.1:54323/hostname] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:41:30.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:41:30.208: INFO: ExecWithOptions: Clientset creation
Feb 27 02:41:30.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.3, port: 54323 02/27/23 02:41:30.269
Feb 27 02:41:30.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.3:54323/hostname] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:41:30.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:41:30.270: INFO: ExecWithOptions: Clientset creation
Feb 27 02:41:30.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.3, port: 54323 UDP 02/27/23 02:41:30.312
Feb 27 02:41:30.312: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.3 54323] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:41:30.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:41:30.312: INFO: ExecWithOptions: Clientset creation
Feb 27 02:41:30.312: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Feb 27 02:41:35.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-4900" for this suite. 02/27/23 02:41:35.399
------------------------------
• [SLOW TEST] [15.315 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:20.091
    Feb 27 02:41:20.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename hostport 02/27/23 02:41:20.092
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:20.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:20.122
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 02/27/23 02:41:20.129
    W0227 02:41:20.139963      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Feb 27 02:41:20.140: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4900" to be "running and ready"
    Feb 27 02:41:20.142: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552247ms
    Feb 27 02:41:20.142: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:22.146: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006607354s
    Feb 27 02:41:22.146: INFO: The phase of Pod pod1 is Running (Ready = true)
    Feb 27 02:41:22.146: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.10.3 on the node which pod1 resides and expect scheduled 02/27/23 02:41:22.146
    W0227 02:41:22.155411      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Feb 27 02:41:22.155: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4900" to be "running and ready"
    Feb 27 02:41:22.158: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.733098ms
    Feb 27 02:41:22.158: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:24.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007219506s
    Feb 27 02:41:24.162: INFO: The phase of Pod pod2 is Running (Ready = true)
    Feb 27 02:41:24.162: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.10.3 but use UDP protocol on the node which pod2 resides 02/27/23 02:41:24.162
    W0227 02:41:24.176712      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Feb 27 02:41:24.176: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4900" to be "running and ready"
    Feb 27 02:41:24.180: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535432ms
    Feb 27 02:41:24.180: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:26.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.007919065s
    Feb 27 02:41:26.184: INFO: The phase of Pod pod3 is Running (Ready = false)
    Feb 27 02:41:28.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.007262146s
    Feb 27 02:41:28.184: INFO: The phase of Pod pod3 is Running (Ready = true)
    Feb 27 02:41:28.184: INFO: Pod "pod3" satisfied condition "running and ready"
    W0227 02:41:28.193995      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Feb 27 02:41:28.194: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4900" to be "running and ready"
    Feb 27 02:41:28.199: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.290839ms
    Feb 27 02:41:28.199: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:41:30.203: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910095s
    Feb 27 02:41:30.204: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Feb 27 02:41:30.204: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 02/27/23 02:41:30.207
    Feb 27 02:41:30.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.10.3 http://127.0.0.1:54323/hostname] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:41:30.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:41:30.208: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:41:30.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.10.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.3, port: 54323 02/27/23 02:41:30.269
    Feb 27 02:41:30.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.10.3:54323/hostname] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:41:30.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:41:30.270: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:41:30.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.10.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.10.3, port: 54323 UDP 02/27/23 02:41:30.312
    Feb 27 02:41:30.312: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.10.3 54323] Namespace:hostport-4900 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:41:30.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:41:30.312: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:41:30.312: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-4900/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.10.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:41:35.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-4900" for this suite. 02/27/23 02:41:35.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:41:35.407
Feb 27 02:41:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:41:35.407
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:35.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:35.429
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5681 02/27/23 02:41:35.431
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 02/27/23 02:41:35.448
Feb 27 02:41:35.460: INFO: Found 0 stateful pods, waiting for 3
Feb 27 02:41:45.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:41:45.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:41:45.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:41:45.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:41:45.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:41:45.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:41:45.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-4 02/27/23 02:41:55.632
Feb 27 02:41:55.655: INFO: Updating stateful set ss2
STEP: Creating a new revision 02/27/23 02:41:55.655
STEP: Updating Pods in reverse ordinal order 02/27/23 02:42:05.673
Feb 27 02:42:05.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:42:05.802: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:42:05.802: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:42:05.802: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 02/27/23 02:42:15.82
Feb 27 02:42:15.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:42:16.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:42:16.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:42:16.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 02:42:26.039: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 02/27/23 02:42:36.056
Feb 27 02:42:36.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:42:36.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:42:36.204: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:42:36.204: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:42:46.225: INFO: Deleting all statefulset in ns statefulset-5681
Feb 27 02:42:46.229: INFO: Scaling statefulset ss2 to 0
Feb 27 02:42:56.266: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:42:56.281: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:42:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5681" for this suite. 02/27/23 02:42:56.298
------------------------------
• [SLOW TEST] [80.900 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:41:35.407
    Feb 27 02:41:35.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:41:35.407
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:41:35.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:41:35.429
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5681 02/27/23 02:41:35.431
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 02/27/23 02:41:35.448
    Feb 27 02:41:35.460: INFO: Found 0 stateful pods, waiting for 3
    Feb 27 02:41:45.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:41:45.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:41:45.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:41:45.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:41:45.611: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:41:45.611: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:41:45.611: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 to armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.39-4 02/27/23 02:41:55.632
    Feb 27 02:41:55.655: INFO: Updating stateful set ss2
    STEP: Creating a new revision 02/27/23 02:41:55.655
    STEP: Updating Pods in reverse ordinal order 02/27/23 02:42:05.673
    Feb 27 02:42:05.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:42:05.802: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:42:05.802: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:42:05.802: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 02/27/23 02:42:15.82
    Feb 27 02:42:15.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:42:16.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:42:16.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:42:16.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 02:42:26.039: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 02/27/23 02:42:36.056
    Feb 27 02:42:36.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-5681 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:42:36.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:42:36.204: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:42:36.204: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:42:46.225: INFO: Deleting all statefulset in ns statefulset-5681
    Feb 27 02:42:46.229: INFO: Scaling statefulset ss2 to 0
    Feb 27 02:42:56.266: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:42:56.281: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:42:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5681" for this suite. 02/27/23 02:42:56.298
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:42:56.307
Feb 27 02:42:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename endpointslice 02/27/23 02:42:56.308
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:56.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:56.328
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 02:42:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9623" for this suite. 02/27/23 02:42:58.413
------------------------------
• [2.114 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:42:56.307
    Feb 27 02:42:56.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename endpointslice 02/27/23 02:42:56.308
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:56.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:56.328
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:42:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9623" for this suite. 02/27/23 02:42:58.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:42:58.421
Feb 27 02:42:58.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 02:42:58.422
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:58.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:58.453
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Feb 27 02:42:58.499: INFO: created pod pod-service-account-defaultsa
Feb 27 02:42:58.499: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 27 02:42:58.510: INFO: created pod pod-service-account-mountsa
Feb 27 02:42:58.510: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 27 02:42:58.519: INFO: created pod pod-service-account-nomountsa
Feb 27 02:42:58.519: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 27 02:42:58.539: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 27 02:42:58.539: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 27 02:42:58.559: INFO: created pod pod-service-account-mountsa-mountspec
Feb 27 02:42:58.559: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 27 02:42:58.569: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 27 02:42:58.569: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 27 02:42:58.578: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 27 02:42:58.578: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 27 02:42:58.589: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 27 02:42:58.589: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 27 02:42:58.595: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 27 02:42:58.595: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 02:42:58.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3507" for this suite. 02/27/23 02:42:58.604
------------------------------
• [0.193 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:42:58.421
    Feb 27 02:42:58.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 02:42:58.422
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:58.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:58.453
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Feb 27 02:42:58.499: INFO: created pod pod-service-account-defaultsa
    Feb 27 02:42:58.499: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Feb 27 02:42:58.510: INFO: created pod pod-service-account-mountsa
    Feb 27 02:42:58.510: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Feb 27 02:42:58.519: INFO: created pod pod-service-account-nomountsa
    Feb 27 02:42:58.519: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Feb 27 02:42:58.539: INFO: created pod pod-service-account-defaultsa-mountspec
    Feb 27 02:42:58.539: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Feb 27 02:42:58.559: INFO: created pod pod-service-account-mountsa-mountspec
    Feb 27 02:42:58.559: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Feb 27 02:42:58.569: INFO: created pod pod-service-account-nomountsa-mountspec
    Feb 27 02:42:58.569: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Feb 27 02:42:58.578: INFO: created pod pod-service-account-defaultsa-nomountspec
    Feb 27 02:42:58.578: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Feb 27 02:42:58.589: INFO: created pod pod-service-account-mountsa-nomountspec
    Feb 27 02:42:58.589: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Feb 27 02:42:58.595: INFO: created pod pod-service-account-nomountsa-nomountspec
    Feb 27 02:42:58.595: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:42:58.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3507" for this suite. 02/27/23 02:42:58.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:42:58.616
Feb 27 02:42:58.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:42:58.617
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:58.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:58.641
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-152e9cf7-4494-4efc-8d66-b3356ddd3aa3 02/27/23 02:42:58.643
STEP: Creating a pod to test consume secrets 02/27/23 02:42:58.654
Feb 27 02:42:58.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4" in namespace "projected-2376" to be "Succeeded or Failed"
Feb 27 02:42:58.676: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473422ms
Feb 27 02:43:00.680: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711069s
Feb 27 02:43:02.680: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006554689s
Feb 27 02:43:04.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007310775s
Feb 27 02:43:06.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007197588s
Feb 27 02:43:08.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.007404544s
STEP: Saw pod success 02/27/23 02:43:08.681
Feb 27 02:43:08.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4" satisfied condition "Succeeded or Failed"
Feb 27 02:43:08.685: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:43:08.693
Feb 27 02:43:08.709: INFO: Waiting for pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 to disappear
Feb 27 02:43:08.712: INFO: Pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 02:43:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2376" for this suite. 02/27/23 02:43:08.716
------------------------------
• [SLOW TEST] [10.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:42:58.616
    Feb 27 02:42:58.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:42:58.617
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:42:58.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:42:58.641
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-152e9cf7-4494-4efc-8d66-b3356ddd3aa3 02/27/23 02:42:58.643
    STEP: Creating a pod to test consume secrets 02/27/23 02:42:58.654
    Feb 27 02:42:58.674: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4" in namespace "projected-2376" to be "Succeeded or Failed"
    Feb 27 02:42:58.676: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473422ms
    Feb 27 02:43:00.680: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711069s
    Feb 27 02:43:02.680: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006554689s
    Feb 27 02:43:04.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007310775s
    Feb 27 02:43:06.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007197588s
    Feb 27 02:43:08.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.007404544s
    STEP: Saw pod success 02/27/23 02:43:08.681
    Feb 27 02:43:08.681: INFO: Pod "pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4" satisfied condition "Succeeded or Failed"
    Feb 27 02:43:08.685: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:43:08.693
    Feb 27 02:43:08.709: INFO: Waiting for pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 to disappear
    Feb 27 02:43:08.712: INFO: Pod pod-projected-secrets-d249eaf9-862e-438f-a619-c64d5e09d6d4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:43:08.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2376" for this suite. 02/27/23 02:43:08.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:43:08.723
Feb 27 02:43:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:43:08.724
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:08.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:08.751
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-bb57d391-d60f-49e2-8d39-7091f9ceeca6 02/27/23 02:43:08.764
STEP: Creating the pod 02/27/23 02:43:08.769
Feb 27 02:43:08.811: INFO: Waiting up to 5m0s for pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2" in namespace "configmap-3630" to be "running"
Feb 27 02:43:08.815: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734254ms
Feb 27 02:43:10.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009026004s
Feb 27 02:43:12.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008794652s
Feb 27 02:43:14.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Running", Reason="", readiness=false. Elapsed: 6.008678757s
Feb 27 02:43:14.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2" satisfied condition "running"
STEP: Waiting for pod with text data 02/27/23 02:43:14.82
STEP: Waiting for pod with binary data 02/27/23 02:43:14.825
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:43:14.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3630" for this suite. 02/27/23 02:43:14.838
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:43:08.723
    Feb 27 02:43:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:43:08.724
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:08.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:08.751
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-bb57d391-d60f-49e2-8d39-7091f9ceeca6 02/27/23 02:43:08.764
    STEP: Creating the pod 02/27/23 02:43:08.769
    Feb 27 02:43:08.811: INFO: Waiting up to 5m0s for pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2" in namespace "configmap-3630" to be "running"
    Feb 27 02:43:08.815: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734254ms
    Feb 27 02:43:10.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009026004s
    Feb 27 02:43:12.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008794652s
    Feb 27 02:43:14.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2": Phase="Running", Reason="", readiness=false. Elapsed: 6.008678757s
    Feb 27 02:43:14.820: INFO: Pod "pod-configmaps-70c8a141-ebb8-43cb-9ef8-e1abc47750e2" satisfied condition "running"
    STEP: Waiting for pod with text data 02/27/23 02:43:14.82
    STEP: Waiting for pod with binary data 02/27/23 02:43:14.825
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:43:14.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3630" for this suite. 02/27/23 02:43:14.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:43:14.845
Feb 27 02:43:14.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:43:14.846
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:14.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:14.866
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 02/27/23 02:43:14.874
STEP: delete the rc 02/27/23 02:43:19.89
STEP: wait for the rc to be deleted 02/27/23 02:43:19.904
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 02:43:24.908
STEP: Gathering metrics 02/27/23 02:43:54.924
Feb 27 02:43:54.954: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 02:43:54.956: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.608342ms
Feb 27 02:43:54.956: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 02:43:54.956: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 02:43:55.004: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 02:43:55.004: INFO: Deleting pod "simpletest.rc-2bkdm" in namespace "gc-3434"
Feb 27 02:43:55.023: INFO: Deleting pod "simpletest.rc-2clxz" in namespace "gc-3434"
Feb 27 02:43:55.040: INFO: Deleting pod "simpletest.rc-2jdp2" in namespace "gc-3434"
Feb 27 02:43:55.058: INFO: Deleting pod "simpletest.rc-45vvq" in namespace "gc-3434"
Feb 27 02:43:55.071: INFO: Deleting pod "simpletest.rc-4bdm4" in namespace "gc-3434"
Feb 27 02:43:55.084: INFO: Deleting pod "simpletest.rc-4fpq2" in namespace "gc-3434"
Feb 27 02:43:55.104: INFO: Deleting pod "simpletest.rc-4fqdk" in namespace "gc-3434"
Feb 27 02:43:55.122: INFO: Deleting pod "simpletest.rc-4g56j" in namespace "gc-3434"
Feb 27 02:43:55.159: INFO: Deleting pod "simpletest.rc-4z67f" in namespace "gc-3434"
Feb 27 02:43:55.171: INFO: Deleting pod "simpletest.rc-54q5q" in namespace "gc-3434"
Feb 27 02:43:55.185: INFO: Deleting pod "simpletest.rc-54xcj" in namespace "gc-3434"
Feb 27 02:43:55.202: INFO: Deleting pod "simpletest.rc-5ckgp" in namespace "gc-3434"
Feb 27 02:43:55.223: INFO: Deleting pod "simpletest.rc-5tn4z" in namespace "gc-3434"
Feb 27 02:43:55.237: INFO: Deleting pod "simpletest.rc-5z2jp" in namespace "gc-3434"
Feb 27 02:43:55.250: INFO: Deleting pod "simpletest.rc-62q2z" in namespace "gc-3434"
Feb 27 02:43:55.265: INFO: Deleting pod "simpletest.rc-67whr" in namespace "gc-3434"
Feb 27 02:43:55.276: INFO: Deleting pod "simpletest.rc-6r6np" in namespace "gc-3434"
Feb 27 02:43:55.295: INFO: Deleting pod "simpletest.rc-6z2gp" in namespace "gc-3434"
Feb 27 02:43:55.309: INFO: Deleting pod "simpletest.rc-7kw97" in namespace "gc-3434"
Feb 27 02:43:55.332: INFO: Deleting pod "simpletest.rc-7m2fb" in namespace "gc-3434"
Feb 27 02:43:55.344: INFO: Deleting pod "simpletest.rc-88js7" in namespace "gc-3434"
Feb 27 02:43:55.356: INFO: Deleting pod "simpletest.rc-8g468" in namespace "gc-3434"
Feb 27 02:43:55.376: INFO: Deleting pod "simpletest.rc-8m22m" in namespace "gc-3434"
Feb 27 02:43:55.396: INFO: Deleting pod "simpletest.rc-8njkx" in namespace "gc-3434"
Feb 27 02:43:55.414: INFO: Deleting pod "simpletest.rc-9447d" in namespace "gc-3434"
Feb 27 02:43:55.434: INFO: Deleting pod "simpletest.rc-9nph7" in namespace "gc-3434"
Feb 27 02:43:55.460: INFO: Deleting pod "simpletest.rc-9qqvs" in namespace "gc-3434"
Feb 27 02:43:55.483: INFO: Deleting pod "simpletest.rc-9sw8g" in namespace "gc-3434"
Feb 27 02:43:55.502: INFO: Deleting pod "simpletest.rc-9szfs" in namespace "gc-3434"
Feb 27 02:43:55.545: INFO: Deleting pod "simpletest.rc-9t5z5" in namespace "gc-3434"
Feb 27 02:43:55.573: INFO: Deleting pod "simpletest.rc-9xzrf" in namespace "gc-3434"
Feb 27 02:43:55.600: INFO: Deleting pod "simpletest.rc-9zm48" in namespace "gc-3434"
Feb 27 02:43:55.622: INFO: Deleting pod "simpletest.rc-b4qnh" in namespace "gc-3434"
Feb 27 02:43:55.647: INFO: Deleting pod "simpletest.rc-b5m2h" in namespace "gc-3434"
Feb 27 02:43:55.682: INFO: Deleting pod "simpletest.rc-bwv7n" in namespace "gc-3434"
Feb 27 02:43:55.716: INFO: Deleting pod "simpletest.rc-bxmkn" in namespace "gc-3434"
Feb 27 02:43:55.743: INFO: Deleting pod "simpletest.rc-c45zj" in namespace "gc-3434"
Feb 27 02:43:55.757: INFO: Deleting pod "simpletest.rc-ccqqm" in namespace "gc-3434"
Feb 27 02:43:55.792: INFO: Deleting pod "simpletest.rc-cfn6k" in namespace "gc-3434"
Feb 27 02:43:55.838: INFO: Deleting pod "simpletest.rc-cjsxm" in namespace "gc-3434"
Feb 27 02:43:55.853: INFO: Deleting pod "simpletest.rc-ddjp8" in namespace "gc-3434"
Feb 27 02:43:55.876: INFO: Deleting pod "simpletest.rc-ddrjj" in namespace "gc-3434"
Feb 27 02:43:55.937: INFO: Deleting pod "simpletest.rc-dhljn" in namespace "gc-3434"
Feb 27 02:43:55.969: INFO: Deleting pod "simpletest.rc-dmhvp" in namespace "gc-3434"
Feb 27 02:43:56.024: INFO: Deleting pod "simpletest.rc-ffd5m" in namespace "gc-3434"
Feb 27 02:43:56.065: INFO: Deleting pod "simpletest.rc-g9299" in namespace "gc-3434"
Feb 27 02:43:56.097: INFO: Deleting pod "simpletest.rc-glp76" in namespace "gc-3434"
Feb 27 02:43:56.120: INFO: Deleting pod "simpletest.rc-gqj5s" in namespace "gc-3434"
Feb 27 02:43:56.148: INFO: Deleting pod "simpletest.rc-h8m6m" in namespace "gc-3434"
Feb 27 02:43:56.201: INFO: Deleting pod "simpletest.rc-hsvdz" in namespace "gc-3434"
Feb 27 02:43:56.219: INFO: Deleting pod "simpletest.rc-hvc5n" in namespace "gc-3434"
Feb 27 02:43:56.244: INFO: Deleting pod "simpletest.rc-jg88q" in namespace "gc-3434"
Feb 27 02:43:56.360: INFO: Deleting pod "simpletest.rc-kfh47" in namespace "gc-3434"
Feb 27 02:43:56.423: INFO: Deleting pod "simpletest.rc-khszs" in namespace "gc-3434"
Feb 27 02:43:56.489: INFO: Deleting pod "simpletest.rc-klk9z" in namespace "gc-3434"
Feb 27 02:43:56.528: INFO: Deleting pod "simpletest.rc-kvmls" in namespace "gc-3434"
Feb 27 02:43:56.549: INFO: Deleting pod "simpletest.rc-l5znz" in namespace "gc-3434"
Feb 27 02:43:56.600: INFO: Deleting pod "simpletest.rc-llvkq" in namespace "gc-3434"
Feb 27 02:43:56.628: INFO: Deleting pod "simpletest.rc-lssjq" in namespace "gc-3434"
Feb 27 02:43:56.668: INFO: Deleting pod "simpletest.rc-m8c5p" in namespace "gc-3434"
Feb 27 02:43:56.707: INFO: Deleting pod "simpletest.rc-mh9xf" in namespace "gc-3434"
Feb 27 02:43:56.732: INFO: Deleting pod "simpletest.rc-mk8qg" in namespace "gc-3434"
Feb 27 02:43:56.787: INFO: Deleting pod "simpletest.rc-mtn5c" in namespace "gc-3434"
Feb 27 02:43:56.806: INFO: Deleting pod "simpletest.rc-n2d8v" in namespace "gc-3434"
Feb 27 02:43:56.835: INFO: Deleting pod "simpletest.rc-n6txr" in namespace "gc-3434"
Feb 27 02:43:56.859: INFO: Deleting pod "simpletest.rc-nbkwj" in namespace "gc-3434"
Feb 27 02:43:56.897: INFO: Deleting pod "simpletest.rc-ntmtl" in namespace "gc-3434"
Feb 27 02:43:56.923: INFO: Deleting pod "simpletest.rc-p9pbm" in namespace "gc-3434"
Feb 27 02:43:56.958: INFO: Deleting pod "simpletest.rc-pgkdj" in namespace "gc-3434"
Feb 27 02:43:56.983: INFO: Deleting pod "simpletest.rc-q7jk7" in namespace "gc-3434"
Feb 27 02:43:57.022: INFO: Deleting pod "simpletest.rc-qgtx9" in namespace "gc-3434"
Feb 27 02:43:57.047: INFO: Deleting pod "simpletest.rc-qqvr9" in namespace "gc-3434"
Feb 27 02:43:57.071: INFO: Deleting pod "simpletest.rc-r9bh4" in namespace "gc-3434"
Feb 27 02:43:57.107: INFO: Deleting pod "simpletest.rc-r9d79" in namespace "gc-3434"
Feb 27 02:43:57.128: INFO: Deleting pod "simpletest.rc-rchdb" in namespace "gc-3434"
Feb 27 02:43:57.178: INFO: Deleting pod "simpletest.rc-rrq5s" in namespace "gc-3434"
Feb 27 02:43:57.194: INFO: Deleting pod "simpletest.rc-rssxg" in namespace "gc-3434"
Feb 27 02:43:57.250: INFO: Deleting pod "simpletest.rc-rw5zg" in namespace "gc-3434"
Feb 27 02:43:57.278: INFO: Deleting pod "simpletest.rc-s2bnh" in namespace "gc-3434"
Feb 27 02:43:57.305: INFO: Deleting pod "simpletest.rc-s9x76" in namespace "gc-3434"
Feb 27 02:43:57.349: INFO: Deleting pod "simpletest.rc-sgsmv" in namespace "gc-3434"
Feb 27 02:43:57.389: INFO: Deleting pod "simpletest.rc-sr8vd" in namespace "gc-3434"
Feb 27 02:43:57.421: INFO: Deleting pod "simpletest.rc-sskzk" in namespace "gc-3434"
Feb 27 02:43:57.473: INFO: Deleting pod "simpletest.rc-sw4j5" in namespace "gc-3434"
Feb 27 02:43:57.513: INFO: Deleting pod "simpletest.rc-t8zk4" in namespace "gc-3434"
Feb 27 02:43:57.536: INFO: Deleting pod "simpletest.rc-tnhg6" in namespace "gc-3434"
Feb 27 02:43:57.570: INFO: Deleting pod "simpletest.rc-tvzvt" in namespace "gc-3434"
Feb 27 02:43:57.602: INFO: Deleting pod "simpletest.rc-twsv6" in namespace "gc-3434"
Feb 27 02:43:57.625: INFO: Deleting pod "simpletest.rc-vcwwp" in namespace "gc-3434"
Feb 27 02:43:57.648: INFO: Deleting pod "simpletest.rc-vdxvz" in namespace "gc-3434"
Feb 27 02:43:57.688: INFO: Deleting pod "simpletest.rc-vr2nj" in namespace "gc-3434"
Feb 27 02:43:57.827: INFO: Deleting pod "simpletest.rc-w9rs9" in namespace "gc-3434"
Feb 27 02:43:57.882: INFO: Deleting pod "simpletest.rc-wckgv" in namespace "gc-3434"
Feb 27 02:43:57.920: INFO: Deleting pod "simpletest.rc-wgmvq" in namespace "gc-3434"
Feb 27 02:43:57.956: INFO: Deleting pod "simpletest.rc-wnvtj" in namespace "gc-3434"
Feb 27 02:43:57.987: INFO: Deleting pod "simpletest.rc-xbq9r" in namespace "gc-3434"
Feb 27 02:43:58.012: INFO: Deleting pod "simpletest.rc-xg7fk" in namespace "gc-3434"
Feb 27 02:43:58.027: INFO: Deleting pod "simpletest.rc-xh2ch" in namespace "gc-3434"
Feb 27 02:43:58.045: INFO: Deleting pod "simpletest.rc-xrxm6" in namespace "gc-3434"
Feb 27 02:43:58.068: INFO: Deleting pod "simpletest.rc-zxck9" in namespace "gc-3434"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:43:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3434" for this suite. 02/27/23 02:43:58.095
------------------------------
• [SLOW TEST] [43.263 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:43:14.845
    Feb 27 02:43:14.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:43:14.846
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:14.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:14.866
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 02/27/23 02:43:14.874
    STEP: delete the rc 02/27/23 02:43:19.89
    STEP: wait for the rc to be deleted 02/27/23 02:43:19.904
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 02/27/23 02:43:24.908
    STEP: Gathering metrics 02/27/23 02:43:54.924
    Feb 27 02:43:54.954: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 02:43:54.956: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.608342ms
    Feb 27 02:43:54.956: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 02:43:54.956: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 02:43:55.004: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 02:43:55.004: INFO: Deleting pod "simpletest.rc-2bkdm" in namespace "gc-3434"
    Feb 27 02:43:55.023: INFO: Deleting pod "simpletest.rc-2clxz" in namespace "gc-3434"
    Feb 27 02:43:55.040: INFO: Deleting pod "simpletest.rc-2jdp2" in namespace "gc-3434"
    Feb 27 02:43:55.058: INFO: Deleting pod "simpletest.rc-45vvq" in namespace "gc-3434"
    Feb 27 02:43:55.071: INFO: Deleting pod "simpletest.rc-4bdm4" in namespace "gc-3434"
    Feb 27 02:43:55.084: INFO: Deleting pod "simpletest.rc-4fpq2" in namespace "gc-3434"
    Feb 27 02:43:55.104: INFO: Deleting pod "simpletest.rc-4fqdk" in namespace "gc-3434"
    Feb 27 02:43:55.122: INFO: Deleting pod "simpletest.rc-4g56j" in namespace "gc-3434"
    Feb 27 02:43:55.159: INFO: Deleting pod "simpletest.rc-4z67f" in namespace "gc-3434"
    Feb 27 02:43:55.171: INFO: Deleting pod "simpletest.rc-54q5q" in namespace "gc-3434"
    Feb 27 02:43:55.185: INFO: Deleting pod "simpletest.rc-54xcj" in namespace "gc-3434"
    Feb 27 02:43:55.202: INFO: Deleting pod "simpletest.rc-5ckgp" in namespace "gc-3434"
    Feb 27 02:43:55.223: INFO: Deleting pod "simpletest.rc-5tn4z" in namespace "gc-3434"
    Feb 27 02:43:55.237: INFO: Deleting pod "simpletest.rc-5z2jp" in namespace "gc-3434"
    Feb 27 02:43:55.250: INFO: Deleting pod "simpletest.rc-62q2z" in namespace "gc-3434"
    Feb 27 02:43:55.265: INFO: Deleting pod "simpletest.rc-67whr" in namespace "gc-3434"
    Feb 27 02:43:55.276: INFO: Deleting pod "simpletest.rc-6r6np" in namespace "gc-3434"
    Feb 27 02:43:55.295: INFO: Deleting pod "simpletest.rc-6z2gp" in namespace "gc-3434"
    Feb 27 02:43:55.309: INFO: Deleting pod "simpletest.rc-7kw97" in namespace "gc-3434"
    Feb 27 02:43:55.332: INFO: Deleting pod "simpletest.rc-7m2fb" in namespace "gc-3434"
    Feb 27 02:43:55.344: INFO: Deleting pod "simpletest.rc-88js7" in namespace "gc-3434"
    Feb 27 02:43:55.356: INFO: Deleting pod "simpletest.rc-8g468" in namespace "gc-3434"
    Feb 27 02:43:55.376: INFO: Deleting pod "simpletest.rc-8m22m" in namespace "gc-3434"
    Feb 27 02:43:55.396: INFO: Deleting pod "simpletest.rc-8njkx" in namespace "gc-3434"
    Feb 27 02:43:55.414: INFO: Deleting pod "simpletest.rc-9447d" in namespace "gc-3434"
    Feb 27 02:43:55.434: INFO: Deleting pod "simpletest.rc-9nph7" in namespace "gc-3434"
    Feb 27 02:43:55.460: INFO: Deleting pod "simpletest.rc-9qqvs" in namespace "gc-3434"
    Feb 27 02:43:55.483: INFO: Deleting pod "simpletest.rc-9sw8g" in namespace "gc-3434"
    Feb 27 02:43:55.502: INFO: Deleting pod "simpletest.rc-9szfs" in namespace "gc-3434"
    Feb 27 02:43:55.545: INFO: Deleting pod "simpletest.rc-9t5z5" in namespace "gc-3434"
    Feb 27 02:43:55.573: INFO: Deleting pod "simpletest.rc-9xzrf" in namespace "gc-3434"
    Feb 27 02:43:55.600: INFO: Deleting pod "simpletest.rc-9zm48" in namespace "gc-3434"
    Feb 27 02:43:55.622: INFO: Deleting pod "simpletest.rc-b4qnh" in namespace "gc-3434"
    Feb 27 02:43:55.647: INFO: Deleting pod "simpletest.rc-b5m2h" in namespace "gc-3434"
    Feb 27 02:43:55.682: INFO: Deleting pod "simpletest.rc-bwv7n" in namespace "gc-3434"
    Feb 27 02:43:55.716: INFO: Deleting pod "simpletest.rc-bxmkn" in namespace "gc-3434"
    Feb 27 02:43:55.743: INFO: Deleting pod "simpletest.rc-c45zj" in namespace "gc-3434"
    Feb 27 02:43:55.757: INFO: Deleting pod "simpletest.rc-ccqqm" in namespace "gc-3434"
    Feb 27 02:43:55.792: INFO: Deleting pod "simpletest.rc-cfn6k" in namespace "gc-3434"
    Feb 27 02:43:55.838: INFO: Deleting pod "simpletest.rc-cjsxm" in namespace "gc-3434"
    Feb 27 02:43:55.853: INFO: Deleting pod "simpletest.rc-ddjp8" in namespace "gc-3434"
    Feb 27 02:43:55.876: INFO: Deleting pod "simpletest.rc-ddrjj" in namespace "gc-3434"
    Feb 27 02:43:55.937: INFO: Deleting pod "simpletest.rc-dhljn" in namespace "gc-3434"
    Feb 27 02:43:55.969: INFO: Deleting pod "simpletest.rc-dmhvp" in namespace "gc-3434"
    Feb 27 02:43:56.024: INFO: Deleting pod "simpletest.rc-ffd5m" in namespace "gc-3434"
    Feb 27 02:43:56.065: INFO: Deleting pod "simpletest.rc-g9299" in namespace "gc-3434"
    Feb 27 02:43:56.097: INFO: Deleting pod "simpletest.rc-glp76" in namespace "gc-3434"
    Feb 27 02:43:56.120: INFO: Deleting pod "simpletest.rc-gqj5s" in namespace "gc-3434"
    Feb 27 02:43:56.148: INFO: Deleting pod "simpletest.rc-h8m6m" in namespace "gc-3434"
    Feb 27 02:43:56.201: INFO: Deleting pod "simpletest.rc-hsvdz" in namespace "gc-3434"
    Feb 27 02:43:56.219: INFO: Deleting pod "simpletest.rc-hvc5n" in namespace "gc-3434"
    Feb 27 02:43:56.244: INFO: Deleting pod "simpletest.rc-jg88q" in namespace "gc-3434"
    Feb 27 02:43:56.360: INFO: Deleting pod "simpletest.rc-kfh47" in namespace "gc-3434"
    Feb 27 02:43:56.423: INFO: Deleting pod "simpletest.rc-khszs" in namespace "gc-3434"
    Feb 27 02:43:56.489: INFO: Deleting pod "simpletest.rc-klk9z" in namespace "gc-3434"
    Feb 27 02:43:56.528: INFO: Deleting pod "simpletest.rc-kvmls" in namespace "gc-3434"
    Feb 27 02:43:56.549: INFO: Deleting pod "simpletest.rc-l5znz" in namespace "gc-3434"
    Feb 27 02:43:56.600: INFO: Deleting pod "simpletest.rc-llvkq" in namespace "gc-3434"
    Feb 27 02:43:56.628: INFO: Deleting pod "simpletest.rc-lssjq" in namespace "gc-3434"
    Feb 27 02:43:56.668: INFO: Deleting pod "simpletest.rc-m8c5p" in namespace "gc-3434"
    Feb 27 02:43:56.707: INFO: Deleting pod "simpletest.rc-mh9xf" in namespace "gc-3434"
    Feb 27 02:43:56.732: INFO: Deleting pod "simpletest.rc-mk8qg" in namespace "gc-3434"
    Feb 27 02:43:56.787: INFO: Deleting pod "simpletest.rc-mtn5c" in namespace "gc-3434"
    Feb 27 02:43:56.806: INFO: Deleting pod "simpletest.rc-n2d8v" in namespace "gc-3434"
    Feb 27 02:43:56.835: INFO: Deleting pod "simpletest.rc-n6txr" in namespace "gc-3434"
    Feb 27 02:43:56.859: INFO: Deleting pod "simpletest.rc-nbkwj" in namespace "gc-3434"
    Feb 27 02:43:56.897: INFO: Deleting pod "simpletest.rc-ntmtl" in namespace "gc-3434"
    Feb 27 02:43:56.923: INFO: Deleting pod "simpletest.rc-p9pbm" in namespace "gc-3434"
    Feb 27 02:43:56.958: INFO: Deleting pod "simpletest.rc-pgkdj" in namespace "gc-3434"
    Feb 27 02:43:56.983: INFO: Deleting pod "simpletest.rc-q7jk7" in namespace "gc-3434"
    Feb 27 02:43:57.022: INFO: Deleting pod "simpletest.rc-qgtx9" in namespace "gc-3434"
    Feb 27 02:43:57.047: INFO: Deleting pod "simpletest.rc-qqvr9" in namespace "gc-3434"
    Feb 27 02:43:57.071: INFO: Deleting pod "simpletest.rc-r9bh4" in namespace "gc-3434"
    Feb 27 02:43:57.107: INFO: Deleting pod "simpletest.rc-r9d79" in namespace "gc-3434"
    Feb 27 02:43:57.128: INFO: Deleting pod "simpletest.rc-rchdb" in namespace "gc-3434"
    Feb 27 02:43:57.178: INFO: Deleting pod "simpletest.rc-rrq5s" in namespace "gc-3434"
    Feb 27 02:43:57.194: INFO: Deleting pod "simpletest.rc-rssxg" in namespace "gc-3434"
    Feb 27 02:43:57.250: INFO: Deleting pod "simpletest.rc-rw5zg" in namespace "gc-3434"
    Feb 27 02:43:57.278: INFO: Deleting pod "simpletest.rc-s2bnh" in namespace "gc-3434"
    Feb 27 02:43:57.305: INFO: Deleting pod "simpletest.rc-s9x76" in namespace "gc-3434"
    Feb 27 02:43:57.349: INFO: Deleting pod "simpletest.rc-sgsmv" in namespace "gc-3434"
    Feb 27 02:43:57.389: INFO: Deleting pod "simpletest.rc-sr8vd" in namespace "gc-3434"
    Feb 27 02:43:57.421: INFO: Deleting pod "simpletest.rc-sskzk" in namespace "gc-3434"
    Feb 27 02:43:57.473: INFO: Deleting pod "simpletest.rc-sw4j5" in namespace "gc-3434"
    Feb 27 02:43:57.513: INFO: Deleting pod "simpletest.rc-t8zk4" in namespace "gc-3434"
    Feb 27 02:43:57.536: INFO: Deleting pod "simpletest.rc-tnhg6" in namespace "gc-3434"
    Feb 27 02:43:57.570: INFO: Deleting pod "simpletest.rc-tvzvt" in namespace "gc-3434"
    Feb 27 02:43:57.602: INFO: Deleting pod "simpletest.rc-twsv6" in namespace "gc-3434"
    Feb 27 02:43:57.625: INFO: Deleting pod "simpletest.rc-vcwwp" in namespace "gc-3434"
    Feb 27 02:43:57.648: INFO: Deleting pod "simpletest.rc-vdxvz" in namespace "gc-3434"
    Feb 27 02:43:57.688: INFO: Deleting pod "simpletest.rc-vr2nj" in namespace "gc-3434"
    Feb 27 02:43:57.827: INFO: Deleting pod "simpletest.rc-w9rs9" in namespace "gc-3434"
    Feb 27 02:43:57.882: INFO: Deleting pod "simpletest.rc-wckgv" in namespace "gc-3434"
    Feb 27 02:43:57.920: INFO: Deleting pod "simpletest.rc-wgmvq" in namespace "gc-3434"
    Feb 27 02:43:57.956: INFO: Deleting pod "simpletest.rc-wnvtj" in namespace "gc-3434"
    Feb 27 02:43:57.987: INFO: Deleting pod "simpletest.rc-xbq9r" in namespace "gc-3434"
    Feb 27 02:43:58.012: INFO: Deleting pod "simpletest.rc-xg7fk" in namespace "gc-3434"
    Feb 27 02:43:58.027: INFO: Deleting pod "simpletest.rc-xh2ch" in namespace "gc-3434"
    Feb 27 02:43:58.045: INFO: Deleting pod "simpletest.rc-xrxm6" in namespace "gc-3434"
    Feb 27 02:43:58.068: INFO: Deleting pod "simpletest.rc-zxck9" in namespace "gc-3434"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:43:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3434" for this suite. 02/27/23 02:43:58.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:43:58.108
Feb 27 02:43:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-webhook 02/27/23 02:43:58.109
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:58.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:58.148
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 02/27/23 02:43:58.156
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 02:43:58.59
STEP: Deploying the custom resource conversion webhook pod 02/27/23 02:43:58.598
STEP: Wait for the deployment to be ready 02/27/23 02:43:58.608
Feb 27 02:43:58.619: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Feb 27 02:44:00.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-86b5dcd96b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:44:02.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-86b5dcd96b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 02/27/23 02:44:04.635
STEP: Verifying the service has paired with the endpoint 02/27/23 02:44:04.655
Feb 27 02:44:05.656: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Feb 27 02:44:05.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Creating a v1 custom resource 02/27/23 02:44:08.249
STEP: Create a v2 custom resource 02/27/23 02:44:08.269
STEP: List CRs in v1 02/27/23 02:44:08.325
STEP: List CRs in v2 02/27/23 02:44:08.328
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3777" for this suite. 02/27/23 02:44:08.929
------------------------------
• [SLOW TEST] [10.833 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:43:58.108
    Feb 27 02:43:58.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-webhook 02/27/23 02:43:58.109
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:43:58.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:43:58.148
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 02/27/23 02:43:58.156
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 02/27/23 02:43:58.59
    STEP: Deploying the custom resource conversion webhook pod 02/27/23 02:43:58.598
    STEP: Wait for the deployment to be ready 02/27/23 02:43:58.608
    Feb 27 02:43:58.619: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Feb 27 02:44:00.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-86b5dcd96b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:44:02.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 43, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-86b5dcd96b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 02/27/23 02:44:04.635
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:44:04.655
    Feb 27 02:44:05.656: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Feb 27 02:44:05.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Creating a v1 custom resource 02/27/23 02:44:08.249
    STEP: Create a v2 custom resource 02/27/23 02:44:08.269
    STEP: List CRs in v1 02/27/23 02:44:08.325
    STEP: List CRs in v2 02/27/23 02:44:08.328
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3777" for this suite. 02/27/23 02:44:08.929
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:08.941
Feb 27 02:44:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 02:44:08.942
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:08.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:08.967
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 02/27/23 02:44:08.97
STEP: Creating hostNetwork=false pod 02/27/23 02:44:08.97
W0227 02:44:09.003725      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
Feb 27 02:44:09.003: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-933" to be "running and ready"
Feb 27 02:44:09.006: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527066ms
Feb 27 02:44:09.006: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:44:11.029: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025984764s
Feb 27 02:44:11.029: INFO: The phase of Pod test-pod is Running (Ready = true)
Feb 27 02:44:11.029: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 02/27/23 02:44:11.033
W0227 02:44:11.044522      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
Feb 27 02:44:11.044: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-933" to be "running and ready"
Feb 27 02:44:11.055: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.869436ms
Feb 27 02:44:11.055: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:44:13.061: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016465233s
Feb 27 02:44:13.061: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Feb 27 02:44:13.061: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 02/27/23 02:44:13.064
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 02:44:13.064
Feb 27 02:44:13.064: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.065: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 02:44:13.163: INFO: Exec stderr: ""
Feb 27 02:44:13.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.163: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.164: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 02:44:13.206: INFO: Exec stderr: ""
Feb 27 02:44:13.206: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.206: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 02:44:13.271: INFO: Exec stderr: ""
Feb 27 02:44:13.271: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.271: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 02:44:13.317: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 02:44:13.317
Feb 27 02:44:13.317: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.318: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.318: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 02:44:13.387: INFO: Exec stderr: ""
Feb 27 02:44:13.387: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.388: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.388: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Feb 27 02:44:13.427: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 02:44:13.427
Feb 27 02:44:13.427: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.427: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 02:44:13.503: INFO: Exec stderr: ""
Feb 27 02:44:13.503: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.504: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.504: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Feb 27 02:44:13.571: INFO: Exec stderr: ""
Feb 27 02:44:13.571: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.571: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.571: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 02:44:13.631: INFO: Exec stderr: ""
Feb 27 02:44:13.631: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:44:13.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:44:13.631: INFO: ExecWithOptions: Clientset creation
Feb 27 02:44:13.631: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Feb 27 02:44:13.707: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-933" for this suite. 02/27/23 02:44:13.714
------------------------------
• [4.779 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:08.941
    Feb 27 02:44:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 02/27/23 02:44:08.942
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:08.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:08.967
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 02/27/23 02:44:08.97
    STEP: Creating hostNetwork=false pod 02/27/23 02:44:08.97
    W0227 02:44:09.003725      23 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
    Feb 27 02:44:09.003: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-933" to be "running and ready"
    Feb 27 02:44:09.006: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.527066ms
    Feb 27 02:44:09.006: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:44:11.029: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.025984764s
    Feb 27 02:44:11.029: INFO: The phase of Pod test-pod is Running (Ready = true)
    Feb 27 02:44:11.029: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 02/27/23 02:44:11.033
    W0227 02:44:11.044522      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
    Feb 27 02:44:11.044: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-933" to be "running and ready"
    Feb 27 02:44:11.055: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.869436ms
    Feb 27 02:44:11.055: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:44:13.061: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016465233s
    Feb 27 02:44:13.061: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Feb 27 02:44:13.061: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 02/27/23 02:44:13.064
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 02/27/23 02:44:13.064
    Feb 27 02:44:13.064: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.065: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 02:44:13.163: INFO: Exec stderr: ""
    Feb 27 02:44:13.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.163: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.164: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 02:44:13.206: INFO: Exec stderr: ""
    Feb 27 02:44:13.206: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.206: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 02:44:13.271: INFO: Exec stderr: ""
    Feb 27 02:44:13.271: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.271: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 02:44:13.317: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 02/27/23 02:44:13.317
    Feb 27 02:44:13.317: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.318: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.318: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 02:44:13.387: INFO: Exec stderr: ""
    Feb 27 02:44:13.387: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.388: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.388: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Feb 27 02:44:13.427: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 02/27/23 02:44:13.427
    Feb 27 02:44:13.427: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.427: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 02:44:13.503: INFO: Exec stderr: ""
    Feb 27 02:44:13.503: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.504: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.504: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Feb 27 02:44:13.571: INFO: Exec stderr: ""
    Feb 27 02:44:13.571: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.571: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.571: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 02:44:13.631: INFO: Exec stderr: ""
    Feb 27 02:44:13.631: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-933 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:44:13.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:44:13.631: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:44:13.631: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-933/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Feb 27 02:44:13.707: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:13.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-933" for this suite. 02/27/23 02:44:13.714
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:13.721
Feb 27 02:44:13.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-runtime 02/27/23 02:44:13.722
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:13.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:13.746
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 02:44:13.758
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 02:44:30.838
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 02:44:30.841
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 02:44:30.849
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 02:44:30.849
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 02:44:30.894
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 02:44:33.91
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 02:44:35.923
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 02:44:35.931
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 02:44:35.931
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 02:44:35.972
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 02:44:36.98
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 02:44:39.996
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 02:44:40.004
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 02:44:40.004
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7044" for this suite. 02/27/23 02:44:40.127
------------------------------
• [SLOW TEST] [26.413 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:13.721
    Feb 27 02:44:13.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-runtime 02/27/23 02:44:13.722
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:13.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:13.746
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 02/27/23 02:44:13.758
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 02/27/23 02:44:30.838
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 02/27/23 02:44:30.841
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 02/27/23 02:44:30.849
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 02/27/23 02:44:30.849
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 02/27/23 02:44:30.894
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 02/27/23 02:44:33.91
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 02/27/23 02:44:35.923
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 02/27/23 02:44:35.931
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 02/27/23 02:44:35.931
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 02/27/23 02:44:35.972
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 02/27/23 02:44:36.98
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 02/27/23 02:44:39.996
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 02/27/23 02:44:40.004
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 02/27/23 02:44:40.004
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7044" for this suite. 02/27/23 02:44:40.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:40.134
Feb 27 02:44:40.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:44:40.135
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:40.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:40.154
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-c394c709-8620-4797-a73d-2b52713017af 02/27/23 02:44:40.157
STEP: Creating a pod to test consume secrets 02/27/23 02:44:40.161
Feb 27 02:44:40.171: INFO: Waiting up to 5m0s for pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3" in namespace "secrets-27" to be "Succeeded or Failed"
Feb 27 02:44:40.197: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.735854ms
Feb 27 02:44:42.201: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030364143s
Feb 27 02:44:44.203: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031511198s
STEP: Saw pod success 02/27/23 02:44:44.203
Feb 27 02:44:44.203: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3" satisfied condition "Succeeded or Failed"
Feb 27 02:44:44.206: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:44:44.213
Feb 27 02:44:44.228: INFO: Waiting for pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 to disappear
Feb 27 02:44:44.230: INFO: Pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:44.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-27" for this suite. 02/27/23 02:44:44.234
------------------------------
• [4.106 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:40.134
    Feb 27 02:44:40.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:44:40.135
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:40.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:40.154
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-c394c709-8620-4797-a73d-2b52713017af 02/27/23 02:44:40.157
    STEP: Creating a pod to test consume secrets 02/27/23 02:44:40.161
    Feb 27 02:44:40.171: INFO: Waiting up to 5m0s for pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3" in namespace "secrets-27" to be "Succeeded or Failed"
    Feb 27 02:44:40.197: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.735854ms
    Feb 27 02:44:42.201: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030364143s
    Feb 27 02:44:44.203: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031511198s
    STEP: Saw pod success 02/27/23 02:44:44.203
    Feb 27 02:44:44.203: INFO: Pod "pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3" satisfied condition "Succeeded or Failed"
    Feb 27 02:44:44.206: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:44:44.213
    Feb 27 02:44:44.228: INFO: Waiting for pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 to disappear
    Feb 27 02:44:44.230: INFO: Pod pod-secrets-8a8e157d-3229-42d7-b52f-1a84e16871b3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:44.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-27" for this suite. 02/27/23 02:44:44.234
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:44.24
Feb 27 02:44:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:44:44.241
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:44.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:44.26
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:44:44.262
Feb 27 02:44:44.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37" in namespace "downward-api-6678" to be "Succeeded or Failed"
Feb 27 02:44:44.280: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371449ms
Feb 27 02:44:46.286: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009540739s
Feb 27 02:44:48.284: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008028604s
STEP: Saw pod success 02/27/23 02:44:48.284
Feb 27 02:44:48.285: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37" satisfied condition "Succeeded or Failed"
Feb 27 02:44:48.288: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 container client-container: <nil>
STEP: delete the pod 02/27/23 02:44:48.294
Feb 27 02:44:48.309: INFO: Waiting for pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 to disappear
Feb 27 02:44:48.313: INFO: Pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:48.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6678" for this suite. 02/27/23 02:44:48.321
------------------------------
• [4.090 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:44.24
    Feb 27 02:44:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:44:44.241
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:44.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:44.26
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:44:44.262
    Feb 27 02:44:44.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37" in namespace "downward-api-6678" to be "Succeeded or Failed"
    Feb 27 02:44:44.280: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Pending", Reason="", readiness=false. Elapsed: 3.371449ms
    Feb 27 02:44:46.286: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009540739s
    Feb 27 02:44:48.284: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008028604s
    STEP: Saw pod success 02/27/23 02:44:48.284
    Feb 27 02:44:48.285: INFO: Pod "downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37" satisfied condition "Succeeded or Failed"
    Feb 27 02:44:48.288: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:44:48.294
    Feb 27 02:44:48.309: INFO: Waiting for pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 to disappear
    Feb 27 02:44:48.313: INFO: Pod downwardapi-volume-b100c34b-82ae-4d5a-95d2-7c693e56aa37 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:48.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6678" for this suite. 02/27/23 02:44:48.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:48.331
Feb 27 02:44:48.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:44:48.332
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:48.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:48.365
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 02:44:48.368
Feb 27 02:44:48.390: INFO: Waiting up to 5m0s for pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7" in namespace "emptydir-4004" to be "Succeeded or Failed"
Feb 27 02:44:48.397: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346574ms
Feb 27 02:44:50.403: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012946939s
Feb 27 02:44:52.401: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010708854s
STEP: Saw pod success 02/27/23 02:44:52.401
Feb 27 02:44:52.401: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7" satisfied condition "Succeeded or Failed"
Feb 27 02:44:52.404: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 container test-container: <nil>
STEP: delete the pod 02/27/23 02:44:52.409
Feb 27 02:44:52.449: INFO: Waiting for pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 to disappear
Feb 27 02:44:52.451: INFO: Pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:52.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4004" for this suite. 02/27/23 02:44:52.456
------------------------------
• [4.132 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:48.331
    Feb 27 02:44:48.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:44:48.332
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:48.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:48.365
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 02/27/23 02:44:48.368
    Feb 27 02:44:48.390: INFO: Waiting up to 5m0s for pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7" in namespace "emptydir-4004" to be "Succeeded or Failed"
    Feb 27 02:44:48.397: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346574ms
    Feb 27 02:44:50.403: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012946939s
    Feb 27 02:44:52.401: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010708854s
    STEP: Saw pod success 02/27/23 02:44:52.401
    Feb 27 02:44:52.401: INFO: Pod "pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7" satisfied condition "Succeeded or Failed"
    Feb 27 02:44:52.404: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:44:52.409
    Feb 27 02:44:52.449: INFO: Waiting for pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 to disappear
    Feb 27 02:44:52.451: INFO: Pod pod-9e0ad7e8-68d2-4c17-b9f8-3fe4986bbae7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:52.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4004" for this suite. 02/27/23 02:44:52.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:52.464
Feb 27 02:44:52.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 02:44:52.465
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:52.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:52.486
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Feb 27 02:44:52.489: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 02:44:53.5
STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 02:44:53.506
STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 02:44:54.513
Feb 27 02:44:54.534: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 02/27/23 02:44:54.534
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:44:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8780" for this suite. 02/27/23 02:44:55.547
------------------------------
• [3.089 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:52.464
    Feb 27 02:44:52.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 02:44:52.465
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:52.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:52.486
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Feb 27 02:44:52.489: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 02/27/23 02:44:53.5
    STEP: Checking rc "condition-test" has the desired failure condition set 02/27/23 02:44:53.506
    STEP: Scaling down rc "condition-test" to satisfy pod quota 02/27/23 02:44:54.513
    Feb 27 02:44:54.534: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 02/27/23 02:44:54.534
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:44:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8780" for this suite. 02/27/23 02:44:55.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:44:55.554
Feb 27 02:44:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replicaset 02/27/23 02:44:55.555
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:55.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:55.585
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 02:44:55.59
Feb 27 02:44:55.601: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 27 02:45:00.610: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 02:45:00.61
STEP: getting scale subresource 02/27/23 02:45:00.61
STEP: updating a scale subresource 02/27/23 02:45:00.616
STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 02:45:00.63
STEP: Patch a scale subresource 02/27/23 02:45:00.633
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:00.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8958" for this suite. 02/27/23 02:45:00.651
------------------------------
• [SLOW TEST] [5.102 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:44:55.554
    Feb 27 02:44:55.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replicaset 02/27/23 02:44:55.555
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:44:55.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:44:55.585
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 02/27/23 02:44:55.59
    Feb 27 02:44:55.601: INFO: Pod name sample-pod: Found 0 pods out of 1
    Feb 27 02:45:00.610: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 02:45:00.61
    STEP: getting scale subresource 02/27/23 02:45:00.61
    STEP: updating a scale subresource 02/27/23 02:45:00.616
    STEP: verifying the replicaset Spec.Replicas was modified 02/27/23 02:45:00.63
    STEP: Patch a scale subresource 02/27/23 02:45:00.633
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:00.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8958" for this suite. 02/27/23 02:45:00.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:00.656
Feb 27 02:45:00.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 02:45:00.657
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:00.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:00.699
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 02/27/23 02:45:00.702
Feb 27 02:45:00.702: INFO: Creating e2e-svc-a-jg7jw
Feb 27 02:45:00.730: INFO: Creating e2e-svc-b-st9jl
Feb 27 02:45:00.769: INFO: Creating e2e-svc-c-7pjvm
STEP: deleting service collection 02/27/23 02:45:00.795
Feb 27 02:45:00.886: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:00.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2764" for this suite. 02/27/23 02:45:00.893
------------------------------
• [0.255 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:00.656
    Feb 27 02:45:00.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 02:45:00.657
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:00.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:00.699
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 02/27/23 02:45:00.702
    Feb 27 02:45:00.702: INFO: Creating e2e-svc-a-jg7jw
    Feb 27 02:45:00.730: INFO: Creating e2e-svc-b-st9jl
    Feb 27 02:45:00.769: INFO: Creating e2e-svc-c-7pjvm
    STEP: deleting service collection 02/27/23 02:45:00.795
    Feb 27 02:45:00.886: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:00.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2764" for this suite. 02/27/23 02:45:00.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:00.912
Feb 27 02:45:00.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:45:00.913
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:00.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:00.966
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-94506bad-bf79-4a34-9dbf-c0067c7f44b8 02/27/23 02:45:00.969
STEP: Creating a pod to test consume secrets 02/27/23 02:45:00.996
Feb 27 02:45:01.044: INFO: Waiting up to 5m0s for pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb" in namespace "secrets-2674" to be "Succeeded or Failed"
Feb 27 02:45:01.057: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.348678ms
Feb 27 02:45:03.060: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015865122s
Feb 27 02:45:05.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017019726s
Feb 27 02:45:07.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01664578s
STEP: Saw pod success 02/27/23 02:45:07.061
Feb 27 02:45:07.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb" satisfied condition "Succeeded or Failed"
Feb 27 02:45:07.066: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:45:07.073
Feb 27 02:45:07.085: INFO: Waiting for pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb to disappear
Feb 27 02:45:07.088: INFO: Pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:07.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2674" for this suite. 02/27/23 02:45:07.092
------------------------------
• [SLOW TEST] [6.188 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:00.912
    Feb 27 02:45:00.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:45:00.913
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:00.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:00.966
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-94506bad-bf79-4a34-9dbf-c0067c7f44b8 02/27/23 02:45:00.969
    STEP: Creating a pod to test consume secrets 02/27/23 02:45:00.996
    Feb 27 02:45:01.044: INFO: Waiting up to 5m0s for pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb" in namespace "secrets-2674" to be "Succeeded or Failed"
    Feb 27 02:45:01.057: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.348678ms
    Feb 27 02:45:03.060: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015865122s
    Feb 27 02:45:05.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017019726s
    Feb 27 02:45:07.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01664578s
    STEP: Saw pod success 02/27/23 02:45:07.061
    Feb 27 02:45:07.061: INFO: Pod "pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb" satisfied condition "Succeeded or Failed"
    Feb 27 02:45:07.066: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:45:07.073
    Feb 27 02:45:07.085: INFO: Waiting for pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb to disappear
    Feb 27 02:45:07.088: INFO: Pod pod-secrets-60cf264b-7bf0-4ad6-a954-3f94faba3adb no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:07.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2674" for this suite. 02/27/23 02:45:07.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:07.101
Feb 27 02:45:07.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:45:07.102
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:07.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:07.119
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Feb 27 02:45:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:08.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-559" for this suite. 02/27/23 02:45:08.15
------------------------------
• [1.058 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:07.101
    Feb 27 02:45:07.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:45:07.102
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:07.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:07.119
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Feb 27 02:45:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:08.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-559" for this suite. 02/27/23 02:45:08.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:08.161
Feb 27 02:45:08.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:45:08.162
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:08.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:08.182
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 02/27/23 02:45:08.185
STEP: delete the rc 02/27/23 02:45:13.192
STEP: wait for all pods to be garbage collected 02/27/23 02:45:13.198
STEP: Gathering metrics 02/27/23 02:45:18.213
Feb 27 02:45:18.242: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 02:45:18.245: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.79374ms
Feb 27 02:45:18.245: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 02:45:18.245: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 02:45:18.278: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1127" for this suite. 02/27/23 02:45:18.286
------------------------------
• [SLOW TEST] [10.133 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:08.161
    Feb 27 02:45:08.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:45:08.162
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:08.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:08.182
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 02/27/23 02:45:08.185
    STEP: delete the rc 02/27/23 02:45:13.192
    STEP: wait for all pods to be garbage collected 02/27/23 02:45:13.198
    STEP: Gathering metrics 02/27/23 02:45:18.213
    Feb 27 02:45:18.242: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 02:45:18.245: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.79374ms
    Feb 27 02:45:18.245: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 02:45:18.245: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 02:45:18.278: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1127" for this suite. 02/27/23 02:45:18.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:18.295
Feb 27 02:45:18.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:45:18.296
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:18.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:18.317
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 02:45:18.319
Feb 27 02:45:18.362: INFO: Waiting up to 5m0s for pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce" in namespace "emptydir-3945" to be "Succeeded or Failed"
Feb 27 02:45:18.372: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.424484ms
Feb 27 02:45:20.376: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014228863s
Feb 27 02:45:22.380: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017878047s
STEP: Saw pod success 02/27/23 02:45:22.38
Feb 27 02:45:22.380: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce" satisfied condition "Succeeded or Failed"
Feb 27 02:45:22.384: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce container test-container: <nil>
STEP: delete the pod 02/27/23 02:45:22.39
Feb 27 02:45:22.404: INFO: Waiting for pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce to disappear
Feb 27 02:45:22.407: INFO: Pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:22.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3945" for this suite. 02/27/23 02:45:22.411
------------------------------
• [4.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:18.295
    Feb 27 02:45:18.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:45:18.296
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:18.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:18.317
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 02/27/23 02:45:18.319
    Feb 27 02:45:18.362: INFO: Waiting up to 5m0s for pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce" in namespace "emptydir-3945" to be "Succeeded or Failed"
    Feb 27 02:45:18.372: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.424484ms
    Feb 27 02:45:20.376: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014228863s
    Feb 27 02:45:22.380: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017878047s
    STEP: Saw pod success 02/27/23 02:45:22.38
    Feb 27 02:45:22.380: INFO: Pod "pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce" satisfied condition "Succeeded or Failed"
    Feb 27 02:45:22.384: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce container test-container: <nil>
    STEP: delete the pod 02/27/23 02:45:22.39
    Feb 27 02:45:22.404: INFO: Waiting for pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce to disappear
    Feb 27 02:45:22.407: INFO: Pod pod-c443d9c8-0dc8-4046-8b7e-5bbbcb869cce no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:22.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3945" for this suite. 02/27/23 02:45:22.411
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:22.418
Feb 27 02:45:22.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:45:22.418
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:22.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:22.444
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 02:45:22.446
Feb 27 02:45:22.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Feb 27 02:45:22.523: INFO: stderr: ""
Feb 27 02:45:22.523: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 02/27/23 02:45:22.523
Feb 27 02:45:22.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4"}]}} --dry-run=server'
Feb 27 02:45:23.254: INFO: stderr: ""
Feb 27 02:45:23.254: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 02:45:23.254
Feb 27 02:45:23.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 delete pods e2e-test-httpd-pod'
Feb 27 02:45:25.908: INFO: stderr: ""
Feb 27 02:45:25.908: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3167" for this suite. 02/27/23 02:45:25.913
------------------------------
• [3.502 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:22.418
    Feb 27 02:45:22.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:45:22.418
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:22.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:22.444
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 02:45:22.446
    Feb 27 02:45:22.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 run e2e-test-httpd-pod --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Feb 27 02:45:22.523: INFO: stderr: ""
    Feb 27 02:45:22.523: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 02/27/23 02:45:22.523
    Feb 27 02:45:22.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/busybox:1.29-4"}]}} --dry-run=server'
    Feb 27 02:45:23.254: INFO: stderr: ""
    Feb 27 02:45:23.254: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 02/27/23 02:45:23.254
    Feb 27 02:45:23.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3167 delete pods e2e-test-httpd-pod'
    Feb 27 02:45:25.908: INFO: stderr: ""
    Feb 27 02:45:25.908: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3167" for this suite. 02/27/23 02:45:25.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:25.921
Feb 27 02:45:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename events 02/27/23 02:45:25.922
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:25.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:25.942
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 02/27/23 02:45:25.944
STEP: get a list of Events with a label in the current namespace 02/27/23 02:45:25.981
STEP: delete a list of events 02/27/23 02:45:25.984
Feb 27 02:45:25.984: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 02/27/23 02:45:26.012
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5878" for this suite. 02/27/23 02:45:26.02
------------------------------
• [0.109 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:25.921
    Feb 27 02:45:25.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename events 02/27/23 02:45:25.922
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:25.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:25.942
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 02/27/23 02:45:25.944
    STEP: get a list of Events with a label in the current namespace 02/27/23 02:45:25.981
    STEP: delete a list of events 02/27/23 02:45:25.984
    Feb 27 02:45:25.984: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 02/27/23 02:45:26.012
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5878" for this suite. 02/27/23 02:45:26.02
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:26.03
Feb 27 02:45:26.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:45:26.031
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:26.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:26.048
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 02/27/23 02:45:26.051
Feb 27 02:45:26.064: INFO: Waiting up to 5m0s for pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9" in namespace "downward-api-7957" to be "running and ready"
Feb 27 02:45:26.067: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78921ms
Feb 27 02:45:26.067: INFO: The phase of Pod annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:45:28.071: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006621644s
Feb 27 02:45:28.071: INFO: The phase of Pod annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9 is Running (Ready = true)
Feb 27 02:45:28.071: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9" satisfied condition "running and ready"
Feb 27 02:45:28.595: INFO: Successfully updated pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7957" for this suite. 02/27/23 02:45:32.62
------------------------------
• [SLOW TEST] [6.601 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:26.03
    Feb 27 02:45:26.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:45:26.031
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:26.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:26.048
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 02/27/23 02:45:26.051
    Feb 27 02:45:26.064: INFO: Waiting up to 5m0s for pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9" in namespace "downward-api-7957" to be "running and ready"
    Feb 27 02:45:26.067: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78921ms
    Feb 27 02:45:26.067: INFO: The phase of Pod annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:45:28.071: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006621644s
    Feb 27 02:45:28.071: INFO: The phase of Pod annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9 is Running (Ready = true)
    Feb 27 02:45:28.071: INFO: Pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9" satisfied condition "running and ready"
    Feb 27 02:45:28.595: INFO: Successfully updated pod "annotationupdate08d008c7-a29a-430a-9c8d-7583fb82d4c9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:32.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7957" for this suite. 02/27/23 02:45:32.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:32.631
Feb 27 02:45:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:45:32.632
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:32.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:32.653
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 02/27/23 02:45:32.655
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 02:45:32.657
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 02:45:32.657
STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 02:45:32.657
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 02:45:32.658
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 02:45:32.658
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 02:45:32.658
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5031" for this suite. 02/27/23 02:45:32.662
------------------------------
• [0.036 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:32.631
    Feb 27 02:45:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename custom-resource-definition 02/27/23 02:45:32.632
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:32.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:32.653
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 02/27/23 02:45:32.655
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 02/27/23 02:45:32.657
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 02/27/23 02:45:32.657
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 02/27/23 02:45:32.657
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 02/27/23 02:45:32.658
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 02:45:32.658
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 02/27/23 02:45:32.658
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:32.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5031" for this suite. 02/27/23 02:45:32.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:32.668
Feb 27 02:45:32.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context-test 02/27/23 02:45:32.668
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:32.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:32.69
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Feb 27 02:45:32.725: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776" in namespace "security-context-test-2656" to be "Succeeded or Failed"
Feb 27 02:45:32.732: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Pending", Reason="", readiness=false. Elapsed: 6.812066ms
Feb 27 02:45:34.739: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013395337s
Feb 27 02:45:36.738: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012710407s
Feb 27 02:45:36.738: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:36.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2656" for this suite. 02/27/23 02:45:36.742
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:32.668
    Feb 27 02:45:32.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context-test 02/27/23 02:45:32.668
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:32.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:32.69
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Feb 27 02:45:32.725: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776" in namespace "security-context-test-2656" to be "Succeeded or Failed"
    Feb 27 02:45:32.732: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Pending", Reason="", readiness=false. Elapsed: 6.812066ms
    Feb 27 02:45:34.739: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013395337s
    Feb 27 02:45:36.738: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012710407s
    Feb 27 02:45:36.738: INFO: Pod "busybox-user-65534-0a39ecac-c7c8-42f8-80aa-62467ec5a776" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:36.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2656" for this suite. 02/27/23 02:45:36.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:36.752
Feb 27 02:45:36.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename proxy 02/27/23 02:45:36.753
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:36.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:36.771
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Feb 27 02:45:36.773: INFO: Creating pod...
Feb 27 02:45:36.782: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6219" to be "running"
Feb 27 02:45:36.790: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821909ms
Feb 27 02:45:38.795: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013269068s
Feb 27 02:45:38.795: INFO: Pod "agnhost" satisfied condition "running"
Feb 27 02:45:38.795: INFO: Creating service...
Feb 27 02:45:38.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/DELETE
Feb 27 02:45:38.818: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 02:45:38.818: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/GET
Feb 27 02:45:38.824: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 02:45:38.824: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/HEAD
Feb 27 02:45:38.829: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 02:45:38.829: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/OPTIONS
Feb 27 02:45:38.831: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 02:45:38.831: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/PATCH
Feb 27 02:45:38.834: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 02:45:38.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/POST
Feb 27 02:45:38.837: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 02:45:38.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/PUT
Feb 27 02:45:38.839: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Feb 27 02:45:38.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/DELETE
Feb 27 02:45:38.844: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Feb 27 02:45:38.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/GET
Feb 27 02:45:38.856: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Feb 27 02:45:38.856: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/HEAD
Feb 27 02:45:38.867: INFO: http.Client request:HEAD | StatusCode:200
Feb 27 02:45:38.867: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/OPTIONS
Feb 27 02:45:38.876: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Feb 27 02:45:38.876: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/PATCH
Feb 27 02:45:38.885: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Feb 27 02:45:38.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/POST
Feb 27 02:45:38.893: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Feb 27 02:45:38.893: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/PUT
Feb 27 02:45:38.897: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:38.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6219" for this suite. 02/27/23 02:45:38.901
------------------------------
• [2.156 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:36.752
    Feb 27 02:45:36.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename proxy 02/27/23 02:45:36.753
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:36.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:36.771
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Feb 27 02:45:36.773: INFO: Creating pod...
    Feb 27 02:45:36.782: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6219" to be "running"
    Feb 27 02:45:36.790: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821909ms
    Feb 27 02:45:38.795: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013269068s
    Feb 27 02:45:38.795: INFO: Pod "agnhost" satisfied condition "running"
    Feb 27 02:45:38.795: INFO: Creating service...
    Feb 27 02:45:38.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/DELETE
    Feb 27 02:45:38.818: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 02:45:38.818: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/GET
    Feb 27 02:45:38.824: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 02:45:38.824: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/HEAD
    Feb 27 02:45:38.829: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 02:45:38.829: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/OPTIONS
    Feb 27 02:45:38.831: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 02:45:38.831: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/PATCH
    Feb 27 02:45:38.834: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 02:45:38.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/POST
    Feb 27 02:45:38.837: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 02:45:38.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/pods/agnhost/proxy/some/path/with/PUT
    Feb 27 02:45:38.839: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Feb 27 02:45:38.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/DELETE
    Feb 27 02:45:38.844: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Feb 27 02:45:38.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/GET
    Feb 27 02:45:38.856: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Feb 27 02:45:38.856: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/HEAD
    Feb 27 02:45:38.867: INFO: http.Client request:HEAD | StatusCode:200
    Feb 27 02:45:38.867: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/OPTIONS
    Feb 27 02:45:38.876: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Feb 27 02:45:38.876: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/PATCH
    Feb 27 02:45:38.885: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Feb 27 02:45:38.885: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/POST
    Feb 27 02:45:38.893: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Feb 27 02:45:38.893: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6219/services/test-service/proxy/some/path/with/PUT
    Feb 27 02:45:38.897: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:38.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6219" for this suite. 02/27/23 02:45:38.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:38.909
Feb 27 02:45:38.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:45:38.91
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:38.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:38.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:45:38.976
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:45:39.499
STEP: Deploying the webhook pod 02/27/23 02:45:39.507
STEP: Wait for the deployment to be ready 02/27/23 02:45:39.52
Feb 27 02:45:39.526: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 02:45:41.536
STEP: Verifying the service has paired with the endpoint 02/27/23 02:45:41.552
Feb 27 02:45:42.553: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 02/27/23 02:45:42.63
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:45:42.657
STEP: Deleting the collection of validation webhooks 02/27/23 02:45:42.678
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:45:42.735
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:45:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-907" for this suite. 02/27/23 02:45:42.858
STEP: Destroying namespace "webhook-907-markers" for this suite. 02/27/23 02:45:42.873
------------------------------
• [3.983 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:38.909
    Feb 27 02:45:38.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:45:38.91
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:38.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:38.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:45:38.976
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:45:39.499
    STEP: Deploying the webhook pod 02/27/23 02:45:39.507
    STEP: Wait for the deployment to be ready 02/27/23 02:45:39.52
    Feb 27 02:45:39.526: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 02:45:41.536
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:45:41.552
    Feb 27 02:45:42.553: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 02/27/23 02:45:42.63
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:45:42.657
    STEP: Deleting the collection of validation webhooks 02/27/23 02:45:42.678
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:45:42.735
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:45:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-907" for this suite. 02/27/23 02:45:42.858
    STEP: Destroying namespace "webhook-907-markers" for this suite. 02/27/23 02:45:42.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:45:42.896
Feb 27 02:45:42.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 02:45:42.898
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:42.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:42.927
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b in namespace container-probe-2891 02/27/23 02:45:42.933
Feb 27 02:45:42.970: INFO: Waiting up to 5m0s for pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b" in namespace "container-probe-2891" to be "not pending"
Feb 27 02:45:42.972: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302581ms
Feb 27 02:45:44.976: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006658333s
Feb 27 02:45:44.976: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b" satisfied condition "not pending"
Feb 27 02:45:44.976: INFO: Started pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b in namespace container-probe-2891
STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:45:44.976
Feb 27 02:45:44.980: INFO: Initial restart count of pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b is 0
STEP: deleting the pod 02/27/23 02:49:45.571
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 02:49:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2891" for this suite. 02/27/23 02:49:45.592
------------------------------
• [SLOW TEST] [242.701 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:45:42.896
    Feb 27 02:45:42.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 02:45:42.898
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:45:42.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:45:42.927
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b in namespace container-probe-2891 02/27/23 02:45:42.933
    Feb 27 02:45:42.970: INFO: Waiting up to 5m0s for pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b" in namespace "container-probe-2891" to be "not pending"
    Feb 27 02:45:42.972: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302581ms
    Feb 27 02:45:44.976: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006658333s
    Feb 27 02:45:44.976: INFO: Pod "busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b" satisfied condition "not pending"
    Feb 27 02:45:44.976: INFO: Started pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b in namespace container-probe-2891
    STEP: checking the pod's current state and verifying that restartCount is present 02/27/23 02:45:44.976
    Feb 27 02:45:44.980: INFO: Initial restart count of pod busybox-d4d4ff4d-739b-4dce-b640-3ab02209fe6b is 0
    STEP: deleting the pod 02/27/23 02:49:45.571
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:49:45.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2891" for this suite. 02/27/23 02:49:45.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:49:45.598
Feb 27 02:49:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:49:45.599
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:45.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:45.623
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:49:45.643
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:49:45.944
STEP: Deploying the webhook pod 02/27/23 02:49:45.953
STEP: Wait for the deployment to be ready 02/27/23 02:49:45.97
Feb 27 02:49:45.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 02:49:47.996
STEP: Verifying the service has paired with the endpoint 02/27/23 02:49:48.012
Feb 27 02:49:49.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 02/27/23 02:49:49.017
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.036
STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 02:49:49.044
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.056
STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 02:49:49.066
STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.073
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:49:49.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9241" for this suite. 02/27/23 02:49:49.171
STEP: Destroying namespace "webhook-9241-markers" for this suite. 02/27/23 02:49:49.179
------------------------------
• [3.591 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:49:45.598
    Feb 27 02:49:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:49:45.599
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:45.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:45.623
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:49:45.643
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:49:45.944
    STEP: Deploying the webhook pod 02/27/23 02:49:45.953
    STEP: Wait for the deployment to be ready 02/27/23 02:49:45.97
    Feb 27 02:49:45.978: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 02:49:47.996
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:49:48.012
    Feb 27 02:49:49.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 02/27/23 02:49:49.017
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.036
    STEP: Updating a validating webhook configuration's rules to not include the create operation 02/27/23 02:49:49.044
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.056
    STEP: Patching a validating webhook configuration's rules to include the create operation 02/27/23 02:49:49.066
    STEP: Creating a configMap that does not comply to the validation webhook rules 02/27/23 02:49:49.073
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:49:49.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9241" for this suite. 02/27/23 02:49:49.171
    STEP: Destroying namespace "webhook-9241-markers" for this suite. 02/27/23 02:49:49.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:49:49.19
Feb 27 02:49:49.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:49:49.191
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:49.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:49.22
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-9089cf15-ca01-4574-9e25-5d1f1fd6761f 02/27/23 02:49:49.228
STEP: Creating configMap with name cm-test-opt-upd-d2b8a243-be0c-4a99-bf6f-f98255d76995 02/27/23 02:49:49.233
STEP: Creating the pod 02/27/23 02:49:49.246
Feb 27 02:49:49.285: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571" in namespace "projected-7284" to be "running and ready"
Feb 27 02:49:49.301: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571": Phase="Pending", Reason="", readiness=false. Elapsed: 16.044224ms
Feb 27 02:49:49.301: INFO: The phase of Pod pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:49:51.305: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571": Phase="Running", Reason="", readiness=true. Elapsed: 2.020478985s
Feb 27 02:49:51.305: INFO: The phase of Pod pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571 is Running (Ready = true)
Feb 27 02:49:51.305: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-9089cf15-ca01-4574-9e25-5d1f1fd6761f 02/27/23 02:49:51.326
STEP: Updating configmap cm-test-opt-upd-d2b8a243-be0c-4a99-bf6f-f98255d76995 02/27/23 02:49:51.331
STEP: Creating configMap with name cm-test-opt-create-c3a417cd-1dbb-4748-91f1-4942e63f7ee4 02/27/23 02:49:51.337
STEP: waiting to observe update in volume 02/27/23 02:49:51.341
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:49:53.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7284" for this suite. 02/27/23 02:49:53.37
------------------------------
• [4.185 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:49:49.19
    Feb 27 02:49:49.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:49:49.191
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:49.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:49.22
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-9089cf15-ca01-4574-9e25-5d1f1fd6761f 02/27/23 02:49:49.228
    STEP: Creating configMap with name cm-test-opt-upd-d2b8a243-be0c-4a99-bf6f-f98255d76995 02/27/23 02:49:49.233
    STEP: Creating the pod 02/27/23 02:49:49.246
    Feb 27 02:49:49.285: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571" in namespace "projected-7284" to be "running and ready"
    Feb 27 02:49:49.301: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571": Phase="Pending", Reason="", readiness=false. Elapsed: 16.044224ms
    Feb 27 02:49:49.301: INFO: The phase of Pod pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:49:51.305: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571": Phase="Running", Reason="", readiness=true. Elapsed: 2.020478985s
    Feb 27 02:49:51.305: INFO: The phase of Pod pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571 is Running (Ready = true)
    Feb 27 02:49:51.305: INFO: Pod "pod-projected-configmaps-e26ee71c-deb2-45b4-9da1-909a5b7f5571" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-9089cf15-ca01-4574-9e25-5d1f1fd6761f 02/27/23 02:49:51.326
    STEP: Updating configmap cm-test-opt-upd-d2b8a243-be0c-4a99-bf6f-f98255d76995 02/27/23 02:49:51.331
    STEP: Creating configMap with name cm-test-opt-create-c3a417cd-1dbb-4748-91f1-4942e63f7ee4 02/27/23 02:49:51.337
    STEP: waiting to observe update in volume 02/27/23 02:49:51.341
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:49:53.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7284" for this suite. 02/27/23 02:49:53.37
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:49:53.375
Feb 27 02:49:53.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:49:53.376
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:53.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:53.399
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 02/27/23 02:49:53.401
Feb 27 02:49:53.417: INFO: Waiting up to 5m0s for pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6" in namespace "projected-2160" to be "running and ready"
Feb 27 02:49:53.422: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737249ms
Feb 27 02:49:53.422: INFO: The phase of Pod annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:49:55.427: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958283s
Feb 27 02:49:55.427: INFO: The phase of Pod annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6 is Running (Ready = true)
Feb 27 02:49:55.427: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6" satisfied condition "running and ready"
Feb 27 02:49:55.957: INFO: Successfully updated pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:49:59.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2160" for this suite. 02/27/23 02:49:59.986
------------------------------
• [SLOW TEST] [6.617 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:49:53.375
    Feb 27 02:49:53.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:49:53.376
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:49:53.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:49:53.399
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 02/27/23 02:49:53.401
    Feb 27 02:49:53.417: INFO: Waiting up to 5m0s for pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6" in namespace "projected-2160" to be "running and ready"
    Feb 27 02:49:53.422: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.737249ms
    Feb 27 02:49:53.422: INFO: The phase of Pod annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:49:55.427: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958283s
    Feb 27 02:49:55.427: INFO: The phase of Pod annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6 is Running (Ready = true)
    Feb 27 02:49:55.427: INFO: Pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6" satisfied condition "running and ready"
    Feb 27 02:49:55.957: INFO: Successfully updated pod "annotationupdate23a52e7b-62e6-4d84-8587-3fed2d1626f6"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:49:59.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2160" for this suite. 02/27/23 02:49:59.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:49:59.992
Feb 27 02:49:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 02:49:59.993
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:00.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:00.008
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 02/27/23 02:50:00.019
STEP: Patching the Job 02/27/23 02:50:00.025
STEP: Watching for Job to be patched 02/27/23 02:50:00.069
Feb 27 02:50:00.071: INFO: Event ADDED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking:]
Feb 27 02:50:00.071: INFO: Event MODIFIED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 02/27/23 02:50:00.071
STEP: Watching for Job to be updated 02/27/23 02:50:00.081
Feb 27 02:50:00.082: INFO: Event MODIFIED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:00.082: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 02/27/23 02:50:00.082
Feb 27 02:50:00.085: INFO: Job: e2e-g9qx8 as labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8]
STEP: Waiting for job to complete 02/27/23 02:50:00.085
STEP: Delete a job collection with a labelselector 02/27/23 02:50:10.09
STEP: Watching for Job to be deleted 02/27/23 02:50:10.102
Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Feb 27 02:50:10.106: INFO: Event DELETED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 02/27/23 02:50:10.106
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:10.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6876" for this suite. 02/27/23 02:50:10.114
------------------------------
• [SLOW TEST] [10.128 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:49:59.992
    Feb 27 02:49:59.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 02:49:59.993
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:00.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:00.008
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 02/27/23 02:50:00.019
    STEP: Patching the Job 02/27/23 02:50:00.025
    STEP: Watching for Job to be patched 02/27/23 02:50:00.069
    Feb 27 02:50:00.071: INFO: Event ADDED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking:]
    Feb 27 02:50:00.071: INFO: Event MODIFIED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 02/27/23 02:50:00.071
    STEP: Watching for Job to be updated 02/27/23 02:50:00.081
    Feb 27 02:50:00.082: INFO: Event MODIFIED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:00.082: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 02/27/23 02:50:00.082
    Feb 27 02:50:00.085: INFO: Job: e2e-g9qx8 as labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8]
    STEP: Waiting for job to complete 02/27/23 02:50:00.085
    STEP: Delete a job collection with a labelselector 02/27/23 02:50:10.09
    STEP: Watching for Job to be deleted 02/27/23 02:50:10.102
    Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:10.105: INFO: Event MODIFIED observed for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Feb 27 02:50:10.106: INFO: Event DELETED found for Job e2e-g9qx8 in namespace job-6876 with labels: map[e2e-g9qx8:patched e2e-job-label:e2e-g9qx8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 02/27/23 02:50:10.106
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:10.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6876" for this suite. 02/27/23 02:50:10.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:10.121
Feb 27 02:50:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:50:10.121
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:10.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:10.151
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:50:10.172
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:50:10.522
STEP: Deploying the webhook pod 02/27/23 02:50:10.578
STEP: Wait for the deployment to be ready 02/27/23 02:50:10.598
Feb 27 02:50:10.620: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 02:50:12.638
STEP: Verifying the service has paired with the endpoint 02/27/23 02:50:12.657
Feb 27 02:50:13.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 02:50:13.66
STEP: create a pod that should be updated by the webhook 02/27/23 02:50:13.674
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:13.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-590" for this suite. 02/27/23 02:50:13.787
STEP: Destroying namespace "webhook-590-markers" for this suite. 02/27/23 02:50:13.797
------------------------------
• [3.687 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:10.121
    Feb 27 02:50:10.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:50:10.121
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:10.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:10.151
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:50:10.172
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:50:10.522
    STEP: Deploying the webhook pod 02/27/23 02:50:10.578
    STEP: Wait for the deployment to be ready 02/27/23 02:50:10.598
    Feb 27 02:50:10.620: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 02:50:12.638
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:50:12.657
    Feb 27 02:50:13.657: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 02/27/23 02:50:13.66
    STEP: create a pod that should be updated by the webhook 02/27/23 02:50:13.674
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:13.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-590" for this suite. 02/27/23 02:50:13.787
    STEP: Destroying namespace "webhook-590-markers" for this suite. 02/27/23 02:50:13.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:13.808
Feb 27 02:50:13.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:50:13.809
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:13.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:13.875
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 02/27/23 02:50:13.886
Feb 27 02:50:13.886: INFO: Creating simple deployment test-deployment-qwvps
Feb 27 02:50:13.918: INFO: deployment "test-deployment-qwvps" doesn't have the required revision set
STEP: Getting /status 02/27/23 02:50:15.936
Feb 27 02:50:15.941: INFO: Deployment test-deployment-qwvps has Conditions: [{Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}]
STEP: updating Deployment Status 02/27/23 02:50:15.941
Feb 27 02:50:15.953: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 50, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qwvps-84865f9c76\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 02/27/23 02:50:15.954
Feb 27 02:50:15.955: INFO: Observed &Deployment event: ADDED
Feb 27 02:50:15.955: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qwvps-84865f9c76" is progressing.}
Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
Feb 27 02:50:15.956: INFO: Found Deployment test-deployment-qwvps in namespace deployment-9221 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 02:50:15.956: INFO: Deployment test-deployment-qwvps has an updated status
STEP: patching the Statefulset Status 02/27/23 02:50:15.956
Feb 27 02:50:15.956: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Feb 27 02:50:15.965: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 02/27/23 02:50:15.965
Feb 27 02:50:15.967: INFO: Observed &Deployment event: ADDED
Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
Feb 27 02:50:15.967: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 02:50:15.967: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qwvps-84865f9c76" is progressing.}
Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
Feb 27 02:50:15.968: INFO: Found deployment test-deployment-qwvps in namespace deployment-9221 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Feb 27 02:50:15.968: INFO: Deployment test-deployment-qwvps has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:50:15.972: INFO: Deployment "test-deployment-qwvps":
&Deployment{ObjectMeta:{test-deployment-qwvps  deployment-9221  6a744d3a-8236-490c-9c48-514b6d38f21b 59019 1 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002de7c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 02:50:15.976: INFO: New ReplicaSet "test-deployment-qwvps-84865f9c76" of Deployment "test-deployment-qwvps":
&ReplicaSet{ObjectMeta:{test-deployment-qwvps-84865f9c76  deployment-9221  75b40498-f81b-4868-b1f1-c91a89ead66d 59008 1 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qwvps 6a744d3a-8236-490c-9c48-514b6d38f21b 0xc0047b43c0 0xc0047b43c1}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a744d3a-8236-490c-9c48-514b6d38f21b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 84865f9c76,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047b4468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:50:15.979: INFO: Pod "test-deployment-qwvps-84865f9c76-cfrbt" is available:
&Pod{ObjectMeta:{test-deployment-qwvps-84865f9c76-cfrbt test-deployment-qwvps-84865f9c76- deployment-9221  230ec556-3542-40c5-8fe4-3da360b7ded2 59007 0 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.191"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-qwvps-84865f9c76 75b40498-f81b-4868-b1f1-c91a89ead66d 0xc006218cb0 0xc006218cb1}] [] [{kube-controller-manager Update v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75b40498-f81b-4868-b1f1-c91a89ead66d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:50:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbfb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbfb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.191,StartTime:2023-02-27 02:50:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:50:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://84800fc3c79a8c0098d6ed2d133fe9c773b82e9ace508527772ff0d7463cc760,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:15.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9221" for this suite. 02/27/23 02:50:15.985
------------------------------
• [2.185 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:13.808
    Feb 27 02:50:13.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:50:13.809
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:13.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:13.875
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 02/27/23 02:50:13.886
    Feb 27 02:50:13.886: INFO: Creating simple deployment test-deployment-qwvps
    Feb 27 02:50:13.918: INFO: deployment "test-deployment-qwvps" doesn't have the required revision set
    STEP: Getting /status 02/27/23 02:50:15.936
    Feb 27 02:50:15.941: INFO: Deployment test-deployment-qwvps has Conditions: [{Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}]
    STEP: updating Deployment Status 02/27/23 02:50:15.941
    Feb 27 02:50:15.953: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 50, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 50, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qwvps-84865f9c76\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 02/27/23 02:50:15.954
    Feb 27 02:50:15.955: INFO: Observed &Deployment event: ADDED
    Feb 27 02:50:15.955: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
    Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qwvps-84865f9c76" is progressing.}
    Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
    Feb 27 02:50:15.956: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 02:50:15.956: INFO: Observed Deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
    Feb 27 02:50:15.956: INFO: Found Deployment test-deployment-qwvps in namespace deployment-9221 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 02:50:15.956: INFO: Deployment test-deployment-qwvps has an updated status
    STEP: patching the Statefulset Status 02/27/23 02:50:15.956
    Feb 27 02:50:15.956: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Feb 27 02:50:15.965: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 02/27/23 02:50:15.965
    Feb 27 02:50:15.967: INFO: Observed &Deployment event: ADDED
    Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
    Feb 27 02:50:15.967: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qwvps-84865f9c76"}
    Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 02:50:15.967: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Feb 27 02:50:15.967: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:13 +0000 UTC 2023-02-27 02:50:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qwvps-84865f9c76" is progressing.}
    Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
    Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-02-27 02:50:15 +0000 UTC 2023-02-27 02:50:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qwvps-84865f9c76" has successfully progressed.}
    Feb 27 02:50:15.968: INFO: Observed deployment test-deployment-qwvps in namespace deployment-9221 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Feb 27 02:50:15.968: INFO: Observed &Deployment event: MODIFIED
    Feb 27 02:50:15.968: INFO: Found deployment test-deployment-qwvps in namespace deployment-9221 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Feb 27 02:50:15.968: INFO: Deployment test-deployment-qwvps has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:50:15.972: INFO: Deployment "test-deployment-qwvps":
    &Deployment{ObjectMeta:{test-deployment-qwvps  deployment-9221  6a744d3a-8236-490c-9c48-514b6d38f21b 59019 1 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002de7c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 02:50:15.976: INFO: New ReplicaSet "test-deployment-qwvps-84865f9c76" of Deployment "test-deployment-qwvps":
    &ReplicaSet{ObjectMeta:{test-deployment-qwvps-84865f9c76  deployment-9221  75b40498-f81b-4868-b1f1-c91a89ead66d 59008 1 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qwvps 6a744d3a-8236-490c-9c48-514b6d38f21b 0xc0047b43c0 0xc0047b43c1}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a744d3a-8236-490c-9c48-514b6d38f21b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 84865f9c76,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047b4468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:50:15.979: INFO: Pod "test-deployment-qwvps-84865f9c76-cfrbt" is available:
    &Pod{ObjectMeta:{test-deployment-qwvps-84865f9c76-cfrbt test-deployment-qwvps-84865f9c76- deployment-9221  230ec556-3542-40c5-8fe4-3da360b7ded2 59007 0 2023-02-27 02:50:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:84865f9c76] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.191"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.191"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-qwvps-84865f9c76 75b40498-f81b-4868-b1f1-c91a89ead66d 0xc006218cb0 0xc006218cb1}] [] [{kube-controller-manager Update v1 2023-02-27 02:50:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"75b40498-f81b-4868-b1f1-c91a89ead66d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 02:50:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 02:50:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbfb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbfb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:50:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.191,StartTime:2023-02-27 02:50:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:50:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://84800fc3c79a8c0098d6ed2d133fe9c773b82e9ace508527772ff0d7463cc760,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:15.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9221" for this suite. 02/27/23 02:50:15.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:15.993
Feb 27 02:50:15.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:50:15.994
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:16.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:16.014
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-9888/configmap-test-695467bf-7fe4-49fa-8d5b-f8247e3584cf 02/27/23 02:50:16.017
STEP: Creating a pod to test consume configMaps 02/27/23 02:50:16.021
Feb 27 02:50:16.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0" in namespace "configmap-9888" to be "Succeeded or Failed"
Feb 27 02:50:16.034: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015082ms
Feb 27 02:50:18.038: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0081036s
Feb 27 02:50:20.039: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009726425s
STEP: Saw pod success 02/27/23 02:50:20.039
Feb 27 02:50:20.040: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0" satisfied condition "Succeeded or Failed"
Feb 27 02:50:20.043: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 container env-test: <nil>
STEP: delete the pod 02/27/23 02:50:20.051
Feb 27 02:50:20.061: INFO: Waiting for pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 to disappear
Feb 27 02:50:20.063: INFO: Pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:20.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9888" for this suite. 02/27/23 02:50:20.067
------------------------------
• [4.080 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:15.993
    Feb 27 02:50:15.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:50:15.994
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:16.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:16.014
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-9888/configmap-test-695467bf-7fe4-49fa-8d5b-f8247e3584cf 02/27/23 02:50:16.017
    STEP: Creating a pod to test consume configMaps 02/27/23 02:50:16.021
    Feb 27 02:50:16.030: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0" in namespace "configmap-9888" to be "Succeeded or Failed"
    Feb 27 02:50:16.034: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015082ms
    Feb 27 02:50:18.038: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0081036s
    Feb 27 02:50:20.039: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009726425s
    STEP: Saw pod success 02/27/23 02:50:20.039
    Feb 27 02:50:20.040: INFO: Pod "pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0" satisfied condition "Succeeded or Failed"
    Feb 27 02:50:20.043: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 container env-test: <nil>
    STEP: delete the pod 02/27/23 02:50:20.051
    Feb 27 02:50:20.061: INFO: Waiting for pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 to disappear
    Feb 27 02:50:20.063: INFO: Pod pod-configmaps-cd000599-85b4-4d67-a7ed-3277c9373ad0 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:20.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9888" for this suite. 02/27/23 02:50:20.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:20.074
Feb 27 02:50:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename var-expansion 02/27/23 02:50:20.074
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:20.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:20.092
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 02/27/23 02:50:20.095
Feb 27 02:50:20.106: INFO: Waiting up to 5m0s for pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3" in namespace "var-expansion-5896" to be "Succeeded or Failed"
Feb 27 02:50:20.110: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670842ms
Feb 27 02:50:22.115: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009706038s
Feb 27 02:50:24.114: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008795863s
STEP: Saw pod success 02/27/23 02:50:24.114
Feb 27 02:50:24.114: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3" satisfied condition "Succeeded or Failed"
Feb 27 02:50:24.118: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 container dapi-container: <nil>
STEP: delete the pod 02/27/23 02:50:24.122
Feb 27 02:50:24.134: INFO: Waiting for pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 to disappear
Feb 27 02:50:24.137: INFO: Pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:24.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5896" for this suite. 02/27/23 02:50:24.142
------------------------------
• [4.076 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:20.074
    Feb 27 02:50:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename var-expansion 02/27/23 02:50:20.074
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:20.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:20.092
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 02/27/23 02:50:20.095
    Feb 27 02:50:20.106: INFO: Waiting up to 5m0s for pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3" in namespace "var-expansion-5896" to be "Succeeded or Failed"
    Feb 27 02:50:20.110: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670842ms
    Feb 27 02:50:22.115: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009706038s
    Feb 27 02:50:24.114: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008795863s
    STEP: Saw pod success 02/27/23 02:50:24.114
    Feb 27 02:50:24.114: INFO: Pod "var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3" satisfied condition "Succeeded or Failed"
    Feb 27 02:50:24.118: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 02:50:24.122
    Feb 27 02:50:24.134: INFO: Waiting for pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 to disappear
    Feb 27 02:50:24.137: INFO: Pod var-expansion-f0dd7b34-1e3a-43e7-876b-4ae951c8f6f3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:24.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5896" for this suite. 02/27/23 02:50:24.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:24.151
Feb 27 02:50:24.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:50:24.152
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:24.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:24.179
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Feb 27 02:50:24.220: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5c6b4d19-e0ea-4d25-9a04-5264b494a205", Controller:(*bool)(0xc005c5bab6), BlockOwnerDeletion:(*bool)(0xc005c5bab7)}}
Feb 27 02:50:24.233: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d7243fef-7034-448b-bb62-3a946e81e47c", Controller:(*bool)(0xc005c5bcfe), BlockOwnerDeletion:(*bool)(0xc005c5bcff)}}
Feb 27 02:50:24.242: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"466f33ce-cd82-41aa-b239-4a0bc80e3380", Controller:(*bool)(0xc005c5bf1e), BlockOwnerDeletion:(*bool)(0xc005c5bf1f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9994" for this suite. 02/27/23 02:50:29.265
------------------------------
• [SLOW TEST] [5.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:24.151
    Feb 27 02:50:24.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:50:24.152
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:24.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:24.179
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Feb 27 02:50:24.220: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5c6b4d19-e0ea-4d25-9a04-5264b494a205", Controller:(*bool)(0xc005c5bab6), BlockOwnerDeletion:(*bool)(0xc005c5bab7)}}
    Feb 27 02:50:24.233: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d7243fef-7034-448b-bb62-3a946e81e47c", Controller:(*bool)(0xc005c5bcfe), BlockOwnerDeletion:(*bool)(0xc005c5bcff)}}
    Feb 27 02:50:24.242: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"466f33ce-cd82-41aa-b239-4a0bc80e3380", Controller:(*bool)(0xc005c5bf1e), BlockOwnerDeletion:(*bool)(0xc005c5bf1f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9994" for this suite. 02/27/23 02:50:29.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:29.273
Feb 27 02:50:29.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:50:29.274
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:29.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:29.3
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-45a6aeaf-8c63-4cd3-b90f-528ffec06df7 02/27/23 02:50:29.308
STEP: Creating secret with name s-test-opt-upd-c75664c9-9d3b-4e0b-94ff-d02e6f632c72 02/27/23 02:50:29.314
STEP: Creating the pod 02/27/23 02:50:29.32
Feb 27 02:50:29.352: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b" in namespace "projected-5642" to be "running and ready"
Feb 27 02:50:29.355: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750867ms
Feb 27 02:50:29.355: INFO: The phase of Pod pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:50:31.360: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307929s
Feb 27 02:50:31.360: INFO: The phase of Pod pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b is Running (Ready = true)
Feb 27 02:50:31.360: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-45a6aeaf-8c63-4cd3-b90f-528ffec06df7 02/27/23 02:50:31.381
STEP: Updating secret s-test-opt-upd-c75664c9-9d3b-4e0b-94ff-d02e6f632c72 02/27/23 02:50:31.388
STEP: Creating secret with name s-test-opt-create-7d562de5-25ea-4722-9c83-4c7d3ab1e1de 02/27/23 02:50:31.392
STEP: waiting to observe update in volume 02/27/23 02:50:31.399
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:33.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5642" for this suite. 02/27/23 02:50:33.424
------------------------------
• [4.158 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:29.273
    Feb 27 02:50:29.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:50:29.274
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:29.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:29.3
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-45a6aeaf-8c63-4cd3-b90f-528ffec06df7 02/27/23 02:50:29.308
    STEP: Creating secret with name s-test-opt-upd-c75664c9-9d3b-4e0b-94ff-d02e6f632c72 02/27/23 02:50:29.314
    STEP: Creating the pod 02/27/23 02:50:29.32
    Feb 27 02:50:29.352: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b" in namespace "projected-5642" to be "running and ready"
    Feb 27 02:50:29.355: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750867ms
    Feb 27 02:50:29.355: INFO: The phase of Pod pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:50:31.360: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307929s
    Feb 27 02:50:31.360: INFO: The phase of Pod pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b is Running (Ready = true)
    Feb 27 02:50:31.360: INFO: Pod "pod-projected-secrets-918f3df0-5cf2-4cd0-9aa5-99a48837fb2b" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-45a6aeaf-8c63-4cd3-b90f-528ffec06df7 02/27/23 02:50:31.381
    STEP: Updating secret s-test-opt-upd-c75664c9-9d3b-4e0b-94ff-d02e6f632c72 02/27/23 02:50:31.388
    STEP: Creating secret with name s-test-opt-create-7d562de5-25ea-4722-9c83-4c7d3ab1e1de 02/27/23 02:50:31.392
    STEP: waiting to observe update in volume 02/27/23 02:50:31.399
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:33.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5642" for this suite. 02/27/23 02:50:33.424
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:33.431
Feb 27 02:50:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 02:50:33.432
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:33.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:33.453
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 02/27/23 02:50:33.456
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:33.473
STEP: Creating a service in the namespace 02/27/23 02:50:33.476
STEP: Deleting the namespace 02/27/23 02:50:33.503
STEP: Waiting for the namespace to be removed. 02/27/23 02:50:33.517
STEP: Recreating the namespace 02/27/23 02:50:39.521
STEP: Verifying there is no service in the namespace 02/27/23 02:50:39.544
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:39.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-608" for this suite. 02/27/23 02:50:39.56
STEP: Destroying namespace "nsdeletetest-7919" for this suite. 02/27/23 02:50:39.567
Feb 27 02:50:39.570: INFO: Namespace nsdeletetest-7919 was already deleted
STEP: Destroying namespace "nsdeletetest-9401" for this suite. 02/27/23 02:50:39.57
------------------------------
• [SLOW TEST] [6.144 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:33.431
    Feb 27 02:50:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 02:50:33.432
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:33.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:33.453
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 02/27/23 02:50:33.456
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:33.473
    STEP: Creating a service in the namespace 02/27/23 02:50:33.476
    STEP: Deleting the namespace 02/27/23 02:50:33.503
    STEP: Waiting for the namespace to be removed. 02/27/23 02:50:33.517
    STEP: Recreating the namespace 02/27/23 02:50:39.521
    STEP: Verifying there is no service in the namespace 02/27/23 02:50:39.544
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:39.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-608" for this suite. 02/27/23 02:50:39.56
    STEP: Destroying namespace "nsdeletetest-7919" for this suite. 02/27/23 02:50:39.567
    Feb 27 02:50:39.570: INFO: Namespace nsdeletetest-7919 was already deleted
    STEP: Destroying namespace "nsdeletetest-9401" for this suite. 02/27/23 02:50:39.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:39.583
Feb 27 02:50:39.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:50:39.583
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:39.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:39.616
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-85f7749b-fe1b-469f-b3a7-9ea1bc16129f 02/27/23 02:50:39.618
STEP: Creating a pod to test consume configMaps 02/27/23 02:50:39.622
Feb 27 02:50:39.649: INFO: Waiting up to 5m0s for pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10" in namespace "configmap-5680" to be "Succeeded or Failed"
Feb 27 02:50:39.652: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.266596ms
Feb 27 02:50:41.662: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01298215s
Feb 27 02:50:43.657: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008437994s
STEP: Saw pod success 02/27/23 02:50:43.657
Feb 27 02:50:43.657: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10" satisfied condition "Succeeded or Failed"
Feb 27 02:50:43.661: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:50:43.668
Feb 27 02:50:43.679: INFO: Waiting for pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 to disappear
Feb 27 02:50:43.682: INFO: Pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:43.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5680" for this suite. 02/27/23 02:50:43.686
------------------------------
• [4.109 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:39.583
    Feb 27 02:50:39.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:50:39.583
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:39.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:39.616
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-85f7749b-fe1b-469f-b3a7-9ea1bc16129f 02/27/23 02:50:39.618
    STEP: Creating a pod to test consume configMaps 02/27/23 02:50:39.622
    Feb 27 02:50:39.649: INFO: Waiting up to 5m0s for pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10" in namespace "configmap-5680" to be "Succeeded or Failed"
    Feb 27 02:50:39.652: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.266596ms
    Feb 27 02:50:41.662: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01298215s
    Feb 27 02:50:43.657: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008437994s
    STEP: Saw pod success 02/27/23 02:50:43.657
    Feb 27 02:50:43.657: INFO: Pod "pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10" satisfied condition "Succeeded or Failed"
    Feb 27 02:50:43.661: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:50:43.668
    Feb 27 02:50:43.679: INFO: Waiting for pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 to disappear
    Feb 27 02:50:43.682: INFO: Pod pod-configmaps-701a41a9-0a48-43d3-96c7-53ef2dd4af10 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:43.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5680" for this suite. 02/27/23 02:50:43.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:43.692
Feb 27 02:50:43.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 02:50:43.693
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:43.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:43.713
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da 02/27/23 02:50:43.715
Feb 27 02:50:43.725: INFO: Pod name my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Found 0 pods out of 1
Feb 27 02:50:48.729: INFO: Pod name my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Found 1 pods out of 1
Feb 27 02:50:48.729: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da" are running
Feb 27 02:50:48.729: INFO: Waiting up to 5m0s for pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" in namespace "replication-controller-1699" to be "running"
Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls": Phase="Running", Reason="", readiness=true. Elapsed: 4.422781ms
Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" satisfied condition "running"
Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:43 +0000 UTC Reason: Message:}])
Feb 27 02:50:48.734: INFO: Trying to dial the pod
Feb 27 02:50:53.746: INFO: Controller my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Got expected result from replica 1 [my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls]: "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:53.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1699" for this suite. 02/27/23 02:50:53.756
------------------------------
• [SLOW TEST] [10.073 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:43.692
    Feb 27 02:50:43.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 02:50:43.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:43.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:43.713
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da 02/27/23 02:50:43.715
    Feb 27 02:50:43.725: INFO: Pod name my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Found 0 pods out of 1
    Feb 27 02:50:48.729: INFO: Pod name my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Found 1 pods out of 1
    Feb 27 02:50:48.729: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da" are running
    Feb 27 02:50:48.729: INFO: Waiting up to 5m0s for pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" in namespace "replication-controller-1699" to be "running"
    Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls": Phase="Running", Reason="", readiness=true. Elapsed: 4.422781ms
    Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" satisfied condition "running"
    Feb 27 02:50:48.734: INFO: Pod "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-02-27 02:50:43 +0000 UTC Reason: Message:}])
    Feb 27 02:50:48.734: INFO: Trying to dial the pod
    Feb 27 02:50:53.746: INFO: Controller my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da: Got expected result from replica 1 [my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls]: "my-hostname-basic-eb6bd597-709d-493f-8a3d-c81b664dd6da-npmls", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:53.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1699" for this suite. 02/27/23 02:50:53.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:53.765
Feb 27 02:50:53.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename certificates 02/27/23 02:50:53.766
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:53.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:53.785
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 02/27/23 02:50:54.316
STEP: getting /apis/certificates.k8s.io 02/27/23 02:50:54.318
STEP: getting /apis/certificates.k8s.io/v1 02/27/23 02:50:54.319
STEP: creating 02/27/23 02:50:54.32
STEP: getting 02/27/23 02:50:54.353
STEP: listing 02/27/23 02:50:54.356
STEP: watching 02/27/23 02:50:54.385
Feb 27 02:50:54.385: INFO: starting watch
STEP: patching 02/27/23 02:50:54.397
STEP: updating 02/27/23 02:50:54.437
Feb 27 02:50:54.447: INFO: waiting for watch events with expected annotations
Feb 27 02:50:54.447: INFO: saw patched and updated annotations
STEP: getting /approval 02/27/23 02:50:54.447
STEP: patching /approval 02/27/23 02:50:54.453
STEP: updating /approval 02/27/23 02:50:54.464
STEP: getting /status 02/27/23 02:50:54.475
STEP: patching /status 02/27/23 02:50:54.48
STEP: updating /status 02/27/23 02:50:54.488
STEP: deleting 02/27/23 02:50:54.494
STEP: deleting a collection 02/27/23 02:50:54.509
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:50:54.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-626" for this suite. 02/27/23 02:50:54.533
------------------------------
• [0.777 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:53.765
    Feb 27 02:50:53.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename certificates 02/27/23 02:50:53.766
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:53.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:53.785
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 02/27/23 02:50:54.316
    STEP: getting /apis/certificates.k8s.io 02/27/23 02:50:54.318
    STEP: getting /apis/certificates.k8s.io/v1 02/27/23 02:50:54.319
    STEP: creating 02/27/23 02:50:54.32
    STEP: getting 02/27/23 02:50:54.353
    STEP: listing 02/27/23 02:50:54.356
    STEP: watching 02/27/23 02:50:54.385
    Feb 27 02:50:54.385: INFO: starting watch
    STEP: patching 02/27/23 02:50:54.397
    STEP: updating 02/27/23 02:50:54.437
    Feb 27 02:50:54.447: INFO: waiting for watch events with expected annotations
    Feb 27 02:50:54.447: INFO: saw patched and updated annotations
    STEP: getting /approval 02/27/23 02:50:54.447
    STEP: patching /approval 02/27/23 02:50:54.453
    STEP: updating /approval 02/27/23 02:50:54.464
    STEP: getting /status 02/27/23 02:50:54.475
    STEP: patching /status 02/27/23 02:50:54.48
    STEP: updating /status 02/27/23 02:50:54.488
    STEP: deleting 02/27/23 02:50:54.494
    STEP: deleting a collection 02/27/23 02:50:54.509
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:50:54.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-626" for this suite. 02/27/23 02:50:54.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:50:54.543
Feb 27 02:50:54.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:50:54.544
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:54.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:54.565
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 02/27/23 02:50:54.568
STEP: Counting existing ResourceQuota 02/27/23 02:50:59.572
STEP: Creating a ResourceQuota 02/27/23 02:51:04.577
STEP: Ensuring resource quota status is calculated 02/27/23 02:51:04.582
STEP: Creating a Secret 02/27/23 02:51:06.586
STEP: Ensuring resource quota status captures secret creation 02/27/23 02:51:06.602
STEP: Deleting a secret 02/27/23 02:51:08.606
STEP: Ensuring resource quota status released usage 02/27/23 02:51:08.615
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7169" for this suite. 02/27/23 02:51:10.626
------------------------------
• [SLOW TEST] [16.122 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:50:54.543
    Feb 27 02:50:54.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:50:54.544
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:50:54.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:50:54.565
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 02/27/23 02:50:54.568
    STEP: Counting existing ResourceQuota 02/27/23 02:50:59.572
    STEP: Creating a ResourceQuota 02/27/23 02:51:04.577
    STEP: Ensuring resource quota status is calculated 02/27/23 02:51:04.582
    STEP: Creating a Secret 02/27/23 02:51:06.586
    STEP: Ensuring resource quota status captures secret creation 02/27/23 02:51:06.602
    STEP: Deleting a secret 02/27/23 02:51:08.606
    STEP: Ensuring resource quota status released usage 02/27/23 02:51:08.615
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:10.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7169" for this suite. 02/27/23 02:51:10.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:10.665
Feb 27 02:51:10.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename subpath 02/27/23 02:51:10.666
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:10.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:10.694
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 02/27/23 02:51:10.699
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-8dzh 02/27/23 02:51:10.716
STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:51:10.716
Feb 27 02:51:10.748: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8dzh" in namespace "subpath-9856" to be "Succeeded or Failed"
Feb 27 02:51:10.751: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16912ms
Feb 27 02:51:12.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007677714s
Feb 27 02:51:14.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 4.008616855s
Feb 27 02:51:16.754: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 6.006339928s
Feb 27 02:51:18.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 8.00868552s
Feb 27 02:51:20.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 10.008463745s
Feb 27 02:51:22.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 12.007592367s
Feb 27 02:51:24.755: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 14.006837343s
Feb 27 02:51:26.755: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 16.006411708s
Feb 27 02:51:28.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 18.008034967s
Feb 27 02:51:30.758: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 20.009559737s
Feb 27 02:51:32.758: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=false. Elapsed: 22.010088227s
Feb 27 02:51:34.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008037869s
STEP: Saw pod success 02/27/23 02:51:34.756
Feb 27 02:51:34.756: INFO: Pod "pod-subpath-test-secret-8dzh" satisfied condition "Succeeded or Failed"
Feb 27 02:51:34.759: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-secret-8dzh container test-container-subpath-secret-8dzh: <nil>
STEP: delete the pod 02/27/23 02:51:34.765
Feb 27 02:51:34.783: INFO: Waiting for pod pod-subpath-test-secret-8dzh to disappear
Feb 27 02:51:34.787: INFO: Pod pod-subpath-test-secret-8dzh no longer exists
STEP: Deleting pod pod-subpath-test-secret-8dzh 02/27/23 02:51:34.787
Feb 27 02:51:34.787: INFO: Deleting pod "pod-subpath-test-secret-8dzh" in namespace "subpath-9856"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:34.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9856" for this suite. 02/27/23 02:51:34.796
------------------------------
• [SLOW TEST] [24.137 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:10.665
    Feb 27 02:51:10.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename subpath 02/27/23 02:51:10.666
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:10.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:10.694
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 02/27/23 02:51:10.699
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-8dzh 02/27/23 02:51:10.716
    STEP: Creating a pod to test atomic-volume-subpath 02/27/23 02:51:10.716
    Feb 27 02:51:10.748: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8dzh" in namespace "subpath-9856" to be "Succeeded or Failed"
    Feb 27 02:51:10.751: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16912ms
    Feb 27 02:51:12.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007677714s
    Feb 27 02:51:14.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 4.008616855s
    Feb 27 02:51:16.754: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 6.006339928s
    Feb 27 02:51:18.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 8.00868552s
    Feb 27 02:51:20.757: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 10.008463745s
    Feb 27 02:51:22.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 12.007592367s
    Feb 27 02:51:24.755: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 14.006837343s
    Feb 27 02:51:26.755: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 16.006411708s
    Feb 27 02:51:28.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 18.008034967s
    Feb 27 02:51:30.758: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=true. Elapsed: 20.009559737s
    Feb 27 02:51:32.758: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Running", Reason="", readiness=false. Elapsed: 22.010088227s
    Feb 27 02:51:34.756: INFO: Pod "pod-subpath-test-secret-8dzh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008037869s
    STEP: Saw pod success 02/27/23 02:51:34.756
    Feb 27 02:51:34.756: INFO: Pod "pod-subpath-test-secret-8dzh" satisfied condition "Succeeded or Failed"
    Feb 27 02:51:34.759: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-subpath-test-secret-8dzh container test-container-subpath-secret-8dzh: <nil>
    STEP: delete the pod 02/27/23 02:51:34.765
    Feb 27 02:51:34.783: INFO: Waiting for pod pod-subpath-test-secret-8dzh to disappear
    Feb 27 02:51:34.787: INFO: Pod pod-subpath-test-secret-8dzh no longer exists
    STEP: Deleting pod pod-subpath-test-secret-8dzh 02/27/23 02:51:34.787
    Feb 27 02:51:34.787: INFO: Deleting pod "pod-subpath-test-secret-8dzh" in namespace "subpath-9856"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:34.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9856" for this suite. 02/27/23 02:51:34.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:34.803
Feb 27 02:51:34.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 02:51:34.804
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:34.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:34.826
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 02/27/23 02:51:34.828
STEP: setting up watch 02/27/23 02:51:34.828
STEP: submitting the pod to kubernetes 02/27/23 02:51:34.933
STEP: verifying the pod is in kubernetes 02/27/23 02:51:34.968
STEP: verifying pod creation was observed 02/27/23 02:51:34.972
Feb 27 02:51:34.972: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef" in namespace "pods-6811" to be "running"
Feb 27 02:51:34.979: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.771113ms
Feb 27 02:51:36.984: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.011819594s
Feb 27 02:51:36.984: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef" satisfied condition "running"
STEP: deleting the pod gracefully 02/27/23 02:51:36.987
STEP: verifying pod deletion was observed 02/27/23 02:51:36.995
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:39.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6811" for this suite. 02/27/23 02:51:39.685
------------------------------
• [4.889 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:34.803
    Feb 27 02:51:34.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 02:51:34.804
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:34.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:34.826
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 02/27/23 02:51:34.828
    STEP: setting up watch 02/27/23 02:51:34.828
    STEP: submitting the pod to kubernetes 02/27/23 02:51:34.933
    STEP: verifying the pod is in kubernetes 02/27/23 02:51:34.968
    STEP: verifying pod creation was observed 02/27/23 02:51:34.972
    Feb 27 02:51:34.972: INFO: Waiting up to 5m0s for pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef" in namespace "pods-6811" to be "running"
    Feb 27 02:51:34.979: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.771113ms
    Feb 27 02:51:36.984: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef": Phase="Running", Reason="", readiness=true. Elapsed: 2.011819594s
    Feb 27 02:51:36.984: INFO: Pod "pod-submit-remove-2c89e700-e13d-459d-b6af-75c35d32e1ef" satisfied condition "running"
    STEP: deleting the pod gracefully 02/27/23 02:51:36.987
    STEP: verifying pod deletion was observed 02/27/23 02:51:36.995
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:39.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6811" for this suite. 02/27/23 02:51:39.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:39.692
Feb 27 02:51:39.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename server-version 02/27/23 02:51:39.693
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:39.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:39.728
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 02/27/23 02:51:39.731
STEP: Confirm major version 02/27/23 02:51:39.732
Feb 27 02:51:39.732: INFO: Major version: 1
STEP: Confirm minor version 02/27/23 02:51:39.732
Feb 27 02:51:39.732: INFO: cleanMinorVersion: 26
Feb 27 02:51:39.732: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-216" for this suite. 02/27/23 02:51:39.746
------------------------------
• [0.141 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:39.692
    Feb 27 02:51:39.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename server-version 02/27/23 02:51:39.693
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:39.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:39.728
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 02/27/23 02:51:39.731
    STEP: Confirm major version 02/27/23 02:51:39.732
    Feb 27 02:51:39.732: INFO: Major version: 1
    STEP: Confirm minor version 02/27/23 02:51:39.732
    Feb 27 02:51:39.732: INFO: cleanMinorVersion: 26
    Feb 27 02:51:39.732: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-216" for this suite. 02/27/23 02:51:39.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:39.834
Feb 27 02:51:39.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:51:39.835
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:39.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:39.858
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:51:39.86
Feb 27 02:51:39.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319" in namespace "downward-api-9359" to be "Succeeded or Failed"
Feb 27 02:51:39.878: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.935946ms
Feb 27 02:51:41.882: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010442968s
Feb 27 02:51:43.882: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010774271s
STEP: Saw pod success 02/27/23 02:51:43.882
Feb 27 02:51:43.883: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319" satisfied condition "Succeeded or Failed"
Feb 27 02:51:43.886: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 container client-container: <nil>
STEP: delete the pod 02/27/23 02:51:43.891
Feb 27 02:51:43.907: INFO: Waiting for pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 to disappear
Feb 27 02:51:43.910: INFO: Pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:43.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9359" for this suite. 02/27/23 02:51:43.914
------------------------------
• [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:39.834
    Feb 27 02:51:39.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:51:39.835
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:39.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:39.858
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:51:39.86
    Feb 27 02:51:39.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319" in namespace "downward-api-9359" to be "Succeeded or Failed"
    Feb 27 02:51:39.878: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Pending", Reason="", readiness=false. Elapsed: 5.935946ms
    Feb 27 02:51:41.882: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010442968s
    Feb 27 02:51:43.882: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010774271s
    STEP: Saw pod success 02/27/23 02:51:43.882
    Feb 27 02:51:43.883: INFO: Pod "downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319" satisfied condition "Succeeded or Failed"
    Feb 27 02:51:43.886: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:51:43.891
    Feb 27 02:51:43.907: INFO: Waiting for pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 to disappear
    Feb 27 02:51:43.910: INFO: Pod downwardapi-volume-7921b627-b83c-43d3-9e6e-ba3cfa8de319 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:43.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9359" for this suite. 02/27/23 02:51:43.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:43.922
Feb 27 02:51:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:51:43.923
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:43.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:43.947
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7044 02/27/23 02:51:43.952
STEP: creating a selector 02/27/23 02:51:43.952
STEP: Creating the service pods in kubernetes 02/27/23 02:51:43.952
Feb 27 02:51:43.952: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Feb 27 02:51:44.014: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7044" to be "running and ready"
Feb 27 02:51:44.033: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.431298ms
Feb 27 02:51:44.033: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:51:46.038: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02460523s
Feb 27 02:51:46.038: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:51:48.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023567885s
Feb 27 02:51:48.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:51:50.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023008842s
Feb 27 02:51:50.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:51:52.039: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025047193s
Feb 27 02:51:52.039: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:51:54.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023697727s
Feb 27 02:51:54.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Feb 27 02:51:56.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.023395495s
Feb 27 02:51:56.037: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Feb 27 02:51:56.037: INFO: Pod "netserver-0" satisfied condition "running and ready"
Feb 27 02:51:56.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7044" to be "running and ready"
Feb 27 02:51:56.043: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.480782ms
Feb 27 02:51:56.043: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Feb 27 02:51:56.043: INFO: Pod "netserver-1" satisfied condition "running and ready"
Feb 27 02:51:56.045: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7044" to be "running and ready"
Feb 27 02:51:56.048: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.878829ms
Feb 27 02:51:56.048: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Feb 27 02:51:56.048: INFO: Pod "netserver-2" satisfied condition "running and ready"
Feb 27 02:51:56.050: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7044" to be "running and ready"
Feb 27 02:51:56.052: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.874815ms
Feb 27 02:51:56.052: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Feb 27 02:51:56.052: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 02/27/23 02:51:56.054
W0227 02:51:56.093158      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Feb 27 02:51:56.093: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7044" to be "running"
Feb 27 02:51:56.098: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995652ms
Feb 27 02:51:58.102: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009395547s
Feb 27 02:51:58.102: INFO: Pod "test-container-pod" satisfied condition "running"
Feb 27 02:51:58.106: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7044" to be "running"
Feb 27 02:51:58.109: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.89275ms
Feb 27 02:51:58.109: INFO: Pod "host-test-container-pod" satisfied condition "running"
Feb 27 02:51:58.111: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Feb 27 02:51:58.111: INFO: Going to poll 192.168.226.87 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 27 02:51:58.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.226.87:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:51:58.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:51:58.114: INFO: ExecWithOptions: Clientset creation
Feb 27 02:51:58.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.226.87%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 02:51:58.174: INFO: Found all 1 expected endpoints: [netserver-0]
Feb 27 02:51:58.174: INFO: Going to poll 192.168.128.34 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 27 02:51:58.178: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.128.34:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:51:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:51:58.178: INFO: ExecWithOptions: Clientset creation
Feb 27 02:51:58.178: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.128.34%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 02:51:58.270: INFO: Found all 1 expected endpoints: [netserver-1]
Feb 27 02:51:58.270: INFO: Going to poll 192.168.214.164 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 27 02:51:58.274: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.214.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:51:58.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:51:58.275: INFO: ExecWithOptions: Clientset creation
Feb 27 02:51:58.275: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.214.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 02:51:58.353: INFO: Found all 1 expected endpoints: [netserver-2]
Feb 27 02:51:58.353: INFO: Going to poll 192.168.21.171 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Feb 27 02:51:58.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.21.171:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:51:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:51:58.358: INFO: ExecWithOptions: Clientset creation
Feb 27 02:51:58.358: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.21.171%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 02:51:58.426: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Feb 27 02:51:58.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7044" for this suite. 02/27/23 02:51:58.432
------------------------------
• [SLOW TEST] [14.518 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:43.922
    Feb 27 02:51:43.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pod-network-test 02/27/23 02:51:43.923
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:43.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:43.947
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7044 02/27/23 02:51:43.952
    STEP: creating a selector 02/27/23 02:51:43.952
    STEP: Creating the service pods in kubernetes 02/27/23 02:51:43.952
    Feb 27 02:51:43.952: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Feb 27 02:51:44.014: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7044" to be "running and ready"
    Feb 27 02:51:44.033: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.431298ms
    Feb 27 02:51:44.033: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:51:46.038: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02460523s
    Feb 27 02:51:46.038: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:51:48.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.023567885s
    Feb 27 02:51:48.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:51:50.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023008842s
    Feb 27 02:51:50.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:51:52.039: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025047193s
    Feb 27 02:51:52.039: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:51:54.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023697727s
    Feb 27 02:51:54.037: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Feb 27 02:51:56.037: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.023395495s
    Feb 27 02:51:56.037: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Feb 27 02:51:56.037: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Feb 27 02:51:56.040: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7044" to be "running and ready"
    Feb 27 02:51:56.043: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.480782ms
    Feb 27 02:51:56.043: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Feb 27 02:51:56.043: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Feb 27 02:51:56.045: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7044" to be "running and ready"
    Feb 27 02:51:56.048: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.878829ms
    Feb 27 02:51:56.048: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Feb 27 02:51:56.048: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Feb 27 02:51:56.050: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7044" to be "running and ready"
    Feb 27 02:51:56.052: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.874815ms
    Feb 27 02:51:56.052: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Feb 27 02:51:56.052: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 02/27/23 02:51:56.054
    W0227 02:51:56.093158      23 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Feb 27 02:51:56.093: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7044" to be "running"
    Feb 27 02:51:56.098: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995652ms
    Feb 27 02:51:58.102: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009395547s
    Feb 27 02:51:58.102: INFO: Pod "test-container-pod" satisfied condition "running"
    Feb 27 02:51:58.106: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7044" to be "running"
    Feb 27 02:51:58.109: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.89275ms
    Feb 27 02:51:58.109: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Feb 27 02:51:58.111: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Feb 27 02:51:58.111: INFO: Going to poll 192.168.226.87 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 02:51:58.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.226.87:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:51:58.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:51:58.114: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:51:58.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.226.87%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 02:51:58.174: INFO: Found all 1 expected endpoints: [netserver-0]
    Feb 27 02:51:58.174: INFO: Going to poll 192.168.128.34 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 02:51:58.178: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.128.34:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:51:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:51:58.178: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:51:58.178: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.128.34%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 02:51:58.270: INFO: Found all 1 expected endpoints: [netserver-1]
    Feb 27 02:51:58.270: INFO: Going to poll 192.168.214.164 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 02:51:58.274: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.214.164:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:51:58.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:51:58.275: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:51:58.275: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.214.164%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 02:51:58.353: INFO: Found all 1 expected endpoints: [netserver-2]
    Feb 27 02:51:58.353: INFO: Going to poll 192.168.21.171 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Feb 27 02:51:58.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.21.171:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7044 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:51:58.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:51:58.358: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:51:58.358: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7044/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.21.171%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 02:51:58.426: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:51:58.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7044" for this suite. 02/27/23 02:51:58.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:51:58.441
Feb 27 02:51:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context-test 02/27/23 02:51:58.441
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:58.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:58.461
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Feb 27 02:51:58.472: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1" in namespace "security-context-test-6009" to be "Succeeded or Failed"
Feb 27 02:51:58.475: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918364ms
Feb 27 02:52:00.479: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006609324s
Feb 27 02:52:02.482: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010040956s
Feb 27 02:52:02.482: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1" satisfied condition "Succeeded or Failed"
Feb 27 02:52:02.488: INFO: Got logs for pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:02.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6009" for this suite. 02/27/23 02:52:02.492
------------------------------
• [4.056 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:51:58.441
    Feb 27 02:51:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context-test 02/27/23 02:51:58.441
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:51:58.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:51:58.461
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Feb 27 02:51:58.472: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1" in namespace "security-context-test-6009" to be "Succeeded or Failed"
    Feb 27 02:51:58.475: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.918364ms
    Feb 27 02:52:00.479: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006609324s
    Feb 27 02:52:02.482: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010040956s
    Feb 27 02:52:02.482: INFO: Pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1" satisfied condition "Succeeded or Failed"
    Feb 27 02:52:02.488: INFO: Got logs for pod "busybox-privileged-false-aed09c6c-58e2-4aae-9758-f941d12da3a1": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:02.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6009" for this suite. 02/27/23 02:52:02.492
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:02.497
Feb 27 02:52:02.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename init-container 02/27/23 02:52:02.498
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:02.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:02.515
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 02/27/23 02:52:02.518
Feb 27 02:52:02.518: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:07.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7397" for this suite. 02/27/23 02:52:07.752
------------------------------
• [SLOW TEST] [5.263 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:02.497
    Feb 27 02:52:02.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename init-container 02/27/23 02:52:02.498
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:02.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:02.515
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 02/27/23 02:52:02.518
    Feb 27 02:52:02.518: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:07.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7397" for this suite. 02/27/23 02:52:07.752
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:07.76
Feb 27 02:52:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:52:07.761
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:07.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:07.78
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 02/27/23 02:52:07.782
Feb 27 02:52:07.816: INFO: Waiting up to 5m0s for pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1" in namespace "downward-api-4313" to be "Succeeded or Failed"
Feb 27 02:52:07.821: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.298696ms
Feb 27 02:52:09.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977337s
Feb 27 02:52:11.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009552786s
STEP: Saw pod success 02/27/23 02:52:11.826
Feb 27 02:52:11.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1" satisfied condition "Succeeded or Failed"
Feb 27 02:52:11.831: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 container dapi-container: <nil>
STEP: delete the pod 02/27/23 02:52:11.838
Feb 27 02:52:11.854: INFO: Waiting for pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 to disappear
Feb 27 02:52:11.856: INFO: Pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:11.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4313" for this suite. 02/27/23 02:52:11.861
------------------------------
• [4.108 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:07.76
    Feb 27 02:52:07.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:52:07.761
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:07.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:07.78
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 02/27/23 02:52:07.782
    Feb 27 02:52:07.816: INFO: Waiting up to 5m0s for pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1" in namespace "downward-api-4313" to be "Succeeded or Failed"
    Feb 27 02:52:07.821: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.298696ms
    Feb 27 02:52:09.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977337s
    Feb 27 02:52:11.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009552786s
    STEP: Saw pod success 02/27/23 02:52:11.826
    Feb 27 02:52:11.826: INFO: Pod "downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1" satisfied condition "Succeeded or Failed"
    Feb 27 02:52:11.831: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 02:52:11.838
    Feb 27 02:52:11.854: INFO: Waiting for pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 to disappear
    Feb 27 02:52:11.856: INFO: Pod downward-api-5c09becf-c65b-4b08-a8fd-6af1f22bb7a1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:11.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4313" for this suite. 02/27/23 02:52:11.861
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:11.868
Feb 27 02:52:11.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 02:52:11.869
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:11.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:11.887
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Feb 27 02:52:11.913: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 02:52:11.92
Feb 27 02:52:11.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:11.925: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 02:52:11.925
Feb 27 02:52:11.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:11.947: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:52:12.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:52:12.951: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 02:52:12.955
Feb 27 02:52:12.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:52:12.979: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Feb 27 02:52:13.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:13.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 02:52:13.982
Feb 27 02:52:13.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:13.992: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:52:14.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:14.997: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:52:15.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:15.997: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:52:17.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:52:17.027: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:52:17.034
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6553, will wait for the garbage collector to delete the pods 02/27/23 02:52:17.034
Feb 27 02:52:17.097: INFO: Deleting DaemonSet.extensions daemon-set took: 6.897161ms
Feb 27 02:52:17.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.952148ms
Feb 27 02:52:19.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:52:19.802: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 02:52:19.805: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60465"},"items":null}

Feb 27 02:52:19.808: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60465"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:19.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6553" for this suite. 02/27/23 02:52:19.838
------------------------------
• [SLOW TEST] [7.979 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:11.868
    Feb 27 02:52:11.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 02:52:11.869
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:11.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:11.887
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Feb 27 02:52:11.913: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 02/27/23 02:52:11.92
    Feb 27 02:52:11.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:11.925: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 02/27/23 02:52:11.925
    Feb 27 02:52:11.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:11.947: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:52:12.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:52:12.951: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 02/27/23 02:52:12.955
    Feb 27 02:52:12.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:52:12.979: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Feb 27 02:52:13.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:13.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 02/27/23 02:52:13.982
    Feb 27 02:52:13.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:13.992: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:52:14.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:14.997: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:52:15.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:15.997: INFO: Node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:52:17.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:52:17.027: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:52:17.034
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6553, will wait for the garbage collector to delete the pods 02/27/23 02:52:17.034
    Feb 27 02:52:17.097: INFO: Deleting DaemonSet.extensions daemon-set took: 6.897161ms
    Feb 27 02:52:17.198: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.952148ms
    Feb 27 02:52:19.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:52:19.802: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 02:52:19.805: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60465"},"items":null}

    Feb 27 02:52:19.808: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60465"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:19.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6553" for this suite. 02/27/23 02:52:19.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:19.847
Feb 27 02:52:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:52:19.848
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:19.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:19.872
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 02/27/23 02:52:19.874
Feb 27 02:52:19.919: INFO: Waiting up to 5m0s for pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11" in namespace "downward-api-5897" to be "Succeeded or Failed"
Feb 27 02:52:19.921: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660907ms
Feb 27 02:52:21.926: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007002599s
Feb 27 02:52:23.927: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008646643s
STEP: Saw pod success 02/27/23 02:52:23.927
Feb 27 02:52:23.927: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11" satisfied condition "Succeeded or Failed"
Feb 27 02:52:23.931: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 container dapi-container: <nil>
STEP: delete the pod 02/27/23 02:52:23.94
Feb 27 02:52:23.954: INFO: Waiting for pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 to disappear
Feb 27 02:52:23.964: INFO: Pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5897" for this suite. 02/27/23 02:52:23.969
------------------------------
• [4.127 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:19.847
    Feb 27 02:52:19.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:52:19.848
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:19.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:19.872
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 02/27/23 02:52:19.874
    Feb 27 02:52:19.919: INFO: Waiting up to 5m0s for pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11" in namespace "downward-api-5897" to be "Succeeded or Failed"
    Feb 27 02:52:19.921: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.660907ms
    Feb 27 02:52:21.926: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007002599s
    Feb 27 02:52:23.927: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008646643s
    STEP: Saw pod success 02/27/23 02:52:23.927
    Feb 27 02:52:23.927: INFO: Pod "downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11" satisfied condition "Succeeded or Failed"
    Feb 27 02:52:23.931: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 container dapi-container: <nil>
    STEP: delete the pod 02/27/23 02:52:23.94
    Feb 27 02:52:23.954: INFO: Waiting for pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 to disappear
    Feb 27 02:52:23.964: INFO: Pod downward-api-57ef3240-5f62-4931-9d69-85e950ccaf11 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5897" for this suite. 02/27/23 02:52:23.969
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:23.975
Feb 27 02:52:23.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:52:23.975
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:23.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:23.992
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:52:24.007
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:52:24.443
STEP: Deploying the webhook pod 02/27/23 02:52:24.455
STEP: Wait for the deployment to be ready 02/27/23 02:52:24.472
Feb 27 02:52:24.478: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 02:52:26.493
STEP: Verifying the service has paired with the endpoint 02/27/23 02:52:26.515
Feb 27 02:52:27.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 02:52:27.523
Feb 27 02:52:27.552: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 02:52:27.66
STEP: Creating a dummy validating-webhook-configuration object 02/27/23 02:52:27.673
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 02:52:27.684
STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 02:52:27.691
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 02:52:27.7
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1846" for this suite. 02/27/23 02:52:27.802
STEP: Destroying namespace "webhook-1846-markers" for this suite. 02/27/23 02:52:27.811
------------------------------
• [3.854 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:23.975
    Feb 27 02:52:23.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:52:23.975
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:23.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:23.992
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:52:24.007
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:52:24.443
    STEP: Deploying the webhook pod 02/27/23 02:52:24.455
    STEP: Wait for the deployment to be ready 02/27/23 02:52:24.472
    Feb 27 02:52:24.478: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 02:52:26.493
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:52:26.515
    Feb 27 02:52:27.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 02:52:27.523
    Feb 27 02:52:27.552: INFO: Waiting for webhook configuration to be ready...
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 02/27/23 02:52:27.66
    STEP: Creating a dummy validating-webhook-configuration object 02/27/23 02:52:27.673
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 02/27/23 02:52:27.684
    STEP: Creating a dummy mutating-webhook-configuration object 02/27/23 02:52:27.691
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 02/27/23 02:52:27.7
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1846" for this suite. 02/27/23 02:52:27.802
    STEP: Destroying namespace "webhook-1846-markers" for this suite. 02/27/23 02:52:27.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:27.83
Feb 27 02:52:27.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:52:27.831
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:27.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:27.85
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Feb 27 02:52:27.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 create -f -'
Feb 27 02:52:28.586: INFO: stderr: ""
Feb 27 02:52:28.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Feb 27 02:52:28.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 create -f -'
Feb 27 02:52:28.746: INFO: stderr: ""
Feb 27 02:52:28.746: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 02/27/23 02:52:28.746
Feb 27 02:52:29.752: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:52:29.752: INFO: Found 0 / 1
Feb 27 02:52:30.750: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:52:30.750: INFO: Found 1 / 1
Feb 27 02:52:30.750: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 27 02:52:30.752: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 27 02:52:30.752: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 27 02:52:30.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe pod agnhost-primary-nfp8l'
Feb 27 02:52:30.827: INFO: stderr: ""
Feb 27 02:52:30.827: INFO: stdout: "Name:             agnhost-primary-nfp8l\nNamespace:        kubectl-575\nPriority:         0\nService Account:  default\nNode:             worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins/10.0.10.3\nStart Time:       Mon, 27 Feb 2023 02:52:28 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.214.153\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.214.153\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               192.168.214.153\nIPs:\n  IP:           192.168.214.153\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://aa60a1aa08c469af7809e04dcf16f6aa8e30c2e85c69f4c6f9edf3f72f2e300d\n    Image:          armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\n    Image ID:       armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 02:52:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-75rvq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-75rvq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-575/agnhost-primary-nfp8l to worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins\n  Normal  AddedInterface  1s    multus             Add eth0 [192.168.214.153/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Feb 27 02:52:30.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe rc agnhost-primary'
Feb 27 02:52:30.898: INFO: stderr: ""
Feb 27 02:52:30.898: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-575\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-nfp8l\n"
Feb 27 02:52:30.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe service agnhost-primary'
Feb 27 02:52:30.976: INFO: stderr: ""
Feb 27 02:52:30.976: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-575\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.217.145\nIPs:               10.100.217.145\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.214.153:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 27 02:52:30.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe node master-0-n92-ci-ibd-23-jenkins'
Feb 27 02:52:31.099: INFO: stderr: ""
Feb 27 02:52:31.099: INFO: stdout: "Name:               master-0-n92-ci-ibd-23-jenkins\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ccd.master.wd\n                    beta.kubernetes.io/os=linux\n                    ccd/version=2.25.0\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0-n92-ci-ibd-23-jenkins\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ccd.master.wd\n                    node.uuid=947eda8c-2f47-409b-8bf0-0ee12458bce3\n                    node.uuid_source=cloud-init\n                    topology.cinder.csi.openstack.org/zone=nova\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"947eda8c-2f47-409b-8bf0-0ee12458bce3\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 00:48:21 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-0-n92-ci-ibd-23-jenkins\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 02:52:27 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Feb 2023 00:54:56 +0000   Mon, 27 Feb 2023 00:54:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:54:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.10.12\n  Hostname:    master-0-n92-ci-ibd-23-jenkins\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25125520Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3045248Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23155679194\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2942848Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 947eda8c2f47409b8bf00ee12458bce3\n  System UUID:                947eda8c-2f47-409b-8bf0-0ee12458bce3\n  Boot ID:                    4c9084b5-2b3c-4079-95fd-e25d4cf2d09e\n  Kernel Version:             5.14.21-150400.24.46-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   openstack:///947eda8c-2f47-409b-8bf0-0ee12458bce3\nNon-terminated Pods:          (17 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-69f6d96cc5-mlp6g                   0 (0%)        0 (0%)      100Mi (3%)       1Gi (35%)      117m\n  kube-system                 calico-node-4hx9c                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 coredns-6f595d4ffd-n6czc                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     116m\n  kube-system                 csi-cinder-controllerplugin-84849d4b58-99mzt               0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 csi-cinder-nodeplugin-s2kvg                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-apiserver-master-0-n92-ci-ibd-23-jenkins              250m (12%)    0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kube-controller-manager-master-0-n92-ci-ibd-23-jenkins     200m (10%)    0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kube-multus-ds-amd64-krmtz                                 100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      117m\n  kube-system                 kube-proxy-mmr49                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         123m\n  kube-system                 kube-scheduler-master-0-n92-ci-ibd-23-jenkins              100m (5%)     0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kucero-rh8rb                                               100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      115m\n  kube-system                 node-local-dns-j6j4w                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         121m\n  kube-system                 openstack-cloud-controller-manager-2xtkf                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 subport-controller-xzx6s                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  monitoring                  eric-pm-node-exporter-x7lzj                                100m (5%)     400m (20%)  100Mi (3%)       500Mi (17%)    101m\n  monitoring                  node-cert-exporter-dsjcm                                   100m (5%)     250m (12%)  128Mi (4%)       256Mi (8%)     100m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-r44s5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1525m (76%)  850m (42%)\n  memory             503Mi (17%)  2050Mi (71%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Feb 27 02:52:31.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe namespace kubectl-575'
Feb 27 02:52:31.177: INFO: stderr: ""
Feb 27 02:52:31.177: INFO: stdout: "Name:         kubectl-575\nLabels:       e2e-framework=kubectl\n              e2e-run=66932bf2-ce63-47ef-bda3-5ca4c4ddd6c8\n              kubernetes.io/metadata.name=kubectl-575\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-575" for this suite. 02/27/23 02:52:31.181
------------------------------
• [3.356 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:27.83
    Feb 27 02:52:27.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:52:27.831
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:27.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:27.85
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Feb 27 02:52:27.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 create -f -'
    Feb 27 02:52:28.586: INFO: stderr: ""
    Feb 27 02:52:28.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Feb 27 02:52:28.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 create -f -'
    Feb 27 02:52:28.746: INFO: stderr: ""
    Feb 27 02:52:28.746: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 02/27/23 02:52:28.746
    Feb 27 02:52:29.752: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:52:29.752: INFO: Found 0 / 1
    Feb 27 02:52:30.750: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:52:30.750: INFO: Found 1 / 1
    Feb 27 02:52:30.750: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Feb 27 02:52:30.752: INFO: Selector matched 1 pods for map[app:agnhost]
    Feb 27 02:52:30.752: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Feb 27 02:52:30.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe pod agnhost-primary-nfp8l'
    Feb 27 02:52:30.827: INFO: stderr: ""
    Feb 27 02:52:30.827: INFO: stdout: "Name:             agnhost-primary-nfp8l\nNamespace:        kubectl-575\nPriority:         0\nService Account:  default\nNode:             worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins/10.0.10.3\nStart Time:       Mon, 27 Feb 2023 02:52:28 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.214.153\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"192.168.214.153\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               192.168.214.153\nIPs:\n  IP:           192.168.214.153\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://aa60a1aa08c469af7809e04dcf16f6aa8e30c2e85c69f4c6f9edf3f72f2e300d\n    Image:          armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\n    Image ID:       armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Feb 2023 02:52:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-75rvq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-75rvq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-575/agnhost-primary-nfp8l to worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins\n  Normal  AddedInterface  1s    multus             Add eth0 [192.168.214.153/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Feb 27 02:52:30.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe rc agnhost-primary'
    Feb 27 02:52:30.898: INFO: stderr: ""
    Feb 27 02:52:30.898: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-575\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-nfp8l\n"
    Feb 27 02:52:30.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe service agnhost-primary'
    Feb 27 02:52:30.976: INFO: stderr: ""
    Feb 27 02:52:30.976: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-575\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.217.145\nIPs:               10.100.217.145\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.214.153:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Feb 27 02:52:30.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe node master-0-n92-ci-ibd-23-jenkins'
    Feb 27 02:52:31.099: INFO: stderr: ""
    Feb 27 02:52:31.099: INFO: stdout: "Name:               master-0-n92-ci-ibd-23-jenkins\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ccd.master.wd\n                    beta.kubernetes.io/os=linux\n                    ccd/version=2.25.0\n                    failure-domain.beta.kubernetes.io/region=RegionOne\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-0-n92-ci-ibd-23-jenkins\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=ccd.master.wd\n                    node.uuid=947eda8c-2f47-409b-8bf0-0ee12458bce3\n                    node.uuid_source=cloud-init\n                    topology.cinder.csi.openstack.org/zone=nova\n                    topology.kubernetes.io/region=RegionOne\n                    topology.kubernetes.io/zone=nova\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cinder.csi.openstack.org\":\"947eda8c-2f47-409b-8bf0-0ee12458bce3\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 27 Feb 2023 00:48:21 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-0-n92-ci-ibd-23-jenkins\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Feb 2023 02:52:27 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 27 Feb 2023 00:54:56 +0000   Mon, 27 Feb 2023 00:54:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:48:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 27 Feb 2023 02:48:39 +0000   Mon, 27 Feb 2023 00:54:49 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.10.12\n  Hostname:    master-0-n92-ci-ibd-23-jenkins\nCapacity:\n  cpu:                2\n  ephemeral-storage:  25125520Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3045248Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  23155679194\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2942848Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 947eda8c2f47409b8bf00ee12458bce3\n  System UUID:                947eda8c-2f47-409b-8bf0-0ee12458bce3\n  Boot ID:                    4c9084b5-2b3c-4079-95fd-e25d4cf2d09e\n  Kernel Version:             5.14.21-150400.24.46-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP4\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.12\n  Kubelet Version:            v1.26.1\n  Kube-Proxy Version:         v1.26.1\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nProviderID:                   openstack:///947eda8c-2f47-409b-8bf0-0ee12458bce3\nNon-terminated Pods:          (17 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-69f6d96cc5-mlp6g                   0 (0%)        0 (0%)      100Mi (3%)       1Gi (35%)      117m\n  kube-system                 calico-node-4hx9c                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 coredns-6f595d4ffd-n6czc                                   100m (5%)     0 (0%)      70Mi (2%)        170Mi (5%)     116m\n  kube-system                 csi-cinder-controllerplugin-84849d4b58-99mzt               0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 csi-cinder-nodeplugin-s2kvg                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-apiserver-master-0-n92-ci-ibd-23-jenkins              250m (12%)    0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kube-controller-manager-master-0-n92-ci-ibd-23-jenkins     200m (10%)    0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kube-multus-ds-amd64-krmtz                                 100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      117m\n  kube-system                 kube-proxy-mmr49                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         123m\n  kube-system                 kube-scheduler-master-0-n92-ci-ibd-23-jenkins              100m (5%)     0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                 kucero-rh8rb                                               100m (5%)     100m (5%)   50Mi (1%)        50Mi (1%)      115m\n  kube-system                 node-local-dns-j6j4w                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         121m\n  kube-system                 openstack-cloud-controller-manager-2xtkf                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 subport-controller-xzx6s                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  monitoring                  eric-pm-node-exporter-x7lzj                                100m (5%)     400m (20%)  100Mi (3%)       500Mi (17%)    101m\n  monitoring                  node-cert-exporter-dsjcm                                   100m (5%)     250m (12%)  128Mi (4%)       256Mi (8%)     100m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-r44s5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1525m (76%)  850m (42%)\n  memory             503Mi (17%)  2050Mi (71%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Feb 27 02:52:31.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-575 describe namespace kubectl-575'
    Feb 27 02:52:31.177: INFO: stderr: ""
    Feb 27 02:52:31.177: INFO: stdout: "Name:         kubectl-575\nLabels:       e2e-framework=kubectl\n              e2e-run=66932bf2-ce63-47ef-bda3-5ca4c4ddd6c8\n              kubernetes.io/metadata.name=kubectl-575\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:31.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-575" for this suite. 02/27/23 02:52:31.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:31.187
Feb 27 02:52:31.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:52:31.188
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:31.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:31.211
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 02:52:31.214
Feb 27 02:52:31.248: INFO: Waiting up to 5m0s for pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59" in namespace "emptydir-3806" to be "Succeeded or Failed"
Feb 27 02:52:31.253: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901391ms
Feb 27 02:52:33.258: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009975064s
Feb 27 02:52:35.257: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00845279s
STEP: Saw pod success 02/27/23 02:52:35.257
Feb 27 02:52:35.257: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59" satisfied condition "Succeeded or Failed"
Feb 27 02:52:35.261: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 container test-container: <nil>
STEP: delete the pod 02/27/23 02:52:35.265
Feb 27 02:52:35.276: INFO: Waiting for pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 to disappear
Feb 27 02:52:35.279: INFO: Pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:52:35.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3806" for this suite. 02/27/23 02:52:35.285
------------------------------
• [4.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:31.187
    Feb 27 02:52:31.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:52:31.188
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:31.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:31.211
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 02/27/23 02:52:31.214
    Feb 27 02:52:31.248: INFO: Waiting up to 5m0s for pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59" in namespace "emptydir-3806" to be "Succeeded or Failed"
    Feb 27 02:52:31.253: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.901391ms
    Feb 27 02:52:33.258: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009975064s
    Feb 27 02:52:35.257: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00845279s
    STEP: Saw pod success 02/27/23 02:52:35.257
    Feb 27 02:52:35.257: INFO: Pod "pod-52686401-aefe-4ab1-b41d-ccbc1d612e59" satisfied condition "Succeeded or Failed"
    Feb 27 02:52:35.261: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:52:35.265
    Feb 27 02:52:35.276: INFO: Waiting for pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 to disappear
    Feb 27 02:52:35.279: INFO: Pod pod-52686401-aefe-4ab1-b41d-ccbc1d612e59 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:52:35.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3806" for this suite. 02/27/23 02:52:35.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:52:35.292
Feb 27 02:52:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename statefulset 02/27/23 02:52:35.293
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:35.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:35.312
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2806 02/27/23 02:52:35.315
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 02:52:35.321
STEP: Creating stateful set ss in namespace statefulset-2806 02/27/23 02:52:35.324
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2806 02/27/23 02:52:35.331
Feb 27 02:52:35.335: INFO: Found 0 stateful pods, waiting for 1
Feb 27 02:52:45.340: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 02:52:45.34
Feb 27 02:52:45.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:52:45.475: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:52:45.475: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:52:45.475: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 02:52:45.479: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 27 02:52:55.487: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 02:52:55.487: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:52:55.501: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999832s
Feb 27 02:52:56.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99773086s
Feb 27 02:52:57.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993591277s
Feb 27 02:52:58.513: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989687309s
Feb 27 02:52:59.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985972568s
Feb 27 02:53:00.529: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974682338s
Feb 27 02:53:01.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970368283s
Feb 27 02:53:02.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966608148s
Feb 27 02:53:03.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960913034s
Feb 27 02:53:04.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.088663ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2806 02/27/23 02:53:05.546
Feb 27 02:53:05.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:53:05.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:53:05.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:53:05.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 02:53:05.685: INFO: Found 1 stateful pods, waiting for 3
Feb 27 02:53:15.691: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:53:15.692: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 27 02:53:15.692: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 02/27/23 02:53:15.692
STEP: Scale down will halt with unhealthy stateful pod 02/27/23 02:53:15.692
Feb 27 02:53:15.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:53:15.920: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:53:15.920: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:53:15.920: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 02:53:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:53:16.056: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:53:16.056: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:53:16.056: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 02:53:16.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 27 02:53:16.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 27 02:53:16.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 27 02:53:16.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 27 02:53:16.207: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:53:16.211: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 27 02:53:26.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 02:53:26.218: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 02:53:26.218: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 27 02:53:26.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999671s
Feb 27 02:53:27.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993511195s
Feb 27 02:53:28.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989044525s
Feb 27 02:53:29.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985232968s
Feb 27 02:53:30.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981439454s
Feb 27 02:53:31.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977343513s
Feb 27 02:53:32.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970735687s
Feb 27 02:53:33.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965142288s
Feb 27 02:53:34.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960850596s
Feb 27 02:53:35.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.992019ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2806 02/27/23 02:53:36.279
Feb 27 02:53:36.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:53:36.425: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:53:36.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:53:36.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 02:53:36.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:53:36.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:53:36.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:53:36.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 02:53:36.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 27 02:53:36.720: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 27 02:53:36.720: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 27 02:53:36.720: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 27 02:53:36.720: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 02:53:46.739
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Feb 27 02:53:46.739: INFO: Deleting all statefulset in ns statefulset-2806
Feb 27 02:53:46.742: INFO: Scaling statefulset ss to 0
Feb 27 02:53:46.750: INFO: Waiting for statefulset status.replicas updated to 0
Feb 27 02:53:46.752: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Feb 27 02:53:46.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2806" for this suite. 02/27/23 02:53:46.771
------------------------------
• [SLOW TEST] [71.492 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:52:35.292
    Feb 27 02:52:35.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename statefulset 02/27/23 02:52:35.293
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:52:35.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:52:35.312
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2806 02/27/23 02:52:35.315
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 02/27/23 02:52:35.321
    STEP: Creating stateful set ss in namespace statefulset-2806 02/27/23 02:52:35.324
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2806 02/27/23 02:52:35.331
    Feb 27 02:52:35.335: INFO: Found 0 stateful pods, waiting for 1
    Feb 27 02:52:45.340: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 02/27/23 02:52:45.34
    Feb 27 02:52:45.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:52:45.475: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:52:45.475: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:52:45.475: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 02:52:45.479: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Feb 27 02:52:55.487: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 02:52:55.487: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:52:55.501: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999832s
    Feb 27 02:52:56.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99773086s
    Feb 27 02:52:57.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993591277s
    Feb 27 02:52:58.513: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989687309s
    Feb 27 02:52:59.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985972568s
    Feb 27 02:53:00.529: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974682338s
    Feb 27 02:53:01.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970368283s
    Feb 27 02:53:02.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966608148s
    Feb 27 02:53:03.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960913034s
    Feb 27 02:53:04.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.088663ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2806 02/27/23 02:53:05.546
    Feb 27 02:53:05.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:53:05.680: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:53:05.680: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:53:05.680: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 02:53:05.685: INFO: Found 1 stateful pods, waiting for 3
    Feb 27 02:53:15.691: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:53:15.692: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Feb 27 02:53:15.692: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 02/27/23 02:53:15.692
    STEP: Scale down will halt with unhealthy stateful pod 02/27/23 02:53:15.692
    Feb 27 02:53:15.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:53:15.920: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:53:15.920: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:53:15.920: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 02:53:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:53:16.056: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:53:16.056: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:53:16.056: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 02:53:16.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Feb 27 02:53:16.207: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Feb 27 02:53:16.207: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Feb 27 02:53:16.207: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Feb 27 02:53:16.207: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:53:16.211: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Feb 27 02:53:26.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 02:53:26.218: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 02:53:26.218: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Feb 27 02:53:26.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999671s
    Feb 27 02:53:27.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993511195s
    Feb 27 02:53:28.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989044525s
    Feb 27 02:53:29.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985232968s
    Feb 27 02:53:30.254: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981439454s
    Feb 27 02:53:31.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977343513s
    Feb 27 02:53:32.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970735687s
    Feb 27 02:53:33.271: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965142288s
    Feb 27 02:53:34.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.960850596s
    Feb 27 02:53:35.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 956.992019ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2806 02/27/23 02:53:36.279
    Feb 27 02:53:36.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:53:36.425: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:53:36.425: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:53:36.425: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 02:53:36.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:53:36.565: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:53:36.565: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:53:36.565: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 02:53:36.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=statefulset-2806 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Feb 27 02:53:36.720: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Feb 27 02:53:36.720: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Feb 27 02:53:36.720: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Feb 27 02:53:36.720: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 02/27/23 02:53:46.739
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Feb 27 02:53:46.739: INFO: Deleting all statefulset in ns statefulset-2806
    Feb 27 02:53:46.742: INFO: Scaling statefulset ss to 0
    Feb 27 02:53:46.750: INFO: Waiting for statefulset status.replicas updated to 0
    Feb 27 02:53:46.752: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:53:46.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2806" for this suite. 02/27/23 02:53:46.771
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:53:46.784
Feb 27 02:53:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:53:46.785
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:46.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:46.806
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Feb 27 02:53:46.840: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2437 to be scheduled
Feb 27 02:53:46.848: INFO: 1 pods are not scheduled: [runtimeclass-2437/test-runtimeclass-runtimeclass-2437-preconfigured-handler-hwbgh(c9aaccf5-f8a5-4d84-a3e2-e954efb40772)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 02:53:48.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2437" for this suite. 02/27/23 02:53:48.866
------------------------------
• [2.089 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:53:46.784
    Feb 27 02:53:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:53:46.785
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:46.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:46.806
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Feb 27 02:53:46.840: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2437 to be scheduled
    Feb 27 02:53:46.848: INFO: 1 pods are not scheduled: [runtimeclass-2437/test-runtimeclass-runtimeclass-2437-preconfigured-handler-hwbgh(c9aaccf5-f8a5-4d84-a3e2-e954efb40772)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:53:48.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2437" for this suite. 02/27/23 02:53:48.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:53:48.876
Feb 27 02:53:48.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 02:53:48.877
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:48.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:48.896
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 02/27/23 02:53:48.899
Feb 27 02:53:48.910: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2875" to be "running and ready"
Feb 27 02:53:48.914: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097189ms
Feb 27 02:53:48.914: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:53:50.921: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010944314s
Feb 27 02:53:50.921: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Feb 27 02:53:50.921: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 02/27/23 02:53:50.925
Feb 27 02:53:50.941: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2875" to be "container debugger running"
Feb 27 02:53:50.947: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.691123ms
Feb 27 02:53:52.952: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010929892s
Feb 27 02:53:54.953: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011835459s
Feb 27 02:53:54.953: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 02/27/23 02:53:54.953
Feb 27 02:53:54.953: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2875 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 02:53:54.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 02:53:54.953: INFO: ExecWithOptions: Clientset creation
Feb 27 02:53:54.953: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2875/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Feb 27 02:53:55.014: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:53:55.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2875" for this suite. 02/27/23 02:53:55.024
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:53:48.876
    Feb 27 02:53:48.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename ephemeral-containers-test 02/27/23 02:53:48.877
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:48.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:48.896
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 02/27/23 02:53:48.899
    Feb 27 02:53:48.910: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2875" to be "running and ready"
    Feb 27 02:53:48.914: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.097189ms
    Feb 27 02:53:48.914: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:53:50.921: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010944314s
    Feb 27 02:53:50.921: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Feb 27 02:53:50.921: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 02/27/23 02:53:50.925
    Feb 27 02:53:50.941: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2875" to be "container debugger running"
    Feb 27 02:53:50.947: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.691123ms
    Feb 27 02:53:52.952: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010929892s
    Feb 27 02:53:54.953: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011835459s
    Feb 27 02:53:54.953: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 02/27/23 02:53:54.953
    Feb 27 02:53:54.953: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2875 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 02:53:54.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 02:53:54.953: INFO: ExecWithOptions: Clientset creation
    Feb 27 02:53:54.953: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-2875/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Feb 27 02:53:55.014: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:53:55.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2875" for this suite. 02/27/23 02:53:55.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:53:55.029
Feb 27 02:53:55.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:53:55.03
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:55.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:55.047
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Feb 27 02:53:55.061: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 27 02:54:55.137: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 02/27/23 02:54:55.14
Feb 27 02:54:55.183: INFO: Created pod: pod0-0-sched-preemption-low-priority
Feb 27 02:54:55.195: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Feb 27 02:54:55.225: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Feb 27 02:54:55.235: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Feb 27 02:54:55.266: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Feb 27 02:54:55.278: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Feb 27 02:54:55.301: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Feb 27 02:54:55.316: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 02/27/23 02:54:55.316
Feb 27 02:54:55.316: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:55.325: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.503566ms
Feb 27 02:54:57.330: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013543273s
Feb 27 02:54:57.330: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Feb 27 02:54:57.330: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.341: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.413933ms
Feb 27 02:54:57.341: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.341: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.345: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.389237ms
Feb 27 02:54:57.345: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.345: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.348: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.950891ms
Feb 27 02:54:57.348: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.348: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.351: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.806927ms
Feb 27 02:54:57.351: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.351: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.353: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.582539ms
Feb 27 02:54:57.353: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.353: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.356: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.258959ms
Feb 27 02:54:57.356: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Feb 27 02:54:57.356: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.358: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.434149ms
Feb 27 02:54:57.358: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 02:54:57.358
Feb 27 02:54:57.366: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8538" to be "running"
Feb 27 02:54:57.370: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844298ms
Feb 27 02:54:59.374: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008271673s
Feb 27 02:55:01.380: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014301895s
Feb 27 02:55:03.374: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007773376s
Feb 27 02:55:03.374: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:03.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8538" for this suite. 02/27/23 02:55:03.458
------------------------------
• [SLOW TEST] [68.435 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:53:55.029
    Feb 27 02:53:55.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-preemption 02/27/23 02:53:55.03
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:53:55.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:53:55.047
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Feb 27 02:53:55.061: INFO: Waiting up to 1m0s for all nodes to be ready
    Feb 27 02:54:55.137: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 02/27/23 02:54:55.14
    Feb 27 02:54:55.183: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Feb 27 02:54:55.195: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Feb 27 02:54:55.225: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Feb 27 02:54:55.235: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Feb 27 02:54:55.266: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Feb 27 02:54:55.278: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Feb 27 02:54:55.301: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Feb 27 02:54:55.316: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 02/27/23 02:54:55.316
    Feb 27 02:54:55.316: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:55.325: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.503566ms
    Feb 27 02:54:57.330: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013543273s
    Feb 27 02:54:57.330: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Feb 27 02:54:57.330: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.341: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.413933ms
    Feb 27 02:54:57.341: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.341: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.345: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.389237ms
    Feb 27 02:54:57.345: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.345: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.348: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.950891ms
    Feb 27 02:54:57.348: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.348: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.351: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.806927ms
    Feb 27 02:54:57.351: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.351: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.353: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.582539ms
    Feb 27 02:54:57.353: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.353: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.356: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.258959ms
    Feb 27 02:54:57.356: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Feb 27 02:54:57.356: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.358: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.434149ms
    Feb 27 02:54:57.358: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 02/27/23 02:54:57.358
    Feb 27 02:54:57.366: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8538" to be "running"
    Feb 27 02:54:57.370: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.844298ms
    Feb 27 02:54:59.374: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008271673s
    Feb 27 02:55:01.380: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014301895s
    Feb 27 02:55:03.374: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007773376s
    Feb 27 02:55:03.374: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:03.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8538" for this suite. 02/27/23 02:55:03.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:03.466
Feb 27 02:55:03.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename emptydir 02/27/23 02:55:03.467
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:03.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:03.488
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 02:55:03.492
Feb 27 02:55:03.529: INFO: Waiting up to 5m0s for pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f" in namespace "emptydir-8196" to be "Succeeded or Failed"
Feb 27 02:55:03.532: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180145ms
Feb 27 02:55:05.536: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006704885s
Feb 27 02:55:07.536: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006950556s
STEP: Saw pod success 02/27/23 02:55:07.536
Feb 27 02:55:07.537: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f" satisfied condition "Succeeded or Failed"
Feb 27 02:55:07.539: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f container test-container: <nil>
STEP: delete the pod 02/27/23 02:55:07.545
Feb 27 02:55:07.556: INFO: Waiting for pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f to disappear
Feb 27 02:55:07.558: INFO: Pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:07.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8196" for this suite. 02/27/23 02:55:07.563
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:03.466
    Feb 27 02:55:03.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename emptydir 02/27/23 02:55:03.467
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:03.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:03.488
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 02/27/23 02:55:03.492
    Feb 27 02:55:03.529: INFO: Waiting up to 5m0s for pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f" in namespace "emptydir-8196" to be "Succeeded or Failed"
    Feb 27 02:55:03.532: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.180145ms
    Feb 27 02:55:05.536: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006704885s
    Feb 27 02:55:07.536: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006950556s
    STEP: Saw pod success 02/27/23 02:55:07.536
    Feb 27 02:55:07.537: INFO: Pod "pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f" satisfied condition "Succeeded or Failed"
    Feb 27 02:55:07.539: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f container test-container: <nil>
    STEP: delete the pod 02/27/23 02:55:07.545
    Feb 27 02:55:07.556: INFO: Waiting for pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f to disappear
    Feb 27 02:55:07.558: INFO: Pod pod-41c572dc-c6ae-42e8-ba3b-86ac9800a81f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:07.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8196" for this suite. 02/27/23 02:55:07.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:07.568
Feb 27 02:55:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 02:55:07.569
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:07.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:07.591
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8169.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 02/27/23 02:55:07.594
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8169.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 02/27/23 02:55:07.594
STEP: creating a pod to probe /etc/hosts 02/27/23 02:55:07.594
STEP: submitting the pod to kubernetes 02/27/23 02:55:07.594
Feb 27 02:55:07.604: INFO: Waiting up to 15m0s for pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6" in namespace "dns-8169" to be "running"
Feb 27 02:55:07.607: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48623ms
Feb 27 02:55:09.611: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006692918s
Feb 27 02:55:09.611: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:55:09.611
STEP: looking for the results for each expected name from probers 02/27/23 02:55:09.613
Feb 27 02:55:09.625: INFO: DNS probes using dns-8169/dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6 succeeded

STEP: deleting the pod 02/27/23 02:55:09.625
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:09.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8169" for this suite. 02/27/23 02:55:09.647
------------------------------
• [2.086 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:07.568
    Feb 27 02:55:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 02:55:07.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:07.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:07.591
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8169.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     02/27/23 02:55:07.594
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8169.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8169.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     02/27/23 02:55:07.594
    STEP: creating a pod to probe /etc/hosts 02/27/23 02:55:07.594
    STEP: submitting the pod to kubernetes 02/27/23 02:55:07.594
    Feb 27 02:55:07.604: INFO: Waiting up to 15m0s for pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6" in namespace "dns-8169" to be "running"
    Feb 27 02:55:07.607: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48623ms
    Feb 27 02:55:09.611: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006692918s
    Feb 27 02:55:09.611: INFO: Pod "dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:55:09.611
    STEP: looking for the results for each expected name from probers 02/27/23 02:55:09.613
    Feb 27 02:55:09.625: INFO: DNS probes using dns-8169/dns-test-1e30eb53-8910-4112-9e2b-b18f154c1fd6 succeeded

    STEP: deleting the pod 02/27/23 02:55:09.625
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:09.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8169" for this suite. 02/27/23 02:55:09.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:09.654
Feb 27 02:55:09.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 02:55:09.655
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:09.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:09.678
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 02:55:09.691
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:55:09.972
STEP: Deploying the webhook pod 02/27/23 02:55:09.979
STEP: Wait for the deployment to be ready 02/27/23 02:55:09.989
Feb 27 02:55:09.996: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 02/27/23 02:55:12.01
STEP: Verifying the service has paired with the endpoint 02/27/23 02:55:12.024
Feb 27 02:55:13.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Feb 27 02:55:13.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2015-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 02:55:13.541
STEP: Creating a custom resource while v1 is storage version 02/27/23 02:55:13.564
STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 02:55:15.623
STEP: Patching the custom resource while v2 is storage version 02/27/23 02:55:15.642
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:16.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3214" for this suite. 02/27/23 02:55:16.338
STEP: Destroying namespace "webhook-3214-markers" for this suite. 02/27/23 02:55:16.35
------------------------------
• [SLOW TEST] [6.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:09.654
    Feb 27 02:55:09.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 02:55:09.655
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:09.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:09.678
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 02:55:09.691
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 02:55:09.972
    STEP: Deploying the webhook pod 02/27/23 02:55:09.979
    STEP: Wait for the deployment to be ready 02/27/23 02:55:09.989
    Feb 27 02:55:09.996: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 02/27/23 02:55:12.01
    STEP: Verifying the service has paired with the endpoint 02/27/23 02:55:12.024
    Feb 27 02:55:13.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Feb 27 02:55:13.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2015-crds.webhook.example.com via the AdmissionRegistration API 02/27/23 02:55:13.541
    STEP: Creating a custom resource while v1 is storage version 02/27/23 02:55:13.564
    STEP: Patching Custom Resource Definition to set v2 as storage 02/27/23 02:55:15.623
    STEP: Patching the custom resource while v2 is storage version 02/27/23 02:55:15.642
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:16.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3214" for this suite. 02/27/23 02:55:16.338
    STEP: Destroying namespace "webhook-3214-markers" for this suite. 02/27/23 02:55:16.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:16.373
Feb 27 02:55:16.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:55:16.374
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:16.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:16.404
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-aaf6e85f-69e5-4658-aab7-1965c4270c59 02/27/23 02:55:16.407
STEP: Creating a pod to test consume configMaps 02/27/23 02:55:16.418
Feb 27 02:55:16.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68" in namespace "projected-3198" to be "Succeeded or Failed"
Feb 27 02:55:16.452: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912318ms
Feb 27 02:55:18.457: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009870788s
Feb 27 02:55:20.459: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01112076s
STEP: Saw pod success 02/27/23 02:55:20.459
Feb 27 02:55:20.459: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68" satisfied condition "Succeeded or Failed"
Feb 27 02:55:20.464: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:55:20.471
Feb 27 02:55:20.485: INFO: Waiting for pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 to disappear
Feb 27 02:55:20.488: INFO: Pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:20.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3198" for this suite. 02/27/23 02:55:20.491
------------------------------
• [4.123 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:16.373
    Feb 27 02:55:16.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:55:16.374
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:16.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:16.404
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-aaf6e85f-69e5-4658-aab7-1965c4270c59 02/27/23 02:55:16.407
    STEP: Creating a pod to test consume configMaps 02/27/23 02:55:16.418
    Feb 27 02:55:16.447: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68" in namespace "projected-3198" to be "Succeeded or Failed"
    Feb 27 02:55:16.452: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912318ms
    Feb 27 02:55:18.457: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009870788s
    Feb 27 02:55:20.459: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01112076s
    STEP: Saw pod success 02/27/23 02:55:20.459
    Feb 27 02:55:20.459: INFO: Pod "pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68" satisfied condition "Succeeded or Failed"
    Feb 27 02:55:20.464: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:55:20.471
    Feb 27 02:55:20.485: INFO: Waiting for pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 to disappear
    Feb 27 02:55:20.488: INFO: Pod pod-projected-configmaps-c797c2db-0f19-4bad-a66d-634e327a7c68 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:20.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3198" for this suite. 02/27/23 02:55:20.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:20.496
Feb 27 02:55:20.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:55:20.497
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:20.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:20.514
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:20.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9208" for this suite. 02/27/23 02:55:20.527
------------------------------
• [0.035 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:20.496
    Feb 27 02:55:20.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 02:55:20.497
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:20.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:20.514
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:20.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9208" for this suite. 02/27/23 02:55:20.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:20.532
Feb 27 02:55:20.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 02:55:20.533
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:20.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:20.553
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 02/27/23 02:55:20.556
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:20.561
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:20.561
STEP: creating a pod to probe DNS 02/27/23 02:55:20.561
STEP: submitting the pod to kubernetes 02/27/23 02:55:20.561
Feb 27 02:55:20.573: INFO: Waiting up to 15m0s for pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098" in namespace "dns-8356" to be "running"
Feb 27 02:55:20.579: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046736ms
Feb 27 02:55:22.583: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098": Phase="Running", Reason="", readiness=true. Elapsed: 2.009959284s
Feb 27 02:55:22.583: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:55:22.583
STEP: looking for the results for each expected name from probers 02/27/23 02:55:22.617
Feb 27 02:55:22.627: INFO: DNS probes using dns-test-4a612803-0a8f-4a7f-b872-2020ea354098 succeeded

STEP: deleting the pod 02/27/23 02:55:22.627
STEP: changing the externalName to bar.example.com 02/27/23 02:55:22.641
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:22.65
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:22.65
STEP: creating a second pod to probe DNS 02/27/23 02:55:22.65
STEP: submitting the pod to kubernetes 02/27/23 02:55:22.65
Feb 27 02:55:22.659: INFO: Waiting up to 15m0s for pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74" in namespace "dns-8356" to be "running"
Feb 27 02:55:22.661: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229399ms
Feb 27 02:55:24.666: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74": Phase="Running", Reason="", readiness=true. Elapsed: 2.007214056s
Feb 27 02:55:24.666: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:55:24.666
STEP: looking for the results for each expected name from probers 02/27/23 02:55:24.67
Feb 27 02:55:24.675: INFO: File wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 02:55:24.679: INFO: File jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 02:55:24.679: INFO: Lookups using dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 failed for: [wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local]

Feb 27 02:55:29.685: INFO: File wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 02:55:29.688: INFO: File jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 27 02:55:29.688: INFO: Lookups using dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 failed for: [wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local]

Feb 27 02:55:34.688: INFO: DNS probes using dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 succeeded

STEP: deleting the pod 02/27/23 02:55:34.688
STEP: changing the service to type=ClusterIP 02/27/23 02:55:34.702
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:34.719
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
 02/27/23 02:55:34.719
STEP: creating a third pod to probe DNS 02/27/23 02:55:34.719
STEP: submitting the pod to kubernetes 02/27/23 02:55:34.722
Feb 27 02:55:34.765: INFO: Waiting up to 15m0s for pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e" in namespace "dns-8356" to be "running"
Feb 27 02:55:34.768: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.747313ms
Feb 27 02:55:36.774: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008480609s
Feb 27 02:55:36.774: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:55:36.774
STEP: looking for the results for each expected name from probers 02/27/23 02:55:36.78
Feb 27 02:55:36.811: INFO: DNS probes using dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e succeeded

STEP: deleting the pod 02/27/23 02:55:36.811
STEP: deleting the test externalName service 02/27/23 02:55:36.831
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:36.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8356" for this suite. 02/27/23 02:55:36.886
------------------------------
• [SLOW TEST] [16.362 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:20.532
    Feb 27 02:55:20.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 02:55:20.533
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:20.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:20.553
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 02/27/23 02:55:20.556
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:20.561
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:20.561
    STEP: creating a pod to probe DNS 02/27/23 02:55:20.561
    STEP: submitting the pod to kubernetes 02/27/23 02:55:20.561
    Feb 27 02:55:20.573: INFO: Waiting up to 15m0s for pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098" in namespace "dns-8356" to be "running"
    Feb 27 02:55:20.579: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046736ms
    Feb 27 02:55:22.583: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098": Phase="Running", Reason="", readiness=true. Elapsed: 2.009959284s
    Feb 27 02:55:22.583: INFO: Pod "dns-test-4a612803-0a8f-4a7f-b872-2020ea354098" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:55:22.583
    STEP: looking for the results for each expected name from probers 02/27/23 02:55:22.617
    Feb 27 02:55:22.627: INFO: DNS probes using dns-test-4a612803-0a8f-4a7f-b872-2020ea354098 succeeded

    STEP: deleting the pod 02/27/23 02:55:22.627
    STEP: changing the externalName to bar.example.com 02/27/23 02:55:22.641
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:22.65
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:22.65
    STEP: creating a second pod to probe DNS 02/27/23 02:55:22.65
    STEP: submitting the pod to kubernetes 02/27/23 02:55:22.65
    Feb 27 02:55:22.659: INFO: Waiting up to 15m0s for pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74" in namespace "dns-8356" to be "running"
    Feb 27 02:55:22.661: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.229399ms
    Feb 27 02:55:24.666: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74": Phase="Running", Reason="", readiness=true. Elapsed: 2.007214056s
    Feb 27 02:55:24.666: INFO: Pod "dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:55:24.666
    STEP: looking for the results for each expected name from probers 02/27/23 02:55:24.67
    Feb 27 02:55:24.675: INFO: File wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 02:55:24.679: INFO: File jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 02:55:24.679: INFO: Lookups using dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 failed for: [wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local]

    Feb 27 02:55:29.685: INFO: File wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 02:55:29.688: INFO: File jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local from pod  dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Feb 27 02:55:29.688: INFO: Lookups using dns-8356/dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 failed for: [wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local]

    Feb 27 02:55:34.688: INFO: DNS probes using dns-test-a858704e-f31e-4857-8cc0-a8f9cd5e2e74 succeeded

    STEP: deleting the pod 02/27/23 02:55:34.688
    STEP: changing the service to type=ClusterIP 02/27/23 02:55:34.702
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:34.719
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8356.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8356.svc.cluster.local; sleep 1; done
     02/27/23 02:55:34.719
    STEP: creating a third pod to probe DNS 02/27/23 02:55:34.719
    STEP: submitting the pod to kubernetes 02/27/23 02:55:34.722
    Feb 27 02:55:34.765: INFO: Waiting up to 15m0s for pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e" in namespace "dns-8356" to be "running"
    Feb 27 02:55:34.768: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.747313ms
    Feb 27 02:55:36.774: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008480609s
    Feb 27 02:55:36.774: INFO: Pod "dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:55:36.774
    STEP: looking for the results for each expected name from probers 02/27/23 02:55:36.78
    Feb 27 02:55:36.811: INFO: DNS probes using dns-test-295d89b5-1209-4ec8-a136-af473ed7c06e succeeded

    STEP: deleting the pod 02/27/23 02:55:36.811
    STEP: deleting the test externalName service 02/27/23 02:55:36.831
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:36.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8356" for this suite. 02/27/23 02:55:36.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:36.894
Feb 27 02:55:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename controllerrevisions 02/27/23 02:55:36.895
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:36.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:36.932
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-tg272-daemon-set" 02/27/23 02:55:36.968
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:55:36.974
Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:36.991: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
Feb 27 02:55:36.991: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:38.001: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
Feb 27 02:55:38.001: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:55:39.000: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 4
Feb 27 02:55:39.000: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-tg272-daemon-set
STEP: Confirm DaemonSet "e2e-tg272-daemon-set" successfully created with "daemonset-name=e2e-tg272-daemon-set" label 02/27/23 02:55:39.003
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tg272-daemon-set" 02/27/23 02:55:39.011
Feb 27 02:55:39.015: INFO: Located ControllerRevision: "e2e-tg272-daemon-set-6f64875598"
STEP: Patching ControllerRevision "e2e-tg272-daemon-set-6f64875598" 02/27/23 02:55:39.018
Feb 27 02:55:39.025: INFO: e2e-tg272-daemon-set-6f64875598 has been patched
STEP: Create a new ControllerRevision 02/27/23 02:55:39.025
Feb 27 02:55:39.030: INFO: Created ControllerRevision: e2e-tg272-daemon-set-58d6f5fd8
STEP: Confirm that there are two ControllerRevisions 02/27/23 02:55:39.03
Feb 27 02:55:39.031: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 02:55:39.034: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-tg272-daemon-set-6f64875598" 02/27/23 02:55:39.034
STEP: Confirm that there is only one ControllerRevision 02/27/23 02:55:39.04
Feb 27 02:55:39.040: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 02:55:39.043: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-tg272-daemon-set-58d6f5fd8" 02/27/23 02:55:39.045
Feb 27 02:55:39.053: INFO: e2e-tg272-daemon-set-58d6f5fd8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 02:55:39.053
W0227 02:55:39.059598      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 02/27/23 02:55:39.059
Feb 27 02:55:39.059: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 02:55:40.062: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 02:55:40.065: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tg272-daemon-set-58d6f5fd8=updated" 02/27/23 02:55:40.065
STEP: Confirm that there is only one ControllerRevision 02/27/23 02:55:40.073
Feb 27 02:55:40.073: INFO: Requesting list of ControllerRevisions to confirm quantity
Feb 27 02:55:40.075: INFO: Found 1 ControllerRevisions
Feb 27 02:55:40.077: INFO: ControllerRevision "e2e-tg272-daemon-set-7bb94d9549" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-tg272-daemon-set" 02/27/23 02:55:40.08
STEP: deleting DaemonSet.extensions e2e-tg272-daemon-set in namespace controllerrevisions-3860, will wait for the garbage collector to delete the pods 02/27/23 02:55:40.08
Feb 27 02:55:40.141: INFO: Deleting DaemonSet.extensions e2e-tg272-daemon-set took: 7.911422ms
Feb 27 02:55:40.241: INFO: Terminating DaemonSet.extensions e2e-tg272-daemon-set pods took: 100.091288ms
Feb 27 02:55:41.944: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
Feb 27 02:55:41.944: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tg272-daemon-set
Feb 27 02:55:41.947: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62657"},"items":null}

Feb 27 02:55:41.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62657"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:41.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3860" for this suite. 02/27/23 02:55:41.974
------------------------------
• [SLOW TEST] [5.086 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:36.894
    Feb 27 02:55:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename controllerrevisions 02/27/23 02:55:36.895
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:36.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:36.932
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-tg272-daemon-set" 02/27/23 02:55:36.968
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:55:36.974
    Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:36.988: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:36.991: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
    Feb 27 02:55:36.991: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:37.998: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:38.001: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
    Feb 27 02:55:38.001: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:38.997: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:55:39.000: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 4
    Feb 27 02:55:39.000: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-tg272-daemon-set
    STEP: Confirm DaemonSet "e2e-tg272-daemon-set" successfully created with "daemonset-name=e2e-tg272-daemon-set" label 02/27/23 02:55:39.003
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-tg272-daemon-set" 02/27/23 02:55:39.011
    Feb 27 02:55:39.015: INFO: Located ControllerRevision: "e2e-tg272-daemon-set-6f64875598"
    STEP: Patching ControllerRevision "e2e-tg272-daemon-set-6f64875598" 02/27/23 02:55:39.018
    Feb 27 02:55:39.025: INFO: e2e-tg272-daemon-set-6f64875598 has been patched
    STEP: Create a new ControllerRevision 02/27/23 02:55:39.025
    Feb 27 02:55:39.030: INFO: Created ControllerRevision: e2e-tg272-daemon-set-58d6f5fd8
    STEP: Confirm that there are two ControllerRevisions 02/27/23 02:55:39.03
    Feb 27 02:55:39.031: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 02:55:39.034: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-tg272-daemon-set-6f64875598" 02/27/23 02:55:39.034
    STEP: Confirm that there is only one ControllerRevision 02/27/23 02:55:39.04
    Feb 27 02:55:39.040: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 02:55:39.043: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-tg272-daemon-set-58d6f5fd8" 02/27/23 02:55:39.045
    Feb 27 02:55:39.053: INFO: e2e-tg272-daemon-set-58d6f5fd8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 02/27/23 02:55:39.053
    W0227 02:55:39.059598      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 02/27/23 02:55:39.059
    Feb 27 02:55:39.059: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 02:55:40.062: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 02:55:40.065: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-tg272-daemon-set-58d6f5fd8=updated" 02/27/23 02:55:40.065
    STEP: Confirm that there is only one ControllerRevision 02/27/23 02:55:40.073
    Feb 27 02:55:40.073: INFO: Requesting list of ControllerRevisions to confirm quantity
    Feb 27 02:55:40.075: INFO: Found 1 ControllerRevisions
    Feb 27 02:55:40.077: INFO: ControllerRevision "e2e-tg272-daemon-set-7bb94d9549" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-tg272-daemon-set" 02/27/23 02:55:40.08
    STEP: deleting DaemonSet.extensions e2e-tg272-daemon-set in namespace controllerrevisions-3860, will wait for the garbage collector to delete the pods 02/27/23 02:55:40.08
    Feb 27 02:55:40.141: INFO: Deleting DaemonSet.extensions e2e-tg272-daemon-set took: 7.911422ms
    Feb 27 02:55:40.241: INFO: Terminating DaemonSet.extensions e2e-tg272-daemon-set pods took: 100.091288ms
    Feb 27 02:55:41.944: INFO: Number of nodes with available pods controlled by daemonset e2e-tg272-daemon-set: 0
    Feb 27 02:55:41.944: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-tg272-daemon-set
    Feb 27 02:55:41.947: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"62657"},"items":null}

    Feb 27 02:55:41.950: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"62657"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:41.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3860" for this suite. 02/27/23 02:55:41.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:41.981
Feb 27 02:55:41.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:55:41.982
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:41.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:42.001
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 02/27/23 02:55:42.008
STEP: delete the rc 02/27/23 02:55:47.016
STEP: wait for the rc to be deleted 02/27/23 02:55:47.024
Feb 27 02:55:48.041: INFO: 80 pods remaining
Feb 27 02:55:48.041: INFO: 80 pods has nil DeletionTimestamp
Feb 27 02:55:48.041: INFO: 
Feb 27 02:55:49.036: INFO: 71 pods remaining
Feb 27 02:55:49.036: INFO: 71 pods has nil DeletionTimestamp
Feb 27 02:55:49.036: INFO: 
Feb 27 02:55:50.038: INFO: 60 pods remaining
Feb 27 02:55:50.038: INFO: 60 pods has nil DeletionTimestamp
Feb 27 02:55:50.038: INFO: 
Feb 27 02:55:51.053: INFO: 40 pods remaining
Feb 27 02:55:51.053: INFO: 40 pods has nil DeletionTimestamp
Feb 27 02:55:51.053: INFO: 
Feb 27 02:55:52.034: INFO: 31 pods remaining
Feb 27 02:55:52.034: INFO: 30 pods has nil DeletionTimestamp
Feb 27 02:55:52.034: INFO: 
Feb 27 02:55:53.031: INFO: 20 pods remaining
Feb 27 02:55:53.031: INFO: 20 pods has nil DeletionTimestamp
Feb 27 02:55:53.031: INFO: 
STEP: Gathering metrics 02/27/23 02:55:54.051
Feb 27 02:55:54.083: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 02:55:54.086: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.678536ms
Feb 27 02:55:54.086: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 02:55:54.086: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 02:55:54.151: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 02:55:54.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8766" for this suite. 02/27/23 02:55:54.161
------------------------------
• [SLOW TEST] [12.185 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:41.981
    Feb 27 02:55:41.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:55:41.982
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:41.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:42.001
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 02/27/23 02:55:42.008
    STEP: delete the rc 02/27/23 02:55:47.016
    STEP: wait for the rc to be deleted 02/27/23 02:55:47.024
    Feb 27 02:55:48.041: INFO: 80 pods remaining
    Feb 27 02:55:48.041: INFO: 80 pods has nil DeletionTimestamp
    Feb 27 02:55:48.041: INFO: 
    Feb 27 02:55:49.036: INFO: 71 pods remaining
    Feb 27 02:55:49.036: INFO: 71 pods has nil DeletionTimestamp
    Feb 27 02:55:49.036: INFO: 
    Feb 27 02:55:50.038: INFO: 60 pods remaining
    Feb 27 02:55:50.038: INFO: 60 pods has nil DeletionTimestamp
    Feb 27 02:55:50.038: INFO: 
    Feb 27 02:55:51.053: INFO: 40 pods remaining
    Feb 27 02:55:51.053: INFO: 40 pods has nil DeletionTimestamp
    Feb 27 02:55:51.053: INFO: 
    Feb 27 02:55:52.034: INFO: 31 pods remaining
    Feb 27 02:55:52.034: INFO: 30 pods has nil DeletionTimestamp
    Feb 27 02:55:52.034: INFO: 
    Feb 27 02:55:53.031: INFO: 20 pods remaining
    Feb 27 02:55:53.031: INFO: 20 pods has nil DeletionTimestamp
    Feb 27 02:55:53.031: INFO: 
    STEP: Gathering metrics 02/27/23 02:55:54.051
    Feb 27 02:55:54.083: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 02:55:54.086: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 2.678536ms
    Feb 27 02:55:54.086: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 02:55:54.086: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 02:55:54.151: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:55:54.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8766" for this suite. 02/27/23 02:55:54.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:55:54.167
Feb 27 02:55:54.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context-test 02/27/23 02:55:54.167
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:54.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:54.195
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Feb 27 02:55:54.236: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1" in namespace "security-context-test-2261" to be "Succeeded or Failed"
Feb 27 02:55:54.239: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186978ms
Feb 27 02:55:56.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0064608s
Feb 27 02:55:58.244: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007978395s
Feb 27 02:56:00.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007223969s
Feb 27 02:56:00.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:00.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2261" for this suite. 02/27/23 02:56:00.25
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:55:54.167
    Feb 27 02:55:54.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context-test 02/27/23 02:55:54.167
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:55:54.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:55:54.195
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Feb 27 02:55:54.236: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1" in namespace "security-context-test-2261" to be "Succeeded or Failed"
    Feb 27 02:55:54.239: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186978ms
    Feb 27 02:55:56.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0064608s
    Feb 27 02:55:58.244: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007978395s
    Feb 27 02:56:00.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007223969s
    Feb 27 02:56:00.243: INFO: Pod "busybox-readonly-false-526e8e80-04ae-44bb-8ddb-34d2312c77c1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:00.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2261" for this suite. 02/27/23 02:56:00.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:00.258
Feb 27 02:56:00.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:56:00.259
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:00.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:00.287
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 02/27/23 02:56:00.29
STEP: Creating a ResourceQuota 02/27/23 02:56:05.295
STEP: Ensuring resource quota status is calculated 02/27/23 02:56:05.301
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:07.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8782" for this suite. 02/27/23 02:56:07.312
------------------------------
• [SLOW TEST] [7.060 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:00.258
    Feb 27 02:56:00.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:56:00.259
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:00.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:00.287
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 02/27/23 02:56:00.29
    STEP: Creating a ResourceQuota 02/27/23 02:56:05.295
    STEP: Ensuring resource quota status is calculated 02/27/23 02:56:05.301
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:07.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8782" for this suite. 02/27/23 02:56:07.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:07.319
Feb 27 02:56:07.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:56:07.32
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:07.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:07.344
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:56:07.347
Feb 27 02:56:07.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216" in namespace "projected-6035" to be "Succeeded or Failed"
Feb 27 02:56:07.378: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801532ms
Feb 27 02:56:09.383: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00853948s
Feb 27 02:56:11.382: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007359412s
STEP: Saw pod success 02/27/23 02:56:11.382
Feb 27 02:56:11.382: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216" satisfied condition "Succeeded or Failed"
Feb 27 02:56:11.385: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 container client-container: <nil>
STEP: delete the pod 02/27/23 02:56:11.39
Feb 27 02:56:11.399: INFO: Waiting for pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 to disappear
Feb 27 02:56:11.402: INFO: Pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:11.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6035" for this suite. 02/27/23 02:56:11.406
------------------------------
• [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:07.319
    Feb 27 02:56:07.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:56:07.32
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:07.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:07.344
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:56:07.347
    Feb 27 02:56:07.375: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216" in namespace "projected-6035" to be "Succeeded or Failed"
    Feb 27 02:56:07.378: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801532ms
    Feb 27 02:56:09.383: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00853948s
    Feb 27 02:56:11.382: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007359412s
    STEP: Saw pod success 02/27/23 02:56:11.382
    Feb 27 02:56:11.382: INFO: Pod "downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216" satisfied condition "Succeeded or Failed"
    Feb 27 02:56:11.385: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 container client-container: <nil>
    STEP: delete the pod 02/27/23 02:56:11.39
    Feb 27 02:56:11.399: INFO: Waiting for pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 to disappear
    Feb 27 02:56:11.402: INFO: Pod downwardapi-volume-23d7c3f2-5cc6-4c7f-a73d-5c4283352216 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:11.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6035" for this suite. 02/27/23 02:56:11.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:11.412
Feb 27 02:56:11.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 02:56:11.413
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:11.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:11.434
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 02/27/23 02:56:11.437
STEP: submitting the pod to kubernetes 02/27/23 02:56:11.438
Feb 27 02:56:11.446: INFO: Waiting up to 5m0s for pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" in namespace "pods-7513" to be "running and ready"
Feb 27 02:56:11.450: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200666ms
Feb 27 02:56:11.450: INFO: The phase of Pod pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:56:13.454: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007713454s
Feb 27 02:56:13.454: INFO: The phase of Pod pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9 is Running (Ready = true)
Feb 27 02:56:13.454: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 02:56:13.457
STEP: updating the pod 02/27/23 02:56:13.46
Feb 27 02:56:13.974: INFO: Successfully updated pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9"
Feb 27 02:56:13.974: INFO: Waiting up to 5m0s for pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" in namespace "pods-7513" to be "running"
Feb 27 02:56:13.977: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.767178ms
Feb 27 02:56:13.977: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 02/27/23 02:56:13.977
Feb 27 02:56:13.980: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:13.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7513" for this suite. 02/27/23 02:56:13.984
------------------------------
• [2.578 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:11.412
    Feb 27 02:56:11.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 02:56:11.413
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:11.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:11.434
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 02/27/23 02:56:11.437
    STEP: submitting the pod to kubernetes 02/27/23 02:56:11.438
    Feb 27 02:56:11.446: INFO: Waiting up to 5m0s for pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" in namespace "pods-7513" to be "running and ready"
    Feb 27 02:56:11.450: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200666ms
    Feb 27 02:56:11.450: INFO: The phase of Pod pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:56:13.454: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007713454s
    Feb 27 02:56:13.454: INFO: The phase of Pod pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9 is Running (Ready = true)
    Feb 27 02:56:13.454: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 02:56:13.457
    STEP: updating the pod 02/27/23 02:56:13.46
    Feb 27 02:56:13.974: INFO: Successfully updated pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9"
    Feb 27 02:56:13.974: INFO: Waiting up to 5m0s for pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" in namespace "pods-7513" to be "running"
    Feb 27 02:56:13.977: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.767178ms
    Feb 27 02:56:13.977: INFO: Pod "pod-update-a5df6af4-954c-4de1-a4b4-f164be9a6ff9" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 02/27/23 02:56:13.977
    Feb 27 02:56:13.980: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:13.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7513" for this suite. 02/27/23 02:56:13.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:13.991
Feb 27 02:56:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename downward-api 02/27/23 02:56:13.992
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:14.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:14.013
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 02/27/23 02:56:14.016
Feb 27 02:56:14.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc" in namespace "downward-api-5232" to be "Succeeded or Failed"
Feb 27 02:56:14.041: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.231488ms
Feb 27 02:56:16.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018414545s
Feb 27 02:56:18.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018085854s
STEP: Saw pod success 02/27/23 02:56:18.046
Feb 27 02:56:18.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc" satisfied condition "Succeeded or Failed"
Feb 27 02:56:18.049: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc container client-container: <nil>
STEP: delete the pod 02/27/23 02:56:18.054
Feb 27 02:56:18.101: INFO: Waiting for pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc to disappear
Feb 27 02:56:18.138: INFO: Pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5232" for this suite. 02/27/23 02:56:18.148
------------------------------
• [4.168 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:13.991
    Feb 27 02:56:13.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename downward-api 02/27/23 02:56:13.992
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:14.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:14.013
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 02/27/23 02:56:14.016
    Feb 27 02:56:14.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc" in namespace "downward-api-5232" to be "Succeeded or Failed"
    Feb 27 02:56:14.041: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.231488ms
    Feb 27 02:56:16.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018414545s
    Feb 27 02:56:18.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018085854s
    STEP: Saw pod success 02/27/23 02:56:18.046
    Feb 27 02:56:18.046: INFO: Pod "downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc" satisfied condition "Succeeded or Failed"
    Feb 27 02:56:18.049: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc container client-container: <nil>
    STEP: delete the pod 02/27/23 02:56:18.054
    Feb 27 02:56:18.101: INFO: Waiting for pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc to disappear
    Feb 27 02:56:18.138: INFO: Pod downwardapi-volume-e1f05d05-bc42-491d-a7d9-f52ed23625fc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5232" for this suite. 02/27/23 02:56:18.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:18.159
Feb 27 02:56:18.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename daemonsets 02/27/23 02:56:18.16
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:18.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:18.187
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 02/27/23 02:56:18.208
STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:56:18.217
Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:18.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:56:18.224: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:19.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Feb 27 02:56:19.232: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:20.234: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.235: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.235: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 02:56:20.239: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 02:56:20.242
Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 02:56:20.268: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:21.275: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:21.281: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:21.281: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:21.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 02:56:21.286: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:22.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 02:56:22.280: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:23.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Feb 27 02:56:23.278: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 27 02:56:24.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Feb 27 02:56:24.277: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:56:24.279
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4531, will wait for the garbage collector to delete the pods 02/27/23 02:56:24.279
Feb 27 02:56:24.347: INFO: Deleting DaemonSet.extensions daemon-set took: 14.362034ms
Feb 27 02:56:24.448: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.328991ms
Feb 27 02:56:26.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Feb 27 02:56:26.852: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Feb 27 02:56:26.854: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65447"},"items":null}

Feb 27 02:56:26.857: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65447"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:26.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4531" for this suite. 02/27/23 02:56:26.875
------------------------------
• [SLOW TEST] [8.721 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:18.159
    Feb 27 02:56:18.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename daemonsets 02/27/23 02:56:18.16
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:18.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:18.187
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 02/27/23 02:56:18.208
    STEP: Check that daemon pods launch on every node of the cluster. 02/27/23 02:56:18.217
    Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:18.222: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:18.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:56:18.224: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:19.229: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:19.232: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Feb 27 02:56:19.232: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:20.234: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.235: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.235: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 02:56:20.239: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 02/27/23 02:56:20.242
    Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.265: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:20.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 02:56:20.268: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:21.275: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:21.281: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:21.281: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:21.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 02:56:21.286: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:22.275: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:22.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 02:56:22.280: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:23.274: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:23.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Feb 27 02:56:23.278: INFO: Node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins is running 0 daemon pod, expected 1
    Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-0-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-1-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:24.274: INFO: DaemonSet pods can't tolerate node master-2-n92-ci-ibd-23-jenkins with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Feb 27 02:56:24.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Feb 27 02:56:24.277: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 02/27/23 02:56:24.279
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4531, will wait for the garbage collector to delete the pods 02/27/23 02:56:24.279
    Feb 27 02:56:24.347: INFO: Deleting DaemonSet.extensions daemon-set took: 14.362034ms
    Feb 27 02:56:24.448: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.328991ms
    Feb 27 02:56:26.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Feb 27 02:56:26.852: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Feb 27 02:56:26.854: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"65447"},"items":null}

    Feb 27 02:56:26.857: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"65447"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:26.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4531" for this suite. 02/27/23 02:56:26.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:26.881
Feb 27 02:56:26.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 02:56:26.882
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:26.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:26.903
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-9cnhx" 02/27/23 02:56:26.906
Feb 27 02:56:26.912: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
Feb 27 02:56:27.916: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
Feb 27 02:56:27.921: INFO: Found 1 replicas for "e2e-rc-9cnhx" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-9cnhx" 02/27/23 02:56:27.921
STEP: Updating a scale subresource 02/27/23 02:56:27.926
STEP: Verifying replicas where modified for replication controller "e2e-rc-9cnhx" 02/27/23 02:56:27.935
Feb 27 02:56:27.935: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
Feb 27 02:56:28.942: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
Feb 27 02:56:28.946: INFO: Found 2 replicas for "e2e-rc-9cnhx" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6842" for this suite. 02/27/23 02:56:28.952
------------------------------
• [2.081 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:26.881
    Feb 27 02:56:26.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 02:56:26.882
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:26.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:26.903
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-9cnhx" 02/27/23 02:56:26.906
    Feb 27 02:56:26.912: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
    Feb 27 02:56:27.916: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
    Feb 27 02:56:27.921: INFO: Found 1 replicas for "e2e-rc-9cnhx" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-9cnhx" 02/27/23 02:56:27.921
    STEP: Updating a scale subresource 02/27/23 02:56:27.926
    STEP: Verifying replicas where modified for replication controller "e2e-rc-9cnhx" 02/27/23 02:56:27.935
    Feb 27 02:56:27.935: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
    Feb 27 02:56:28.942: INFO: Get Replication Controller "e2e-rc-9cnhx" to confirm replicas
    Feb 27 02:56:28.946: INFO: Found 2 replicas for "e2e-rc-9cnhx" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:28.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6842" for this suite. 02/27/23 02:56:28.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:28.963
Feb 27 02:56:28.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-runtime 02/27/23 02:56:28.963
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:28.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:28.985
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 02/27/23 02:56:28.989
STEP: wait for the container to reach Succeeded 02/27/23 02:56:29.018
STEP: get the container status 02/27/23 02:56:33.041
STEP: the container should be terminated 02/27/23 02:56:33.044
STEP: the termination message should be set 02/27/23 02:56:33.044
Feb 27 02:56:33.044: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 02/27/23 02:56:33.044
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:33.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6787" for this suite. 02/27/23 02:56:33.063
------------------------------
• [4.106 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:28.963
    Feb 27 02:56:28.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-runtime 02/27/23 02:56:28.963
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:28.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:28.985
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 02/27/23 02:56:28.989
    STEP: wait for the container to reach Succeeded 02/27/23 02:56:29.018
    STEP: get the container status 02/27/23 02:56:33.041
    STEP: the container should be terminated 02/27/23 02:56:33.044
    STEP: the termination message should be set 02/27/23 02:56:33.044
    Feb 27 02:56:33.044: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 02/27/23 02:56:33.044
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:33.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6787" for this suite. 02/27/23 02:56:33.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:33.069
Feb 27 02:56:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename init-container 02/27/23 02:56:33.069
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:33.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:33.09
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 02/27/23 02:56:33.092
Feb 27 02:56:33.092: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-8158" for this suite. 02/27/23 02:56:36.543
------------------------------
• [3.479 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:33.069
    Feb 27 02:56:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename init-container 02/27/23 02:56:33.069
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:33.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:33.09
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 02/27/23 02:56:33.092
    Feb 27 02:56:33.092: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:36.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-8158" for this suite. 02/27/23 02:56:36.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:36.549
Feb 27 02:56:36.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename security-context 02/27/23 02:56:36.549
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:36.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:36.57
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 02:56:36.572
Feb 27 02:56:36.582: INFO: Waiting up to 5m0s for pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159" in namespace "security-context-7484" to be "Succeeded or Failed"
Feb 27 02:56:36.585: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631599ms
Feb 27 02:56:38.591: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009398859s
Feb 27 02:56:40.590: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007926618s
STEP: Saw pod success 02/27/23 02:56:40.59
Feb 27 02:56:40.590: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159" satisfied condition "Succeeded or Failed"
Feb 27 02:56:40.593: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 container test-container: <nil>
STEP: delete the pod 02/27/23 02:56:40.599
Feb 27 02:56:40.614: INFO: Waiting for pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 to disappear
Feb 27 02:56:40.616: INFO: Pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:40.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7484" for this suite. 02/27/23 02:56:40.621
------------------------------
• [4.079 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:36.549
    Feb 27 02:56:36.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename security-context 02/27/23 02:56:36.549
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:36.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:36.57
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 02/27/23 02:56:36.572
    Feb 27 02:56:36.582: INFO: Waiting up to 5m0s for pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159" in namespace "security-context-7484" to be "Succeeded or Failed"
    Feb 27 02:56:36.585: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631599ms
    Feb 27 02:56:38.591: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009398859s
    Feb 27 02:56:40.590: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007926618s
    STEP: Saw pod success 02/27/23 02:56:40.59
    Feb 27 02:56:40.590: INFO: Pod "security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159" satisfied condition "Succeeded or Failed"
    Feb 27 02:56:40.593: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 container test-container: <nil>
    STEP: delete the pod 02/27/23 02:56:40.599
    Feb 27 02:56:40.614: INFO: Waiting for pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 to disappear
    Feb 27 02:56:40.616: INFO: Pod security-context-8cf9fd6e-e984-48c3-a310-bde0e78bd159 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:40.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7484" for this suite. 02/27/23 02:56:40.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:40.629
Feb 27 02:56:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:56:40.63
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:40.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:40.652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 02/27/23 02:56:40.654
Feb 27 02:56:40.673: INFO: Waiting up to 5m0s for pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4" in namespace "projected-6404" to be "running and ready"
Feb 27 02:56:40.676: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948646ms
Feb 27 02:56:40.676: INFO: The phase of Pod labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:56:42.681: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007980621s
Feb 27 02:56:42.681: INFO: The phase of Pod labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4 is Running (Ready = true)
Feb 27 02:56:42.681: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4" satisfied condition "running and ready"
Feb 27 02:56:43.205: INFO: Successfully updated pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 02:56:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6404" for this suite. 02/27/23 02:56:47.231
------------------------------
• [SLOW TEST] [6.612 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:40.629
    Feb 27 02:56:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:56:40.63
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:40.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:40.652
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 02/27/23 02:56:40.654
    Feb 27 02:56:40.673: INFO: Waiting up to 5m0s for pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4" in namespace "projected-6404" to be "running and ready"
    Feb 27 02:56:40.676: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.948646ms
    Feb 27 02:56:40.676: INFO: The phase of Pod labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:56:42.681: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007980621s
    Feb 27 02:56:42.681: INFO: The phase of Pod labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4 is Running (Ready = true)
    Feb 27 02:56:42.681: INFO: Pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4" satisfied condition "running and ready"
    Feb 27 02:56:43.205: INFO: Successfully updated pod "labelsupdatef83a3ec1-c9c0-4ec0-a54a-3ab6608563f4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:56:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6404" for this suite. 02/27/23 02:56:47.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:56:47.242
Feb 27 02:56:47.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-watch 02/27/23 02:56:47.243
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:47.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:47.261
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Feb 27 02:56:47.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Creating first CR  02/27/23 02:56:49.819
Feb 27 02:56:49.823: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:56:49Z]] name:name1 resourceVersion:65820 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 02/27/23 02:56:59.824
Feb 27 02:56:59.830: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:56:59Z]] name:name2 resourceVersion:65884 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 02/27/23 02:57:09.83
Feb 27 02:57:09.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:09Z]] name:name1 resourceVersion:65930 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 02/27/23 02:57:19.842
Feb 27 02:57:19.851: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:19Z]] name:name2 resourceVersion:65978 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 02/27/23 02:57:29.851
Feb 27 02:57:29.859: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:09Z]] name:name1 resourceVersion:66024 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 02/27/23 02:57:39.861
Feb 27 02:57:39.868: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:19Z]] name:name2 resourceVersion:66070 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:57:50.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-957" for this suite. 02/27/23 02:57:50.391
------------------------------
• [SLOW TEST] [63.157 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:56:47.242
    Feb 27 02:56:47.243: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-watch 02/27/23 02:56:47.243
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:56:47.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:56:47.261
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Feb 27 02:56:47.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Creating first CR  02/27/23 02:56:49.819
    Feb 27 02:56:49.823: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:56:49Z]] name:name1 resourceVersion:65820 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 02/27/23 02:56:59.824
    Feb 27 02:56:59.830: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:56:59Z]] name:name2 resourceVersion:65884 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 02/27/23 02:57:09.83
    Feb 27 02:57:09.841: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:09Z]] name:name1 resourceVersion:65930 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 02/27/23 02:57:19.842
    Feb 27 02:57:19.851: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:19Z]] name:name2 resourceVersion:65978 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 02/27/23 02:57:29.851
    Feb 27 02:57:29.859: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:09Z]] name:name1 resourceVersion:66024 uid:7a3dad92-3358-4f3e-9423-14f072aef981] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 02/27/23 02:57:39.861
    Feb 27 02:57:39.868: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-02-27T02:56:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-02-27T02:57:19Z]] name:name2 resourceVersion:66070 uid:690f723d-386b-426a-b9a0-7d70d2273599] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:57:50.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-957" for this suite. 02/27/23 02:57:50.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:57:50.4
Feb 27 02:57:50.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:57:50.401
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:57:50.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:57:50.431
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7036/configmap-test-f1e6dc29-2813-4af9-b180-942ed8d4219b 02/27/23 02:57:50.434
STEP: Creating a pod to test consume configMaps 02/27/23 02:57:50.44
Feb 27 02:57:50.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d" in namespace "configmap-7036" to be "Succeeded or Failed"
Feb 27 02:57:50.480: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.326755ms
Feb 27 02:57:52.484: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007406719s
Feb 27 02:57:54.485: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008928781s
STEP: Saw pod success 02/27/23 02:57:54.485
Feb 27 02:57:54.485: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d" satisfied condition "Succeeded or Failed"
Feb 27 02:57:54.489: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d container env-test: <nil>
STEP: delete the pod 02/27/23 02:57:54.496
Feb 27 02:57:54.510: INFO: Waiting for pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d to disappear
Feb 27 02:57:54.513: INFO: Pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:57:54.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7036" for this suite. 02/27/23 02:57:54.517
------------------------------
• [4.122 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:57:50.4
    Feb 27 02:57:50.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:57:50.401
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:57:50.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:57:50.431
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7036/configmap-test-f1e6dc29-2813-4af9-b180-942ed8d4219b 02/27/23 02:57:50.434
    STEP: Creating a pod to test consume configMaps 02/27/23 02:57:50.44
    Feb 27 02:57:50.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d" in namespace "configmap-7036" to be "Succeeded or Failed"
    Feb 27 02:57:50.480: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.326755ms
    Feb 27 02:57:52.484: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007406719s
    Feb 27 02:57:54.485: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008928781s
    STEP: Saw pod success 02/27/23 02:57:54.485
    Feb 27 02:57:54.485: INFO: Pod "pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d" satisfied condition "Succeeded or Failed"
    Feb 27 02:57:54.489: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d container env-test: <nil>
    STEP: delete the pod 02/27/23 02:57:54.496
    Feb 27 02:57:54.510: INFO: Waiting for pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d to disappear
    Feb 27 02:57:54.513: INFO: Pod pod-configmaps-91bb8219-58ee-443f-8695-b227faa1fd6d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:57:54.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7036" for this suite. 02/27/23 02:57:54.517
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:57:54.522
Feb 27 02:57:54.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 02:57:54.523
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:57:54.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:57:54.545
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Feb 27 02:57:54.557: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 27 02:57:59.561: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 02/27/23 02:57:59.561
Feb 27 02:57:59.561: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 27 02:58:01.566: INFO: Creating deployment "test-rollover-deployment"
Feb 27 02:58:01.577: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 27 02:58:03.587: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 27 02:58:03.593: INFO: Ensure that both replica sets have 1 created replica
Feb 27 02:58:03.598: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 27 02:58:03.609: INFO: Updating deployment test-rollover-deployment
Feb 27 02:58:03.609: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 27 02:58:05.616: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 27 02:58:05.621: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 27 02:58:05.626: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 02:58:05.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:58:07.633: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 02:58:07.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:58:09.635: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 02:58:09.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:58:11.633: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 02:58:11.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:58:13.633: INFO: all replica sets need to contain the pod-template-hash label
Feb 27 02:58:13.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 27 02:58:15.635: INFO: 
Feb 27 02:58:15.635: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 02:58:15.643: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6161  9cfe4665-9478-4ad1-96dc-44c1bfbf7600 66376 2 2023-02-27 02:58:01 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8ad98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 02:58:01 +0000 UTC,LastTransitionTime:2023-02-27 02:58:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-85c7b5545b" has successfully progressed.,LastUpdateTime:2023-02-27 02:58:14 +0000 UTC,LastTransitionTime:2023-02-27 02:58:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 27 02:58:15.646: INFO: New ReplicaSet "test-rollover-deployment-85c7b5545b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-85c7b5545b  deployment-6161  c2be123a-c898-44b8-ab04-2ca0b22c3950 66366 2 2023-02-27 02:58:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b607 0xc005d8b608}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 85c7b5545b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:58:15.646: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 27 02:58:15.646: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6161  587c4bee-b66b-46df-92d4-b92a19217f99 66375 2 2023-02-27 02:57:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b3b7 0xc005d8b3b8}] [] [{e2e.test Update apps/v1 2023-02-27 02:57:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d8b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:58:15.646: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6161  e762d444-b247-4b14-bdec-ac8d68bce146 66292 2 2023-02-27 02:58:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b4e7 0xc005d8b4e8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 27 02:58:15.648: INFO: Pod "test-rollover-deployment-85c7b5545b-228qb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-85c7b5545b-228qb test-rollover-deployment-85c7b5545b- deployment-6161  43449fa5-af11-4836-aa04-121e794b03b4 66314 0 2023-02-27 02:58:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.136"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.136"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-85c7b5545b c2be123a-c898-44b8-ab04-2ca0b22c3950 0xc005d8bc07 0xc005d8bc08}] [] [{kube-controller-manager Update v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2be123a-c898-44b8-ab04-2ca0b22c3950\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:58:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:58:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2p6mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2p6mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.136,StartTime:2023-02-27 02:58:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:58:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://259b60c27f942bab494b37fdb624c90ff648aeb9c2dde1245549e9d0bacea9cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:15.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6161" for this suite. 02/27/23 02:58:15.652
------------------------------
• [SLOW TEST] [21.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:57:54.522
    Feb 27 02:57:54.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 02:57:54.523
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:57:54.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:57:54.545
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Feb 27 02:57:54.557: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Feb 27 02:57:59.561: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 02/27/23 02:57:59.561
    Feb 27 02:57:59.561: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Feb 27 02:58:01.566: INFO: Creating deployment "test-rollover-deployment"
    Feb 27 02:58:01.577: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Feb 27 02:58:03.587: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Feb 27 02:58:03.593: INFO: Ensure that both replica sets have 1 created replica
    Feb 27 02:58:03.598: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Feb 27 02:58:03.609: INFO: Updating deployment test-rollover-deployment
    Feb 27 02:58:03.609: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Feb 27 02:58:05.616: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Feb 27 02:58:05.621: INFO: Make sure deployment "test-rollover-deployment" is complete
    Feb 27 02:58:05.626: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 02:58:05.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:58:07.633: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 02:58:07.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:58:09.635: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 02:58:09.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:58:11.633: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 02:58:11.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:58:13.633: INFO: all replica sets need to contain the pod-template-hash label
    Feb 27 02:58:13.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.February, 27, 2, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.February, 27, 2, 58, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-85c7b5545b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Feb 27 02:58:15.635: INFO: 
    Feb 27 02:58:15.635: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 02:58:15.643: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6161  9cfe4665-9478-4ad1-96dc-44c1bfbf7600 66376 2 2023-02-27 02:58:01 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8ad98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-02-27 02:58:01 +0000 UTC,LastTransitionTime:2023-02-27 02:58:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-85c7b5545b" has successfully progressed.,LastUpdateTime:2023-02-27 02:58:14 +0000 UTC,LastTransitionTime:2023-02-27 02:58:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Feb 27 02:58:15.646: INFO: New ReplicaSet "test-rollover-deployment-85c7b5545b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-85c7b5545b  deployment-6161  c2be123a-c898-44b8-ab04-2ca0b22c3950 66366 2 2023-02-27 02:58:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b607 0xc005d8b608}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 85c7b5545b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[] [] [] []} {[] [] [{agnhost armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:58:15.646: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Feb 27 02:58:15.646: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6161  587c4bee-b66b-46df-92d4-b92a19217f99 66375 2 2023-02-27 02:57:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b3b7 0xc005d8b3b8}] [] [{e2e.test Update apps/v1 2023-02-27 02:57:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005d8b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:58:15.646: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-6161  e762d444-b247-4b14-bdec-ac8d68bce146 66292 2 2023-02-27 02:58:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 9cfe4665-9478-4ad1-96dc-44c1bfbf7600 0xc005d8b4e7 0xc005d8b4e8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfe4665-9478-4ad1-96dc-44c1bfbf7600\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Feb 27 02:58:15.648: INFO: Pod "test-rollover-deployment-85c7b5545b-228qb" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-85c7b5545b-228qb test-rollover-deployment-85c7b5545b- deployment-6161  43449fa5-af11-4836-aa04-121e794b03b4 66314 0 2023-02-27 02:58:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:85c7b5545b] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.136"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.136"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rollover-deployment-85c7b5545b c2be123a-c898-44b8-ab04-2ca0b22c3950 0xc005d8bc07 0xc005d8bc08}] [] [{kube-controller-manager Update v1 2023-02-27 02:58:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c2be123a-c898-44b8-ab04-2ca0b22c3950\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 02:58:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 02:58:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2p6mg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2p6mg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 02:58:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.136,StartTime:2023-02-27 02:58:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 02:58:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost@sha256:c338e9c1470d7e0e40b21e2fe191b99c1855ce0a6c265a2eacf70f7ed6a979f6,ContainerID:containerd://259b60c27f942bab494b37fdb624c90ff648aeb9c2dde1245549e9d0bacea9cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:15.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6161" for this suite. 02/27/23 02:58:15.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:15.658
Feb 27 02:58:15.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 02:58:15.658
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:15.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:15.681
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 02/27/23 02:58:15.687
Feb 27 02:58:15.721: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4756" to be "running and ready"
Feb 27 02:58:15.729: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.545926ms
Feb 27 02:58:15.729: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:58:17.734: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013238862s
Feb 27 02:58:17.734: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Feb 27 02:58:17.734: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 02/27/23 02:58:17.74
Feb 27 02:58:17.748: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4756" to be "running and ready"
Feb 27 02:58:17.752: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700186ms
Feb 27 02:58:17.752: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:58:19.760: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011873119s
Feb 27 02:58:19.760: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Feb 27 02:58:19.760: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 02/27/23 02:58:19.765
STEP: delete the pod with lifecycle hook 02/27/23 02:58:19.774
Feb 27 02:58:19.781: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 02:58:19.786: INFO: Pod pod-with-poststart-http-hook still exists
Feb 27 02:58:21.788: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 02:58:21.793: INFO: Pod pod-with-poststart-http-hook still exists
Feb 27 02:58:23.786: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 27 02:58:23.789: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4756" for this suite. 02/27/23 02:58:23.794
------------------------------
• [SLOW TEST] [8.143 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:15.658
    Feb 27 02:58:15.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-lifecycle-hook 02/27/23 02:58:15.658
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:15.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:15.681
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 02/27/23 02:58:15.687
    Feb 27 02:58:15.721: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4756" to be "running and ready"
    Feb 27 02:58:15.729: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.545926ms
    Feb 27 02:58:15.729: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:58:17.734: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013238862s
    Feb 27 02:58:17.734: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Feb 27 02:58:17.734: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 02/27/23 02:58:17.74
    Feb 27 02:58:17.748: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4756" to be "running and ready"
    Feb 27 02:58:17.752: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700186ms
    Feb 27 02:58:17.752: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:58:19.760: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011873119s
    Feb 27 02:58:19.760: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Feb 27 02:58:19.760: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 02/27/23 02:58:19.765
    STEP: delete the pod with lifecycle hook 02/27/23 02:58:19.774
    Feb 27 02:58:19.781: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 02:58:19.786: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 27 02:58:21.788: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 02:58:21.793: INFO: Pod pod-with-poststart-http-hook still exists
    Feb 27 02:58:23.786: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Feb 27 02:58:23.789: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4756" for this suite. 02/27/23 02:58:23.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:23.802
Feb 27 02:58:23.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:23.802
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:23.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:23.826
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Feb 27 02:58:23.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 02:58:25.668
Feb 27 02:58:25.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 create -f -'
Feb 27 02:58:26.354: INFO: stderr: ""
Feb 27 02:58:26.354: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 02:58:26.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 delete e2e-test-crd-publish-openapi-6618-crds test-cr'
Feb 27 02:58:26.453: INFO: stderr: ""
Feb 27 02:58:26.453: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 27 02:58:26.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 apply -f -'
Feb 27 02:58:27.004: INFO: stderr: ""
Feb 27 02:58:27.004: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 27 02:58:27.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 delete e2e-test-crd-publish-openapi-6618-crds test-cr'
Feb 27 02:58:27.073: INFO: stderr: ""
Feb 27 02:58:27.073: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 02/27/23 02:58:27.073
Feb 27 02:58:27.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 explain e2e-test-crd-publish-openapi-6618-crds'
Feb 27 02:58:27.618: INFO: stderr: ""
Feb 27 02:58:27.618: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6618-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:29.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6927" for this suite. 02/27/23 02:58:29.442
------------------------------
• [SLOW TEST] [5.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:23.802
    Feb 27 02:58:23.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:23.802
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:23.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:23.826
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Feb 27 02:58:23.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 02:58:25.668
    Feb 27 02:58:25.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 create -f -'
    Feb 27 02:58:26.354: INFO: stderr: ""
    Feb 27 02:58:26.354: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 02:58:26.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 delete e2e-test-crd-publish-openapi-6618-crds test-cr'
    Feb 27 02:58:26.453: INFO: stderr: ""
    Feb 27 02:58:26.453: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Feb 27 02:58:26.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 apply -f -'
    Feb 27 02:58:27.004: INFO: stderr: ""
    Feb 27 02:58:27.004: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Feb 27 02:58:27.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 --namespace=crd-publish-openapi-6927 delete e2e-test-crd-publish-openapi-6618-crds test-cr'
    Feb 27 02:58:27.073: INFO: stderr: ""
    Feb 27 02:58:27.073: INFO: stdout: "e2e-test-crd-publish-openapi-6618-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 02/27/23 02:58:27.073
    Feb 27 02:58:27.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-6927 explain e2e-test-crd-publish-openapi-6618-crds'
    Feb 27 02:58:27.618: INFO: stderr: ""
    Feb 27 02:58:27.618: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6618-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:29.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6927" for this suite. 02/27/23 02:58:29.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:29.449
Feb 27 02:58:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename containers 02/27/23 02:58:29.45
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:29.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:29.474
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 02/27/23 02:58:29.477
Feb 27 02:58:29.515: INFO: Waiting up to 5m0s for pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856" in namespace "containers-3857" to be "Succeeded or Failed"
Feb 27 02:58:29.520: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57209ms
Feb 27 02:58:31.524: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009304364s
Feb 27 02:58:33.526: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010570201s
STEP: Saw pod success 02/27/23 02:58:33.526
Feb 27 02:58:33.526: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856" satisfied condition "Succeeded or Failed"
Feb 27 02:58:33.529: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:58:33.534
Feb 27 02:58:33.549: INFO: Waiting for pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 to disappear
Feb 27 02:58:33.552: INFO: Pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:33.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3857" for this suite. 02/27/23 02:58:33.557
------------------------------
• [4.113 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:29.449
    Feb 27 02:58:29.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename containers 02/27/23 02:58:29.45
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:29.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:29.474
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 02/27/23 02:58:29.477
    Feb 27 02:58:29.515: INFO: Waiting up to 5m0s for pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856" in namespace "containers-3857" to be "Succeeded or Failed"
    Feb 27 02:58:29.520: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57209ms
    Feb 27 02:58:31.524: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009304364s
    Feb 27 02:58:33.526: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010570201s
    STEP: Saw pod success 02/27/23 02:58:33.526
    Feb 27 02:58:33.526: INFO: Pod "client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856" satisfied condition "Succeeded or Failed"
    Feb 27 02:58:33.529: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:58:33.534
    Feb 27 02:58:33.549: INFO: Waiting for pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 to disappear
    Feb 27 02:58:33.552: INFO: Pod client-containers-d3ee29db-5a7f-460e-bbbe-6e8927138856 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:33.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3857" for this suite. 02/27/23 02:58:33.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:33.563
Feb 27 02:58:33.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename watch 02/27/23 02:58:33.564
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:33.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:33.584
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 02/27/23 02:58:33.589
STEP: starting a background goroutine to produce watch events 02/27/23 02:58:33.591
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 02:58:33.591
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6795" for this suite. 02/27/23 02:58:36.421
------------------------------
• [2.908 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:33.563
    Feb 27 02:58:33.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename watch 02/27/23 02:58:33.564
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:33.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:33.584
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 02/27/23 02:58:33.589
    STEP: starting a background goroutine to produce watch events 02/27/23 02:58:33.591
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 02/27/23 02:58:33.591
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:36.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6795" for this suite. 02/27/23 02:58:36.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:36.471
Feb 27 02:58:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename limitrange 02/27/23 02:58:36.472
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:36.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:36.489
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-dgwxz" in namespace "limitrange-2858" 02/27/23 02:58:36.492
STEP: Creating another limitRange in another namespace 02/27/23 02:58:36.499
Feb 27 02:58:36.515: INFO: Namespace "e2e-limitrange-dgwxz-4126" created
Feb 27 02:58:36.515: INFO: Creating LimitRange "e2e-limitrange-dgwxz" in namespace "e2e-limitrange-dgwxz-4126"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dgwxz" 02/27/23 02:58:36.52
Feb 27 02:58:36.523: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-dgwxz" in "limitrange-2858" namespace 02/27/23 02:58:36.523
Feb 27 02:58:36.529: INFO: LimitRange "e2e-limitrange-dgwxz" has been patched
STEP: Delete LimitRange "e2e-limitrange-dgwxz" by Collection with labelSelector: "e2e-limitrange-dgwxz=patched" 02/27/23 02:58:36.529
STEP: Confirm that the limitRange "e2e-limitrange-dgwxz" has been deleted 02/27/23 02:58:36.538
Feb 27 02:58:36.538: INFO: Requesting list of LimitRange to confirm quantity
Feb 27 02:58:36.541: INFO: Found 0 LimitRange with label "e2e-limitrange-dgwxz=patched"
Feb 27 02:58:36.541: INFO: LimitRange "e2e-limitrange-dgwxz" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dgwxz" 02/27/23 02:58:36.541
Feb 27 02:58:36.543: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:36.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2858" for this suite. 02/27/23 02:58:36.548
STEP: Destroying namespace "e2e-limitrange-dgwxz-4126" for this suite. 02/27/23 02:58:36.554
------------------------------
• [0.089 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:36.471
    Feb 27 02:58:36.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename limitrange 02/27/23 02:58:36.472
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:36.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:36.489
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-dgwxz" in namespace "limitrange-2858" 02/27/23 02:58:36.492
    STEP: Creating another limitRange in another namespace 02/27/23 02:58:36.499
    Feb 27 02:58:36.515: INFO: Namespace "e2e-limitrange-dgwxz-4126" created
    Feb 27 02:58:36.515: INFO: Creating LimitRange "e2e-limitrange-dgwxz" in namespace "e2e-limitrange-dgwxz-4126"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-dgwxz" 02/27/23 02:58:36.52
    Feb 27 02:58:36.523: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-dgwxz" in "limitrange-2858" namespace 02/27/23 02:58:36.523
    Feb 27 02:58:36.529: INFO: LimitRange "e2e-limitrange-dgwxz" has been patched
    STEP: Delete LimitRange "e2e-limitrange-dgwxz" by Collection with labelSelector: "e2e-limitrange-dgwxz=patched" 02/27/23 02:58:36.529
    STEP: Confirm that the limitRange "e2e-limitrange-dgwxz" has been deleted 02/27/23 02:58:36.538
    Feb 27 02:58:36.538: INFO: Requesting list of LimitRange to confirm quantity
    Feb 27 02:58:36.541: INFO: Found 0 LimitRange with label "e2e-limitrange-dgwxz=patched"
    Feb 27 02:58:36.541: INFO: LimitRange "e2e-limitrange-dgwxz" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-dgwxz" 02/27/23 02:58:36.541
    Feb 27 02:58:36.543: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:36.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2858" for this suite. 02/27/23 02:58:36.548
    STEP: Destroying namespace "e2e-limitrange-dgwxz-4126" for this suite. 02/27/23 02:58:36.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:36.561
Feb 27 02:58:36.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:36.562
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:36.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:36.579
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Feb 27 02:58:36.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 02:58:38.545
Feb 27 02:58:38.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 create -f -'
Feb 27 02:58:39.118: INFO: stderr: ""
Feb 27 02:58:39.118: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 02:58:39.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 delete e2e-test-crd-publish-openapi-6490-crds test-cr'
Feb 27 02:58:39.188: INFO: stderr: ""
Feb 27 02:58:39.188: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 27 02:58:39.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 apply -f -'
Feb 27 02:58:39.740: INFO: stderr: ""
Feb 27 02:58:39.740: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 27 02:58:39.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 delete e2e-test-crd-publish-openapi-6490-crds test-cr'
Feb 27 02:58:39.811: INFO: stderr: ""
Feb 27 02:58:39.811: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 02/27/23 02:58:39.811
Feb 27 02:58:39.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 explain e2e-test-crd-publish-openapi-6490-crds'
Feb 27 02:58:40.258: INFO: stderr: ""
Feb 27 02:58:40.258: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6490-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:42.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7818" for this suite. 02/27/23 02:58:42.18
------------------------------
• [SLOW TEST] [5.626 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:36.561
    Feb 27 02:58:36.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:36.562
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:36.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:36.579
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Feb 27 02:58:36.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 02/27/23 02:58:38.545
    Feb 27 02:58:38.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 create -f -'
    Feb 27 02:58:39.118: INFO: stderr: ""
    Feb 27 02:58:39.118: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 02:58:39.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 delete e2e-test-crd-publish-openapi-6490-crds test-cr'
    Feb 27 02:58:39.188: INFO: stderr: ""
    Feb 27 02:58:39.188: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Feb 27 02:58:39.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 apply -f -'
    Feb 27 02:58:39.740: INFO: stderr: ""
    Feb 27 02:58:39.740: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Feb 27 02:58:39.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 --namespace=crd-publish-openapi-7818 delete e2e-test-crd-publish-openapi-6490-crds test-cr'
    Feb 27 02:58:39.811: INFO: stderr: ""
    Feb 27 02:58:39.811: INFO: stdout: "e2e-test-crd-publish-openapi-6490-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 02/27/23 02:58:39.811
    Feb 27 02:58:39.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=crd-publish-openapi-7818 explain e2e-test-crd-publish-openapi-6490-crds'
    Feb 27 02:58:40.258: INFO: stderr: ""
    Feb 27 02:58:40.258: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6490-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:42.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7818" for this suite. 02/27/23 02:58:42.18
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:42.187
Feb 27 02:58:42.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename replication-controller 02/27/23 02:58:42.188
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:42.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:42.213
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 02:58:42.215
Feb 27 02:58:42.247: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5543" to be "running and ready"
Feb 27 02:58:42.251: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434974ms
Feb 27 02:58:42.251: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Feb 27 02:58:44.255: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008037909s
Feb 27 02:58:44.255: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Feb 27 02:58:44.255: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 02/27/23 02:58:44.258
STEP: Then the orphan pod is adopted 02/27/23 02:58:44.265
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:45.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5543" for this suite. 02/27/23 02:58:45.278
------------------------------
• [3.099 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:42.187
    Feb 27 02:58:42.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename replication-controller 02/27/23 02:58:42.188
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:42.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:42.213
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 02/27/23 02:58:42.215
    Feb 27 02:58:42.247: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5543" to be "running and ready"
    Feb 27 02:58:42.251: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434974ms
    Feb 27 02:58:42.251: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 02:58:44.255: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008037909s
    Feb 27 02:58:44.255: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Feb 27 02:58:44.255: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 02/27/23 02:58:44.258
    STEP: Then the orphan pod is adopted 02/27/23 02:58:44.265
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:45.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5543" for this suite. 02/27/23 02:58:45.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:45.287
Feb 27 02:58:45.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:45.287
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:45.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:45.308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 02/27/23 02:58:45.312
Feb 27 02:58:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: rename a version 02/27/23 02:58:49.27
STEP: check the new version name is served 02/27/23 02:58:49.326
STEP: check the old version name is removed 02/27/23 02:58:50.671
STEP: check the other version is not changed 02/27/23 02:58:51.385
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:54.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1998" for this suite. 02/27/23 02:58:54.608
------------------------------
• [SLOW TEST] [9.328 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:45.287
    Feb 27 02:58:45.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 02:58:45.287
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:45.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:45.308
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 02/27/23 02:58:45.312
    Feb 27 02:58:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: rename a version 02/27/23 02:58:49.27
    STEP: check the new version name is served 02/27/23 02:58:49.326
    STEP: check the old version name is removed 02/27/23 02:58:50.671
    STEP: check the other version is not changed 02/27/23 02:58:51.385
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:54.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1998" for this suite. 02/27/23 02:58:54.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:54.615
Feb 27 02:58:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 02:58:54.616
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:54.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:54.635
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-dd1c1350-f8a0-4287-a878-be87d1a76628 02/27/23 02:58:54.638
STEP: Creating a pod to test consume configMaps 02/27/23 02:58:54.645
Feb 27 02:58:54.673: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12" in namespace "projected-6779" to be "Succeeded or Failed"
Feb 27 02:58:54.676: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899251ms
Feb 27 02:58:56.680: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00669543s
Feb 27 02:58:58.681: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008150825s
STEP: Saw pod success 02/27/23 02:58:58.681
Feb 27 02:58:58.681: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12" satisfied condition "Succeeded or Failed"
Feb 27 02:58:58.684: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 container agnhost-container: <nil>
STEP: delete the pod 02/27/23 02:58:58.691
Feb 27 02:58:58.703: INFO: Waiting for pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 to disappear
Feb 27 02:58:58.707: INFO: Pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:58.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6779" for this suite. 02/27/23 02:58:58.715
------------------------------
• [4.107 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:54.615
    Feb 27 02:58:54.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 02:58:54.616
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:54.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:54.635
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-dd1c1350-f8a0-4287-a878-be87d1a76628 02/27/23 02:58:54.638
    STEP: Creating a pod to test consume configMaps 02/27/23 02:58:54.645
    Feb 27 02:58:54.673: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12" in namespace "projected-6779" to be "Succeeded or Failed"
    Feb 27 02:58:54.676: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899251ms
    Feb 27 02:58:56.680: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00669543s
    Feb 27 02:58:58.681: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008150825s
    STEP: Saw pod success 02/27/23 02:58:58.681
    Feb 27 02:58:58.681: INFO: Pod "pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12" satisfied condition "Succeeded or Failed"
    Feb 27 02:58:58.684: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 02:58:58.691
    Feb 27 02:58:58.703: INFO: Waiting for pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 to disappear
    Feb 27 02:58:58.707: INFO: Pod pod-projected-configmaps-56863c11-7291-49ab-9b6e-39c536562c12 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:58.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6779" for this suite. 02/27/23 02:58:58.715
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:58.722
Feb 27 02:58:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 02:58:58.723
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.739
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Feb 27 02:58:58.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-1594 version'
Feb 27 02:58:58.799: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Feb 27 02:58:58.799: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"23650373dd86a4d09220930060a90eeb356e0851\", GitTreeState:\"clean\", BuildDate:\"2023-01-25T13:34:31Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:58.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1594" for this suite. 02/27/23 02:58:58.804
------------------------------
• [0.088 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:58.722
    Feb 27 02:58:58.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 02:58:58.723
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.739
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Feb 27 02:58:58.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-1594 version'
    Feb 27 02:58:58.799: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Feb 27 02:58:58.799: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"23650373dd86a4d09220930060a90eeb356e0851\", GitTreeState:\"clean\", BuildDate:\"2023-01-25T13:34:31Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:58.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1594" for this suite. 02/27/23 02:58:58.804
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:58.81
Feb 27 02:58:58.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename lease-test 02/27/23 02:58:58.811
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.838
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:58.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7490" for this suite. 02/27/23 02:58:58.898
------------------------------
• [0.096 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:58.81
    Feb 27 02:58:58.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename lease-test 02/27/23 02:58:58.811
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.838
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:58.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7490" for this suite. 02/27/23 02:58:58.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:58.906
Feb 27 02:58:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename endpointslice 02/27/23 02:58:58.907
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.93
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 02/27/23 02:58:58.933
STEP: getting /apis/discovery.k8s.io 02/27/23 02:58:58.935
STEP: getting /apis/discovery.k8s.iov1 02/27/23 02:58:58.936
STEP: creating 02/27/23 02:58:58.937
STEP: getting 02/27/23 02:58:58.971
STEP: listing 02/27/23 02:58:58.974
STEP: watching 02/27/23 02:58:58.976
Feb 27 02:58:58.976: INFO: starting watch
STEP: cluster-wide listing 02/27/23 02:58:58.977
STEP: cluster-wide watching 02/27/23 02:58:58.981
Feb 27 02:58:58.981: INFO: starting watch
STEP: patching 02/27/23 02:58:58.982
STEP: updating 02/27/23 02:58:58.99
Feb 27 02:58:58.998: INFO: waiting for watch events with expected annotations
Feb 27 02:58:58.998: INFO: saw patched and updated annotations
STEP: deleting 02/27/23 02:58:58.998
STEP: deleting a collection 02/27/23 02:58:59.008
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:59.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4678" for this suite. 02/27/23 02:58:59.024
------------------------------
• [0.127 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:58.906
    Feb 27 02:58:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename endpointslice 02/27/23 02:58:58.907
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:58.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:58.93
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 02/27/23 02:58:58.933
    STEP: getting /apis/discovery.k8s.io 02/27/23 02:58:58.935
    STEP: getting /apis/discovery.k8s.iov1 02/27/23 02:58:58.936
    STEP: creating 02/27/23 02:58:58.937
    STEP: getting 02/27/23 02:58:58.971
    STEP: listing 02/27/23 02:58:58.974
    STEP: watching 02/27/23 02:58:58.976
    Feb 27 02:58:58.976: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 02:58:58.977
    STEP: cluster-wide watching 02/27/23 02:58:58.981
    Feb 27 02:58:58.981: INFO: starting watch
    STEP: patching 02/27/23 02:58:58.982
    STEP: updating 02/27/23 02:58:58.99
    Feb 27 02:58:58.998: INFO: waiting for watch events with expected annotations
    Feb 27 02:58:58.998: INFO: saw patched and updated annotations
    STEP: deleting 02/27/23 02:58:58.998
    STEP: deleting a collection 02/27/23 02:58:59.008
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:59.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4678" for this suite. 02/27/23 02:58:59.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:59.034
Feb 27 02:58:59.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 02:58:59.035
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:59.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:59.053
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 02/27/23 02:58:59.055
STEP: watching for the ServiceAccount to be added 02/27/23 02:58:59.066
STEP: patching the ServiceAccount 02/27/23 02:58:59.067
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 02:58:59.072
STEP: deleting the ServiceAccount 02/27/23 02:58:59.075
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 02:58:59.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8864" for this suite. 02/27/23 02:58:59.093
------------------------------
• [0.072 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:59.034
    Feb 27 02:58:59.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 02:58:59.035
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:59.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:59.053
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 02/27/23 02:58:59.055
    STEP: watching for the ServiceAccount to be added 02/27/23 02:58:59.066
    STEP: patching the ServiceAccount 02/27/23 02:58:59.067
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 02/27/23 02:58:59.072
    STEP: deleting the ServiceAccount 02/27/23 02:58:59.075
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:58:59.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8864" for this suite. 02/27/23 02:58:59.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:58:59.107
Feb 27 02:58:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 02:58:59.108
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:59.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:59.128
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 02/27/23 02:58:59.13
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local;sleep 1; done
 02/27/23 02:58:59.135
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local;sleep 1; done
 02/27/23 02:58:59.135
STEP: creating a pod to probe DNS 02/27/23 02:58:59.135
STEP: submitting the pod to kubernetes 02/27/23 02:58:59.135
Feb 27 02:58:59.150: INFO: Waiting up to 15m0s for pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed" in namespace "dns-8254" to be "running"
Feb 27 02:58:59.152: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608067ms
Feb 27 02:59:01.157: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.006721442s
Feb 27 02:59:01.157: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed" satisfied condition "running"
STEP: retrieving the pod 02/27/23 02:59:01.157
STEP: looking for the results for each expected name from probers 02/27/23 02:59:01.161
Feb 27 02:59:01.165: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.168: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.173: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.176: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.179: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.183: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.185: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.188: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:01.188: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:06.194: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.205: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.209: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.214: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.217: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.219: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.221: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:06.224: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:11.196: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.202: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.208: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.211: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.215: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.218: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.221: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:11.224: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:16.194: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.198: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.202: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.206: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.209: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.212: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.215: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.218: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:16.218: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:21.195: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.199: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.202: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.206: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.211: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.215: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.225: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.229: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:21.229: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:26.193: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.197: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.201: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.205: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.208: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.211: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.214: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.217: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
Feb 27 02:59:26.217: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

Feb 27 02:59:31.217: INFO: DNS probes using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed succeeded

STEP: deleting the pod 02/27/23 02:59:31.217
STEP: deleting the test headless service 02/27/23 02:59:31.23
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:31.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8254" for this suite. 02/27/23 02:59:31.26
------------------------------
• [SLOW TEST] [32.161 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:58:59.107
    Feb 27 02:58:59.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 02:58:59.108
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:58:59.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:58:59.128
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 02/27/23 02:58:59.13
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local;sleep 1; done
     02/27/23 02:58:59.135
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8254.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local;sleep 1; done
     02/27/23 02:58:59.135
    STEP: creating a pod to probe DNS 02/27/23 02:58:59.135
    STEP: submitting the pod to kubernetes 02/27/23 02:58:59.135
    Feb 27 02:58:59.150: INFO: Waiting up to 15m0s for pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed" in namespace "dns-8254" to be "running"
    Feb 27 02:58:59.152: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.608067ms
    Feb 27 02:59:01.157: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.006721442s
    Feb 27 02:59:01.157: INFO: Pod "dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed" satisfied condition "running"
    STEP: retrieving the pod 02/27/23 02:59:01.157
    STEP: looking for the results for each expected name from probers 02/27/23 02:59:01.161
    Feb 27 02:59:01.165: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.168: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.173: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.176: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.179: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.183: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.185: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.188: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:01.188: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:06.194: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.205: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.209: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.214: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.217: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.219: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.221: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:06.224: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:11.196: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.202: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.208: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.211: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.215: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.218: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.221: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.224: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:11.224: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:16.194: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.198: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.202: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.206: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.209: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.212: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.215: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.218: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:16.218: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:21.195: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.199: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.202: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.206: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.211: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.215: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.225: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.229: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:21.229: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:26.193: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.197: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.201: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.205: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.208: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.211: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.214: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.217: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local from pod dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed: the server could not find the requested resource (get pods dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed)
    Feb 27 02:59:26.217: INFO: Lookups using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8254.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8254.svc.cluster.local jessie_udp@dns-test-service-2.dns-8254.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8254.svc.cluster.local]

    Feb 27 02:59:31.217: INFO: DNS probes using dns-8254/dns-test-43c3a4dc-6f20-411e-a8bb-b0dc1ee581ed succeeded

    STEP: deleting the pod 02/27/23 02:59:31.217
    STEP: deleting the test headless service 02/27/23 02:59:31.23
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:31.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8254" for this suite. 02/27/23 02:59:31.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:31.268
Feb 27 02:59:31.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename configmap 02/27/23 02:59:31.269
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:31.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:31.291
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:31.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2335" for this suite. 02/27/23 02:59:31.33
------------------------------
• [0.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:31.268
    Feb 27 02:59:31.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename configmap 02/27/23 02:59:31.269
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:31.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:31.291
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:31.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2335" for this suite. 02/27/23 02:59:31.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:31.337
Feb 27 02:59:31.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 02:59:31.338
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:31.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:31.359
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 02/27/23 02:59:31.376
Feb 27 02:59:31.385: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 02/27/23 02:59:33.39
Feb 27 02:59:33.400: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 02/27/23 02:59:35.403
Feb 27 02:59:35.418: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-4006" for this suite. 02/27/23 02:59:37.43
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:31.337
    Feb 27 02:59:31.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename endpointslicemirroring 02/27/23 02:59:31.338
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:31.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:31.359
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 02/27/23 02:59:31.376
    Feb 27 02:59:31.385: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 02/27/23 02:59:33.39
    Feb 27 02:59:33.400: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 02/27/23 02:59:35.403
    Feb 27 02:59:35.418: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:37.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-4006" for this suite. 02/27/23 02:59:37.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:37.438
Feb 27 02:59:37.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 02:59:37.439
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:37.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:37.462
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 02/27/23 02:59:37.466
STEP: Ensuring job reaches completions 02/27/23 02:59:37.474
STEP: Ensuring pods with index for job exist 02/27/23 02:59:47.48
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:47.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2547" for this suite. 02/27/23 02:59:47.492
------------------------------
• [SLOW TEST] [10.059 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:37.438
    Feb 27 02:59:37.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 02:59:37.439
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:37.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:37.462
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 02/27/23 02:59:37.466
    STEP: Ensuring job reaches completions 02/27/23 02:59:37.474
    STEP: Ensuring pods with index for job exist 02/27/23 02:59:47.48
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:47.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2547" for this suite. 02/27/23 02:59:47.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:47.498
Feb 27 02:59:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename resourcequota 02/27/23 02:59:47.499
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:47.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:47.526
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 02/27/23 02:59:47.529
STEP: Getting a ResourceQuota 02/27/23 02:59:47.534
STEP: Updating a ResourceQuota 02/27/23 02:59:47.537
STEP: Verifying a ResourceQuota was modified 02/27/23 02:59:47.543
STEP: Deleting a ResourceQuota 02/27/23 02:59:47.55
STEP: Verifying the deleted ResourceQuota 02/27/23 02:59:47.555
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:47.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2882" for this suite. 02/27/23 02:59:47.561
------------------------------
• [0.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:47.498
    Feb 27 02:59:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename resourcequota 02/27/23 02:59:47.499
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:47.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:47.526
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 02/27/23 02:59:47.529
    STEP: Getting a ResourceQuota 02/27/23 02:59:47.534
    STEP: Updating a ResourceQuota 02/27/23 02:59:47.537
    STEP: Verifying a ResourceQuota was modified 02/27/23 02:59:47.543
    STEP: Deleting a ResourceQuota 02/27/23 02:59:47.55
    STEP: Verifying the deleted ResourceQuota 02/27/23 02:59:47.555
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:47.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2882" for this suite. 02/27/23 02:59:47.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:47.568
Feb 27 02:59:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename job 02/27/23 02:59:47.569
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:47.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:47.589
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 02/27/23 02:59:47.592
STEP: Ensure pods equal to parallelism count is attached to the job 02/27/23 02:59:47.598
STEP: patching /status 02/27/23 02:59:49.604
STEP: updating /status 02/27/23 02:59:49.614
STEP: get /status 02/27/23 02:59:49.65
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4563" for this suite. 02/27/23 02:59:49.659
------------------------------
• [2.097 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:47.568
    Feb 27 02:59:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename job 02/27/23 02:59:47.569
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:47.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:47.589
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 02/27/23 02:59:47.592
    STEP: Ensure pods equal to parallelism count is attached to the job 02/27/23 02:59:47.598
    STEP: patching /status 02/27/23 02:59:49.604
    STEP: updating /status 02/27/23 02:59:49.614
    STEP: get /status 02/27/23 02:59:49.65
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:49.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4563" for this suite. 02/27/23 02:59:49.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:49.666
Feb 27 02:59:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename secrets 02/27/23 02:59:49.666
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:49.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:49.683
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-e4457d75-af43-4e41-a98d-9d1c0adfa782 02/27/23 02:59:49.687
STEP: Creating a pod to test consume secrets 02/27/23 02:59:49.693
Feb 27 02:59:49.722: INFO: Waiting up to 5m0s for pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2" in namespace "secrets-6341" to be "Succeeded or Failed"
Feb 27 02:59:49.728: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.883123ms
Feb 27 02:59:51.734: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011163477s
Feb 27 02:59:53.733: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010250819s
STEP: Saw pod success 02/27/23 02:59:53.733
Feb 27 02:59:53.733: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2" satisfied condition "Succeeded or Failed"
Feb 27 02:59:53.735: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 container secret-volume-test: <nil>
STEP: delete the pod 02/27/23 02:59:53.741
Feb 27 02:59:53.757: INFO: Waiting for pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 to disappear
Feb 27 02:59:53.759: INFO: Pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:53.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6341" for this suite. 02/27/23 02:59:53.763
------------------------------
• [4.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:49.666
    Feb 27 02:59:49.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename secrets 02/27/23 02:59:49.666
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:49.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:49.683
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-e4457d75-af43-4e41-a98d-9d1c0adfa782 02/27/23 02:59:49.687
    STEP: Creating a pod to test consume secrets 02/27/23 02:59:49.693
    Feb 27 02:59:49.722: INFO: Waiting up to 5m0s for pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2" in namespace "secrets-6341" to be "Succeeded or Failed"
    Feb 27 02:59:49.728: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.883123ms
    Feb 27 02:59:51.734: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011163477s
    Feb 27 02:59:53.733: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010250819s
    STEP: Saw pod success 02/27/23 02:59:53.733
    Feb 27 02:59:53.733: INFO: Pod "pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2" satisfied condition "Succeeded or Failed"
    Feb 27 02:59:53.735: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 container secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 02:59:53.741
    Feb 27 02:59:53.757: INFO: Waiting for pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 to disappear
    Feb 27 02:59:53.759: INFO: Pod pod-secrets-b0e7a557-49ea-479f-939e-b3517752dab2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:53.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6341" for this suite. 02/27/23 02:59:53.763
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:53.768
Feb 27 02:59:53.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename namespaces 02/27/23 02:59:53.769
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:53.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:53.791
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 02/27/23 02:59:53.795
STEP: patching the Namespace 02/27/23 02:59:53.813
STEP: get the Namespace and ensuring it has the label 02/27/23 02:59:53.819
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1745" for this suite. 02/27/23 02:59:53.838
STEP: Destroying namespace "nspatchtest-0f8c3018-b3e0-4069-9043-b42e991d2f51-1265" for this suite. 02/27/23 02:59:53.844
------------------------------
• [0.082 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:53.768
    Feb 27 02:59:53.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename namespaces 02/27/23 02:59:53.769
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:53.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:53.791
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 02/27/23 02:59:53.795
    STEP: patching the Namespace 02/27/23 02:59:53.813
    STEP: get the Namespace and ensuring it has the label 02/27/23 02:59:53.819
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:53.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1745" for this suite. 02/27/23 02:59:53.838
    STEP: Destroying namespace "nspatchtest-0f8c3018-b3e0-4069-9043-b42e991d2f51-1265" for this suite. 02/27/23 02:59:53.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:53.85
Feb 27 02:59:53.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename sched-pred 02/27/23 02:59:53.851
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:53.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:53.869
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Feb 27 02:59:53.871: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 27 02:59:53.881: INFO: Waiting for terminating namespaces to be deleted...
Feb 27 02:59:53.884: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
Feb 27 02:59:53.896: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:59:53.896: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:59:53.896: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container registry ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container sidecar ready: true, restart count 0
Feb 27 02:59:53.896: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
Feb 27 02:59:53.896: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container controller ready: true, restart count 0
Feb 27 02:59:53.896: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:59:53.896: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:59:53.896: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:59:53.896: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:59:53.896: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:59:53.896: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:59:53.896: INFO: isp-logger-74bf5ff65d-b59dr from monitoring started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container isp-logger ready: true, restart count 0
Feb 27 02:59:53.896: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:59:53.896: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container e2e ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:59:53.896: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:59:53.896: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
Feb 27 02:59:53.914: INFO: default-http-backend-67df9bcb5b-fspzf from ingress-nginx started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 27 02:59:53.914: INFO: nginx-ingress-controller-67d95699d-qglqv from ingress-nginx started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 02:59:53.914: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:59:53.914: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container calicoctl ready: true, restart count 0
Feb 27 02:59:53.914: INFO: ccd-license-consumer-69f48d6d8f-bfqz9 from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container ccd-license-consumer ready: true, restart count 0
Feb 27 02:59:53.914: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:59:53.914: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:59:53.914: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:59:53.914: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:59:53.914: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container metrics-server ready: true, restart count 0
Feb 27 02:59:53.914: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 02:59:53.914: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:59:53.914: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 	Container vmagent-config-reload ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 02:28:08 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
Feb 27 02:59:53.914: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:59:53.914: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 27 02:59:53.914: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.914: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:59:53.914: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
Feb 27 02:59:53.926: INFO: suspend-false-to-true-rs6vh from job-4563 started at 2023-02-27 02:59:47 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container c ready: true, restart count 0
Feb 27 02:59:53.926: INFO: suspend-false-to-true-w9z2k from job-4563 started at 2023-02-27 02:59:47 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container c ready: true, restart count 0
Feb 27 02:59:53.926: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:59:53.926: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:59:53.926: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:59:53.926: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:59:53.926: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:59:53.926: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:59:53.926: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:59:53.926: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:59:53.926: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container node-cache ready: true, restart count 0
Feb 27 02:59:53.926: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:59:53.926: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:59:53.926: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.926: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:59:53.926: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 27 02:59:53.926: INFO: 
Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
Feb 27 02:59:53.942: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 27 02:59:53.942: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container calico-node ready: true, restart count 0
Feb 27 02:59:53.942: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Feb 27 02:59:53.942: INFO: 	Container liveness-probe ready: true, restart count 0
Feb 27 02:59:53.942: INFO: 	Container node-driver-registrar ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container speaker ready: true, restart count 0
Feb 27 02:59:53.942: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container kube-multus ready: true, restart count 0
Feb 27 02:59:53.942: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container kube-proxy ready: true, restart count 1
Feb 27 02:59:53.942: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container kucero ready: true, restart count 0
Feb 27 02:59:53.942: INFO: network-resources-injector-545655c748-mzkpv from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container webhook-server ready: true, restart count 0
Feb 27 02:59:53.942: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container node-cache ready: true, restart count 1
Feb 27 02:59:53.942: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
Feb 27 02:59:53.942: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
Feb 27 02:59:53.942: INFO: 	Container vmalert-config-reload ready: true, restart count 0
Feb 27 02:59:53.942: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container node-cert-exporter ready: true, restart count 0
Feb 27 02:59:53.942: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
Feb 27 02:59:53.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 27 02:59:53.942: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 02:59:53.942
Feb 27 02:59:53.952: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6374" to be "running"
Feb 27 02:59:53.955: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834103ms
Feb 27 02:59:55.960: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008542786s
Feb 27 02:59:55.960: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 02:59:55.964
STEP: Trying to apply a random label on the found node. 02/27/23 02:59:55.977
STEP: verifying the node has the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d 42 02/27/23 02:59:55.989
STEP: Trying to relaunch the pod, now with labels. 02/27/23 02:59:55.993
Feb 27 02:59:56.002: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6374" to be "not pending"
Feb 27 02:59:56.005: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105689ms
Feb 27 02:59:58.014: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011925447s
Feb 27 02:59:58.014: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d off the node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:59:58.018
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d 02/27/23 02:59:58.034
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Feb 27 02:59:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6374" for this suite. 02/27/23 02:59:58.042
------------------------------
• [4.201 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:53.85
    Feb 27 02:59:53.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename sched-pred 02/27/23 02:59:53.851
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:53.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:53.869
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Feb 27 02:59:53.871: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Feb 27 02:59:53.881: INFO: Waiting for terminating namespaces to be deleted...
    Feb 27 02:59:53.884: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-58bsk712-n92-ci-ibd-23-jenkins before test
    Feb 27 02:59:53.896: INFO: calico-node-5k85w from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: csi-cinder-nodeplugin-bds62 from kube-system started at 2023-02-27 01:02:44 +0000 UTC (3 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: eric-lcm-container-registry-registry-54ddd9cdb9-2l27b from kube-system started at 2023-02-27 01:08:19 +0000 UTC (3 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container nginx-tls-terminator ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container registry ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container sidecar ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: eric-lm-combined-server-license-server-client-76bf797c48-g6zcr from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container eric-lm-license-server-client ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: eric-tm-external-connectivity-frontend-controller-57c598db58dsg from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container controller ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: eric-tm-external-connectivity-frontend-speaker-hkwmb from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: kube-multus-ds-amd64-g6mkh from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: kube-proxy-8w2tp from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:59:53.896: INFO: kucero-zsvpx from kube-system started at 2023-02-27 01:02:44 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: node-local-dns-jghdz from kube-system started at 2023-02-27 01:02:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:59:53.896: INFO: eric-pm-node-exporter-z4dl2 from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: isp-logger-74bf5ff65d-b59dr from monitoring started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container isp-logger ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: node-cert-exporter-jgb7v from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: sonobuoy-e2e-job-ce280b850091407c from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container e2e ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-lcd97 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:59:53.896: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-losn7d81-n92-ci-ibd-23-jenkins before test
    Feb 27 02:59:53.914: INFO: default-http-backend-67df9bcb5b-fspzf from ingress-nginx started at 2023-02-27 02:28:07 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container default-http-backend ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: nginx-ingress-controller-67d95699d-qglqv from ingress-nginx started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: calico-node-7chz2 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: calicoctl-64848f7f7c-ssjb9 from kube-system started at 2023-02-27 01:19:45 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container calicoctl ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: ccd-license-consumer-69f48d6d8f-bfqz9 from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container ccd-license-consumer ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: csi-cinder-nodeplugin-46cv8 from kube-system started at 2023-02-27 01:02:51 +0000 UTC (3 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-data-document-database-pg-0 from kube-system started at 2023-02-27 01:16:04 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-data-document-database-pg ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-tm-external-connectivity-frontend-speaker-xjcj4 from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: kube-multus-ds-amd64-sqs2s from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: kube-proxy-jcbgm from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:59:53.914: INFO: kucero-8svhq from kube-system started at 2023-02-27 01:02:51 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: metrics-server-697d576bc4-2hwrw from kube-system started at 2023-02-27 01:13:34 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container metrics-server ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: network-resources-injector-545655c748-8ttr4 from kube-system started at 2023-02-27 01:18:48 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: node-local-dns-jmd95 from kube-system started at 2023-02-27 01:02:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:59:53.914: INFO: eric-pm-alertmanager-547b74fff-2zrcr from monitoring started at 2023-02-27 01:11:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-pm-alertmanager ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 	Container eric-pm-alertmanager-configmap-reload ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-pm-node-exporter-4mg5c from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-victoria-metrics-agent-6fb8955b7b-x7hlx from monitoring started at 2023-02-27 01:10:00 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-agent ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 	Container vmagent-config-reload ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vminsert-5bdbcb79b-nbr8s from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vminsert ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vmselect-54b7564777-xvc4l from monitoring started at 2023-02-27 01:09:22 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vmselect ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: eric-victoria-metrics-cluster-vmstorage-0 from monitoring started at 2023-02-27 02:28:08 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container eric-victoria-metrics-cluster-vmstorage ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: node-cert-exporter-gngw2 from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: sonobuoy from sonobuoy started at 2023-02-27 01:31:21 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-6snsc from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.914: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:59:53.914: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins before test
    Feb 27 02:59:53.926: INFO: suspend-false-to-true-rs6vh from job-4563 started at 2023-02-27 02:59:47 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container c ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: suspend-false-to-true-w9z2k from job-4563 started at 2023-02-27 02:59:47 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container c ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: calico-node-42t9d from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: csi-cinder-nodeplugin-ptfx8 from kube-system started at 2023-02-27 01:02:33 +0000 UTC (3 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: eric-tm-external-connectivity-frontend-speaker-4xqtd from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: kube-multus-ds-amd64-nlglc from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: kube-proxy-pvz7k from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:59:53.926: INFO: kucero-lgwqj from kube-system started at 2023-02-27 01:02:33 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: node-local-dns-99jdq from kube-system started at 2023-02-27 01:02:02 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container node-cache ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: eric-pm-node-exporter-tg4hb from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: node-cert-exporter-7h7nd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-bg9hs from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.926: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: 	Container systemd-logs ready: true, restart count 0
    Feb 27 02:59:53.926: INFO: 
    Logging pods the apiserver thinks is on node worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins before test
    Feb 27 02:59:53.942: INFO: nginx-ingress-controller-67d95699d-nkkpt from ingress-nginx started at 2023-02-27 01:07:20 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: calico-node-q6vpb from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container calico-node ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: csi-cinder-nodeplugin-r96td from kube-system started at 2023-02-27 01:02:35 +0000 UTC (3 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: 	Container liveness-probe ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-app-sys-info-handler-674c6dfbf5-jt2mw from kube-system started at 2023-02-27 01:14:46 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-si-application-sys-info-handler ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-lm-combined-server-license-consumer-handler-58b4fc75cgnl7q from kube-system started at 2023-02-27 01:17:01 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-lm-license-consumer-handler ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-tm-external-connectivity-frontend-speaker-7zm7w from kube-system started at 2023-02-27 01:04:14 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container speaker ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: kube-multus-ds-amd64-m4dgt from kube-system started at 2023-02-27 01:02:35 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container kube-multus ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: kube-proxy-wdnsg from kube-system started at 2023-02-27 01:02:06 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container kube-proxy ready: true, restart count 1
    Feb 27 02:59:53.942: INFO: kucero-w7c79 from kube-system started at 2023-02-27 01:02:36 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container kucero ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: network-resources-injector-545655c748-mzkpv from kube-system started at 2023-02-27 01:58:12 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container webhook-server ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: node-local-dns-s8t4t from kube-system started at 2023-02-27 01:02:04 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container node-cache ready: true, restart count 1
    Feb 27 02:59:53.942: INFO: eric-pm-kube-state-metrics-8488b76fc5-pcnjq from monitoring started at 2023-02-27 01:11:10 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-pm-kube-state-metrics ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-pm-node-exporter-pthcx from monitoring started at 2023-02-27 01:10:40 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-pm-node-exporter ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-pm-server-utils-56888b6858-kjswh from monitoring started at 2023-02-27 01:11:48 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-pm-server-utils ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: eric-victoria-metrics-alert-server-77cc8f97f-2pcrj from monitoring started at 2023-02-27 01:10:15 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container eric-victoria-metrics-alert-server ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: 	Container vmalert-config-reload ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: node-cert-exporter-sdthd from monitoring started at 2023-02-27 01:11:39 +0000 UTC (1 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container node-cert-exporter ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: sonobuoy-systemd-logs-daemon-set-788b720f1c67459a-m9bm4 from sonobuoy started at 2023-02-27 01:31:23 +0000 UTC (2 container statuses recorded)
    Feb 27 02:59:53.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Feb 27 02:59:53.942: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 02/27/23 02:59:53.942
    Feb 27 02:59:53.952: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6374" to be "running"
    Feb 27 02:59:53.955: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.834103ms
    Feb 27 02:59:55.960: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008542786s
    Feb 27 02:59:55.960: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 02/27/23 02:59:55.964
    STEP: Trying to apply a random label on the found node. 02/27/23 02:59:55.977
    STEP: verifying the node has the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d 42 02/27/23 02:59:55.989
    STEP: Trying to relaunch the pod, now with labels. 02/27/23 02:59:55.993
    Feb 27 02:59:56.002: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6374" to be "not pending"
    Feb 27 02:59:56.005: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.105689ms
    Feb 27 02:59:58.014: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.011925447s
    Feb 27 02:59:58.014: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d off the node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins 02/27/23 02:59:58.018
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5e508a6d-2296-4213-9e06-a709d4d93c9d 02/27/23 02:59:58.034
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Feb 27 02:59:58.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6374" for this suite. 02/27/23 02:59:58.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 02:59:58.052
Feb 27 02:59:58.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename gc 02/27/23 02:59:58.053
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:58.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:58.078
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 02/27/23 02:59:58.088
STEP: create the rc2 02/27/23 02:59:58.093
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 03:00:03.117
STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 03:00:03.898
STEP: wait for the rc to be deleted 02/27/23 03:00:03.912
Feb 27 03:00:08.924: INFO: 63 pods remaining
Feb 27 03:00:08.924: INFO: 63 pods has nil DeletionTimestamp
Feb 27 03:00:08.924: INFO: 
STEP: Gathering metrics 02/27/23 03:00:13.926
Feb 27 03:00:14.053: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
Feb 27 03:00:14.058: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 5.068626ms
Feb 27 03:00:14.058: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
Feb 27 03:00:14.058: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
Feb 27 03:00:14.106: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Feb 27 03:00:14.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-22vsc" in namespace "gc-7860"
Feb 27 03:00:14.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-294hk" in namespace "gc-7860"
Feb 27 03:00:14.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fdmj" in namespace "gc-7860"
Feb 27 03:00:14.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-2h8gq" in namespace "gc-7860"
Feb 27 03:00:14.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l2tp" in namespace "gc-7860"
Feb 27 03:00:14.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rhx5" in namespace "gc-7860"
Feb 27 03:00:14.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n65w" in namespace "gc-7860"
Feb 27 03:00:14.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nfnw" in namespace "gc-7860"
Feb 27 03:00:14.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bdpp" in namespace "gc-7860"
Feb 27 03:00:14.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jl56" in namespace "gc-7860"
Feb 27 03:00:14.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xbt4" in namespace "gc-7860"
Feb 27 03:00:14.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p986" in namespace "gc-7860"
Feb 27 03:00:14.252: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vfzq" in namespace "gc-7860"
Feb 27 03:00:14.262: INFO: Deleting pod "simpletest-rc-to-be-deleted-772zf" in namespace "gc-7860"
Feb 27 03:00:14.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-77t6p" in namespace "gc-7860"
Feb 27 03:00:14.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gc29" in namespace "gc-7860"
Feb 27 03:00:14.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gjbv" in namespace "gc-7860"
Feb 27 03:00:14.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dk2s" in namespace "gc-7860"
Feb 27 03:00:14.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nzcn" in namespace "gc-7860"
Feb 27 03:00:14.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xjrc" in namespace "gc-7860"
Feb 27 03:00:14.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-94w4z" in namespace "gc-7860"
Feb 27 03:00:14.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jxkq" in namespace "gc-7860"
Feb 27 03:00:14.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2shk" in namespace "gc-7860"
Feb 27 03:00:14.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj52v" in namespace "gc-7860"
Feb 27 03:00:14.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccdhg" in namespace "gc-7860"
Feb 27 03:00:14.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccjdq" in namespace "gc-7860"
Feb 27 03:00:14.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfr8g" in namespace "gc-7860"
Feb 27 03:00:14.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjvn5" in namespace "gc-7860"
Feb 27 03:00:14.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5jnd" in namespace "gc-7860"
Feb 27 03:00:14.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8gf9" in namespace "gc-7860"
Feb 27 03:00:14.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddpvv" in namespace "gc-7860"
Feb 27 03:00:14.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-f246b" in namespace "gc-7860"
Feb 27 03:00:14.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-frjmk" in namespace "gc-7860"
Feb 27 03:00:14.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftdrn" in namespace "gc-7860"
Feb 27 03:00:14.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftgd4" in namespace "gc-7860"
Feb 27 03:00:14.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq2rq" in namespace "gc-7860"
Feb 27 03:00:15.004: INFO: Deleting pod "simpletest-rc-to-be-deleted-gv4n7" in namespace "gc-7860"
Feb 27 03:00:15.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzr8d" in namespace "gc-7860"
Feb 27 03:00:15.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbfmt" in namespace "gc-7860"
Feb 27 03:00:15.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbr6d" in namespace "gc-7860"
Feb 27 03:00:15.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqvpl" in namespace "gc-7860"
Feb 27 03:00:15.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4m7q" in namespace "gc-7860"
Feb 27 03:00:15.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-j52mr" in namespace "gc-7860"
Feb 27 03:00:15.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-jckzg" in namespace "gc-7860"
Feb 27 03:00:15.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmdql" in namespace "gc-7860"
Feb 27 03:00:15.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-jv6vv" in namespace "gc-7860"
Feb 27 03:00:15.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2jbs" in namespace "gc-7860"
Feb 27 03:00:15.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-k428g" in namespace "gc-7860"
Feb 27 03:00:15.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-kx57h" in namespace "gc-7860"
Feb 27 03:00:15.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzmd5" in namespace "gc-7860"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:15.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7860" for this suite. 02/27/23 03:00:15.356
------------------------------
• [SLOW TEST] [17.315 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 02:59:58.052
    Feb 27 02:59:58.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename gc 02/27/23 02:59:58.053
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 02:59:58.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 02:59:58.078
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 02/27/23 02:59:58.088
    STEP: create the rc2 02/27/23 02:59:58.093
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 02/27/23 03:00:03.117
    STEP: delete the rc simpletest-rc-to-be-deleted 02/27/23 03:00:03.898
    STEP: wait for the rc to be deleted 02/27/23 03:00:03.912
    Feb 27 03:00:08.924: INFO: 63 pods remaining
    Feb 27 03:00:08.924: INFO: 63 pods has nil DeletionTimestamp
    Feb 27 03:00:08.924: INFO: 
    STEP: Gathering metrics 02/27/23 03:00:13.926
    Feb 27 03:00:14.053: INFO: Waiting up to 5m0s for pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" in namespace "kube-system" to be "running and ready"
    Feb 27 03:00:14.058: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins": Phase="Running", Reason="", readiness=true. Elapsed: 5.068626ms
    Feb 27 03:00:14.058: INFO: The phase of Pod kube-controller-manager-master-2-n92-ci-ibd-23-jenkins is Running (Ready = true)
    Feb 27 03:00:14.058: INFO: Pod "kube-controller-manager-master-2-n92-ci-ibd-23-jenkins" satisfied condition "running and ready"
    Feb 27 03:00:14.106: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Feb 27 03:00:14.106: INFO: Deleting pod "simpletest-rc-to-be-deleted-22vsc" in namespace "gc-7860"
    Feb 27 03:00:14.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-294hk" in namespace "gc-7860"
    Feb 27 03:00:14.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fdmj" in namespace "gc-7860"
    Feb 27 03:00:14.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-2h8gq" in namespace "gc-7860"
    Feb 27 03:00:14.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-2l2tp" in namespace "gc-7860"
    Feb 27 03:00:14.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rhx5" in namespace "gc-7860"
    Feb 27 03:00:14.179: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n65w" in namespace "gc-7860"
    Feb 27 03:00:14.189: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nfnw" in namespace "gc-7860"
    Feb 27 03:00:14.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bdpp" in namespace "gc-7860"
    Feb 27 03:00:14.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jl56" in namespace "gc-7860"
    Feb 27 03:00:14.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xbt4" in namespace "gc-7860"
    Feb 27 03:00:14.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p986" in namespace "gc-7860"
    Feb 27 03:00:14.252: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vfzq" in namespace "gc-7860"
    Feb 27 03:00:14.262: INFO: Deleting pod "simpletest-rc-to-be-deleted-772zf" in namespace "gc-7860"
    Feb 27 03:00:14.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-77t6p" in namespace "gc-7860"
    Feb 27 03:00:14.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gc29" in namespace "gc-7860"
    Feb 27 03:00:14.334: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gjbv" in namespace "gc-7860"
    Feb 27 03:00:14.403: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dk2s" in namespace "gc-7860"
    Feb 27 03:00:14.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nzcn" in namespace "gc-7860"
    Feb 27 03:00:14.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xjrc" in namespace "gc-7860"
    Feb 27 03:00:14.612: INFO: Deleting pod "simpletest-rc-to-be-deleted-94w4z" in namespace "gc-7860"
    Feb 27 03:00:14.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jxkq" in namespace "gc-7860"
    Feb 27 03:00:14.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2shk" in namespace "gc-7860"
    Feb 27 03:00:14.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj52v" in namespace "gc-7860"
    Feb 27 03:00:14.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccdhg" in namespace "gc-7860"
    Feb 27 03:00:14.748: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccjdq" in namespace "gc-7860"
    Feb 27 03:00:14.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfr8g" in namespace "gc-7860"
    Feb 27 03:00:14.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjvn5" in namespace "gc-7860"
    Feb 27 03:00:14.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5jnd" in namespace "gc-7860"
    Feb 27 03:00:14.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8gf9" in namespace "gc-7860"
    Feb 27 03:00:14.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddpvv" in namespace "gc-7860"
    Feb 27 03:00:14.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-f246b" in namespace "gc-7860"
    Feb 27 03:00:14.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-frjmk" in namespace "gc-7860"
    Feb 27 03:00:14.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftdrn" in namespace "gc-7860"
    Feb 27 03:00:14.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftgd4" in namespace "gc-7860"
    Feb 27 03:00:14.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq2rq" in namespace "gc-7860"
    Feb 27 03:00:15.004: INFO: Deleting pod "simpletest-rc-to-be-deleted-gv4n7" in namespace "gc-7860"
    Feb 27 03:00:15.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzr8d" in namespace "gc-7860"
    Feb 27 03:00:15.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbfmt" in namespace "gc-7860"
    Feb 27 03:00:15.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbr6d" in namespace "gc-7860"
    Feb 27 03:00:15.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqvpl" in namespace "gc-7860"
    Feb 27 03:00:15.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4m7q" in namespace "gc-7860"
    Feb 27 03:00:15.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-j52mr" in namespace "gc-7860"
    Feb 27 03:00:15.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-jckzg" in namespace "gc-7860"
    Feb 27 03:00:15.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmdql" in namespace "gc-7860"
    Feb 27 03:00:15.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-jv6vv" in namespace "gc-7860"
    Feb 27 03:00:15.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2jbs" in namespace "gc-7860"
    Feb 27 03:00:15.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-k428g" in namespace "gc-7860"
    Feb 27 03:00:15.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-kx57h" in namespace "gc-7860"
    Feb 27 03:00:15.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzmd5" in namespace "gc-7860"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:15.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7860" for this suite. 02/27/23 03:00:15.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:15.368
Feb 27 03:00:15.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename container-probe 02/27/23 03:00:15.369
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:15.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:15.408
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Feb 27 03:00:15.465: INFO: Waiting up to 5m0s for pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda" in namespace "container-probe-1993" to be "running and ready"
Feb 27 03:00:15.471: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041107ms
Feb 27 03:00:15.471: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Pending, waiting for it to be Running (with Ready = true)
Feb 27 03:00:17.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012013484s
Feb 27 03:00:17.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Pending, waiting for it to be Running (with Ready = true)
Feb 27 03:00:19.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 4.010140612s
Feb 27 03:00:19.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:21.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 6.010766996s
Feb 27 03:00:21.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:23.475: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 8.010040704s
Feb 27 03:00:23.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:25.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 10.01117661s
Feb 27 03:00:25.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:27.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 12.010091615s
Feb 27 03:00:27.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:29.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 14.011242544s
Feb 27 03:00:29.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:31.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 16.010665989s
Feb 27 03:00:31.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:33.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 18.010750738s
Feb 27 03:00:33.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:35.478: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 20.012363102s
Feb 27 03:00:35.478: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
Feb 27 03:00:37.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=true. Elapsed: 22.010948979s
Feb 27 03:00:37.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = true)
Feb 27 03:00:37.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda" satisfied condition "running and ready"
Feb 27 03:00:37.481: INFO: Container started at 2023-02-27 03:00:16 +0000 UTC, pod became ready at 2023-02-27 03:00:36 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:37.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1993" for this suite. 02/27/23 03:00:37.488
------------------------------
• [SLOW TEST] [22.128 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:15.368
    Feb 27 03:00:15.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename container-probe 02/27/23 03:00:15.369
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:15.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:15.408
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Feb 27 03:00:15.465: INFO: Waiting up to 5m0s for pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda" in namespace "container-probe-1993" to be "running and ready"
    Feb 27 03:00:15.471: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041107ms
    Feb 27 03:00:15.471: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 03:00:17.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012013484s
    Feb 27 03:00:17.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 03:00:19.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 4.010140612s
    Feb 27 03:00:19.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:21.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 6.010766996s
    Feb 27 03:00:21.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:23.475: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 8.010040704s
    Feb 27 03:00:23.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:25.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 10.01117661s
    Feb 27 03:00:25.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:27.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 12.010091615s
    Feb 27 03:00:27.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:29.477: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 14.011242544s
    Feb 27 03:00:29.477: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:31.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 16.010665989s
    Feb 27 03:00:31.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:33.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 18.010750738s
    Feb 27 03:00:33.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:35.478: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=false. Elapsed: 20.012363102s
    Feb 27 03:00:35.478: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = false)
    Feb 27 03:00:37.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda": Phase="Running", Reason="", readiness=true. Elapsed: 22.010948979s
    Feb 27 03:00:37.476: INFO: The phase of Pod test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda is Running (Ready = true)
    Feb 27 03:00:37.476: INFO: Pod "test-webserver-44ad9c60-179e-418e-91b1-8fa2fac52dda" satisfied condition "running and ready"
    Feb 27 03:00:37.481: INFO: Container started at 2023-02-27 03:00:16 +0000 UTC, pod became ready at 2023-02-27 03:00:36 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:37.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1993" for this suite. 02/27/23 03:00:37.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:37.497
Feb 27 03:00:37.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 03:00:37.497
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:37.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:37.518
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-ed599203-2fcf-4c4a-a54f-f72dc0f612ec 02/27/23 03:00:37.521
STEP: Creating a pod to test consume secrets 02/27/23 03:00:37.525
Feb 27 03:00:37.556: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e" in namespace "projected-3380" to be "Succeeded or Failed"
Feb 27 03:00:37.558: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300229ms
Feb 27 03:00:39.563: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007293804s
Feb 27 03:00:41.563: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007711088s
STEP: Saw pod success 02/27/23 03:00:41.563
Feb 27 03:00:41.564: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e" satisfied condition "Succeeded or Failed"
Feb 27 03:00:41.566: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e container projected-secret-volume-test: <nil>
STEP: delete the pod 02/27/23 03:00:41.575
Feb 27 03:00:41.584: INFO: Waiting for pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e to disappear
Feb 27 03:00:41.586: INFO: Pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:41.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3380" for this suite. 02/27/23 03:00:41.591
------------------------------
• [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:37.497
    Feb 27 03:00:37.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 03:00:37.497
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:37.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:37.518
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-ed599203-2fcf-4c4a-a54f-f72dc0f612ec 02/27/23 03:00:37.521
    STEP: Creating a pod to test consume secrets 02/27/23 03:00:37.525
    Feb 27 03:00:37.556: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e" in namespace "projected-3380" to be "Succeeded or Failed"
    Feb 27 03:00:37.558: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300229ms
    Feb 27 03:00:39.563: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007293804s
    Feb 27 03:00:41.563: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007711088s
    STEP: Saw pod success 02/27/23 03:00:41.563
    Feb 27 03:00:41.564: INFO: Pod "pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e" satisfied condition "Succeeded or Failed"
    Feb 27 03:00:41.566: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e container projected-secret-volume-test: <nil>
    STEP: delete the pod 02/27/23 03:00:41.575
    Feb 27 03:00:41.584: INFO: Waiting for pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e to disappear
    Feb 27 03:00:41.586: INFO: Pod pod-projected-secrets-9f8b20ca-7f09-4635-a9a7-b148473e222e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:41.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3380" for this suite. 02/27/23 03:00:41.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:41.597
Feb 27 03:00:41.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubelet-test 02/27/23 03:00:41.598
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:41.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:41.615
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:41.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9382" for this suite. 02/27/23 03:00:41.645
------------------------------
• [0.053 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:41.597
    Feb 27 03:00:41.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubelet-test 02/27/23 03:00:41.598
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:41.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:41.615
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:41.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9382" for this suite. 02/27/23 03:00:41.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:41.651
Feb 27 03:00:41.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename projected 02/27/23 03:00:41.652
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:41.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:41.678
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 02/27/23 03:00:41.681
Feb 27 03:00:41.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51" in namespace "projected-3916" to be "Succeeded or Failed"
Feb 27 03:00:41.697: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583229ms
Feb 27 03:00:43.700: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007214882s
Feb 27 03:00:45.704: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010412211s
STEP: Saw pod success 02/27/23 03:00:45.704
Feb 27 03:00:45.704: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51" satisfied condition "Succeeded or Failed"
Feb 27 03:00:45.707: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 container client-container: <nil>
STEP: delete the pod 02/27/23 03:00:45.712
Feb 27 03:00:45.722: INFO: Waiting for pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 to disappear
Feb 27 03:00:45.724: INFO: Pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3916" for this suite. 02/27/23 03:00:45.729
------------------------------
• [4.083 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:41.651
    Feb 27 03:00:41.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename projected 02/27/23 03:00:41.652
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:41.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:41.678
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 02/27/23 03:00:41.681
    Feb 27 03:00:41.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51" in namespace "projected-3916" to be "Succeeded or Failed"
    Feb 27 03:00:41.697: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583229ms
    Feb 27 03:00:43.700: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007214882s
    Feb 27 03:00:45.704: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010412211s
    STEP: Saw pod success 02/27/23 03:00:45.704
    Feb 27 03:00:45.704: INFO: Pod "downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51" satisfied condition "Succeeded or Failed"
    Feb 27 03:00:45.707: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 container client-container: <nil>
    STEP: delete the pod 02/27/23 03:00:45.712
    Feb 27 03:00:45.722: INFO: Waiting for pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 to disappear
    Feb 27 03:00:45.724: INFO: Pod downwardapi-volume-1aa7c83f-216a-4f4b-b4fe-47c652d25f51 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:45.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3916" for this suite. 02/27/23 03:00:45.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:45.734
Feb 27 03:00:45.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 03:00:45.735
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:45.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:45.756
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 03:00:45.759
Feb 27 03:00:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 03:00:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9631" for this suite. 02/27/23 03:00:54.772
------------------------------
• [SLOW TEST] [9.045 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:45.734
    Feb 27 03:00:45.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename crd-publish-openapi 02/27/23 03:00:45.735
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:45.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:45.756
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 02/27/23 03:00:45.759
    Feb 27 03:00:45.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 03:00:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:54.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9631" for this suite. 02/27/23 03:00:54.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:54.78
Feb 27 03:00:54.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename cronjob 02/27/23 03:00:54.781
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:54.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:54.798
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 02/27/23 03:00:54.8
STEP: creating 02/27/23 03:00:54.8
STEP: getting 02/27/23 03:00:54.812
STEP: listing 02/27/23 03:00:54.815
STEP: watching 02/27/23 03:00:54.817
Feb 27 03:00:54.817: INFO: starting watch
STEP: cluster-wide listing 02/27/23 03:00:54.818
STEP: cluster-wide watching 02/27/23 03:00:54.82
Feb 27 03:00:54.821: INFO: starting watch
STEP: patching 02/27/23 03:00:54.821
STEP: updating 02/27/23 03:00:54.827
Feb 27 03:00:54.834: INFO: waiting for watch events with expected annotations
Feb 27 03:00:54.834: INFO: saw patched and updated annotations
STEP: patching /status 02/27/23 03:00:54.834
STEP: updating /status 02/27/23 03:00:54.84
STEP: get /status 02/27/23 03:00:54.847
STEP: deleting 02/27/23 03:00:54.849
STEP: deleting a collection 02/27/23 03:00:54.861
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4881" for this suite. 02/27/23 03:00:54.879
------------------------------
• [0.103 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:54.78
    Feb 27 03:00:54.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename cronjob 02/27/23 03:00:54.781
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:54.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:54.798
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 02/27/23 03:00:54.8
    STEP: creating 02/27/23 03:00:54.8
    STEP: getting 02/27/23 03:00:54.812
    STEP: listing 02/27/23 03:00:54.815
    STEP: watching 02/27/23 03:00:54.817
    Feb 27 03:00:54.817: INFO: starting watch
    STEP: cluster-wide listing 02/27/23 03:00:54.818
    STEP: cluster-wide watching 02/27/23 03:00:54.82
    Feb 27 03:00:54.821: INFO: starting watch
    STEP: patching 02/27/23 03:00:54.821
    STEP: updating 02/27/23 03:00:54.827
    Feb 27 03:00:54.834: INFO: waiting for watch events with expected annotations
    Feb 27 03:00:54.834: INFO: saw patched and updated annotations
    STEP: patching /status 02/27/23 03:00:54.834
    STEP: updating /status 02/27/23 03:00:54.84
    STEP: get /status 02/27/23 03:00:54.847
    STEP: deleting 02/27/23 03:00:54.849
    STEP: deleting a collection 02/27/23 03:00:54.861
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4881" for this suite. 02/27/23 03:00:54.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:54.884
Feb 27 03:00:54.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename dns 02/27/23 03:00:54.885
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:54.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:54.9
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 03:00:54.903
Feb 27 03:00:54.930: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7666  997c0de2-17c9-4987-b3ae-bf46a4b8cc7b 70880 0 2023-02-27 03:00:54 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 03:00:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-44kwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-44kwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 27 03:00:54.930: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7666" to be "running and ready"
Feb 27 03:00:54.934: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.151288ms
Feb 27 03:00:54.934: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Feb 27 03:00:56.938: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007395245s
Feb 27 03:00:56.938: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Feb 27 03:00:56.938: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 03:00:56.938
Feb 27 03:00:56.938: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7666 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 03:00:56.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 03:00:56.938: INFO: ExecWithOptions: Clientset creation
Feb 27 03:00:56.938: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7666/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 02/27/23 03:00:57.021
Feb 27 03:00:57.021: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7666 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Feb 27 03:00:57.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
Feb 27 03:00:57.022: INFO: ExecWithOptions: Clientset creation
Feb 27 03:00:57.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7666/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Feb 27 03:00:57.110: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7666" for this suite. 02/27/23 03:00:57.128
------------------------------
• [2.250 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:54.884
    Feb 27 03:00:54.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename dns 02/27/23 03:00:54.885
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:54.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:54.9
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 02/27/23 03:00:54.903
    Feb 27 03:00:54.930: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7666  997c0de2-17c9-4987-b3ae-bf46a4b8cc7b 70880 0 2023-02-27 03:00:54 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-02-27 03:00:54 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-44kwl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-44kwl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Feb 27 03:00:54.930: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-7666" to be "running and ready"
    Feb 27 03:00:54.934: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.151288ms
    Feb 27 03:00:54.934: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 03:00:56.938: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007395245s
    Feb 27 03:00:56.938: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Feb 27 03:00:56.938: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 02/27/23 03:00:56.938
    Feb 27 03:00:56.938: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7666 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 03:00:56.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 03:00:56.938: INFO: ExecWithOptions: Clientset creation
    Feb 27 03:00:56.938: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7666/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 02/27/23 03:00:57.021
    Feb 27 03:00:57.021: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7666 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Feb 27 03:00:57.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    Feb 27 03:00:57.022: INFO: ExecWithOptions: Clientset creation
    Feb 27 03:00:57.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-7666/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Feb 27 03:00:57.110: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7666" for this suite. 02/27/23 03:00:57.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:57.135
Feb 27 03:00:57.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename svcaccounts 02/27/23 03:00:57.135
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:57.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:57.156
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Feb 27 03:00:57.167: INFO: Got root ca configmap in namespace "svcaccounts-3945"
Feb 27 03:00:57.172: INFO: Deleted root ca configmap in namespace "svcaccounts-3945"
STEP: waiting for a new root ca configmap created 02/27/23 03:00:57.673
Feb 27 03:00:57.676: INFO: Recreated root ca configmap in namespace "svcaccounts-3945"
Feb 27 03:00:57.682: INFO: Updated root ca configmap in namespace "svcaccounts-3945"
STEP: waiting for the root ca configmap reconciled 02/27/23 03:00:58.182
Feb 27 03:00:58.185: INFO: Reconciled root ca configmap in namespace "svcaccounts-3945"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:58.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3945" for this suite. 02/27/23 03:00:58.189
------------------------------
• [1.060 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:57.135
    Feb 27 03:00:57.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename svcaccounts 02/27/23 03:00:57.135
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:57.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:57.156
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Feb 27 03:00:57.167: INFO: Got root ca configmap in namespace "svcaccounts-3945"
    Feb 27 03:00:57.172: INFO: Deleted root ca configmap in namespace "svcaccounts-3945"
    STEP: waiting for a new root ca configmap created 02/27/23 03:00:57.673
    Feb 27 03:00:57.676: INFO: Recreated root ca configmap in namespace "svcaccounts-3945"
    Feb 27 03:00:57.682: INFO: Updated root ca configmap in namespace "svcaccounts-3945"
    STEP: waiting for the root ca configmap reconciled 02/27/23 03:00:58.182
    Feb 27 03:00:58.185: INFO: Reconciled root ca configmap in namespace "svcaccounts-3945"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:58.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3945" for this suite. 02/27/23 03:00:58.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:58.195
Feb 27 03:00:58.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename services 02/27/23 03:00:58.196
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:58.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:58.212
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Feb 27 03:00:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8542" for this suite. 02/27/23 03:00:58.22
------------------------------
• [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:58.195
    Feb 27 03:00:58.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename services 02/27/23 03:00:58.196
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:58.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:58.212
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:00:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8542" for this suite. 02/27/23 03:00:58.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:00:58.227
Feb 27 03:00:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename webhook 02/27/23 03:00:58.228
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:58.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:58.241
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 02/27/23 03:00:58.268
STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 03:00:58.632
STEP: Deploying the webhook pod 02/27/23 03:00:58.644
STEP: Wait for the deployment to be ready 02/27/23 03:00:58.659
Feb 27 03:00:58.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 02/27/23 03:01:00.681
STEP: Verifying the service has paired with the endpoint 02/27/23 03:01:00.698
Feb 27 03:01:01.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 02/27/23 03:01:01.796
STEP: Creating a configMap that should be mutated 02/27/23 03:01:01.807
STEP: Deleting the collection of validation webhooks 02/27/23 03:01:01.842
STEP: Creating a configMap that should not be mutated 02/27/23 03:01:01.888
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:01.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1641" for this suite. 02/27/23 03:01:01.962
STEP: Destroying namespace "webhook-1641-markers" for this suite. 02/27/23 03:01:01.972
------------------------------
• [3.761 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:00:58.227
    Feb 27 03:00:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename webhook 02/27/23 03:00:58.228
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:00:58.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:00:58.241
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 02/27/23 03:00:58.268
    STEP: Create role binding to let webhook read extension-apiserver-authentication 02/27/23 03:00:58.632
    STEP: Deploying the webhook pod 02/27/23 03:00:58.644
    STEP: Wait for the deployment to be ready 02/27/23 03:00:58.659
    Feb 27 03:00:58.666: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 02/27/23 03:01:00.681
    STEP: Verifying the service has paired with the endpoint 02/27/23 03:01:00.698
    Feb 27 03:01:01.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 02/27/23 03:01:01.796
    STEP: Creating a configMap that should be mutated 02/27/23 03:01:01.807
    STEP: Deleting the collection of validation webhooks 02/27/23 03:01:01.842
    STEP: Creating a configMap that should not be mutated 02/27/23 03:01:01.888
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:01.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1641" for this suite. 02/27/23 03:01:01.962
    STEP: Destroying namespace "webhook-1641-markers" for this suite. 02/27/23 03:01:01.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:01.988
Feb 27 03:01:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename kubectl 02/27/23 03:01:01.989
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:02.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:02.014
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 02/27/23 03:01:02.016
Feb 27 03:01:02.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 run logs-generator --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 27 03:01:02.099: INFO: stderr: ""
Feb 27 03:01:02.099: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 02/27/23 03:01:02.099
Feb 27 03:01:02.100: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 27 03:01:02.100: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3774" to be "running and ready, or succeeded"
Feb 27 03:01:02.104: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000104ms
Feb 27 03:01:02.104: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
Feb 27 03:01:04.109: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008882889s
Feb 27 03:01:04.109: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 27 03:01:04.109: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 02/27/23 03:01:04.109
Feb 27 03:01:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator'
Feb 27 03:01:04.175: INFO: stderr: ""
Feb 27 03:01:04.175: INFO: stdout: "I0227 03:01:02.889700       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zhrh 355\nI0227 03:01:03.089870       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/87n 336\nI0227 03:01:03.290449       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/qkf6 569\nI0227 03:01:03.489732       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/s8mh 599\nI0227 03:01:03.690062       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l2z9 309\nI0227 03:01:03.890324       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wwhv 200\nI0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
STEP: limiting log lines 02/27/23 03:01:04.175
Feb 27 03:01:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --tail=1'
Feb 27 03:01:04.243: INFO: stderr: ""
Feb 27 03:01:04.243: INFO: stdout: "I0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
Feb 27 03:01:04.243: INFO: got output "I0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
STEP: limiting log bytes 02/27/23 03:01:04.243
Feb 27 03:01:04.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --limit-bytes=1'
Feb 27 03:01:04.319: INFO: stderr: ""
Feb 27 03:01:04.319: INFO: stdout: "I"
Feb 27 03:01:04.319: INFO: got output "I"
STEP: exposing timestamps 02/27/23 03:01:04.319
Feb 27 03:01:04.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --tail=1 --timestamps'
Feb 27 03:01:04.427: INFO: stderr: ""
Feb 27 03:01:04.427: INFO: stdout: "2023-02-27T05:01:04.289932952+02:00 I0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\n"
Feb 27 03:01:04.427: INFO: got output "2023-02-27T05:01:04.289932952+02:00 I0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\n"
STEP: restricting to a time range 02/27/23 03:01:04.427
Feb 27 03:01:06.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --since=1s'
Feb 27 03:01:07.006: INFO: stderr: ""
Feb 27 03:01:07.006: INFO: stdout: "I0227 03:01:06.090706       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/7wmk 346\nI0227 03:01:06.290081       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nz86 279\nI0227 03:01:06.490448       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/gxm5 243\nI0227 03:01:06.690768       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/9r7s 282\nI0227 03:01:06.890092       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/t9c 543\n"
Feb 27 03:01:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --since=24h'
Feb 27 03:01:07.082: INFO: stderr: ""
Feb 27 03:01:07.082: INFO: stdout: "I0227 03:01:02.889700       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zhrh 355\nI0227 03:01:03.089870       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/87n 336\nI0227 03:01:03.290449       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/qkf6 569\nI0227 03:01:03.489732       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/s8mh 599\nI0227 03:01:03.690062       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l2z9 309\nI0227 03:01:03.890324       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wwhv 200\nI0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\nI0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\nI0227 03:01:04.490144       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/f5d 429\nI0227 03:01:04.690436       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/qnpk 515\nI0227 03:01:04.890794       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/fk2 222\nI0227 03:01:05.090095       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/w2nx 394\nI0227 03:01:05.290447       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/mtln 308\nI0227 03:01:05.489742       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/gzhz 384\nI0227 03:01:05.690025       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/nb5 239\nI0227 03:01:05.890386       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/p5jp 296\nI0227 03:01:06.090706       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/7wmk 346\nI0227 03:01:06.290081       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nz86 279\nI0227 03:01:06.490448       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/gxm5 243\nI0227 03:01:06.690768       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/9r7s 282\nI0227 03:01:06.890092       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/t9c 543\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Feb 27 03:01:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 delete pod logs-generator'
Feb 27 03:01:08.375: INFO: stderr: ""
Feb 27 03:01:08.375: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:08.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3774" for this suite. 02/27/23 03:01:08.379
------------------------------
• [SLOW TEST] [6.396 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:01.988
    Feb 27 03:01:01.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename kubectl 02/27/23 03:01:01.989
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:02.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:02.014
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 02/27/23 03:01:02.016
    Feb 27 03:01:02.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 run logs-generator --image=armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Feb 27 03:01:02.099: INFO: stderr: ""
    Feb 27 03:01:02.099: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 02/27/23 03:01:02.099
    Feb 27 03:01:02.100: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Feb 27 03:01:02.100: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3774" to be "running and ready, or succeeded"
    Feb 27 03:01:02.104: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000104ms
    Feb 27 03:01:02.104: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins' to be 'Running' but was 'Pending'
    Feb 27 03:01:04.109: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008882889s
    Feb 27 03:01:04.109: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Feb 27 03:01:04.109: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 02/27/23 03:01:04.109
    Feb 27 03:01:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator'
    Feb 27 03:01:04.175: INFO: stderr: ""
    Feb 27 03:01:04.175: INFO: stdout: "I0227 03:01:02.889700       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zhrh 355\nI0227 03:01:03.089870       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/87n 336\nI0227 03:01:03.290449       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/qkf6 569\nI0227 03:01:03.489732       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/s8mh 599\nI0227 03:01:03.690062       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l2z9 309\nI0227 03:01:03.890324       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wwhv 200\nI0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
    STEP: limiting log lines 02/27/23 03:01:04.175
    Feb 27 03:01:04.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --tail=1'
    Feb 27 03:01:04.243: INFO: stderr: ""
    Feb 27 03:01:04.243: INFO: stdout: "I0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
    Feb 27 03:01:04.243: INFO: got output "I0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\n"
    STEP: limiting log bytes 02/27/23 03:01:04.243
    Feb 27 03:01:04.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --limit-bytes=1'
    Feb 27 03:01:04.319: INFO: stderr: ""
    Feb 27 03:01:04.319: INFO: stdout: "I"
    Feb 27 03:01:04.319: INFO: got output "I"
    STEP: exposing timestamps 02/27/23 03:01:04.319
    Feb 27 03:01:04.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --tail=1 --timestamps'
    Feb 27 03:01:04.427: INFO: stderr: ""
    Feb 27 03:01:04.427: INFO: stdout: "2023-02-27T05:01:04.289932952+02:00 I0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\n"
    Feb 27 03:01:04.427: INFO: got output "2023-02-27T05:01:04.289932952+02:00 I0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\n"
    STEP: restricting to a time range 02/27/23 03:01:04.427
    Feb 27 03:01:06.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --since=1s'
    Feb 27 03:01:07.006: INFO: stderr: ""
    Feb 27 03:01:07.006: INFO: stdout: "I0227 03:01:06.090706       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/7wmk 346\nI0227 03:01:06.290081       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nz86 279\nI0227 03:01:06.490448       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/gxm5 243\nI0227 03:01:06.690768       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/9r7s 282\nI0227 03:01:06.890092       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/t9c 543\n"
    Feb 27 03:01:07.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 logs logs-generator logs-generator --since=24h'
    Feb 27 03:01:07.082: INFO: stderr: ""
    Feb 27 03:01:07.082: INFO: stdout: "I0227 03:01:02.889700       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/zhrh 355\nI0227 03:01:03.089870       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/87n 336\nI0227 03:01:03.290449       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/qkf6 569\nI0227 03:01:03.489732       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/s8mh 599\nI0227 03:01:03.690062       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l2z9 309\nI0227 03:01:03.890324       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/wwhv 200\nI0227 03:01:04.090653       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/5v4 234\nI0227 03:01:04.289807       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/dnsc 217\nI0227 03:01:04.490144       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/f5d 429\nI0227 03:01:04.690436       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/qnpk 515\nI0227 03:01:04.890794       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/fk2 222\nI0227 03:01:05.090095       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/w2nx 394\nI0227 03:01:05.290447       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/mtln 308\nI0227 03:01:05.489742       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/gzhz 384\nI0227 03:01:05.690025       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/nb5 239\nI0227 03:01:05.890386       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/p5jp 296\nI0227 03:01:06.090706       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/7wmk 346\nI0227 03:01:06.290081       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nz86 279\nI0227 03:01:06.490448       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/gxm5 243\nI0227 03:01:06.690768       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/9r7s 282\nI0227 03:01:06.890092       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/t9c 543\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Feb 27 03:01:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2276000635 --namespace=kubectl-3774 delete pod logs-generator'
    Feb 27 03:01:08.375: INFO: stderr: ""
    Feb 27 03:01:08.375: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:08.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3774" for this suite. 02/27/23 03:01:08.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:08.384
Feb 27 03:01:08.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename pods 02/27/23 03:01:08.385
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:08.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:08.406
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 02/27/23 03:01:08.408
STEP: submitting the pod to kubernetes 02/27/23 03:01:08.408
Feb 27 03:01:08.441: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" in namespace "pods-4758" to be "running and ready"
Feb 27 03:01:08.446: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270743ms
Feb 27 03:01:08.446: INFO: The phase of Pod pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86 is Pending, waiting for it to be Running (with Ready = true)
Feb 27 03:01:10.450: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 2.009470427s
Feb 27 03:01:10.450: INFO: The phase of Pod pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86 is Running (Ready = true)
Feb 27 03:01:10.450: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 02/27/23 03:01:10.453
STEP: updating the pod 02/27/23 03:01:10.456
Feb 27 03:01:10.990: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86"
Feb 27 03:01:10.990: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" in namespace "pods-4758" to be "terminated with reason DeadlineExceeded"
Feb 27 03:01:10.994: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 3.797906ms
Feb 27 03:01:12.998: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 2.007986307s
Feb 27 03:01:14.998: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=false. Elapsed: 4.007989416s
Feb 27 03:01:16.999: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008919044s
Feb 27 03:01:16.999: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:16.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4758" for this suite. 02/27/23 03:01:17.003
------------------------------
• [SLOW TEST] [8.626 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:08.384
    Feb 27 03:01:08.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename pods 02/27/23 03:01:08.385
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:08.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:08.406
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 02/27/23 03:01:08.408
    STEP: submitting the pod to kubernetes 02/27/23 03:01:08.408
    Feb 27 03:01:08.441: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" in namespace "pods-4758" to be "running and ready"
    Feb 27 03:01:08.446: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.270743ms
    Feb 27 03:01:08.446: INFO: The phase of Pod pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86 is Pending, waiting for it to be Running (with Ready = true)
    Feb 27 03:01:10.450: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 2.009470427s
    Feb 27 03:01:10.450: INFO: The phase of Pod pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86 is Running (Ready = true)
    Feb 27 03:01:10.450: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 02/27/23 03:01:10.453
    STEP: updating the pod 02/27/23 03:01:10.456
    Feb 27 03:01:10.990: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86"
    Feb 27 03:01:10.990: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" in namespace "pods-4758" to be "terminated with reason DeadlineExceeded"
    Feb 27 03:01:10.994: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 3.797906ms
    Feb 27 03:01:12.998: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=true. Elapsed: 2.007986307s
    Feb 27 03:01:14.998: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Running", Reason="", readiness=false. Elapsed: 4.007989416s
    Feb 27 03:01:16.999: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008919044s
    Feb 27 03:01:16.999: INFO: Pod "pod-update-activedeadlineseconds-7b139355-7d3c-472f-bd2e-2e3c5c132c86" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:16.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4758" for this suite. 02/27/23 03:01:17.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:17.011
Feb 27 03:01:17.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename containers 02/27/23 03:01:17.012
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:17.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:17.025
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 02/27/23 03:01:17.027
Feb 27 03:01:17.053: INFO: Waiting up to 5m0s for pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b" in namespace "containers-1817" to be "Succeeded or Failed"
Feb 27 03:01:17.059: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055588ms
Feb 27 03:01:19.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010062206s
Feb 27 03:01:21.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009931544s
STEP: Saw pod success 02/27/23 03:01:21.063
Feb 27 03:01:21.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b" satisfied condition "Succeeded or Failed"
Feb 27 03:01:21.066: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b container agnhost-container: <nil>
STEP: delete the pod 02/27/23 03:01:21.071
Feb 27 03:01:21.085: INFO: Waiting for pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b to disappear
Feb 27 03:01:21.089: INFO: Pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:21.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1817" for this suite. 02/27/23 03:01:21.095
------------------------------
• [4.094 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:17.011
    Feb 27 03:01:17.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename containers 02/27/23 03:01:17.012
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:17.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:17.025
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 02/27/23 03:01:17.027
    Feb 27 03:01:17.053: INFO: Waiting up to 5m0s for pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b" in namespace "containers-1817" to be "Succeeded or Failed"
    Feb 27 03:01:17.059: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055588ms
    Feb 27 03:01:19.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010062206s
    Feb 27 03:01:21.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009931544s
    STEP: Saw pod success 02/27/23 03:01:21.063
    Feb 27 03:01:21.063: INFO: Pod "client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b" satisfied condition "Succeeded or Failed"
    Feb 27 03:01:21.066: INFO: Trying to get logs from node worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b container agnhost-container: <nil>
    STEP: delete the pod 02/27/23 03:01:21.071
    Feb 27 03:01:21.085: INFO: Waiting for pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b to disappear
    Feb 27 03:01:21.089: INFO: Pod client-containers-cd7f8891-bf36-4386-a2fd-728674b8fc8b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:21.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1817" for this suite. 02/27/23 03:01:21.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:21.11
Feb 27 03:01:21.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename endpointslice 02/27/23 03:01:21.111
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:21.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:21.13
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 02/27/23 03:01:26.341
STEP: referencing matching pods with named port 02/27/23 03:01:31.347
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 03:01:36.355
STEP: recreating EndpointSlices after they've been deleted 02/27/23 03:01:41.362
Feb 27 03:01:41.387: INFO: EndpointSlice for Service endpointslice-8924/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:51.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8924" for this suite. 02/27/23 03:01:51.402
------------------------------
• [SLOW TEST] [30.300 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:21.11
    Feb 27 03:01:21.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename endpointslice 02/27/23 03:01:21.111
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:21.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:21.13
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 02/27/23 03:01:26.341
    STEP: referencing matching pods with named port 02/27/23 03:01:31.347
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 02/27/23 03:01:36.355
    STEP: recreating EndpointSlices after they've been deleted 02/27/23 03:01:41.362
    Feb 27 03:01:41.387: INFO: EndpointSlice for Service endpointslice-8924/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:51.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8924" for this suite. 02/27/23 03:01:51.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:51.41
Feb 27 03:01:51.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename deployment 02/27/23 03:01:51.411
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:51.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:51.428
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 02/27/23 03:01:51.435
STEP: waiting for Deployment to be created 02/27/23 03:01:51.44
STEP: waiting for all Replicas to be Ready 02/27/23 03:01:51.441
Feb 27 03:01:51.442: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.442: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.453: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.453: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.463: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.463: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.510: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:51.510: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Feb 27 03:01:52.462: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 03:01:52.462: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Feb 27 03:01:52.747: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 02/27/23 03:01:52.747
W0227 03:01:52.754986      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Feb 27 03:01:52.757: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 02/27/23 03:01:52.757
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.766: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.766: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.803: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.803: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:52.819: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:52.819: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:52.827: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:52.827: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:54.506: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:54.506: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:54.527: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
STEP: listing Deployments 02/27/23 03:01:54.527
Feb 27 03:01:54.531: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 02/27/23 03:01:54.531
Feb 27 03:01:54.543: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 02/27/23 03:01:54.543
Feb 27 03:01:54.549: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:54.553: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:54.576: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:54.592: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:54.627: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:55.503: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:55.531: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:55.539: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:55.556: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:55.584: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Feb 27 03:01:56.782: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 02/27/23 03:01:56.838
STEP: fetching the DeploymentStatus 02/27/23 03:01:56.845
Feb 27 03:01:56.852: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:56.852: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 3
STEP: deleting the Deployment 02/27/23 03:01:56.853
Feb 27 03:01:56.863: INFO: observed event type MODIFIED
Feb 27 03:01:56.863: INFO: observed event type MODIFIED
Feb 27 03:01:56.863: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
Feb 27 03:01:56.864: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Feb 27 03:01:56.871: INFO: Log out all the ReplicaSets if there is no deployment created
Feb 27 03:01:56.875: INFO: ReplicaSet "test-deployment-6b47df49d":
&ReplicaSet{ObjectMeta:{test-deployment-6b47df49d  deployment-3978  fb032df5-3529-48bd-bf36-32961134786d 71582 3 2023-02-27 03:01:51 +0000 UTC <nil> <nil> map[pod-template-hash:6b47df49d test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b6d7 0xc005d8b6d8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b47df49d,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b47df49d test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 27 03:01:56.879: INFO: ReplicaSet "test-deployment-7569b7f845":
&ReplicaSet{ObjectMeta:{test-deployment-7569b7f845  deployment-3978  8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 71714 4 2023-02-27 03:01:52 +0000 UTC <nil> <nil> map[pod-template-hash:7569b7f845 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b7c7 0xc005d8b7c8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7569b7f845,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7569b7f845 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Feb 27 03:01:56.883: INFO: pod: "test-deployment-7569b7f845-4g9gd":
&Pod{ObjectMeta:{test-deployment-7569b7f845-4g9gd test-deployment-7569b7f845- deployment-3978  df14eed9-07c9-49b0-8482-f43e947fd318 71710 0 2023-02-27 03:01:52 +0000 UTC 2023-02-27 03:01:57 +0000 UTC 0xc0031ab1a8 map[pod-template-hash:7569b7f845 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.191"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.191"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7569b7f845 8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 0xc0031ab1d7 0xc0031ab1d8}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 03:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54vzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54vzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.191,StartTime:2023-02-27 03:01:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:containerd://dbbd28266b79ca9c6ae481716f7344d8d6cc052a0c00b0d9d0d4b16a090182e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 03:01:56.884: INFO: pod: "test-deployment-7569b7f845-qczx5":
&Pod{ObjectMeta:{test-deployment-7569b7f845-qczx5 test-deployment-7569b7f845- deployment-3978  db459f69-4577-4ae7-9b30-9e93243bf630 71663 0 2023-02-27 03:01:54 +0000 UTC 2023-02-27 03:01:56 +0000 UTC 0xc0031ab3b0 map[pod-template-hash:7569b7f845 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.88"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.226.88"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7569b7f845 8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 0xc0031ab3e7 0xc0031ab3e8}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdbzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdbzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.88,StartTime:2023-02-27 03:01:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:containerd://b2d5601a497d219c6616bcb8ca3b5533cf5bdb4646d60fb2d285d5dfd25c43f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 03:01:56.884: INFO: ReplicaSet "test-deployment-7c88d8f495":
&ReplicaSet{ObjectMeta:{test-deployment-7c88d8f495  deployment-3978  845539ba-62dc-4f00-9178-8ae7c446ee16 71703 2 2023-02-27 03:01:54 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b8b7 0xc005d8b8b8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c88d8f495,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b940 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Feb 27 03:01:56.888: INFO: pod: "test-deployment-7c88d8f495-2trw8":
&Pod{ObjectMeta:{test-deployment-7c88d8f495-2trw8 test-deployment-7c88d8f495- deployment-3978  48925b3f-912c-402d-b283-d532b56f42a9 71636 0 2023-02-27 03:01:54 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.140"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.214.140"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7c88d8f495 845539ba-62dc-4f00-9178-8ae7c446ee16 0xc004b3af67 0xc004b3af68}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"845539ba-62dc-4f00-9178-8ae7c446ee16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-644w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-644w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.140,StartTime:2023-02-27 03:01:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://a7bd3af6aaba66dd0ab3c76548ca78f431c9e81945b652de62be8d9bb03abf16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Feb 27 03:01:56.888: INFO: pod: "test-deployment-7c88d8f495-dmg55":
&Pod{ObjectMeta:{test-deployment-7c88d8f495-dmg55 test-deployment-7c88d8f495- deployment-3978  16d8a6c9-1555-4f25-8b5d-ac338f248512 71702 0 2023-02-27 03:01:55 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.173"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "192.168.21.173"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7c88d8f495 845539ba-62dc-4f00-9178-8ae7c446ee16 0xc004b3b177 0xc004b3b178}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"845539ba-62dc-4f00-9178-8ae7c446ee16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62j9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62j9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.173,StartTime:2023-02-27 03:01:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d43df86b786c017d48f76a6ab716a8a175ccc41bb45edfab60f7f1cd805ceb9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3978" for this suite. 02/27/23 03:01:56.892
------------------------------
• [SLOW TEST] [5.493 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:51.41
    Feb 27 03:01:51.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename deployment 02/27/23 03:01:51.411
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:51.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:51.428
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 02/27/23 03:01:51.435
    STEP: waiting for Deployment to be created 02/27/23 03:01:51.44
    STEP: waiting for all Replicas to be Ready 02/27/23 03:01:51.441
    Feb 27 03:01:51.442: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.442: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.453: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.453: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.463: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.463: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.510: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:51.510: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Feb 27 03:01:52.462: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 03:01:52.462: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Feb 27 03:01:52.747: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 02/27/23 03:01:52.747
    W0227 03:01:52.754986      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Feb 27 03:01:52.757: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 02/27/23 03:01:52.757
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 0
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.759: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.766: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.766: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.803: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.803: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:52.819: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:52.819: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:52.827: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:52.827: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:54.506: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:54.506: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:54.527: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    STEP: listing Deployments 02/27/23 03:01:54.527
    Feb 27 03:01:54.531: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 02/27/23 03:01:54.531
    Feb 27 03:01:54.543: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 02/27/23 03:01:54.543
    Feb 27 03:01:54.549: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:54.553: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:54.576: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:54.592: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:54.627: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:55.503: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:55.531: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:55.539: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:55.556: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:55.584: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Feb 27 03:01:56.782: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 02/27/23 03:01:56.838
    STEP: fetching the DeploymentStatus 02/27/23 03:01:56.845
    Feb 27 03:01:56.852: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:56.852: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 1
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 2
    Feb 27 03:01:56.853: INFO: observed Deployment test-deployment in namespace deployment-3978 with ReadyReplicas 3
    STEP: deleting the Deployment 02/27/23 03:01:56.853
    Feb 27 03:01:56.863: INFO: observed event type MODIFIED
    Feb 27 03:01:56.863: INFO: observed event type MODIFIED
    Feb 27 03:01:56.863: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    Feb 27 03:01:56.864: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Feb 27 03:01:56.871: INFO: Log out all the ReplicaSets if there is no deployment created
    Feb 27 03:01:56.875: INFO: ReplicaSet "test-deployment-6b47df49d":
    &ReplicaSet{ObjectMeta:{test-deployment-6b47df49d  deployment-3978  fb032df5-3529-48bd-bf36-32961134786d 71582 3 2023-02-27 03:01:51 +0000 UTC <nil> <nil> map[pod-template-hash:6b47df49d test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b6d7 0xc005d8b6d8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b47df49d,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b47df49d test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 03:01:56.879: INFO: ReplicaSet "test-deployment-7569b7f845":
    &ReplicaSet{ObjectMeta:{test-deployment-7569b7f845  deployment-3978  8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 71714 4 2023-02-27 03:01:52 +0000 UTC <nil> <nil> map[pod-template-hash:7569b7f845 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b7c7 0xc005d8b7c8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7569b7f845,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7569b7f845 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 03:01:56.883: INFO: pod: "test-deployment-7569b7f845-4g9gd":
    &Pod{ObjectMeta:{test-deployment-7569b7f845-4g9gd test-deployment-7569b7f845- deployment-3978  df14eed9-07c9-49b0-8482-f43e947fd318 71710 0 2023-02-27 03:01:52 +0000 UTC 2023-02-27 03:01:57 +0000 UTC 0xc0031ab1a8 map[pod-template-hash:7569b7f845 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.191"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.191"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7569b7f845 8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 0xc0031ab1d7 0xc0031ab1d8}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-02-27 03:01:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54vzg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54vzg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.191,StartTime:2023-02-27 03:01:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:containerd://dbbd28266b79ca9c6ae481716f7344d8d6cc052a0c00b0d9d0d4b16a090182e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 03:01:56.884: INFO: pod: "test-deployment-7569b7f845-qczx5":
    &Pod{ObjectMeta:{test-deployment-7569b7f845-qczx5 test-deployment-7569b7f845- deployment-3978  db459f69-4577-4ae7-9b30-9e93243bf630 71663 0 2023-02-27 03:01:54 +0000 UTC 2023-02-27 03:01:56 +0000 UTC 0xc0031ab3b0 map[pod-template-hash:7569b7f845 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.88"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.226.88"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7569b7f845 8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f 0xc0031ab3e7 0xc0031ab3e8}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a88feb1-5f5d-42ea-9ae2-36f2d4f71d0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.226.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdbzd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdbzd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-58bsk712-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.15,PodIP:192.168.226.88,StartTime:2023-02-27 03:01:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause:3.9,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:containerd://b2d5601a497d219c6616bcb8ca3b5533cf5bdb4646d60fb2d285d5dfd25c43f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.226.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 03:01:56.884: INFO: ReplicaSet "test-deployment-7c88d8f495":
    &ReplicaSet{ObjectMeta:{test-deployment-7c88d8f495  deployment-3978  845539ba-62dc-4f00-9178-8ae7c446ee16 71703 2 2023-02-27 03:01:54 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 059a7efa-3570-4e67-a28e-4dcb0ac024b0 0xc005d8b8b7 0xc005d8b8b8}] [] [{kube-controller-manager Update apps/v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"059a7efa-3570-4e67-a28e-4dcb0ac024b0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c88d8f495,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d8b940 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Feb 27 03:01:56.888: INFO: pod: "test-deployment-7c88d8f495-2trw8":
    &Pod{ObjectMeta:{test-deployment-7c88d8f495-2trw8 test-deployment-7c88d8f495- deployment-3978  48925b3f-912c-402d-b283-d532b56f42a9 71636 0 2023-02-27 03:01:54 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.140"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.214.140"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7c88d8f495 845539ba-62dc-4f00-9178-8ae7c446ee16 0xc004b3af67 0xc004b3af68}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"845539ba-62dc-4f00-9178-8ae7c446ee16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.214.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-644w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-644w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r0tg68ia-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.3,PodIP:192.168.214.140,StartTime:2023-02-27 03:01:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://a7bd3af6aaba66dd0ab3c76548ca78f431c9e81945b652de62be8d9bb03abf16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.214.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Feb 27 03:01:56.888: INFO: pod: "test-deployment-7c88d8f495-dmg55":
    &Pod{ObjectMeta:{test-deployment-7c88d8f495-dmg55 test-deployment-7c88d8f495- deployment-3978  16d8a6c9-1555-4f25-8b5d-ac338f248512 71702 0 2023-02-27 03:01:55 +0000 UTC <nil> <nil> map[pod-template-hash:7c88d8f495 test-deployment-static:true] map[k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.173"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "192.168.21.173"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7c88d8f495 845539ba-62dc-4f00-9178-8ae7c446ee16 0xc004b3b177 0xc004b3b178}] [] [{kube-controller-manager Update v1 2023-02-27 03:01:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"845539ba-62dc-4f00-9178-8ae7c446ee16\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.21.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-02-27 03:01:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62j9n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62j9n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-pool1-r5deh8a3-n92-ci-ibd-23-jenkins,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-02-27 03:01:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.10.5,PodIP:192.168.21.173,StartTime:2023-02-27 03:01:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-02-27 03:01:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd:2.4.38-4,ImageID:armdocker.rnd.ericsson.se/proj_kds/ccd/3pp/sonobuoy/httpd@sha256:8b745b6e9cb6a6184353110aabe28d8d6f6145d3fe7c0e1899aa70c8228c0fd3,ContainerID:containerd://d43df86b786c017d48f76a6ab716a8a175ccc41bb45edfab60f7f1cd805ceb9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.21.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:56.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3978" for this suite. 02/27/23 03:01:56.892
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 02/27/23 03:01:56.903
Feb 27 03:01:56.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
STEP: Building a namespace api object, basename runtimeclass 02/27/23 03:01:56.904
STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:56.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:56.926
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5341-delete-me 02/27/23 03:01:56.938
STEP: Waiting for the RuntimeClass to disappear 02/27/23 03:01:56.943
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Feb 27 03:01:56.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5341" for this suite. 02/27/23 03:01:56.968
------------------------------
• [0.071 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 02/27/23 03:01:56.903
    Feb 27 03:01:56.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2276000635
    STEP: Building a namespace api object, basename runtimeclass 02/27/23 03:01:56.904
    STEP: Waiting for a default service account to be provisioned in namespace 02/27/23 03:01:56.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 02/27/23 03:01:56.926
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5341-delete-me 02/27/23 03:01:56.938
    STEP: Waiting for the RuntimeClass to disappear 02/27/23 03:01:56.943
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Feb 27 03:01:56.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5341" for this suite. 02/27/23 03:01:56.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Feb 27 03:01:56.976: INFO: Running AfterSuite actions on node 1
Feb 27 03:01:56.976: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Feb 27 03:01:56.976: INFO: Running AfterSuite actions on node 1
    Feb 27 03:01:56.976: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.083 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5418.992 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h30m19.30696151s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

