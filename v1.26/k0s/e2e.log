I0112 15:23:48.773137      22 e2e.go:126] Starting e2e run "40cab34b-98f1-488c-8c7d-dad3d0da3836" on Ginkgo node 1
Jan 12 15:23:48.787: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1673537028 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 12 15:23:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
E0112 15:23:48.906912      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 12 15:23:48.907: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 12 15:23:48.916: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 12 15:23:48.936: INFO: The status of Pod coredns-9864b985-9m6mg is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:48.936: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:48.936: INFO: 7 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 12 15:23:48.936: INFO: expected 3 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Jan 12 15:23:48.936: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
Jan 12 15:23:48.936: INFO: coredns-9864b985-9m6mg           worker-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:48.936: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:48.936: INFO: 
Jan 12 15:23:50.949: INFO: The status of Pod coredns-9864b985-9m6mg is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:50.949: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:50.949: INFO: 7 / 9 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Jan 12 15:23:50.949: INFO: expected 3 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Jan 12 15:23:50.949: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
Jan 12 15:23:50.949: INFO: coredns-9864b985-9m6mg           worker-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:50.949: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:50.949: INFO: 
Jan 12 15:23:52.947: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:52.947: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Jan 12 15:23:52.947: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 12 15:23:52.947: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
Jan 12 15:23:52.947: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:52.948: INFO: 
Jan 12 15:23:54.947: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:54.947: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Jan 12 15:23:54.947: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 12 15:23:54.947: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
Jan 12 15:23:54.947: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:54.947: INFO: 
Jan 12 15:23:56.948: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 15:23:56.948: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Jan 12 15:23:56.948: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 12 15:23:56.948: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
Jan 12 15:23:56.948: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
Jan 12 15:23:56.948: INFO: 
Jan 12 15:23:58.947: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Jan 12 15:23:58.947: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 12 15:23:58.947: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-router' (0 seconds elapsed)
Jan 12 15:23:58.950: INFO: e2e test version: v1.26.0
Jan 12 15:23:58.951: INFO: kube-apiserver version: v1.26.0+k0s
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 12 15:23:58.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:23:58.954: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [10.049 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 12 15:23:48.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    E0112 15:23:48.906912      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 12 15:23:48.907: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 12 15:23:48.916: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 12 15:23:48.936: INFO: The status of Pod coredns-9864b985-9m6mg is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:48.936: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:48.936: INFO: 7 / 9 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 12 15:23:48.936: INFO: expected 3 pod replicas in namespace 'kube-system', 1 are Running and Ready.
    Jan 12 15:23:48.936: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
    Jan 12 15:23:48.936: INFO: coredns-9864b985-9m6mg           worker-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:48.936: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:48.936: INFO: 
    Jan 12 15:23:50.949: INFO: The status of Pod coredns-9864b985-9m6mg is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:50.949: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:50.949: INFO: 7 / 9 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
    Jan 12 15:23:50.949: INFO: expected 3 pod replicas in namespace 'kube-system', 1 are Running and Ready.
    Jan 12 15:23:50.949: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
    Jan 12 15:23:50.949: INFO: coredns-9864b985-9m6mg           worker-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:50.949: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:50.949: INFO: 
    Jan 12 15:23:52.947: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:52.947: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
    Jan 12 15:23:52.947: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 12 15:23:52.947: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
    Jan 12 15:23:52.947: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:52.948: INFO: 
    Jan 12 15:23:54.947: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:54.947: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
    Jan 12 15:23:54.947: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 12 15:23:54.947: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
    Jan 12 15:23:54.947: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:54.947: INFO: 
    Jan 12 15:23:56.948: INFO: The status of Pod metrics-server-7446cc488c-tsnl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 15:23:56.948: INFO: 8 / 9 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
    Jan 12 15:23:56.948: INFO: expected 3 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 12 15:23:56.948: INFO: POD                              NODE      PHASE    GRACE  CONDITIONS
    Jan 12 15:23:56.948: INFO: metrics-server-7446cc488c-tsnl2  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC ContainersNotReady containers with unready status: [metrics-server]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 15:23:17 +0000 UTC  }]
    Jan 12 15:23:56.948: INFO: 
    Jan 12 15:23:58.947: INFO: 9 / 9 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
    Jan 12 15:23:58.947: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jan 12 15:23:58.947: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 12 15:23:58.950: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-router' (0 seconds elapsed)
    Jan 12 15:23:58.950: INFO: e2e test version: v1.26.0
    Jan 12 15:23:58.951: INFO: kube-apiserver version: v1.26.0+k0s
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 12 15:23:58.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:23:58.954: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:23:58.972
Jan 12 15:23:58.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename proxy 01/12/23 15:23:58.972
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:23:58.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:23:58.982
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/12/23 15:23:58.995
STEP: creating replication controller proxy-service-4xgc8 in namespace proxy-9591 01/12/23 15:23:58.996
I0112 15:23:59.002702      22 runners.go:193] Created replication controller with name: proxy-service-4xgc8, namespace: proxy-9591, replica count: 1
I0112 15:24:00.053285      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 15:24:01.053491      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 15:24:02.053628      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 15:24:03.053858      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 15:24:04.054048      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 15:24:04.056: INFO: setup took 5.071480503s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/12/23 15:24:04.056
Jan 12 15:24:04.070: INFO: (0) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 13.488779ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 14.348585ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 14.672281ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 15.239587ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 15.072592ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 15.176545ms)
Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 15.127183ms)
Jan 12 15:24:04.072: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 15.410753ms)
Jan 12 15:24:04.074: INFO: (0) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 18.025115ms)
Jan 12 15:24:04.075: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 18.26597ms)
Jan 12 15:24:04.075: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 18.1784ms)
Jan 12 15:24:04.081: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 24.734409ms)
Jan 12 15:24:04.081: INFO: (0) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 24.891354ms)
Jan 12 15:24:04.082: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 25.626854ms)
Jan 12 15:24:04.082: INFO: (0) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 25.651616ms)
Jan 12 15:24:04.084: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 27.732949ms)
Jan 12 15:24:04.090: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.30505ms)
Jan 12 15:24:04.090: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.252664ms)
Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.375923ms)
Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.506221ms)
Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 8.175208ms)
Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.097451ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.546695ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.853687ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.952636ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.928163ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.901657ms)
Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.063111ms)
Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.155029ms)
Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.230174ms)
Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.310159ms)
Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.149867ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 12.234612ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 12.070549ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 12.182036ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 12.271515ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 12.563526ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.675955ms)
Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 12.864606ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 14.162618ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 13.745349ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 13.708948ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 13.804708ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 14.129051ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 14.074747ms)
Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 14.142628ms)
Jan 12 15:24:04.110: INFO: (2) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 14.838887ms)
Jan 12 15:24:04.110: INFO: (2) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 15.20573ms)
Jan 12 15:24:04.113: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 3.456337ms)
Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 5.475065ms)
Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 5.735249ms)
Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.574726ms)
Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 6.49167ms)
Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.33738ms)
Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 6.768526ms)
Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.152037ms)
Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.022344ms)
Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 7.53952ms)
Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 7.826395ms)
Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 8.135385ms)
Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.500612ms)
Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.64647ms)
Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.841161ms)
Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.84209ms)
Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.086041ms)
Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.315874ms)
Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.163121ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.249028ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.383997ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.720492ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.988037ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.947277ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.74362ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 9.327677ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.908755ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 9.312772ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.487144ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.583992ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.901284ms)
Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.005457ms)
Jan 12 15:24:04.134: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.904135ms)
Jan 12 15:24:04.137: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.051843ms)
Jan 12 15:24:04.137: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.057461ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.178612ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 7.673728ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.111658ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.354523ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.186732ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.322046ms)
Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.930523ms)
Jan 12 15:24:04.139: INFO: (5) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.648549ms)
Jan 12 15:24:04.139: INFO: (5) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.159821ms)
Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.991826ms)
Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.596615ms)
Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.639533ms)
Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.734146ms)
Jan 12 15:24:04.145: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.447523ms)
Jan 12 15:24:04.145: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.6227ms)
Jan 12 15:24:04.148: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.049851ms)
Jan 12 15:24:04.148: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.084523ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.094225ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.172732ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.272355ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.976157ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.081417ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.745981ms)
Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.996762ms)
Jan 12 15:24:04.150: INFO: (6) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.766666ms)
Jan 12 15:24:04.150: INFO: (6) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.889465ms)
Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.466854ms)
Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.967214ms)
Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.903885ms)
Jan 12 15:24:04.155: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 3.731796ms)
Jan 12 15:24:04.156: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.042589ms)
Jan 12 15:24:04.158: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.89466ms)
Jan 12 15:24:04.159: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.625743ms)
Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.17014ms)
Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.676029ms)
Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.232825ms)
Jan 12 15:24:04.161: INFO: (7) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.289585ms)
Jan 12 15:24:04.161: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.157781ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.804146ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.060181ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 10.274156ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 10.507911ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.615953ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 10.747854ms)
Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 10.585887ms)
Jan 12 15:24:04.166: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 3.765124ms)
Jan 12 15:24:04.166: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.108175ms)
Jan 12 15:24:04.169: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.762857ms)
Jan 12 15:24:04.170: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 6.933093ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.455506ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.278341ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.603108ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.780383ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.223296ms)
Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.309443ms)
Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.702697ms)
Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.770696ms)
Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.166556ms)
Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.241841ms)
Jan 12 15:24:04.173: INFO: (8) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.542427ms)
Jan 12 15:24:04.173: INFO: (8) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.055102ms)
Jan 12 15:24:04.176: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 2.644175ms)
Jan 12 15:24:04.176: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 3.020492ms)
Jan 12 15:24:04.177: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.078137ms)
Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.587118ms)
Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.59546ms)
Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.068621ms)
Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.148469ms)
Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.373389ms)
Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.07399ms)
Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 10.058775ms)
Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.058702ms)
Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.109131ms)
Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.76307ms)
Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.891611ms)
Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.923796ms)
Jan 12 15:24:04.185: INFO: (9) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.852974ms)
Jan 12 15:24:04.190: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.441051ms)
Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.604108ms)
Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 5.695321ms)
Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 5.729684ms)
Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 5.833429ms)
Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 11.675071ms)
Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.191099ms)
Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 12.136034ms)
Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 12.135992ms)
Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 12.118729ms)
Jan 12 15:24:04.198: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 13.209051ms)
Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 13.276378ms)
Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 13.177942ms)
Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 13.497741ms)
Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 13.963981ms)
Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 13.871891ms)
Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.21133ms)
Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.575608ms)
Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.56638ms)
Jan 12 15:24:04.208: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.487961ms)
Jan 12 15:24:04.208: INFO: (11) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.503932ms)
Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.352082ms)
Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.865593ms)
Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 9.572218ms)
Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.631818ms)
Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.91088ms)
Jan 12 15:24:04.210: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 10.233126ms)
Jan 12 15:24:04.210: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.226952ms)
Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 11.172917ms)
Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 11.149293ms)
Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.295339ms)
Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.552931ms)
Jan 12 15:24:04.216: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 4.239783ms)
Jan 12 15:24:04.217: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 6.20558ms)
Jan 12 15:24:04.217: INFO: (12) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 5.825408ms)
Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.787626ms)
Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 7.346886ms)
Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.013823ms)
Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.367783ms)
Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.163459ms)
Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.625099ms)
Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.545784ms)
Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.478501ms)
Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.423293ms)
Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.951774ms)
Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.907237ms)
Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.133084ms)
Jan 12 15:24:04.222: INFO: (12) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.549382ms)
Jan 12 15:24:04.226: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 4.273344ms)
Jan 12 15:24:04.227: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 5.224308ms)
Jan 12 15:24:04.227: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.943137ms)
Jan 12 15:24:04.229: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.186453ms)
Jan 12 15:24:04.229: INFO: (13) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 7.074507ms)
Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 7.641683ms)
Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.653108ms)
Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.913769ms)
Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.922816ms)
Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.017122ms)
Jan 12 15:24:04.231: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.144952ms)
Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.036565ms)
Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.906755ms)
Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.907853ms)
Jan 12 15:24:04.233: INFO: (13) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.239512ms)
Jan 12 15:24:04.233: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.293143ms)
Jan 12 15:24:04.237: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.379132ms)
Jan 12 15:24:04.238: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 5.172531ms)
Jan 12 15:24:04.240: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.145938ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.082224ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.056869ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.234175ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.369052ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.276319ms)
Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.234344ms)
Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.508392ms)
Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.227697ms)
Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 9.060818ms)
Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.773377ms)
Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.905019ms)
Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.857253ms)
Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 10.543238ms)
Jan 12 15:24:04.248: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.751907ms)
Jan 12 15:24:04.250: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 6.176483ms)
Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 6.920245ms)
Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.89292ms)
Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.265658ms)
Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.547799ms)
Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 10.183118ms)
Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 10.447527ms)
Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.649127ms)
Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.699071ms)
Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.75384ms)
Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.720945ms)
Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.003345ms)
Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 11.016101ms)
Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.698215ms)
Jan 12 15:24:04.256: INFO: (15) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 11.993989ms)
Jan 12 15:24:04.262: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.509458ms)
Jan 12 15:24:04.263: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.024403ms)
Jan 12 15:24:04.263: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.122827ms)
Jan 12 15:24:04.264: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.936209ms)
Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 9.038114ms)
Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.911341ms)
Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.270893ms)
Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.361708ms)
Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.357361ms)
Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.694293ms)
Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 10.339717ms)
Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.961079ms)
Jan 12 15:24:04.267: INFO: (16) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.260399ms)
Jan 12 15:24:04.267: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.655175ms)
Jan 12 15:24:04.268: INFO: (16) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.672373ms)
Jan 12 15:24:04.269: INFO: (16) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.795023ms)
Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.953022ms)
Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.655669ms)
Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 4.983461ms)
Jan 12 15:24:04.278: INFO: (17) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.15988ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.821382ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 10.118025ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.843671ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.952071ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 9.919591ms)
Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 9.976368ms)
Jan 12 15:24:04.280: INFO: (17) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.386956ms)
Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 11.5566ms)
Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 11.311177ms)
Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 12.080238ms)
Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 12.047042ms)
Jan 12 15:24:04.282: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 12.335454ms)
Jan 12 15:24:04.288: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 5.925946ms)
Jan 12 15:24:04.288: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.826635ms)
Jan 12 15:24:04.289: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.403087ms)
Jan 12 15:24:04.289: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 6.727259ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.667389ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.006324ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.354144ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.1963ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.777459ms)
Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.159054ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.769505ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.581671ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.219631ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.752288ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.595302ms)
Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.780992ms)
Jan 12 15:24:04.297: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.661154ms)
Jan 12 15:24:04.299: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.12975ms)
Jan 12 15:24:04.300: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.018458ms)
Jan 12 15:24:04.301: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.411505ms)
Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.002748ms)
Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.117363ms)
Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.814724ms)
Jan 12 15:24:04.304: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 10.791928ms)
Jan 12 15:24:04.304: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 11.214104ms)
Jan 12 15:24:04.305: INFO: (19) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 12.773116ms)
Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 14.752018ms)
Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 15.398168ms)
Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 15.342433ms)
Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 15.399637ms)
Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 15.501976ms)
Jan 12 15:24:04.309: INFO: (19) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 16.091077ms)
STEP: deleting ReplicationController proxy-service-4xgc8 in namespace proxy-9591, will wait for the garbage collector to delete the pods 01/12/23 15:24:04.309
Jan 12 15:24:04.366: INFO: Deleting ReplicationController proxy-service-4xgc8 took: 3.964432ms
Jan 12 15:24:04.466: INFO: Terminating ReplicationController proxy-service-4xgc8 pods took: 100.294378ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9591" for this suite. 01/12/23 15:24:12.37
------------------------------
 [SLOW TEST] [13.402 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:23:58.972
    Jan 12 15:23:58.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename proxy 01/12/23 15:23:58.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:23:58.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:23:58.982
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/12/23 15:23:58.995
    STEP: creating replication controller proxy-service-4xgc8 in namespace proxy-9591 01/12/23 15:23:58.996
    I0112 15:23:59.002702      22 runners.go:193] Created replication controller with name: proxy-service-4xgc8, namespace: proxy-9591, replica count: 1
    I0112 15:24:00.053285      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 15:24:01.053491      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 15:24:02.053628      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 15:24:03.053858      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 15:24:04.054048      22 runners.go:193] proxy-service-4xgc8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 15:24:04.056: INFO: setup took 5.071480503s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/12/23 15:24:04.056
    Jan 12 15:24:04.070: INFO: (0) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 13.488779ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 14.348585ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 14.672281ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 15.239587ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 15.072592ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 15.176545ms)
    Jan 12 15:24:04.071: INFO: (0) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 15.127183ms)
    Jan 12 15:24:04.072: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 15.410753ms)
    Jan 12 15:24:04.074: INFO: (0) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 18.025115ms)
    Jan 12 15:24:04.075: INFO: (0) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 18.26597ms)
    Jan 12 15:24:04.075: INFO: (0) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 18.1784ms)
    Jan 12 15:24:04.081: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 24.734409ms)
    Jan 12 15:24:04.081: INFO: (0) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 24.891354ms)
    Jan 12 15:24:04.082: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 25.626854ms)
    Jan 12 15:24:04.082: INFO: (0) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 25.651616ms)
    Jan 12 15:24:04.084: INFO: (0) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 27.732949ms)
    Jan 12 15:24:04.090: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.30505ms)
    Jan 12 15:24:04.090: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.252664ms)
    Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.375923ms)
    Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.506221ms)
    Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 8.175208ms)
    Jan 12 15:24:04.092: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.097451ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.546695ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.853687ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.952636ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.928163ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.901657ms)
    Jan 12 15:24:04.093: INFO: (1) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.063111ms)
    Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.155029ms)
    Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.230174ms)
    Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.310159ms)
    Jan 12 15:24:04.094: INFO: (1) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.149867ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 12.234612ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 12.070549ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 12.182036ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 12.271515ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 12.563526ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.675955ms)
    Jan 12 15:24:04.107: INFO: (2) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 12.864606ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 14.162618ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 13.745349ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 13.708948ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 13.804708ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 14.129051ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 14.074747ms)
    Jan 12 15:24:04.109: INFO: (2) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 14.142628ms)
    Jan 12 15:24:04.110: INFO: (2) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 14.838887ms)
    Jan 12 15:24:04.110: INFO: (2) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 15.20573ms)
    Jan 12 15:24:04.113: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 3.456337ms)
    Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 5.475065ms)
    Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 5.735249ms)
    Jan 12 15:24:04.116: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.574726ms)
    Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 6.49167ms)
    Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.33738ms)
    Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 6.768526ms)
    Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.152037ms)
    Jan 12 15:24:04.117: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.022344ms)
    Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 7.53952ms)
    Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 7.826395ms)
    Jan 12 15:24:04.118: INFO: (3) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 8.135385ms)
    Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.500612ms)
    Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.64647ms)
    Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.841161ms)
    Jan 12 15:24:04.119: INFO: (3) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.84209ms)
    Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.086041ms)
    Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.315874ms)
    Jan 12 15:24:04.128: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.163121ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.249028ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.383997ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.720492ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.988037ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.947277ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.74362ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 9.327677ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.908755ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 9.312772ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.487144ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.583992ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.901284ms)
    Jan 12 15:24:04.129: INFO: (4) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.005457ms)
    Jan 12 15:24:04.134: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.904135ms)
    Jan 12 15:24:04.137: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.051843ms)
    Jan 12 15:24:04.137: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.057461ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.178612ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 7.673728ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.111658ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.354523ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.186732ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.322046ms)
    Jan 12 15:24:04.138: INFO: (5) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.930523ms)
    Jan 12 15:24:04.139: INFO: (5) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.648549ms)
    Jan 12 15:24:04.139: INFO: (5) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.159821ms)
    Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.991826ms)
    Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.596615ms)
    Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.639533ms)
    Jan 12 15:24:04.140: INFO: (5) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.734146ms)
    Jan 12 15:24:04.145: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.447523ms)
    Jan 12 15:24:04.145: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.6227ms)
    Jan 12 15:24:04.148: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.049851ms)
    Jan 12 15:24:04.148: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.084523ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.094225ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.172732ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.272355ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.976157ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.081417ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.745981ms)
    Jan 12 15:24:04.149: INFO: (6) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.996762ms)
    Jan 12 15:24:04.150: INFO: (6) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.766666ms)
    Jan 12 15:24:04.150: INFO: (6) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.889465ms)
    Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.466854ms)
    Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.967214ms)
    Jan 12 15:24:04.151: INFO: (6) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.903885ms)
    Jan 12 15:24:04.155: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 3.731796ms)
    Jan 12 15:24:04.156: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.042589ms)
    Jan 12 15:24:04.158: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.89466ms)
    Jan 12 15:24:04.159: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.625743ms)
    Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.17014ms)
    Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.676029ms)
    Jan 12 15:24:04.160: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.232825ms)
    Jan 12 15:24:04.161: INFO: (7) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.289585ms)
    Jan 12 15:24:04.161: INFO: (7) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.157781ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.804146ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.060181ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 10.274156ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 10.507911ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.615953ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 10.747854ms)
    Jan 12 15:24:04.162: INFO: (7) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 10.585887ms)
    Jan 12 15:24:04.166: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 3.765124ms)
    Jan 12 15:24:04.166: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.108175ms)
    Jan 12 15:24:04.169: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.762857ms)
    Jan 12 15:24:04.170: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 6.933093ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.455506ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.278341ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.603108ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.780383ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.223296ms)
    Jan 12 15:24:04.171: INFO: (8) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.309443ms)
    Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.702697ms)
    Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 8.770696ms)
    Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.166556ms)
    Jan 12 15:24:04.172: INFO: (8) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.241841ms)
    Jan 12 15:24:04.173: INFO: (8) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.542427ms)
    Jan 12 15:24:04.173: INFO: (8) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.055102ms)
    Jan 12 15:24:04.176: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 2.644175ms)
    Jan 12 15:24:04.176: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 3.020492ms)
    Jan 12 15:24:04.177: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.078137ms)
    Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.587118ms)
    Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.59546ms)
    Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.068621ms)
    Jan 12 15:24:04.182: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.148469ms)
    Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.373389ms)
    Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.07399ms)
    Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 10.058775ms)
    Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.058702ms)
    Jan 12 15:24:04.183: INFO: (9) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.109131ms)
    Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.76307ms)
    Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.891611ms)
    Jan 12 15:24:04.184: INFO: (9) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 10.923796ms)
    Jan 12 15:24:04.185: INFO: (9) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.852974ms)
    Jan 12 15:24:04.190: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.441051ms)
    Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.604108ms)
    Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 5.695321ms)
    Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 5.729684ms)
    Jan 12 15:24:04.191: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 5.833429ms)
    Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 11.675071ms)
    Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.191099ms)
    Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 12.136034ms)
    Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 12.135992ms)
    Jan 12 15:24:04.197: INFO: (10) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 12.118729ms)
    Jan 12 15:24:04.198: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 13.209051ms)
    Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 13.276378ms)
    Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 13.177942ms)
    Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 13.497741ms)
    Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 13.963981ms)
    Jan 12 15:24:04.199: INFO: (10) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 13.871891ms)
    Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.21133ms)
    Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.575608ms)
    Jan 12 15:24:04.204: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.56638ms)
    Jan 12 15:24:04.208: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.487961ms)
    Jan 12 15:24:04.208: INFO: (11) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 8.503932ms)
    Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.352082ms)
    Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.865593ms)
    Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 9.572218ms)
    Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.631818ms)
    Jan 12 15:24:04.209: INFO: (11) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.91088ms)
    Jan 12 15:24:04.210: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 10.233126ms)
    Jan 12 15:24:04.210: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.226952ms)
    Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 11.172917ms)
    Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 11.149293ms)
    Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.295339ms)
    Jan 12 15:24:04.211: INFO: (11) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.552931ms)
    Jan 12 15:24:04.216: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 4.239783ms)
    Jan 12 15:24:04.217: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 6.20558ms)
    Jan 12 15:24:04.217: INFO: (12) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 5.825408ms)
    Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.787626ms)
    Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 7.346886ms)
    Jan 12 15:24:04.219: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.013823ms)
    Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.367783ms)
    Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.163459ms)
    Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.625099ms)
    Jan 12 15:24:04.220: INFO: (12) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.545784ms)
    Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.478501ms)
    Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.423293ms)
    Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.951774ms)
    Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.907237ms)
    Jan 12 15:24:04.221: INFO: (12) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.133084ms)
    Jan 12 15:24:04.222: INFO: (12) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.549382ms)
    Jan 12 15:24:04.226: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 4.273344ms)
    Jan 12 15:24:04.227: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 5.224308ms)
    Jan 12 15:24:04.227: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.943137ms)
    Jan 12 15:24:04.229: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.186453ms)
    Jan 12 15:24:04.229: INFO: (13) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 7.074507ms)
    Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 7.641683ms)
    Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.653108ms)
    Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 7.913769ms)
    Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 7.922816ms)
    Jan 12 15:24:04.230: INFO: (13) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 8.017122ms)
    Jan 12 15:24:04.231: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.144952ms)
    Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.036565ms)
    Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.906755ms)
    Jan 12 15:24:04.232: INFO: (13) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.907853ms)
    Jan 12 15:24:04.233: INFO: (13) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 10.239512ms)
    Jan 12 15:24:04.233: INFO: (13) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 10.293143ms)
    Jan 12 15:24:04.237: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 4.379132ms)
    Jan 12 15:24:04.238: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 5.172531ms)
    Jan 12 15:24:04.240: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 7.145938ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.082224ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 8.056869ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.234175ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 8.369052ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 8.276319ms)
    Jan 12 15:24:04.241: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.234344ms)
    Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.508392ms)
    Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.227697ms)
    Jan 12 15:24:04.242: INFO: (14) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 9.060818ms)
    Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.773377ms)
    Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.905019ms)
    Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.857253ms)
    Jan 12 15:24:04.243: INFO: (14) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 10.543238ms)
    Jan 12 15:24:04.248: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.751907ms)
    Jan 12 15:24:04.250: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 6.176483ms)
    Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 6.920245ms)
    Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.89292ms)
    Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.265658ms)
    Jan 12 15:24:04.251: INFO: (15) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.547799ms)
    Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 10.183118ms)
    Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 10.447527ms)
    Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.649127ms)
    Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.699071ms)
    Jan 12 15:24:04.254: INFO: (15) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.75384ms)
    Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.720945ms)
    Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.003345ms)
    Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 11.016101ms)
    Jan 12 15:24:04.255: INFO: (15) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.698215ms)
    Jan 12 15:24:04.256: INFO: (15) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 11.993989ms)
    Jan 12 15:24:04.262: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.509458ms)
    Jan 12 15:24:04.263: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 7.024403ms)
    Jan 12 15:24:04.263: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 7.122827ms)
    Jan 12 15:24:04.264: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 7.936209ms)
    Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 9.038114ms)
    Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.911341ms)
    Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.270893ms)
    Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.361708ms)
    Jan 12 15:24:04.265: INFO: (16) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.357361ms)
    Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 9.694293ms)
    Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 10.339717ms)
    Jan 12 15:24:04.266: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.961079ms)
    Jan 12 15:24:04.267: INFO: (16) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 10.260399ms)
    Jan 12 15:24:04.267: INFO: (16) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.655175ms)
    Jan 12 15:24:04.268: INFO: (16) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 11.672373ms)
    Jan 12 15:24:04.269: INFO: (16) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 12.795023ms)
    Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 4.953022ms)
    Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 4.655669ms)
    Jan 12 15:24:04.274: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 4.983461ms)
    Jan 12 15:24:04.278: INFO: (17) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.15988ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 9.821382ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 10.118025ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.843671ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.952071ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 9.919591ms)
    Jan 12 15:24:04.279: INFO: (17) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 9.976368ms)
    Jan 12 15:24:04.280: INFO: (17) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 11.386956ms)
    Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 11.5566ms)
    Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 11.311177ms)
    Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 12.080238ms)
    Jan 12 15:24:04.281: INFO: (17) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 12.047042ms)
    Jan 12 15:24:04.282: INFO: (17) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 12.335454ms)
    Jan 12 15:24:04.288: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 5.925946ms)
    Jan 12 15:24:04.288: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 5.826635ms)
    Jan 12 15:24:04.289: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 6.403087ms)
    Jan 12 15:24:04.289: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 6.727259ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 8.667389ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 8.006324ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 8.354144ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 9.1963ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 8.777459ms)
    Jan 12 15:24:04.291: INFO: (18) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 9.159054ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 8.769505ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 9.581671ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 9.219631ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 9.752288ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 9.595302ms)
    Jan 12 15:24:04.292: INFO: (18) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 9.780992ms)
    Jan 12 15:24:04.297: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:460/proxy/: tls baz (200; 4.661154ms)
    Jan 12 15:24:04.299: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 6.12975ms)
    Jan 12 15:24:04.300: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">... (200; 7.018458ms)
    Jan 12 15:24:04.301: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4/proxy/rewriteme">test</a> (200; 8.411505ms)
    Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 10.002748ms)
    Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:443/proxy/tlsrewritem... (200; 10.117363ms)
    Jan 12 15:24:04.303: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:160/proxy/: foo (200; 10.814724ms)
    Jan 12 15:24:04.304: INFO: (19) /api/v1/namespaces/proxy-9591/pods/https:proxy-service-4xgc8-8v2c4:462/proxy/: tls qux (200; 10.791928ms)
    Jan 12 15:24:04.304: INFO: (19) /api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/: <a href="/api/v1/namespaces/proxy-9591/pods/proxy-service-4xgc8-8v2c4:1080/proxy/rewriteme">test<... (200; 11.214104ms)
    Jan 12 15:24:04.305: INFO: (19) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname1/proxy/: foo (200; 12.773116ms)
    Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname2/proxy/: tls qux (200; 14.752018ms)
    Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname1/proxy/: foo (200; 15.398168ms)
    Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/https:proxy-service-4xgc8:tlsportname1/proxy/: tls baz (200; 15.342433ms)
    Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/services/http:proxy-service-4xgc8:portname2/proxy/: bar (200; 15.399637ms)
    Jan 12 15:24:04.308: INFO: (19) /api/v1/namespaces/proxy-9591/pods/http:proxy-service-4xgc8-8v2c4:162/proxy/: bar (200; 15.501976ms)
    Jan 12 15:24:04.309: INFO: (19) /api/v1/namespaces/proxy-9591/services/proxy-service-4xgc8:portname2/proxy/: bar (200; 16.091077ms)
    STEP: deleting ReplicationController proxy-service-4xgc8 in namespace proxy-9591, will wait for the garbage collector to delete the pods 01/12/23 15:24:04.309
    Jan 12 15:24:04.366: INFO: Deleting ReplicationController proxy-service-4xgc8 took: 3.964432ms
    Jan 12 15:24:04.466: INFO: Terminating ReplicationController proxy-service-4xgc8 pods took: 100.294378ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9591" for this suite. 01/12/23 15:24:12.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:12.374
Jan 12 15:24:12.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 15:24:12.375
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.388
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-dv8l9" 01/12/23 15:24:12.391
Jan 12 15:24:12.401: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-dv8l9-4820" 01/12/23 15:24:12.401
Jan 12 15:24:12.406: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-dv8l9-4820" 01/12/23 15:24:12.406
Jan 12 15:24:12.411: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7059" for this suite. 01/12/23 15:24:12.414
STEP: Destroying namespace "e2e-ns-dv8l9-4820" for this suite. 01/12/23 15:24:12.42
------------------------------
 [0.052 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:12.374
    Jan 12 15:24:12.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 15:24:12.375
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.388
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-dv8l9" 01/12/23 15:24:12.391
    Jan 12 15:24:12.401: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-dv8l9-4820" 01/12/23 15:24:12.401
    Jan 12 15:24:12.406: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-dv8l9-4820" 01/12/23 15:24:12.406
    Jan 12 15:24:12.411: INFO: Namespace "e2e-ns-dv8l9-4820" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:12.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7059" for this suite. 01/12/23 15:24:12.414
    STEP: Destroying namespace "e2e-ns-dv8l9-4820" for this suite. 01/12/23 15:24:12.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:12.427
Jan 12 15:24:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 15:24:12.428
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.438
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/12/23 15:24:12.44
Jan 12 15:24:12.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4958 cluster-info'
Jan 12 15:24:12.521: INFO: stderr: ""
Jan 12 15:24:12.521: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:12.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4958" for this suite. 01/12/23 15:24:12.523
------------------------------
 [0.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:12.427
    Jan 12 15:24:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 15:24:12.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.438
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/12/23 15:24:12.44
    Jan 12 15:24:12.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4958 cluster-info'
    Jan 12 15:24:12.521: INFO: stderr: ""
    Jan 12 15:24:12.521: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:12.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4958" for this suite. 01/12/23 15:24:12.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:12.532
Jan 12 15:24:12.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 15:24:12.533
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.544
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 15:24:12.546
Jan 12 15:24:12.552: INFO: Waiting up to 5m0s for pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5" in namespace "emptydir-1120" to be "Succeeded or Failed"
Jan 12 15:24:12.554: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.907572ms
Jan 12 15:24:14.557: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004517404s
Jan 12 15:24:16.558: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005094059s
STEP: Saw pod success 01/12/23 15:24:16.558
Jan 12 15:24:16.558: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5" satisfied condition "Succeeded or Failed"
Jan 12 15:24:16.560: INFO: Trying to get logs from node worker-1 pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 container test-container: <nil>
STEP: delete the pod 01/12/23 15:24:16.572
Jan 12 15:24:16.582: INFO: Waiting for pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 to disappear
Jan 12 15:24:16.584: INFO: Pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:16.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1120" for this suite. 01/12/23 15:24:16.586
------------------------------
 [4.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:12.532
    Jan 12 15:24:12.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 15:24:12.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:12.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:12.544
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 15:24:12.546
    Jan 12 15:24:12.552: INFO: Waiting up to 5m0s for pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5" in namespace "emptydir-1120" to be "Succeeded or Failed"
    Jan 12 15:24:12.554: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.907572ms
    Jan 12 15:24:14.557: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004517404s
    Jan 12 15:24:16.558: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005094059s
    STEP: Saw pod success 01/12/23 15:24:16.558
    Jan 12 15:24:16.558: INFO: Pod "pod-1c70d117-0025-4f83-a06d-bd0db718d1b5" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:16.560: INFO: Trying to get logs from node worker-1 pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 container test-container: <nil>
    STEP: delete the pod 01/12/23 15:24:16.572
    Jan 12 15:24:16.582: INFO: Waiting for pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 to disappear
    Jan 12 15:24:16.584: INFO: Pod pod-1c70d117-0025-4f83-a06d-bd0db718d1b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:16.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1120" for this suite. 01/12/23 15:24:16.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:16.592
Jan 12 15:24:16.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context-test 01/12/23 15:24:16.592
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:16.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:16.604
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 12 15:24:16.617: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988" in namespace "security-context-test-4834" to be "Succeeded or Failed"
Jan 12 15:24:16.619: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Pending", Reason="", readiness=false. Elapsed: 1.886682ms
Jan 12 15:24:18.621: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004360854s
Jan 12 15:24:20.623: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005936968s
Jan 12 15:24:20.623: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988" satisfied condition "Succeeded or Failed"
Jan 12 15:24:20.627: INFO: Got logs for pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:20.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4834" for this suite. 01/12/23 15:24:20.63
------------------------------
 [4.042 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:16.592
    Jan 12 15:24:16.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context-test 01/12/23 15:24:16.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:16.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:16.604
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 12 15:24:16.617: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988" in namespace "security-context-test-4834" to be "Succeeded or Failed"
    Jan 12 15:24:16.619: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Pending", Reason="", readiness=false. Elapsed: 1.886682ms
    Jan 12 15:24:18.621: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004360854s
    Jan 12 15:24:20.623: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005936968s
    Jan 12 15:24:20.623: INFO: Pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:20.627: INFO: Got logs for pod "busybox-privileged-false-6f7f75ac-ea43-4b4a-9043-cfbe460df988": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:20.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4834" for this suite. 01/12/23 15:24:20.63
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:20.633
Jan 12 15:24:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:24:20.635
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:20.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:20.644
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/12/23 15:24:20.646
Jan 12 15:24:20.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a" in namespace "projected-4261" to be "Succeeded or Failed"
Jan 12 15:24:20.653: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755258ms
Jan 12 15:24:22.656: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004738355s
Jan 12 15:24:24.655: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004094911s
STEP: Saw pod success 01/12/23 15:24:24.655
Jan 12 15:24:24.656: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a" satisfied condition "Succeeded or Failed"
Jan 12 15:24:24.657: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a container client-container: <nil>
STEP: delete the pod 01/12/23 15:24:24.662
Jan 12 15:24:24.670: INFO: Waiting for pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a to disappear
Jan 12 15:24:24.672: INFO: Pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4261" for this suite. 01/12/23 15:24:24.674
------------------------------
 [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:20.633
    Jan 12 15:24:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:24:20.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:20.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:20.644
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/12/23 15:24:20.646
    Jan 12 15:24:20.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a" in namespace "projected-4261" to be "Succeeded or Failed"
    Jan 12 15:24:20.653: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755258ms
    Jan 12 15:24:22.656: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004738355s
    Jan 12 15:24:24.655: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004094911s
    STEP: Saw pod success 01/12/23 15:24:24.655
    Jan 12 15:24:24.656: INFO: Pod "downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:24.657: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a container client-container: <nil>
    STEP: delete the pod 01/12/23 15:24:24.662
    Jan 12 15:24:24.670: INFO: Waiting for pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a to disappear
    Jan 12 15:24:24.672: INFO: Pod downwardapi-volume-fefdde0f-1051-415d-92cb-576c88bb575a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4261" for this suite. 01/12/23 15:24:24.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:24.68
Jan 12 15:24:24.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename runtimeclass 01/12/23 15:24:24.681
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:24.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:24.693
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:24.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-742" for this suite. 01/12/23 15:24:24.701
------------------------------
 [0.024 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:24.68
    Jan 12 15:24:24.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 15:24:24.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:24.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:24.693
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:24.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-742" for this suite. 01/12/23 15:24:24.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:24.707
Jan 12 15:24:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:24:24.707
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:24.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:24.722
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-569190ca-31ab-438f-9ddd-62d2a58b13bf 01/12/23 15:24:24.725
STEP: Creating a pod to test consume configMaps 01/12/23 15:24:24.727
Jan 12 15:24:24.734: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d" in namespace "projected-7636" to be "Succeeded or Failed"
Jan 12 15:24:24.736: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900153ms
Jan 12 15:24:26.738: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Running", Reason="", readiness=false. Elapsed: 2.00430376s
Jan 12 15:24:28.738: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004568003s
STEP: Saw pod success 01/12/23 15:24:28.738
Jan 12 15:24:28.739: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d" satisfied condition "Succeeded or Failed"
Jan 12 15:24:28.740: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:24:28.745
Jan 12 15:24:28.755: INFO: Waiting for pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d to disappear
Jan 12 15:24:28.757: INFO: Pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:28.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7636" for this suite. 01/12/23 15:24:28.759
------------------------------
 [4.056 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:24.707
    Jan 12 15:24:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:24:24.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:24.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:24.722
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-569190ca-31ab-438f-9ddd-62d2a58b13bf 01/12/23 15:24:24.725
    STEP: Creating a pod to test consume configMaps 01/12/23 15:24:24.727
    Jan 12 15:24:24.734: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d" in namespace "projected-7636" to be "Succeeded or Failed"
    Jan 12 15:24:24.736: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900153ms
    Jan 12 15:24:26.738: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Running", Reason="", readiness=false. Elapsed: 2.00430376s
    Jan 12 15:24:28.738: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004568003s
    STEP: Saw pod success 01/12/23 15:24:28.738
    Jan 12 15:24:28.739: INFO: Pod "pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:28.740: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:24:28.745
    Jan 12 15:24:28.755: INFO: Waiting for pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d to disappear
    Jan 12 15:24:28.757: INFO: Pod pod-projected-configmaps-40f5ddde-ec70-4e62-926d-9816aaba660d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:28.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7636" for this suite. 01/12/23 15:24:28.759
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:28.763
Jan 12 15:24:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 15:24:28.764
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:28.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:28.774
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/12/23 15:24:28.776
Jan 12 15:24:28.783: INFO: Waiting up to 5m0s for pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935" in namespace "svcaccounts-6317" to be "Succeeded or Failed"
Jan 12 15:24:28.785: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949248ms
Jan 12 15:24:30.788: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004865573s
Jan 12 15:24:32.789: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006115855s
STEP: Saw pod success 01/12/23 15:24:32.789
Jan 12 15:24:32.789: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935" satisfied condition "Succeeded or Failed"
Jan 12 15:24:32.791: INFO: Trying to get logs from node worker-1 pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:24:32.799
Jan 12 15:24:32.806: INFO: Waiting for pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 to disappear
Jan 12 15:24:32.808: INFO: Pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:32.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6317" for this suite. 01/12/23 15:24:32.81
------------------------------
 [4.051 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:28.763
    Jan 12 15:24:28.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 15:24:28.764
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:28.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:28.774
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/12/23 15:24:28.776
    Jan 12 15:24:28.783: INFO: Waiting up to 5m0s for pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935" in namespace "svcaccounts-6317" to be "Succeeded or Failed"
    Jan 12 15:24:28.785: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949248ms
    Jan 12 15:24:30.788: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004865573s
    Jan 12 15:24:32.789: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006115855s
    STEP: Saw pod success 01/12/23 15:24:32.789
    Jan 12 15:24:32.789: INFO: Pod "test-pod-b63295a8-639d-4312-8520-3ed97b753935" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:32.791: INFO: Trying to get logs from node worker-1 pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:24:32.799
    Jan 12 15:24:32.806: INFO: Waiting for pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 to disappear
    Jan 12 15:24:32.808: INFO: Pod test-pod-b63295a8-639d-4312-8520-3ed97b753935 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:32.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6317" for this suite. 01/12/23 15:24:32.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:32.815
Jan 12 15:24:32.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 15:24:32.816
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:32.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:32.84
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-b8568710-dd24-4e73-8a35-c1fad368ecad 01/12/23 15:24:32.854
STEP: Creating a pod to test consume secrets 01/12/23 15:24:32.856
Jan 12 15:24:32.868: INFO: Waiting up to 5m0s for pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202" in namespace "secrets-4207" to be "Succeeded or Failed"
Jan 12 15:24:32.869: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61819ms
Jan 12 15:24:34.872: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004471402s
Jan 12 15:24:36.873: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00567238s
STEP: Saw pod success 01/12/23 15:24:36.873
Jan 12 15:24:36.874: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202" satisfied condition "Succeeded or Failed"
Jan 12 15:24:36.875: INFO: Trying to get logs from node worker-1 pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 15:24:36.879
Jan 12 15:24:36.887: INFO: Waiting for pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 to disappear
Jan 12 15:24:36.888: INFO: Pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4207" for this suite. 01/12/23 15:24:36.89
STEP: Destroying namespace "secret-namespace-5227" for this suite. 01/12/23 15:24:36.894
------------------------------
 [4.082 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:32.815
    Jan 12 15:24:32.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 15:24:32.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:32.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:32.84
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-b8568710-dd24-4e73-8a35-c1fad368ecad 01/12/23 15:24:32.854
    STEP: Creating a pod to test consume secrets 01/12/23 15:24:32.856
    Jan 12 15:24:32.868: INFO: Waiting up to 5m0s for pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202" in namespace "secrets-4207" to be "Succeeded or Failed"
    Jan 12 15:24:32.869: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61819ms
    Jan 12 15:24:34.872: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004471402s
    Jan 12 15:24:36.873: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00567238s
    STEP: Saw pod success 01/12/23 15:24:36.873
    Jan 12 15:24:36.874: INFO: Pod "pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:36.875: INFO: Trying to get logs from node worker-1 pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:24:36.879
    Jan 12 15:24:36.887: INFO: Waiting for pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 to disappear
    Jan 12 15:24:36.888: INFO: Pod pod-secrets-71efc649-93f7-43b1-980f-8f7c556b1202 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4207" for this suite. 01/12/23 15:24:36.89
    STEP: Destroying namespace "secret-namespace-5227" for this suite. 01/12/23 15:24:36.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:36.898
Jan 12 15:24:36.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename containers 01/12/23 15:24:36.899
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:36.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:36.908
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/12/23 15:24:36.91
Jan 12 15:24:36.916: INFO: Waiting up to 5m0s for pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7" in namespace "containers-8382" to be "Succeeded or Failed"
Jan 12 15:24:36.917: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767768ms
Jan 12 15:24:38.920: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004690228s
Jan 12 15:24:40.922: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006397994s
STEP: Saw pod success 01/12/23 15:24:40.922
Jan 12 15:24:40.922: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7" satisfied condition "Succeeded or Failed"
Jan 12 15:24:40.924: INFO: Trying to get logs from node worker-1 pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:24:40.928
Jan 12 15:24:40.935: INFO: Waiting for pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 to disappear
Jan 12 15:24:40.936: INFO: Pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8382" for this suite. 01/12/23 15:24:40.939
------------------------------
 [4.044 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:36.898
    Jan 12 15:24:36.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename containers 01/12/23 15:24:36.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:36.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:36.908
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/12/23 15:24:36.91
    Jan 12 15:24:36.916: INFO: Waiting up to 5m0s for pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7" in namespace "containers-8382" to be "Succeeded or Failed"
    Jan 12 15:24:36.917: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767768ms
    Jan 12 15:24:38.920: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004690228s
    Jan 12 15:24:40.922: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006397994s
    STEP: Saw pod success 01/12/23 15:24:40.922
    Jan 12 15:24:40.922: INFO: Pod "client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7" satisfied condition "Succeeded or Failed"
    Jan 12 15:24:40.924: INFO: Trying to get logs from node worker-1 pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:24:40.928
    Jan 12 15:24:40.935: INFO: Waiting for pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 to disappear
    Jan 12 15:24:40.936: INFO: Pod client-containers-fc6dced1-0b1d-4650-9e01-111b7285aec7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:40.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8382" for this suite. 01/12/23 15:24:40.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:40.943
Jan 12 15:24:40.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sysctl 01/12/23 15:24:40.944
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:40.951
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:40.954
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/12/23 15:24:40.956
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:24:40.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1128" for this suite. 01/12/23 15:24:40.961
------------------------------
 [0.022 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:40.943
    Jan 12 15:24:40.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sysctl 01/12/23 15:24:40.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:40.951
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:40.954
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/12/23 15:24:40.956
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:24:40.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1128" for this suite. 01/12/23 15:24:40.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:24:40.966
Jan 12 15:24:40.966: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename subpath 01/12/23 15:24:40.967
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:40.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:40.977
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 15:24:40.98
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-tncf 01/12/23 15:24:40.988
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 15:24:40.989
Jan 12 15:24:40.993: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tncf" in namespace "subpath-745" to be "Succeeded or Failed"
Jan 12 15:24:40.995: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837836ms
Jan 12 15:24:42.997: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 2.004278821s
Jan 12 15:24:44.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 4.004536988s
Jan 12 15:24:46.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 6.004913205s
Jan 12 15:24:48.997: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 8.00425615s
Jan 12 15:24:50.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 10.005460752s
Jan 12 15:24:52.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 12.005946263s
Jan 12 15:24:54.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 14.005525392s
Jan 12 15:24:56.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 16.0060162s
Jan 12 15:24:58.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 18.004771678s
Jan 12 15:25:01.000: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 20.0063736s
Jan 12 15:25:02.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=false. Elapsed: 22.005598416s
Jan 12 15:25:04.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005040518s
STEP: Saw pod success 01/12/23 15:25:04.998
Jan 12 15:25:04.998: INFO: Pod "pod-subpath-test-secret-tncf" satisfied condition "Succeeded or Failed"
Jan 12 15:25:05.000: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-secret-tncf container test-container-subpath-secret-tncf: <nil>
STEP: delete the pod 01/12/23 15:25:05.005
Jan 12 15:25:05.014: INFO: Waiting for pod pod-subpath-test-secret-tncf to disappear
Jan 12 15:25:05.015: INFO: Pod pod-subpath-test-secret-tncf no longer exists
STEP: Deleting pod pod-subpath-test-secret-tncf 01/12/23 15:25:05.015
Jan 12 15:25:05.016: INFO: Deleting pod "pod-subpath-test-secret-tncf" in namespace "subpath-745"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 15:25:05.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-745" for this suite. 01/12/23 15:25:05.019
------------------------------
 [SLOW TEST] [24.057 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:24:40.966
    Jan 12 15:24:40.966: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename subpath 01/12/23 15:24:40.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:24:40.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:24:40.977
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 15:24:40.98
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-tncf 01/12/23 15:24:40.988
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 15:24:40.989
    Jan 12 15:24:40.993: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tncf" in namespace "subpath-745" to be "Succeeded or Failed"
    Jan 12 15:24:40.995: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837836ms
    Jan 12 15:24:42.997: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 2.004278821s
    Jan 12 15:24:44.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 4.004536988s
    Jan 12 15:24:46.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 6.004913205s
    Jan 12 15:24:48.997: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 8.00425615s
    Jan 12 15:24:50.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 10.005460752s
    Jan 12 15:24:52.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 12.005946263s
    Jan 12 15:24:54.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 14.005525392s
    Jan 12 15:24:56.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 16.0060162s
    Jan 12 15:24:58.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 18.004771678s
    Jan 12 15:25:01.000: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=true. Elapsed: 20.0063736s
    Jan 12 15:25:02.999: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Running", Reason="", readiness=false. Elapsed: 22.005598416s
    Jan 12 15:25:04.998: INFO: Pod "pod-subpath-test-secret-tncf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005040518s
    STEP: Saw pod success 01/12/23 15:25:04.998
    Jan 12 15:25:04.998: INFO: Pod "pod-subpath-test-secret-tncf" satisfied condition "Succeeded or Failed"
    Jan 12 15:25:05.000: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-secret-tncf container test-container-subpath-secret-tncf: <nil>
    STEP: delete the pod 01/12/23 15:25:05.005
    Jan 12 15:25:05.014: INFO: Waiting for pod pod-subpath-test-secret-tncf to disappear
    Jan 12 15:25:05.015: INFO: Pod pod-subpath-test-secret-tncf no longer exists
    STEP: Deleting pod pod-subpath-test-secret-tncf 01/12/23 15:25:05.015
    Jan 12 15:25:05.016: INFO: Deleting pod "pod-subpath-test-secret-tncf" in namespace "subpath-745"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:25:05.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-745" for this suite. 01/12/23 15:25:05.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:25:05.025
Jan 12 15:25:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:25:05.026
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:25:05.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:25:05.035
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-f961fc8d-f26e-4c7a-847c-a67ace2c6e2c 01/12/23 15:25:05.037
STEP: Creating a pod to test consume configMaps 01/12/23 15:25:05.041
Jan 12 15:25:05.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e" in namespace "configmap-5476" to be "Succeeded or Failed"
Jan 12 15:25:05.050: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772407ms
Jan 12 15:25:07.054: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006138604s
Jan 12 15:25:09.053: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005422388s
STEP: Saw pod success 01/12/23 15:25:09.053
Jan 12 15:25:09.053: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e" satisfied condition "Succeeded or Failed"
Jan 12 15:25:09.055: INFO: Trying to get logs from node worker-1 pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:25:09.06
Jan 12 15:25:09.068: INFO: Waiting for pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e to disappear
Jan 12 15:25:09.069: INFO: Pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:25:09.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5476" for this suite. 01/12/23 15:25:09.073
------------------------------
 [4.052 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:25:05.025
    Jan 12 15:25:05.025: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:25:05.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:25:05.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:25:05.035
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-f961fc8d-f26e-4c7a-847c-a67ace2c6e2c 01/12/23 15:25:05.037
    STEP: Creating a pod to test consume configMaps 01/12/23 15:25:05.041
    Jan 12 15:25:05.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e" in namespace "configmap-5476" to be "Succeeded or Failed"
    Jan 12 15:25:05.050: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.772407ms
    Jan 12 15:25:07.054: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006138604s
    Jan 12 15:25:09.053: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005422388s
    STEP: Saw pod success 01/12/23 15:25:09.053
    Jan 12 15:25:09.053: INFO: Pod "pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e" satisfied condition "Succeeded or Failed"
    Jan 12 15:25:09.055: INFO: Trying to get logs from node worker-1 pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:25:09.06
    Jan 12 15:25:09.068: INFO: Waiting for pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e to disappear
    Jan 12 15:25:09.069: INFO: Pod pod-configmaps-17e93c17-b3ea-466d-bcea-21dd899d821e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:25:09.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5476" for this suite. 01/12/23 15:25:09.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:25:09.079
Jan 12 15:25:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename cronjob 01/12/23 15:25:09.079
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:25:09.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:25:09.089
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/12/23 15:25:09.091
STEP: Ensuring a job is scheduled 01/12/23 15:25:09.095
STEP: Ensuring exactly one is scheduled 01/12/23 15:26:01.098
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 15:26:01.1
STEP: Ensuring no more jobs are scheduled 01/12/23 15:26:01.102
STEP: Removing cronjob 01/12/23 15:31:01.107
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6785" for this suite. 01/12/23 15:31:01.118
------------------------------
 [SLOW TEST] [352.046 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:25:09.079
    Jan 12 15:25:09.079: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename cronjob 01/12/23 15:25:09.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:25:09.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:25:09.089
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/12/23 15:25:09.091
    STEP: Ensuring a job is scheduled 01/12/23 15:25:09.095
    STEP: Ensuring exactly one is scheduled 01/12/23 15:26:01.098
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 15:26:01.1
    STEP: Ensuring no more jobs are scheduled 01/12/23 15:26:01.102
    STEP: Removing cronjob 01/12/23 15:31:01.107
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:01.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6785" for this suite. 01/12/23 15:31:01.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:01.126
Jan 12 15:31:01.126: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:31:01.127
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:01.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:01.141
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:31:01.153
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:01.511
STEP: Deploying the webhook pod 01/12/23 15:31:01.515
STEP: Wait for the deployment to be ready 01/12/23 15:31:01.527
Jan 12 15:31:01.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:31:03.538
STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:03.547
Jan 12 15:31:04.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/12/23 15:31:04.55
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/12/23 15:31:04.573
STEP: Creating a configMap that should not be mutated 01/12/23 15:31:04.577
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/12/23 15:31:04.584
STEP: Creating a configMap that should be mutated 01/12/23 15:31:04.59
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4820" for this suite. 01/12/23 15:31:04.634
STEP: Destroying namespace "webhook-4820-markers" for this suite. 01/12/23 15:31:04.642
------------------------------
 [3.521 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:01.126
    Jan 12 15:31:01.126: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:31:01.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:01.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:01.141
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:31:01.153
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:01.511
    STEP: Deploying the webhook pod 01/12/23 15:31:01.515
    STEP: Wait for the deployment to be ready 01/12/23 15:31:01.527
    Jan 12 15:31:01.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:31:03.538
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:03.547
    Jan 12 15:31:04.548: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/12/23 15:31:04.55
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/12/23 15:31:04.573
    STEP: Creating a configMap that should not be mutated 01/12/23 15:31:04.577
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/12/23 15:31:04.584
    STEP: Creating a configMap that should be mutated 01/12/23 15:31:04.59
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4820" for this suite. 01/12/23 15:31:04.634
    STEP: Destroying namespace "webhook-4820-markers" for this suite. 01/12/23 15:31:04.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:04.65
Jan 12 15:31:04.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:31:04.651
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:04.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:04.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:31:04.672
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:05.265
STEP: Deploying the webhook pod 01/12/23 15:31:05.271
STEP: Wait for the deployment to be ready 01/12/23 15:31:05.281
Jan 12 15:31:05.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:31:07.296
STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:07.305
Jan 12 15:31:08.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 15:31:08.307
Jan 12 15:31:13.323: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 15:31:13.433
STEP: Creating a dummy validating-webhook-configuration object 01/12/23 15:31:13.448
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/12/23 15:31:13.455
STEP: Creating a dummy mutating-webhook-configuration object 01/12/23 15:31:13.459
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/12/23 15:31:13.469
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7467" for this suite. 01/12/23 15:31:13.508
STEP: Destroying namespace "webhook-7467-markers" for this suite. 01/12/23 15:31:13.515
------------------------------
 [SLOW TEST] [8.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:04.65
    Jan 12 15:31:04.650: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:31:04.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:04.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:04.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:31:04.672
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:05.265
    STEP: Deploying the webhook pod 01/12/23 15:31:05.271
    STEP: Wait for the deployment to be ready 01/12/23 15:31:05.281
    Jan 12 15:31:05.287: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:31:07.296
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:07.305
    Jan 12 15:31:08.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 15:31:08.307
    Jan 12 15:31:13.323: INFO: Waiting for webhook configuration to be ready...
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/12/23 15:31:13.433
    STEP: Creating a dummy validating-webhook-configuration object 01/12/23 15:31:13.448
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/12/23 15:31:13.455
    STEP: Creating a dummy mutating-webhook-configuration object 01/12/23 15:31:13.459
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/12/23 15:31:13.469
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:13.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7467" for this suite. 01/12/23 15:31:13.508
    STEP: Destroying namespace "webhook-7467-markers" for this suite. 01/12/23 15:31:13.515
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:13.521
Jan 12 15:31:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 15:31:13.522
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:13.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:13.534
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8982.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8982.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/12/23 15:31:13.536
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8982.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8982.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/12/23 15:31:13.536
STEP: creating a pod to probe /etc/hosts 01/12/23 15:31:13.536
STEP: submitting the pod to kubernetes 01/12/23 15:31:13.536
Jan 12 15:31:13.544: INFO: Waiting up to 15m0s for pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043" in namespace "dns-8982" to be "running"
Jan 12 15:31:13.546: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 1.922808ms
Jan 12 15:31:15.549: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004642296s
Jan 12 15:31:17.550: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006219502s
Jan 12 15:31:19.549: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004582031s
Jan 12 15:31:21.548: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Running", Reason="", readiness=true. Elapsed: 8.00436833s
Jan 12 15:31:21.548: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043" satisfied condition "running"
STEP: retrieving the pod 01/12/23 15:31:21.548
STEP: looking for the results for each expected name from probers 01/12/23 15:31:21.55
Jan 12 15:31:21.562: INFO: DNS probes using dns-8982/dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043 succeeded

STEP: deleting the pod 01/12/23 15:31:21.562
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:21.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8982" for this suite. 01/12/23 15:31:21.573
------------------------------
 [SLOW TEST] [8.056 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:13.521
    Jan 12 15:31:13.521: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 15:31:13.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:13.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:13.534
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8982.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8982.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/12/23 15:31:13.536
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8982.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8982.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/12/23 15:31:13.536
    STEP: creating a pod to probe /etc/hosts 01/12/23 15:31:13.536
    STEP: submitting the pod to kubernetes 01/12/23 15:31:13.536
    Jan 12 15:31:13.544: INFO: Waiting up to 15m0s for pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043" in namespace "dns-8982" to be "running"
    Jan 12 15:31:13.546: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 1.922808ms
    Jan 12 15:31:15.549: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004642296s
    Jan 12 15:31:17.550: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006219502s
    Jan 12 15:31:19.549: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004582031s
    Jan 12 15:31:21.548: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043": Phase="Running", Reason="", readiness=true. Elapsed: 8.00436833s
    Jan 12 15:31:21.548: INFO: Pod "dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 15:31:21.548
    STEP: looking for the results for each expected name from probers 01/12/23 15:31:21.55
    Jan 12 15:31:21.562: INFO: DNS probes using dns-8982/dns-test-633b6e86-0c57-4378-bcb5-cfd9323bb043 succeeded

    STEP: deleting the pod 01/12/23 15:31:21.562
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:21.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8982" for this suite. 01/12/23 15:31:21.573
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:21.578
Jan 12 15:31:21.578: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 15:31:21.578
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:21.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:21.64
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3029 01/12/23 15:31:21.643
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 12 15:31:21.658: INFO: Found 0 stateful pods, waiting for 1
Jan 12 15:31:31.662: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/12/23 15:31:31.665
W0112 15:31:31.673103      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 15:31:31.678: INFO: Found 1 stateful pods, waiting for 2
Jan 12 15:31:41.683: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 15:31:41.683: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/12/23 15:31:41.687
STEP: Delete all of the StatefulSets 01/12/23 15:31:41.689
STEP: Verify that StatefulSets have been deleted 01/12/23 15:31:41.693
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 15:31:41.695: INFO: Deleting all statefulset in ns statefulset-3029
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3029" for this suite. 01/12/23 15:31:41.705
------------------------------
 [SLOW TEST] [20.133 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:21.578
    Jan 12 15:31:21.578: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 15:31:21.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:21.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:21.64
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3029 01/12/23 15:31:21.643
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 12 15:31:21.658: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 15:31:31.662: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/12/23 15:31:31.665
    W0112 15:31:31.673103      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 15:31:31.678: INFO: Found 1 stateful pods, waiting for 2
    Jan 12 15:31:41.683: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 15:31:41.683: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/12/23 15:31:41.687
    STEP: Delete all of the StatefulSets 01/12/23 15:31:41.689
    STEP: Verify that StatefulSets have been deleted 01/12/23 15:31:41.693
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 15:31:41.695: INFO: Deleting all statefulset in ns statefulset-3029
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:41.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3029" for this suite. 01/12/23 15:31:41.705
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:41.711
Jan 12 15:31:41.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:31:41.712
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:41.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:41.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:31:41.738
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:42.282
STEP: Deploying the webhook pod 01/12/23 15:31:42.288
STEP: Wait for the deployment to be ready 01/12/23 15:31:42.296
Jan 12 15:31:42.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:31:44.309
STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:44.32
Jan 12 15:31:45.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/12/23 15:31:45.322
STEP: create a configmap that should be updated by the webhook 01/12/23 15:31:45.344
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:45.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9394" for this suite. 01/12/23 15:31:45.386
STEP: Destroying namespace "webhook-9394-markers" for this suite. 01/12/23 15:31:45.392
------------------------------
 [3.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:41.711
    Jan 12 15:31:41.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:31:41.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:41.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:41.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:31:41.738
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:31:42.282
    STEP: Deploying the webhook pod 01/12/23 15:31:42.288
    STEP: Wait for the deployment to be ready 01/12/23 15:31:42.296
    Jan 12 15:31:42.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:31:44.309
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:31:44.32
    Jan 12 15:31:45.320: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/12/23 15:31:45.322
    STEP: create a configmap that should be updated by the webhook 01/12/23 15:31:45.344
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:45.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9394" for this suite. 01/12/23 15:31:45.386
    STEP: Destroying namespace "webhook-9394-markers" for this suite. 01/12/23 15:31:45.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:45.4
Jan 12 15:31:45.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:31:45.402
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:45.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:45.413
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 12 15:31:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/12/23 15:31:46.799
Jan 12 15:31:46.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
Jan 12 15:31:47.397: INFO: stderr: ""
Jan 12 15:31:47.398: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 12 15:31:47.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 delete e2e-test-crd-publish-openapi-8427-crds test-foo'
Jan 12 15:31:47.456: INFO: stderr: ""
Jan 12 15:31:47.456: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 12 15:31:47.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
Jan 12 15:31:47.681: INFO: stderr: ""
Jan 12 15:31:47.681: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 12 15:31:47.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 delete e2e-test-crd-publish-openapi-8427-crds test-foo'
Jan 12 15:31:47.746: INFO: stderr: ""
Jan 12 15:31:47.746: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/12/23 15:31:47.746
Jan 12 15:31:47.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
Jan 12 15:31:47.936: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/12/23 15:31:47.936
Jan 12 15:31:47.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
Jan 12 15:31:48.122: INFO: rc: 1
Jan 12 15:31:48.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
Jan 12 15:31:48.320: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/12/23 15:31:48.32
Jan 12 15:31:48.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
Jan 12 15:31:48.531: INFO: rc: 1
Jan 12 15:31:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
Jan 12 15:31:48.698: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/12/23 15:31:48.698
Jan 12 15:31:48.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds'
Jan 12 15:31:48.861: INFO: stderr: ""
Jan 12 15:31:48.861: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/12/23 15:31:48.862
Jan 12 15:31:48.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.metadata'
Jan 12 15:31:49.039: INFO: stderr: ""
Jan 12 15:31:49.039: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 12 15:31:49.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec'
Jan 12 15:31:49.212: INFO: stderr: ""
Jan 12 15:31:49.212: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 12 15:31:49.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec.bars'
Jan 12 15:31:49.372: INFO: stderr: ""
Jan 12 15:31:49.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/12/23 15:31:49.373
Jan 12 15:31:49.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec.bars2'
Jan 12 15:31:49.569: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:50.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-443" for this suite. 01/12/23 15:31:50.941
------------------------------
 [SLOW TEST] [5.544 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:45.4
    Jan 12 15:31:45.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:31:45.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:45.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:45.413
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 12 15:31:45.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/12/23 15:31:46.799
    Jan 12 15:31:46.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
    Jan 12 15:31:47.397: INFO: stderr: ""
    Jan 12 15:31:47.398: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 12 15:31:47.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 delete e2e-test-crd-publish-openapi-8427-crds test-foo'
    Jan 12 15:31:47.456: INFO: stderr: ""
    Jan 12 15:31:47.456: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 12 15:31:47.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
    Jan 12 15:31:47.681: INFO: stderr: ""
    Jan 12 15:31:47.681: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 12 15:31:47.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 delete e2e-test-crd-publish-openapi-8427-crds test-foo'
    Jan 12 15:31:47.746: INFO: stderr: ""
    Jan 12 15:31:47.746: INFO: stdout: "e2e-test-crd-publish-openapi-8427-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/12/23 15:31:47.746
    Jan 12 15:31:47.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
    Jan 12 15:31:47.936: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/12/23 15:31:47.936
    Jan 12 15:31:47.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
    Jan 12 15:31:48.122: INFO: rc: 1
    Jan 12 15:31:48.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
    Jan 12 15:31:48.320: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/12/23 15:31:48.32
    Jan 12 15:31:48.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 create -f -'
    Jan 12 15:31:48.531: INFO: rc: 1
    Jan 12 15:31:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 --namespace=crd-publish-openapi-443 apply -f -'
    Jan 12 15:31:48.698: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/12/23 15:31:48.698
    Jan 12 15:31:48.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds'
    Jan 12 15:31:48.861: INFO: stderr: ""
    Jan 12 15:31:48.861: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/12/23 15:31:48.862
    Jan 12 15:31:48.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.metadata'
    Jan 12 15:31:49.039: INFO: stderr: ""
    Jan 12 15:31:49.039: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 12 15:31:49.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec'
    Jan 12 15:31:49.212: INFO: stderr: ""
    Jan 12 15:31:49.212: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 12 15:31:49.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec.bars'
    Jan 12 15:31:49.372: INFO: stderr: ""
    Jan 12 15:31:49.372: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8427-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/12/23 15:31:49.373
    Jan 12 15:31:49.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-443 explain e2e-test-crd-publish-openapi-8427-crds.spec.bars2'
    Jan 12 15:31:49.569: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:50.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-443" for this suite. 01/12/23 15:31:50.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:50.946
Jan 12 15:31:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 15:31:50.947
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:50.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:50.957
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/12/23 15:31:50.961
STEP: Verify that the required pods have come up. 01/12/23 15:31:50.964
Jan 12 15:31:50.966: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 15:31:55.969: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 15:31:55.969
STEP: Getting /status 01/12/23 15:31:55.969
Jan 12 15:31:55.973: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/12/23 15:31:55.973
Jan 12 15:31:55.983: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/12/23 15:31:55.983
Jan 12 15:31:55.984: INFO: Observed &ReplicaSet event: ADDED
Jan 12 15:31:55.984: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.985: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.985: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.985: INFO: Found replicaset test-rs in namespace replicaset-3276 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 15:31:55.985: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/12/23 15:31:55.985
Jan 12 15:31:55.985: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 15:31:55.990: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/12/23 15:31:55.99
Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: ADDED
Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.992: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.992: INFO: Observed replicaset test-rs in namespace replicaset-3276 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 15:31:55.992: INFO: Observed &ReplicaSet event: MODIFIED
Jan 12 15:31:55.992: INFO: Found replicaset test-rs in namespace replicaset-3276 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 12 15:31:55.992: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3276" for this suite. 01/12/23 15:31:55.994
------------------------------
 [SLOW TEST] [5.052 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:50.946
    Jan 12 15:31:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 15:31:50.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:50.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:50.957
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/12/23 15:31:50.961
    STEP: Verify that the required pods have come up. 01/12/23 15:31:50.964
    Jan 12 15:31:50.966: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 15:31:55.969: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 15:31:55.969
    STEP: Getting /status 01/12/23 15:31:55.969
    Jan 12 15:31:55.973: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/12/23 15:31:55.973
    Jan 12 15:31:55.983: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/12/23 15:31:55.983
    Jan 12 15:31:55.984: INFO: Observed &ReplicaSet event: ADDED
    Jan 12 15:31:55.984: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.985: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.985: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.985: INFO: Found replicaset test-rs in namespace replicaset-3276 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 15:31:55.985: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/12/23 15:31:55.985
    Jan 12 15:31:55.985: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 15:31:55.990: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/12/23 15:31:55.99
    Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: ADDED
    Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.991: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.992: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.992: INFO: Observed replicaset test-rs in namespace replicaset-3276 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 15:31:55.992: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 12 15:31:55.992: INFO: Found replicaset test-rs in namespace replicaset-3276 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 12 15:31:55.992: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:55.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3276" for this suite. 01/12/23 15:31:55.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:56
Jan 12 15:31:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename lease-test 01/12/23 15:31:56.001
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:56.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:56.012
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 12 15:31:56.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-4018" for this suite. 01/12/23 15:31:56.058
------------------------------
 [0.062 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:56
    Jan 12 15:31:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename lease-test 01/12/23 15:31:56.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:56.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:56.012
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:31:56.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-4018" for this suite. 01/12/23 15:31:56.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:31:56.064
Jan 12 15:31:56.064: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:31:56.064
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:56.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:56.08
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-0994fd50-3bfb-47ad-9498-47dcd148661b 01/12/23 15:31:56.083
STEP: Creating a pod to test consume configMaps 01/12/23 15:31:56.086
Jan 12 15:31:56.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590" in namespace "projected-9791" to be "Succeeded or Failed"
Jan 12 15:31:56.094: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958707ms
Jan 12 15:31:58.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004461302s
Jan 12 15:32:00.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004802108s
STEP: Saw pod success 01/12/23 15:32:00.097
Jan 12 15:32:00.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590" satisfied condition "Succeeded or Failed"
Jan 12 15:32:00.099: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/12/23 15:32:00.118
Jan 12 15:32:00.127: INFO: Waiting for pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 to disappear
Jan 12 15:32:00.130: INFO: Pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:32:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9791" for this suite. 01/12/23 15:32:00.133
------------------------------
 [4.075 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:31:56.064
    Jan 12 15:31:56.064: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:31:56.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:31:56.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:31:56.08
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-0994fd50-3bfb-47ad-9498-47dcd148661b 01/12/23 15:31:56.083
    STEP: Creating a pod to test consume configMaps 01/12/23 15:31:56.086
    Jan 12 15:31:56.092: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590" in namespace "projected-9791" to be "Succeeded or Failed"
    Jan 12 15:31:56.094: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Pending", Reason="", readiness=false. Elapsed: 1.958707ms
    Jan 12 15:31:58.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004461302s
    Jan 12 15:32:00.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004802108s
    STEP: Saw pod success 01/12/23 15:32:00.097
    Jan 12 15:32:00.097: INFO: Pod "pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590" satisfied condition "Succeeded or Failed"
    Jan 12 15:32:00.099: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:32:00.118
    Jan 12 15:32:00.127: INFO: Waiting for pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 to disappear
    Jan 12 15:32:00.130: INFO: Pod pod-projected-configmaps-28396247-877c-409d-b40c-b8b087da2590 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:32:00.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9791" for this suite. 01/12/23 15:32:00.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:32:00.142
Jan 12 15:32:00.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:32:00.143
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:00.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:00.155
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9973 01/12/23 15:32:00.158
STEP: creating service affinity-nodeport in namespace services-9973 01/12/23 15:32:00.158
STEP: creating replication controller affinity-nodeport in namespace services-9973 01/12/23 15:32:00.171
I0112 15:32:00.177502      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9973, replica count: 3
I0112 15:32:03.229662      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0112 15:32:06.230469      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 15:32:06.236: INFO: Creating new exec pod
Jan 12 15:32:06.240: INFO: Waiting up to 5m0s for pod "execpod-affinity7l74v" in namespace "services-9973" to be "running"
Jan 12 15:32:06.248: INFO: Pod "execpod-affinity7l74v": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981234ms
Jan 12 15:32:08.252: INFO: Pod "execpod-affinity7l74v": Phase="Running", Reason="", readiness=true. Elapsed: 2.011640943s
Jan 12 15:32:08.252: INFO: Pod "execpod-affinity7l74v" satisfied condition "running"
Jan 12 15:32:09.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 12 15:32:09.375: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 12 15:32:09.375: INFO: stdout: ""
Jan 12 15:32:09.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.101.220.21 80'
Jan 12 15:32:09.489: INFO: stderr: "+ nc -v -z -w 2 10.101.220.21 80\nConnection to 10.101.220.21 80 port [tcp/http] succeeded!\n"
Jan 12 15:32:09.489: INFO: stdout: ""
Jan 12 15:32:09.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 30361'
Jan 12 15:32:09.602: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 30361\nConnection to 10.0.42.160 30361 port [tcp/*] succeeded!\n"
Jan 12 15:32:09.602: INFO: stdout: ""
Jan 12 15:32:09.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 30361'
Jan 12 15:32:09.713: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 30361\nConnection to 10.0.40.50 30361 port [tcp/*] succeeded!\n"
Jan 12 15:32:09.713: INFO: stdout: ""
Jan 12 15:32:09.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:30361/ ; done'
Jan 12 15:32:09.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n"
Jan 12 15:32:09.894: INFO: stdout: "\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp"
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
Jan 12 15:32:09.894: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9973, will wait for the garbage collector to delete the pods 01/12/23 15:32:09.904
Jan 12 15:32:09.959: INFO: Deleting ReplicationController affinity-nodeport took: 3.166531ms
Jan 12 15:32:10.060: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.418314ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:32:13.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9973" for this suite. 01/12/23 15:32:13.279
------------------------------
 [SLOW TEST] [13.142 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:32:00.142
    Jan 12 15:32:00.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:32:00.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:00.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:00.155
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9973 01/12/23 15:32:00.158
    STEP: creating service affinity-nodeport in namespace services-9973 01/12/23 15:32:00.158
    STEP: creating replication controller affinity-nodeport in namespace services-9973 01/12/23 15:32:00.171
    I0112 15:32:00.177502      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9973, replica count: 3
    I0112 15:32:03.229662      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0112 15:32:06.230469      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 15:32:06.236: INFO: Creating new exec pod
    Jan 12 15:32:06.240: INFO: Waiting up to 5m0s for pod "execpod-affinity7l74v" in namespace "services-9973" to be "running"
    Jan 12 15:32:06.248: INFO: Pod "execpod-affinity7l74v": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981234ms
    Jan 12 15:32:08.252: INFO: Pod "execpod-affinity7l74v": Phase="Running", Reason="", readiness=true. Elapsed: 2.011640943s
    Jan 12 15:32:08.252: INFO: Pod "execpod-affinity7l74v" satisfied condition "running"
    Jan 12 15:32:09.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 12 15:32:09.375: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 12 15:32:09.375: INFO: stdout: ""
    Jan 12 15:32:09.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.101.220.21 80'
    Jan 12 15:32:09.489: INFO: stderr: "+ nc -v -z -w 2 10.101.220.21 80\nConnection to 10.101.220.21 80 port [tcp/http] succeeded!\n"
    Jan 12 15:32:09.489: INFO: stdout: ""
    Jan 12 15:32:09.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 30361'
    Jan 12 15:32:09.602: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 30361\nConnection to 10.0.42.160 30361 port [tcp/*] succeeded!\n"
    Jan 12 15:32:09.602: INFO: stdout: ""
    Jan 12 15:32:09.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 30361'
    Jan 12 15:32:09.713: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 30361\nConnection to 10.0.40.50 30361 port [tcp/*] succeeded!\n"
    Jan 12 15:32:09.713: INFO: stdout: ""
    Jan 12 15:32:09.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9973 exec execpod-affinity7l74v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:30361/ ; done'
    Jan 12 15:32:09.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:30361/\n"
    Jan 12 15:32:09.894: INFO: stdout: "\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp\naffinity-nodeport-tnjvp"
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Received response from host: affinity-nodeport-tnjvp
    Jan 12 15:32:09.894: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9973, will wait for the garbage collector to delete the pods 01/12/23 15:32:09.904
    Jan 12 15:32:09.959: INFO: Deleting ReplicationController affinity-nodeport took: 3.166531ms
    Jan 12 15:32:10.060: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.418314ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:32:13.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9973" for this suite. 01/12/23 15:32:13.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:32:13.286
Jan 12 15:32:13.286: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename proxy 01/12/23 15:32:13.286
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:13.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:13.297
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 12 15:32:13.299: INFO: Creating pod...
Jan 12 15:32:13.305: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4384" to be "running"
Jan 12 15:32:13.307: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72731ms
Jan 12 15:32:15.310: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004554089s
Jan 12 15:32:15.310: INFO: Pod "agnhost" satisfied condition "running"
Jan 12 15:32:15.310: INFO: Creating service...
Jan 12 15:32:15.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/DELETE
Jan 12 15:32:15.330: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 15:32:15.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/GET
Jan 12 15:32:15.332: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 12 15:32:15.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/HEAD
Jan 12 15:32:15.334: INFO: http.Client request:HEAD | StatusCode:200
Jan 12 15:32:15.335: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 12 15:32:15.337: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 15:32:15.337: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/PATCH
Jan 12 15:32:15.339: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 15:32:15.339: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/POST
Jan 12 15:32:15.342: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 15:32:15.342: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/PUT
Jan 12 15:32:15.344: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 15:32:15.344: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/DELETE
Jan 12 15:32:15.347: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 15:32:15.347: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/GET
Jan 12 15:32:15.350: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 12 15:32:15.350: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/HEAD
Jan 12 15:32:15.353: INFO: http.Client request:HEAD | StatusCode:200
Jan 12 15:32:15.353: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/OPTIONS
Jan 12 15:32:15.356: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 15:32:15.356: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/PATCH
Jan 12 15:32:15.359: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 15:32:15.359: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/POST
Jan 12 15:32:15.362: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 15:32:15.362: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/PUT
Jan 12 15:32:15.365: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 15:32:15.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4384" for this suite. 01/12/23 15:32:15.367
------------------------------
 [2.084 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:32:13.286
    Jan 12 15:32:13.286: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename proxy 01/12/23 15:32:13.286
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:13.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:13.297
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 12 15:32:13.299: INFO: Creating pod...
    Jan 12 15:32:13.305: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4384" to be "running"
    Jan 12 15:32:13.307: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.72731ms
    Jan 12 15:32:15.310: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004554089s
    Jan 12 15:32:15.310: INFO: Pod "agnhost" satisfied condition "running"
    Jan 12 15:32:15.310: INFO: Creating service...
    Jan 12 15:32:15.321: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/DELETE
    Jan 12 15:32:15.330: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 15:32:15.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/GET
    Jan 12 15:32:15.332: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 12 15:32:15.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/HEAD
    Jan 12 15:32:15.334: INFO: http.Client request:HEAD | StatusCode:200
    Jan 12 15:32:15.335: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 12 15:32:15.337: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 15:32:15.337: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/PATCH
    Jan 12 15:32:15.339: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 15:32:15.339: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/POST
    Jan 12 15:32:15.342: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 15:32:15.342: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/pods/agnhost/proxy/some/path/with/PUT
    Jan 12 15:32:15.344: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 15:32:15.344: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/DELETE
    Jan 12 15:32:15.347: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 15:32:15.347: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/GET
    Jan 12 15:32:15.350: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 12 15:32:15.350: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/HEAD
    Jan 12 15:32:15.353: INFO: http.Client request:HEAD | StatusCode:200
    Jan 12 15:32:15.353: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/OPTIONS
    Jan 12 15:32:15.356: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 15:32:15.356: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/PATCH
    Jan 12 15:32:15.359: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 15:32:15.359: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/POST
    Jan 12 15:32:15.362: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 15:32:15.362: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4384/services/test-service/proxy/some/path/with/PUT
    Jan 12 15:32:15.365: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:32:15.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4384" for this suite. 01/12/23 15:32:15.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:32:15.371
Jan 12 15:32:15.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 15:32:15.372
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:15.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:15.381
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 15:32:15.386
Jan 12 15:32:15.390: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1166" to be "running and ready"
Jan 12 15:32:15.400: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466045ms
Jan 12 15:32:15.400: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:32:17.404: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253005s
Jan 12 15:32:17.404: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 15:32:17.404: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/12/23 15:32:17.406
Jan 12 15:32:17.410: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1166" to be "running and ready"
Jan 12 15:32:17.412: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183058ms
Jan 12 15:32:17.412: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:32:19.414: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004922731s
Jan 12 15:32:19.415: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 12 15:32:19.415: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/12/23 15:32:19.416
Jan 12 15:32:19.422: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 12 15:32:19.424: INFO: Pod pod-with-prestop-http-hook still exists
Jan 12 15:32:21.424: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 12 15:32:21.426: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/12/23 15:32:21.426
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 15:32:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1166" for this suite. 01/12/23 15:32:21.44
------------------------------
 [SLOW TEST] [6.074 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:32:15.371
    Jan 12 15:32:15.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 15:32:15.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:15.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:15.381
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 15:32:15.386
    Jan 12 15:32:15.390: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1166" to be "running and ready"
    Jan 12 15:32:15.400: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 9.466045ms
    Jan 12 15:32:15.400: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:32:17.404: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013253005s
    Jan 12 15:32:17.404: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 15:32:17.404: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/12/23 15:32:17.406
    Jan 12 15:32:17.410: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1166" to be "running and ready"
    Jan 12 15:32:17.412: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183058ms
    Jan 12 15:32:17.412: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:32:19.414: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004922731s
    Jan 12 15:32:19.415: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 12 15:32:19.415: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/12/23 15:32:19.416
    Jan 12 15:32:19.422: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 12 15:32:19.424: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 12 15:32:21.424: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 12 15:32:21.426: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/12/23 15:32:21.426
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:32:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1166" for this suite. 01/12/23 15:32:21.44
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:32:21.446
Jan 12 15:32:21.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 15:32:21.447
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:21.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:21.46
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/12/23 15:32:21.462
STEP: waiting for pod running 01/12/23 15:32:21.467
Jan 12 15:32:21.467: INFO: Waiting up to 2m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578" to be "running"
Jan 12 15:32:21.469: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677057ms
Jan 12 15:32:23.472: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004630346s
Jan 12 15:32:23.472: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" satisfied condition "running"
STEP: creating a file in subpath 01/12/23 15:32:23.472
Jan 12 15:32:23.474: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3578 PodName:var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:32:23.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:32:23.475: INFO: ExecWithOptions: Clientset creation
Jan 12 15:32:23.475: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3578/pods/var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/12/23 15:32:23.532
Jan 12 15:32:23.534: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3578 PodName:var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:32:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:32:23.534: INFO: ExecWithOptions: Clientset creation
Jan 12 15:32:23.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3578/pods/var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/12/23 15:32:23.573
Jan 12 15:32:24.082: INFO: Successfully updated pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5"
STEP: waiting for annotated pod running 01/12/23 15:32:24.082
Jan 12 15:32:24.082: INFO: Waiting up to 2m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578" to be "running"
Jan 12 15:32:24.084: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Running", Reason="", readiness=true. Elapsed: 1.776071ms
Jan 12 15:32:24.084: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 15:32:24.084
Jan 12 15:32:24.084: INFO: Deleting pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578"
Jan 12 15:32:24.091: INFO: Wait up to 5m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 15:32:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3578" for this suite. 01/12/23 15:32:58.097
------------------------------
 [SLOW TEST] [36.655 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:32:21.446
    Jan 12 15:32:21.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 15:32:21.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:21.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:21.46
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/12/23 15:32:21.462
    STEP: waiting for pod running 01/12/23 15:32:21.467
    Jan 12 15:32:21.467: INFO: Waiting up to 2m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578" to be "running"
    Jan 12 15:32:21.469: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677057ms
    Jan 12 15:32:23.472: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004630346s
    Jan 12 15:32:23.472: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" satisfied condition "running"
    STEP: creating a file in subpath 01/12/23 15:32:23.472
    Jan 12 15:32:23.474: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3578 PodName:var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:32:23.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:32:23.475: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:32:23.475: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3578/pods/var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/12/23 15:32:23.532
    Jan 12 15:32:23.534: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3578 PodName:var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:32:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:32:23.534: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:32:23.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3578/pods/var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/12/23 15:32:23.573
    Jan 12 15:32:24.082: INFO: Successfully updated pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5"
    STEP: waiting for annotated pod running 01/12/23 15:32:24.082
    Jan 12 15:32:24.082: INFO: Waiting up to 2m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578" to be "running"
    Jan 12 15:32:24.084: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5": Phase="Running", Reason="", readiness=true. Elapsed: 1.776071ms
    Jan 12 15:32:24.084: INFO: Pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 15:32:24.084
    Jan 12 15:32:24.084: INFO: Deleting pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" in namespace "var-expansion-3578"
    Jan 12 15:32:24.091: INFO: Wait up to 5m0s for pod "var-expansion-fbef129b-9b5d-4294-b8ac-9ff9cf6b7ed5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:32:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3578" for this suite. 01/12/23 15:32:58.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:32:58.102
Jan 12 15:32:58.102: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 15:32:58.103
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:58.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:58.114
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/12/23 15:32:58.116
STEP: Ensuring active pods == parallelism 01/12/23 15:32:58.121
STEP: Orphaning one of the Job's Pods 01/12/23 15:33:00.125
Jan 12 15:33:00.636: INFO: Successfully updated pod "adopt-release-cn8wz"
STEP: Checking that the Job readopts the Pod 01/12/23 15:33:00.636
Jan 12 15:33:00.636: INFO: Waiting up to 15m0s for pod "adopt-release-cn8wz" in namespace "job-8031" to be "adopted"
Jan 12 15:33:00.639: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.854963ms
Jan 12 15:33:02.641: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005243052s
Jan 12 15:33:02.642: INFO: Pod "adopt-release-cn8wz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/12/23 15:33:02.642
Jan 12 15:33:03.153: INFO: Successfully updated pod "adopt-release-cn8wz"
STEP: Checking that the Job releases the Pod 01/12/23 15:33:03.153
Jan 12 15:33:03.153: INFO: Waiting up to 15m0s for pod "adopt-release-cn8wz" in namespace "job-8031" to be "released"
Jan 12 15:33:03.154: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 1.640376ms
Jan 12 15:33:05.157: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004600797s
Jan 12 15:33:05.157: INFO: Pod "adopt-release-cn8wz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:05.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8031" for this suite. 01/12/23 15:33:05.16
------------------------------
 [SLOW TEST] [7.061 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:32:58.102
    Jan 12 15:32:58.102: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 15:32:58.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:32:58.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:32:58.114
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/12/23 15:32:58.116
    STEP: Ensuring active pods == parallelism 01/12/23 15:32:58.121
    STEP: Orphaning one of the Job's Pods 01/12/23 15:33:00.125
    Jan 12 15:33:00.636: INFO: Successfully updated pod "adopt-release-cn8wz"
    STEP: Checking that the Job readopts the Pod 01/12/23 15:33:00.636
    Jan 12 15:33:00.636: INFO: Waiting up to 15m0s for pod "adopt-release-cn8wz" in namespace "job-8031" to be "adopted"
    Jan 12 15:33:00.639: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.854963ms
    Jan 12 15:33:02.641: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.005243052s
    Jan 12 15:33:02.642: INFO: Pod "adopt-release-cn8wz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/12/23 15:33:02.642
    Jan 12 15:33:03.153: INFO: Successfully updated pod "adopt-release-cn8wz"
    STEP: Checking that the Job releases the Pod 01/12/23 15:33:03.153
    Jan 12 15:33:03.153: INFO: Waiting up to 15m0s for pod "adopt-release-cn8wz" in namespace "job-8031" to be "released"
    Jan 12 15:33:03.154: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 1.640376ms
    Jan 12 15:33:05.157: INFO: Pod "adopt-release-cn8wz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004600797s
    Jan 12 15:33:05.157: INFO: Pod "adopt-release-cn8wz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:05.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8031" for this suite. 01/12/23 15:33:05.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:05.164
Jan 12 15:33:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename events 01/12/23 15:33:05.165
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:05.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:05.177
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/12/23 15:33:05.179
STEP: listing events in all namespaces 01/12/23 15:33:05.182
STEP: listing events in test namespace 01/12/23 15:33:05.186
STEP: listing events with field selection filtering on source 01/12/23 15:33:05.188
STEP: listing events with field selection filtering on reportingController 01/12/23 15:33:05.189
STEP: getting the test event 01/12/23 15:33:05.191
STEP: patching the test event 01/12/23 15:33:05.192
STEP: getting the test event 01/12/23 15:33:05.199
STEP: updating the test event 01/12/23 15:33:05.2
STEP: getting the test event 01/12/23 15:33:05.205
STEP: deleting the test event 01/12/23 15:33:05.207
STEP: listing events in all namespaces 01/12/23 15:33:05.211
STEP: listing events in test namespace 01/12/23 15:33:05.215
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:05.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9158" for this suite. 01/12/23 15:33:05.221
------------------------------
 [0.061 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:05.164
    Jan 12 15:33:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename events 01/12/23 15:33:05.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:05.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:05.177
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/12/23 15:33:05.179
    STEP: listing events in all namespaces 01/12/23 15:33:05.182
    STEP: listing events in test namespace 01/12/23 15:33:05.186
    STEP: listing events with field selection filtering on source 01/12/23 15:33:05.188
    STEP: listing events with field selection filtering on reportingController 01/12/23 15:33:05.189
    STEP: getting the test event 01/12/23 15:33:05.191
    STEP: patching the test event 01/12/23 15:33:05.192
    STEP: getting the test event 01/12/23 15:33:05.199
    STEP: updating the test event 01/12/23 15:33:05.2
    STEP: getting the test event 01/12/23 15:33:05.205
    STEP: deleting the test event 01/12/23 15:33:05.207
    STEP: listing events in all namespaces 01/12/23 15:33:05.211
    STEP: listing events in test namespace 01/12/23 15:33:05.215
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:05.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9158" for this suite. 01/12/23 15:33:05.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:05.227
Jan 12 15:33:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 15:33:05.228
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:05.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:05.24
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-3e84f499-142c-485c-aeb2-8e6029ba1005 01/12/23 15:33:05.242
STEP: Creating a pod to test consume secrets 01/12/23 15:33:05.245
Jan 12 15:33:05.250: INFO: Waiting up to 5m0s for pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7" in namespace "secrets-1148" to be "Succeeded or Failed"
Jan 12 15:33:05.252: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029532ms
Jan 12 15:33:07.255: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005441014s
Jan 12 15:33:09.256: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005582789s
STEP: Saw pod success 01/12/23 15:33:09.256
Jan 12 15:33:09.256: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7" satisfied condition "Succeeded or Failed"
Jan 12 15:33:09.257: INFO: Trying to get logs from node worker-0 pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 container secret-env-test: <nil>
STEP: delete the pod 01/12/23 15:33:09.265
Jan 12 15:33:09.273: INFO: Waiting for pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 to disappear
Jan 12 15:33:09.274: INFO: Pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:09.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1148" for this suite. 01/12/23 15:33:09.277
------------------------------
 [4.053 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:05.227
    Jan 12 15:33:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 15:33:05.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:05.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:05.24
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-3e84f499-142c-485c-aeb2-8e6029ba1005 01/12/23 15:33:05.242
    STEP: Creating a pod to test consume secrets 01/12/23 15:33:05.245
    Jan 12 15:33:05.250: INFO: Waiting up to 5m0s for pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7" in namespace "secrets-1148" to be "Succeeded or Failed"
    Jan 12 15:33:05.252: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029532ms
    Jan 12 15:33:07.255: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005441014s
    Jan 12 15:33:09.256: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005582789s
    STEP: Saw pod success 01/12/23 15:33:09.256
    Jan 12 15:33:09.256: INFO: Pod "pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7" satisfied condition "Succeeded or Failed"
    Jan 12 15:33:09.257: INFO: Trying to get logs from node worker-0 pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 container secret-env-test: <nil>
    STEP: delete the pod 01/12/23 15:33:09.265
    Jan 12 15:33:09.273: INFO: Waiting for pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 to disappear
    Jan 12 15:33:09.274: INFO: Pod pod-secrets-263d09be-4b7a-41fb-89de-7dcebc8884a7 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:09.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1148" for this suite. 01/12/23 15:33:09.277
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:09.281
Jan 12 15:33:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:33:09.282
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:09.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:09.295
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:33:09.305
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:33:10.533
STEP: Deploying the webhook pod 01/12/23 15:33:10.538
STEP: Wait for the deployment to be ready 01/12/23 15:33:10.547
Jan 12 15:33:10.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:33:12.561
STEP: Verifying the service has paired with the endpoint 01/12/23 15:33:12.577
Jan 12 15:33:13.578: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/12/23 15:33:13.581
STEP: create a namespace for the webhook 01/12/23 15:33:13.596
STEP: create a configmap should be unconditionally rejected by the webhook 01/12/23 15:33:13.601
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5848" for this suite. 01/12/23 15:33:13.652
STEP: Destroying namespace "webhook-5848-markers" for this suite. 01/12/23 15:33:13.658
------------------------------
 [4.383 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:09.281
    Jan 12 15:33:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:33:09.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:09.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:09.295
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:33:09.305
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:33:10.533
    STEP: Deploying the webhook pod 01/12/23 15:33:10.538
    STEP: Wait for the deployment to be ready 01/12/23 15:33:10.547
    Jan 12 15:33:10.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:33:12.561
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:33:12.577
    Jan 12 15:33:13.578: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/12/23 15:33:13.581
    STEP: create a namespace for the webhook 01/12/23 15:33:13.596
    STEP: create a configmap should be unconditionally rejected by the webhook 01/12/23 15:33:13.601
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5848" for this suite. 01/12/23 15:33:13.652
    STEP: Destroying namespace "webhook-5848-markers" for this suite. 01/12/23 15:33:13.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:13.666
Jan 12 15:33:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename ingress 01/12/23 15:33:13.667
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:13.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:13.677
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/12/23 15:33:13.68
STEP: getting /apis/networking.k8s.io 01/12/23 15:33:13.682
STEP: getting /apis/networking.k8s.iov1 01/12/23 15:33:13.683
STEP: creating 01/12/23 15:33:13.684
STEP: getting 01/12/23 15:33:13.693
STEP: listing 01/12/23 15:33:13.697
STEP: watching 01/12/23 15:33:13.699
Jan 12 15:33:13.699: INFO: starting watch
STEP: cluster-wide listing 01/12/23 15:33:13.7
STEP: cluster-wide watching 01/12/23 15:33:13.701
Jan 12 15:33:13.701: INFO: starting watch
STEP: patching 01/12/23 15:33:13.702
STEP: updating 01/12/23 15:33:13.706
Jan 12 15:33:13.712: INFO: waiting for watch events with expected annotations
Jan 12 15:33:13.712: INFO: saw patched and updated annotations
STEP: patching /status 01/12/23 15:33:13.712
STEP: updating /status 01/12/23 15:33:13.715
STEP: get /status 01/12/23 15:33:13.72
STEP: deleting 01/12/23 15:33:13.722
STEP: deleting a collection 01/12/23 15:33:13.729
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:13.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8928" for this suite. 01/12/23 15:33:13.74
------------------------------
 [0.078 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:13.666
    Jan 12 15:33:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename ingress 01/12/23 15:33:13.667
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:13.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:13.677
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/12/23 15:33:13.68
    STEP: getting /apis/networking.k8s.io 01/12/23 15:33:13.682
    STEP: getting /apis/networking.k8s.iov1 01/12/23 15:33:13.683
    STEP: creating 01/12/23 15:33:13.684
    STEP: getting 01/12/23 15:33:13.693
    STEP: listing 01/12/23 15:33:13.697
    STEP: watching 01/12/23 15:33:13.699
    Jan 12 15:33:13.699: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 15:33:13.7
    STEP: cluster-wide watching 01/12/23 15:33:13.701
    Jan 12 15:33:13.701: INFO: starting watch
    STEP: patching 01/12/23 15:33:13.702
    STEP: updating 01/12/23 15:33:13.706
    Jan 12 15:33:13.712: INFO: waiting for watch events with expected annotations
    Jan 12 15:33:13.712: INFO: saw patched and updated annotations
    STEP: patching /status 01/12/23 15:33:13.712
    STEP: updating /status 01/12/23 15:33:13.715
    STEP: get /status 01/12/23 15:33:13.72
    STEP: deleting 01/12/23 15:33:13.722
    STEP: deleting a collection 01/12/23 15:33:13.729
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:13.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8928" for this suite. 01/12/23 15:33:13.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:13.744
Jan 12 15:33:13.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 15:33:13.745
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:13.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:13.755
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/12/23 15:33:13.757
STEP: Ensuring job reaches completions 01/12/23 15:33:13.762
STEP: Ensuring pods with index for job exist 01/12/23 15:33:21.764
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:21.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3547" for this suite. 01/12/23 15:33:21.768
------------------------------
 [SLOW TEST] [8.028 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:13.744
    Jan 12 15:33:13.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 15:33:13.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:13.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:13.755
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/12/23 15:33:13.757
    STEP: Ensuring job reaches completions 01/12/23 15:33:13.762
    STEP: Ensuring pods with index for job exist 01/12/23 15:33:21.764
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:21.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3547" for this suite. 01/12/23 15:33:21.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:21.775
Jan 12 15:33:21.775: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 15:33:21.776
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:21.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:21.787
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/12/23 15:33:21.789
Jan 12 15:33:21.793: INFO: Waiting up to 5m0s for pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942" in namespace "var-expansion-59" to be "Succeeded or Failed"
Jan 12 15:33:21.795: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833767ms
Jan 12 15:33:23.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Running", Reason="", readiness=false. Elapsed: 2.004507274s
Jan 12 15:33:25.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004373738s
STEP: Saw pod success 01/12/23 15:33:25.798
Jan 12 15:33:25.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942" satisfied condition "Succeeded or Failed"
Jan 12 15:33:25.800: INFO: Trying to get logs from node worker-1 pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 container dapi-container: <nil>
STEP: delete the pod 01/12/23 15:33:25.804
Jan 12 15:33:25.816: INFO: Waiting for pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 to disappear
Jan 12 15:33:25.818: INFO: Pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:25.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-59" for this suite. 01/12/23 15:33:25.82
------------------------------
 [4.050 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:21.775
    Jan 12 15:33:21.775: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 15:33:21.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:21.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:21.787
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/12/23 15:33:21.789
    Jan 12 15:33:21.793: INFO: Waiting up to 5m0s for pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942" in namespace "var-expansion-59" to be "Succeeded or Failed"
    Jan 12 15:33:21.795: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Pending", Reason="", readiness=false. Elapsed: 1.833767ms
    Jan 12 15:33:23.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Running", Reason="", readiness=false. Elapsed: 2.004507274s
    Jan 12 15:33:25.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004373738s
    STEP: Saw pod success 01/12/23 15:33:25.798
    Jan 12 15:33:25.798: INFO: Pod "var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942" satisfied condition "Succeeded or Failed"
    Jan 12 15:33:25.800: INFO: Trying to get logs from node worker-1 pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 15:33:25.804
    Jan 12 15:33:25.816: INFO: Waiting for pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 to disappear
    Jan 12 15:33:25.818: INFO: Pod var-expansion-1c31dce5-bb18-421a-87fc-db2ffde4c942 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:25.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-59" for this suite. 01/12/23 15:33:25.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:25.827
Jan 12 15:33:25.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:33:25.828
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:25.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:25.838
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-50242867-5ea5-45c4-84b1-53440e0c616f 01/12/23 15:33:25.84
STEP: Creating a pod to test consume configMaps 01/12/23 15:33:25.845
Jan 12 15:33:25.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16" in namespace "configmap-6797" to be "Succeeded or Failed"
Jan 12 15:33:25.851: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.534002ms
Jan 12 15:33:27.854: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004566406s
Jan 12 15:33:29.855: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005167388s
STEP: Saw pod success 01/12/23 15:33:29.855
Jan 12 15:33:29.855: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16" satisfied condition "Succeeded or Failed"
Jan 12 15:33:29.857: INFO: Trying to get logs from node worker-0 pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 container configmap-volume-test: <nil>
STEP: delete the pod 01/12/23 15:33:29.861
Jan 12 15:33:29.868: INFO: Waiting for pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 to disappear
Jan 12 15:33:29.869: INFO: Pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:33:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6797" for this suite. 01/12/23 15:33:29.871
------------------------------
 [4.048 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:25.827
    Jan 12 15:33:25.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:33:25.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:25.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:25.838
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-50242867-5ea5-45c4-84b1-53440e0c616f 01/12/23 15:33:25.84
    STEP: Creating a pod to test consume configMaps 01/12/23 15:33:25.845
    Jan 12 15:33:25.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16" in namespace "configmap-6797" to be "Succeeded or Failed"
    Jan 12 15:33:25.851: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.534002ms
    Jan 12 15:33:27.854: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004566406s
    Jan 12 15:33:29.855: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005167388s
    STEP: Saw pod success 01/12/23 15:33:29.855
    Jan 12 15:33:29.855: INFO: Pod "pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16" satisfied condition "Succeeded or Failed"
    Jan 12 15:33:29.857: INFO: Trying to get logs from node worker-0 pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 container configmap-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:33:29.861
    Jan 12 15:33:29.868: INFO: Waiting for pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 to disappear
    Jan 12 15:33:29.869: INFO: Pod pod-configmaps-5e56912a-3b1f-42ed-aec3-718e0049dd16 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:33:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6797" for this suite. 01/12/23 15:33:29.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:33:29.876
Jan 12 15:33:29.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 15:33:29.877
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:29.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:29.889
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/12/23 15:33:29.893
STEP: delete the rc 01/12/23 15:33:34.899
STEP: wait for the rc to be deleted 01/12/23 15:33:34.903
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/12/23 15:33:39.906
STEP: Gathering metrics 01/12/23 15:34:09.916
W0112 15:34:09.919705      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 15:34:09.919: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 12 15:34:09.919: INFO: Deleting pod "simpletest.rc-25z2c" in namespace "gc-1991"
Jan 12 15:34:09.928: INFO: Deleting pod "simpletest.rc-2g5qw" in namespace "gc-1991"
Jan 12 15:34:09.937: INFO: Deleting pod "simpletest.rc-2lgjx" in namespace "gc-1991"
Jan 12 15:34:09.946: INFO: Deleting pod "simpletest.rc-2w887" in namespace "gc-1991"
Jan 12 15:34:09.952: INFO: Deleting pod "simpletest.rc-42p68" in namespace "gc-1991"
Jan 12 15:34:09.959: INFO: Deleting pod "simpletest.rc-49p95" in namespace "gc-1991"
Jan 12 15:34:09.964: INFO: Deleting pod "simpletest.rc-49wjf" in namespace "gc-1991"
Jan 12 15:34:09.974: INFO: Deleting pod "simpletest.rc-4d6xs" in namespace "gc-1991"
Jan 12 15:34:09.982: INFO: Deleting pod "simpletest.rc-4fbhp" in namespace "gc-1991"
Jan 12 15:34:09.989: INFO: Deleting pod "simpletest.rc-4lbgg" in namespace "gc-1991"
Jan 12 15:34:09.996: INFO: Deleting pod "simpletest.rc-4z6bf" in namespace "gc-1991"
Jan 12 15:34:10.010: INFO: Deleting pod "simpletest.rc-545tr" in namespace "gc-1991"
Jan 12 15:34:10.018: INFO: Deleting pod "simpletest.rc-56q2k" in namespace "gc-1991"
Jan 12 15:34:10.025: INFO: Deleting pod "simpletest.rc-5cnvr" in namespace "gc-1991"
Jan 12 15:34:10.033: INFO: Deleting pod "simpletest.rc-6dx5q" in namespace "gc-1991"
Jan 12 15:34:10.046: INFO: Deleting pod "simpletest.rc-6nk8r" in namespace "gc-1991"
Jan 12 15:34:10.052: INFO: Deleting pod "simpletest.rc-6sb9r" in namespace "gc-1991"
Jan 12 15:34:10.059: INFO: Deleting pod "simpletest.rc-6xkfg" in namespace "gc-1991"
Jan 12 15:34:10.070: INFO: Deleting pod "simpletest.rc-75xbl" in namespace "gc-1991"
Jan 12 15:34:10.076: INFO: Deleting pod "simpletest.rc-7qgj4" in namespace "gc-1991"
Jan 12 15:34:10.085: INFO: Deleting pod "simpletest.rc-8hwzc" in namespace "gc-1991"
Jan 12 15:34:10.101: INFO: Deleting pod "simpletest.rc-8kkkj" in namespace "gc-1991"
Jan 12 15:34:10.109: INFO: Deleting pod "simpletest.rc-92n9b" in namespace "gc-1991"
Jan 12 15:34:10.118: INFO: Deleting pod "simpletest.rc-95zf8" in namespace "gc-1991"
Jan 12 15:34:10.126: INFO: Deleting pod "simpletest.rc-97tjv" in namespace "gc-1991"
Jan 12 15:34:10.138: INFO: Deleting pod "simpletest.rc-9kh2j" in namespace "gc-1991"
Jan 12 15:34:10.146: INFO: Deleting pod "simpletest.rc-9nqbq" in namespace "gc-1991"
Jan 12 15:34:10.154: INFO: Deleting pod "simpletest.rc-b2j46" in namespace "gc-1991"
Jan 12 15:34:10.162: INFO: Deleting pod "simpletest.rc-bbmnz" in namespace "gc-1991"
Jan 12 15:34:10.169: INFO: Deleting pod "simpletest.rc-bdl87" in namespace "gc-1991"
Jan 12 15:34:10.187: INFO: Deleting pod "simpletest.rc-bfgwm" in namespace "gc-1991"
Jan 12 15:34:10.215: INFO: Deleting pod "simpletest.rc-bkdnb" in namespace "gc-1991"
Jan 12 15:34:10.223: INFO: Deleting pod "simpletest.rc-c5rp9" in namespace "gc-1991"
Jan 12 15:34:10.231: INFO: Deleting pod "simpletest.rc-cfgv2" in namespace "gc-1991"
Jan 12 15:34:10.236: INFO: Deleting pod "simpletest.rc-cjgfj" in namespace "gc-1991"
Jan 12 15:34:10.246: INFO: Deleting pod "simpletest.rc-cr9wc" in namespace "gc-1991"
Jan 12 15:34:10.256: INFO: Deleting pod "simpletest.rc-dcrth" in namespace "gc-1991"
Jan 12 15:34:10.261: INFO: Deleting pod "simpletest.rc-djnzp" in namespace "gc-1991"
Jan 12 15:34:10.278: INFO: Deleting pod "simpletest.rc-f6mbn" in namespace "gc-1991"
Jan 12 15:34:10.292: INFO: Deleting pod "simpletest.rc-ftx5n" in namespace "gc-1991"
Jan 12 15:34:10.298: INFO: Deleting pod "simpletest.rc-gn5bf" in namespace "gc-1991"
Jan 12 15:34:10.307: INFO: Deleting pod "simpletest.rc-h99jx" in namespace "gc-1991"
Jan 12 15:34:10.314: INFO: Deleting pod "simpletest.rc-hpwqq" in namespace "gc-1991"
Jan 12 15:34:10.320: INFO: Deleting pod "simpletest.rc-htxvb" in namespace "gc-1991"
Jan 12 15:34:10.327: INFO: Deleting pod "simpletest.rc-hxrh8" in namespace "gc-1991"
Jan 12 15:34:10.336: INFO: Deleting pod "simpletest.rc-j8kgf" in namespace "gc-1991"
Jan 12 15:34:10.345: INFO: Deleting pod "simpletest.rc-jh5dr" in namespace "gc-1991"
Jan 12 15:34:10.356: INFO: Deleting pod "simpletest.rc-kzd2g" in namespace "gc-1991"
Jan 12 15:34:10.368: INFO: Deleting pod "simpletest.rc-lqpdd" in namespace "gc-1991"
Jan 12 15:34:10.381: INFO: Deleting pod "simpletest.rc-m5drs" in namespace "gc-1991"
Jan 12 15:34:10.394: INFO: Deleting pod "simpletest.rc-m785x" in namespace "gc-1991"
Jan 12 15:34:10.399: INFO: Deleting pod "simpletest.rc-m8sbx" in namespace "gc-1991"
Jan 12 15:34:10.407: INFO: Deleting pod "simpletest.rc-mbttg" in namespace "gc-1991"
Jan 12 15:34:10.416: INFO: Deleting pod "simpletest.rc-mdr66" in namespace "gc-1991"
Jan 12 15:34:10.423: INFO: Deleting pod "simpletest.rc-mh4qk" in namespace "gc-1991"
Jan 12 15:34:10.429: INFO: Deleting pod "simpletest.rc-mzv56" in namespace "gc-1991"
Jan 12 15:34:10.438: INFO: Deleting pod "simpletest.rc-n85qs" in namespace "gc-1991"
Jan 12 15:34:10.445: INFO: Deleting pod "simpletest.rc-nltrq" in namespace "gc-1991"
Jan 12 15:34:10.453: INFO: Deleting pod "simpletest.rc-nlvfq" in namespace "gc-1991"
Jan 12 15:34:10.468: INFO: Deleting pod "simpletest.rc-npk6t" in namespace "gc-1991"
Jan 12 15:34:10.516: INFO: Deleting pod "simpletest.rc-nqs5w" in namespace "gc-1991"
Jan 12 15:34:10.568: INFO: Deleting pod "simpletest.rc-ns6ks" in namespace "gc-1991"
Jan 12 15:34:10.616: INFO: Deleting pod "simpletest.rc-p7mkb" in namespace "gc-1991"
Jan 12 15:34:10.664: INFO: Deleting pod "simpletest.rc-pfkrp" in namespace "gc-1991"
Jan 12 15:34:10.719: INFO: Deleting pod "simpletest.rc-pv42n" in namespace "gc-1991"
Jan 12 15:34:10.768: INFO: Deleting pod "simpletest.rc-pzf74" in namespace "gc-1991"
Jan 12 15:34:10.816: INFO: Deleting pod "simpletest.rc-q22dh" in namespace "gc-1991"
Jan 12 15:34:10.867: INFO: Deleting pod "simpletest.rc-q2fs2" in namespace "gc-1991"
Jan 12 15:34:10.915: INFO: Deleting pod "simpletest.rc-qf4dz" in namespace "gc-1991"
Jan 12 15:34:10.973: INFO: Deleting pod "simpletest.rc-r2b5r" in namespace "gc-1991"
Jan 12 15:34:11.015: INFO: Deleting pod "simpletest.rc-sgz4h" in namespace "gc-1991"
Jan 12 15:34:11.063: INFO: Deleting pod "simpletest.rc-stgkd" in namespace "gc-1991"
Jan 12 15:34:11.115: INFO: Deleting pod "simpletest.rc-swdfl" in namespace "gc-1991"
Jan 12 15:34:11.168: INFO: Deleting pod "simpletest.rc-swqpc" in namespace "gc-1991"
Jan 12 15:34:11.220: INFO: Deleting pod "simpletest.rc-tkx95" in namespace "gc-1991"
Jan 12 15:34:11.268: INFO: Deleting pod "simpletest.rc-ttnww" in namespace "gc-1991"
Jan 12 15:34:11.316: INFO: Deleting pod "simpletest.rc-tv96d" in namespace "gc-1991"
Jan 12 15:34:11.378: INFO: Deleting pod "simpletest.rc-tvnrr" in namespace "gc-1991"
Jan 12 15:34:11.418: INFO: Deleting pod "simpletest.rc-vb7rb" in namespace "gc-1991"
Jan 12 15:34:11.466: INFO: Deleting pod "simpletest.rc-vf4kd" in namespace "gc-1991"
Jan 12 15:34:11.518: INFO: Deleting pod "simpletest.rc-vgkt8" in namespace "gc-1991"
Jan 12 15:34:11.567: INFO: Deleting pod "simpletest.rc-vlqql" in namespace "gc-1991"
Jan 12 15:34:11.618: INFO: Deleting pod "simpletest.rc-vlsv7" in namespace "gc-1991"
Jan 12 15:34:11.664: INFO: Deleting pod "simpletest.rc-vq6x5" in namespace "gc-1991"
Jan 12 15:34:11.722: INFO: Deleting pod "simpletest.rc-vtfqp" in namespace "gc-1991"
Jan 12 15:34:11.765: INFO: Deleting pod "simpletest.rc-wbq76" in namespace "gc-1991"
Jan 12 15:34:11.817: INFO: Deleting pod "simpletest.rc-whn6j" in namespace "gc-1991"
Jan 12 15:34:11.866: INFO: Deleting pod "simpletest.rc-x82w6" in namespace "gc-1991"
Jan 12 15:34:11.916: INFO: Deleting pod "simpletest.rc-xfnh2" in namespace "gc-1991"
Jan 12 15:34:11.966: INFO: Deleting pod "simpletest.rc-xjmtv" in namespace "gc-1991"
Jan 12 15:34:12.013: INFO: Deleting pod "simpletest.rc-xqfm8" in namespace "gc-1991"
Jan 12 15:34:12.065: INFO: Deleting pod "simpletest.rc-xs6n7" in namespace "gc-1991"
Jan 12 15:34:12.122: INFO: Deleting pod "simpletest.rc-xshqn" in namespace "gc-1991"
Jan 12 15:34:12.169: INFO: Deleting pod "simpletest.rc-xsj9k" in namespace "gc-1991"
Jan 12 15:34:12.216: INFO: Deleting pod "simpletest.rc-xvck5" in namespace "gc-1991"
Jan 12 15:34:12.267: INFO: Deleting pod "simpletest.rc-z7nk7" in namespace "gc-1991"
Jan 12 15:34:12.319: INFO: Deleting pod "simpletest.rc-zf8qd" in namespace "gc-1991"
Jan 12 15:34:12.367: INFO: Deleting pod "simpletest.rc-zgjr9" in namespace "gc-1991"
Jan 12 15:34:12.416: INFO: Deleting pod "simpletest.rc-zh8qb" in namespace "gc-1991"
Jan 12 15:34:12.472: INFO: Deleting pod "simpletest.rc-zn96j" in namespace "gc-1991"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:12.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1991" for this suite. 01/12/23 15:34:12.562
------------------------------
 [SLOW TEST] [42.735 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:33:29.876
    Jan 12 15:33:29.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 15:33:29.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:33:29.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:33:29.889
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/12/23 15:33:29.893
    STEP: delete the rc 01/12/23 15:33:34.899
    STEP: wait for the rc to be deleted 01/12/23 15:33:34.903
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/12/23 15:33:39.906
    STEP: Gathering metrics 01/12/23 15:34:09.916
    W0112 15:34:09.919705      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 15:34:09.919: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 12 15:34:09.919: INFO: Deleting pod "simpletest.rc-25z2c" in namespace "gc-1991"
    Jan 12 15:34:09.928: INFO: Deleting pod "simpletest.rc-2g5qw" in namespace "gc-1991"
    Jan 12 15:34:09.937: INFO: Deleting pod "simpletest.rc-2lgjx" in namespace "gc-1991"
    Jan 12 15:34:09.946: INFO: Deleting pod "simpletest.rc-2w887" in namespace "gc-1991"
    Jan 12 15:34:09.952: INFO: Deleting pod "simpletest.rc-42p68" in namespace "gc-1991"
    Jan 12 15:34:09.959: INFO: Deleting pod "simpletest.rc-49p95" in namespace "gc-1991"
    Jan 12 15:34:09.964: INFO: Deleting pod "simpletest.rc-49wjf" in namespace "gc-1991"
    Jan 12 15:34:09.974: INFO: Deleting pod "simpletest.rc-4d6xs" in namespace "gc-1991"
    Jan 12 15:34:09.982: INFO: Deleting pod "simpletest.rc-4fbhp" in namespace "gc-1991"
    Jan 12 15:34:09.989: INFO: Deleting pod "simpletest.rc-4lbgg" in namespace "gc-1991"
    Jan 12 15:34:09.996: INFO: Deleting pod "simpletest.rc-4z6bf" in namespace "gc-1991"
    Jan 12 15:34:10.010: INFO: Deleting pod "simpletest.rc-545tr" in namespace "gc-1991"
    Jan 12 15:34:10.018: INFO: Deleting pod "simpletest.rc-56q2k" in namespace "gc-1991"
    Jan 12 15:34:10.025: INFO: Deleting pod "simpletest.rc-5cnvr" in namespace "gc-1991"
    Jan 12 15:34:10.033: INFO: Deleting pod "simpletest.rc-6dx5q" in namespace "gc-1991"
    Jan 12 15:34:10.046: INFO: Deleting pod "simpletest.rc-6nk8r" in namespace "gc-1991"
    Jan 12 15:34:10.052: INFO: Deleting pod "simpletest.rc-6sb9r" in namespace "gc-1991"
    Jan 12 15:34:10.059: INFO: Deleting pod "simpletest.rc-6xkfg" in namespace "gc-1991"
    Jan 12 15:34:10.070: INFO: Deleting pod "simpletest.rc-75xbl" in namespace "gc-1991"
    Jan 12 15:34:10.076: INFO: Deleting pod "simpletest.rc-7qgj4" in namespace "gc-1991"
    Jan 12 15:34:10.085: INFO: Deleting pod "simpletest.rc-8hwzc" in namespace "gc-1991"
    Jan 12 15:34:10.101: INFO: Deleting pod "simpletest.rc-8kkkj" in namespace "gc-1991"
    Jan 12 15:34:10.109: INFO: Deleting pod "simpletest.rc-92n9b" in namespace "gc-1991"
    Jan 12 15:34:10.118: INFO: Deleting pod "simpletest.rc-95zf8" in namespace "gc-1991"
    Jan 12 15:34:10.126: INFO: Deleting pod "simpletest.rc-97tjv" in namespace "gc-1991"
    Jan 12 15:34:10.138: INFO: Deleting pod "simpletest.rc-9kh2j" in namespace "gc-1991"
    Jan 12 15:34:10.146: INFO: Deleting pod "simpletest.rc-9nqbq" in namespace "gc-1991"
    Jan 12 15:34:10.154: INFO: Deleting pod "simpletest.rc-b2j46" in namespace "gc-1991"
    Jan 12 15:34:10.162: INFO: Deleting pod "simpletest.rc-bbmnz" in namespace "gc-1991"
    Jan 12 15:34:10.169: INFO: Deleting pod "simpletest.rc-bdl87" in namespace "gc-1991"
    Jan 12 15:34:10.187: INFO: Deleting pod "simpletest.rc-bfgwm" in namespace "gc-1991"
    Jan 12 15:34:10.215: INFO: Deleting pod "simpletest.rc-bkdnb" in namespace "gc-1991"
    Jan 12 15:34:10.223: INFO: Deleting pod "simpletest.rc-c5rp9" in namespace "gc-1991"
    Jan 12 15:34:10.231: INFO: Deleting pod "simpletest.rc-cfgv2" in namespace "gc-1991"
    Jan 12 15:34:10.236: INFO: Deleting pod "simpletest.rc-cjgfj" in namespace "gc-1991"
    Jan 12 15:34:10.246: INFO: Deleting pod "simpletest.rc-cr9wc" in namespace "gc-1991"
    Jan 12 15:34:10.256: INFO: Deleting pod "simpletest.rc-dcrth" in namespace "gc-1991"
    Jan 12 15:34:10.261: INFO: Deleting pod "simpletest.rc-djnzp" in namespace "gc-1991"
    Jan 12 15:34:10.278: INFO: Deleting pod "simpletest.rc-f6mbn" in namespace "gc-1991"
    Jan 12 15:34:10.292: INFO: Deleting pod "simpletest.rc-ftx5n" in namespace "gc-1991"
    Jan 12 15:34:10.298: INFO: Deleting pod "simpletest.rc-gn5bf" in namespace "gc-1991"
    Jan 12 15:34:10.307: INFO: Deleting pod "simpletest.rc-h99jx" in namespace "gc-1991"
    Jan 12 15:34:10.314: INFO: Deleting pod "simpletest.rc-hpwqq" in namespace "gc-1991"
    Jan 12 15:34:10.320: INFO: Deleting pod "simpletest.rc-htxvb" in namespace "gc-1991"
    Jan 12 15:34:10.327: INFO: Deleting pod "simpletest.rc-hxrh8" in namespace "gc-1991"
    Jan 12 15:34:10.336: INFO: Deleting pod "simpletest.rc-j8kgf" in namespace "gc-1991"
    Jan 12 15:34:10.345: INFO: Deleting pod "simpletest.rc-jh5dr" in namespace "gc-1991"
    Jan 12 15:34:10.356: INFO: Deleting pod "simpletest.rc-kzd2g" in namespace "gc-1991"
    Jan 12 15:34:10.368: INFO: Deleting pod "simpletest.rc-lqpdd" in namespace "gc-1991"
    Jan 12 15:34:10.381: INFO: Deleting pod "simpletest.rc-m5drs" in namespace "gc-1991"
    Jan 12 15:34:10.394: INFO: Deleting pod "simpletest.rc-m785x" in namespace "gc-1991"
    Jan 12 15:34:10.399: INFO: Deleting pod "simpletest.rc-m8sbx" in namespace "gc-1991"
    Jan 12 15:34:10.407: INFO: Deleting pod "simpletest.rc-mbttg" in namespace "gc-1991"
    Jan 12 15:34:10.416: INFO: Deleting pod "simpletest.rc-mdr66" in namespace "gc-1991"
    Jan 12 15:34:10.423: INFO: Deleting pod "simpletest.rc-mh4qk" in namespace "gc-1991"
    Jan 12 15:34:10.429: INFO: Deleting pod "simpletest.rc-mzv56" in namespace "gc-1991"
    Jan 12 15:34:10.438: INFO: Deleting pod "simpletest.rc-n85qs" in namespace "gc-1991"
    Jan 12 15:34:10.445: INFO: Deleting pod "simpletest.rc-nltrq" in namespace "gc-1991"
    Jan 12 15:34:10.453: INFO: Deleting pod "simpletest.rc-nlvfq" in namespace "gc-1991"
    Jan 12 15:34:10.468: INFO: Deleting pod "simpletest.rc-npk6t" in namespace "gc-1991"
    Jan 12 15:34:10.516: INFO: Deleting pod "simpletest.rc-nqs5w" in namespace "gc-1991"
    Jan 12 15:34:10.568: INFO: Deleting pod "simpletest.rc-ns6ks" in namespace "gc-1991"
    Jan 12 15:34:10.616: INFO: Deleting pod "simpletest.rc-p7mkb" in namespace "gc-1991"
    Jan 12 15:34:10.664: INFO: Deleting pod "simpletest.rc-pfkrp" in namespace "gc-1991"
    Jan 12 15:34:10.719: INFO: Deleting pod "simpletest.rc-pv42n" in namespace "gc-1991"
    Jan 12 15:34:10.768: INFO: Deleting pod "simpletest.rc-pzf74" in namespace "gc-1991"
    Jan 12 15:34:10.816: INFO: Deleting pod "simpletest.rc-q22dh" in namespace "gc-1991"
    Jan 12 15:34:10.867: INFO: Deleting pod "simpletest.rc-q2fs2" in namespace "gc-1991"
    Jan 12 15:34:10.915: INFO: Deleting pod "simpletest.rc-qf4dz" in namespace "gc-1991"
    Jan 12 15:34:10.973: INFO: Deleting pod "simpletest.rc-r2b5r" in namespace "gc-1991"
    Jan 12 15:34:11.015: INFO: Deleting pod "simpletest.rc-sgz4h" in namespace "gc-1991"
    Jan 12 15:34:11.063: INFO: Deleting pod "simpletest.rc-stgkd" in namespace "gc-1991"
    Jan 12 15:34:11.115: INFO: Deleting pod "simpletest.rc-swdfl" in namespace "gc-1991"
    Jan 12 15:34:11.168: INFO: Deleting pod "simpletest.rc-swqpc" in namespace "gc-1991"
    Jan 12 15:34:11.220: INFO: Deleting pod "simpletest.rc-tkx95" in namespace "gc-1991"
    Jan 12 15:34:11.268: INFO: Deleting pod "simpletest.rc-ttnww" in namespace "gc-1991"
    Jan 12 15:34:11.316: INFO: Deleting pod "simpletest.rc-tv96d" in namespace "gc-1991"
    Jan 12 15:34:11.378: INFO: Deleting pod "simpletest.rc-tvnrr" in namespace "gc-1991"
    Jan 12 15:34:11.418: INFO: Deleting pod "simpletest.rc-vb7rb" in namespace "gc-1991"
    Jan 12 15:34:11.466: INFO: Deleting pod "simpletest.rc-vf4kd" in namespace "gc-1991"
    Jan 12 15:34:11.518: INFO: Deleting pod "simpletest.rc-vgkt8" in namespace "gc-1991"
    Jan 12 15:34:11.567: INFO: Deleting pod "simpletest.rc-vlqql" in namespace "gc-1991"
    Jan 12 15:34:11.618: INFO: Deleting pod "simpletest.rc-vlsv7" in namespace "gc-1991"
    Jan 12 15:34:11.664: INFO: Deleting pod "simpletest.rc-vq6x5" in namespace "gc-1991"
    Jan 12 15:34:11.722: INFO: Deleting pod "simpletest.rc-vtfqp" in namespace "gc-1991"
    Jan 12 15:34:11.765: INFO: Deleting pod "simpletest.rc-wbq76" in namespace "gc-1991"
    Jan 12 15:34:11.817: INFO: Deleting pod "simpletest.rc-whn6j" in namespace "gc-1991"
    Jan 12 15:34:11.866: INFO: Deleting pod "simpletest.rc-x82w6" in namespace "gc-1991"
    Jan 12 15:34:11.916: INFO: Deleting pod "simpletest.rc-xfnh2" in namespace "gc-1991"
    Jan 12 15:34:11.966: INFO: Deleting pod "simpletest.rc-xjmtv" in namespace "gc-1991"
    Jan 12 15:34:12.013: INFO: Deleting pod "simpletest.rc-xqfm8" in namespace "gc-1991"
    Jan 12 15:34:12.065: INFO: Deleting pod "simpletest.rc-xs6n7" in namespace "gc-1991"
    Jan 12 15:34:12.122: INFO: Deleting pod "simpletest.rc-xshqn" in namespace "gc-1991"
    Jan 12 15:34:12.169: INFO: Deleting pod "simpletest.rc-xsj9k" in namespace "gc-1991"
    Jan 12 15:34:12.216: INFO: Deleting pod "simpletest.rc-xvck5" in namespace "gc-1991"
    Jan 12 15:34:12.267: INFO: Deleting pod "simpletest.rc-z7nk7" in namespace "gc-1991"
    Jan 12 15:34:12.319: INFO: Deleting pod "simpletest.rc-zf8qd" in namespace "gc-1991"
    Jan 12 15:34:12.367: INFO: Deleting pod "simpletest.rc-zgjr9" in namespace "gc-1991"
    Jan 12 15:34:12.416: INFO: Deleting pod "simpletest.rc-zh8qb" in namespace "gc-1991"
    Jan 12 15:34:12.472: INFO: Deleting pod "simpletest.rc-zn96j" in namespace "gc-1991"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:12.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1991" for this suite. 01/12/23 15:34:12.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:12.613
Jan 12 15:34:12.613: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 15:34:12.627
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:12.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:12.644
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 12 15:34:12.646: INFO: Creating deployment "test-recreate-deployment"
Jan 12 15:34:12.650: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 12 15:34:12.654: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 12 15:34:14.659: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 12 15:34:14.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:34:16.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:34:18.664: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 12 15:34:18.671: INFO: Updating deployment test-recreate-deployment
Jan 12 15:34:18.671: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 15:34:18.733: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9960  f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 5096 2 2023-01-12 15:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a8408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 15:34:18 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-12 15:34:18 +0000 UTC,LastTransitionTime:2023-01-12 15:34:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 12 15:34:18.735: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9960  5f431d53-41dd-471f-9d74-12318835e7f9 5093 1 2023-01-12 15:34:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 0xc0009a9670 0xc0009a9671}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a98d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:34:18.735: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 12 15:34:18.735: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9960  93a58bf0-3231-4b9b-bfe5-19620cd84e06 5079 2 2023-01-12 15:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 0xc0009a90e7 0xc0009a90e8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a93a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:34:18.738: INFO: Pod "test-recreate-deployment-cff6dc657-qpgbz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qpgbz test-recreate-deployment-cff6dc657- deployment-9960  32403981-e2e4-49f6-95c2-87aacfe62f33 5094 0 2023-01-12 15:34:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 5f431d53-41dd-471f-9d74-12318835e7f9 0xc0038de2a0 0xc0038de2a1}] [] [{kube-controller-manager Update v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f431d53-41dd-471f-9d74-12318835e7f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4tz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4tz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 15:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:18.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9960" for this suite. 01/12/23 15:34:18.74
------------------------------
 [SLOW TEST] [6.131 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:12.613
    Jan 12 15:34:12.613: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 15:34:12.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:12.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:12.644
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 12 15:34:12.646: INFO: Creating deployment "test-recreate-deployment"
    Jan 12 15:34:12.650: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 12 15:34:12.654: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 12 15:34:14.659: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 12 15:34:14.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:34:16.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 34, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:34:18.664: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 12 15:34:18.671: INFO: Updating deployment test-recreate-deployment
    Jan 12 15:34:18.671: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 15:34:18.733: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9960  f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 5096 2 2023-01-12 15:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a8408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 15:34:18 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-12 15:34:18 +0000 UTC,LastTransitionTime:2023-01-12 15:34:12 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 12 15:34:18.735: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9960  5f431d53-41dd-471f-9d74-12318835e7f9 5093 1 2023-01-12 15:34:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 0xc0009a9670 0xc0009a9671}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a98d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:34:18.735: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 12 15:34:18.735: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9960  93a58bf0-3231-4b9b-bfe5-19620cd84e06 5079 2 2023-01-12 15:34:12 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e 0xc0009a90e7 0xc0009a90e8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31e89c9-bf62-4604-b3fa-b48ce6f0eb6e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0009a93a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:34:18.738: INFO: Pod "test-recreate-deployment-cff6dc657-qpgbz" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qpgbz test-recreate-deployment-cff6dc657- deployment-9960  32403981-e2e4-49f6-95c2-87aacfe62f33 5094 0 2023-01-12 15:34:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 5f431d53-41dd-471f-9d74-12318835e7f9 0xc0038de2a0 0xc0038de2a1}] [] [{kube-controller-manager Update v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f431d53-41dd-471f-9d74-12318835e7f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:34:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4tz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4tz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:34:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 15:34:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:18.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9960" for this suite. 01/12/23 15:34:18.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:18.745
Jan 12 15:34:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:34:18.746
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:18.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:18.767
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:34:18.778
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:34:19.287
STEP: Deploying the webhook pod 01/12/23 15:34:19.292
STEP: Wait for the deployment to be ready 01/12/23 15:34:19.306
Jan 12 15:34:19.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:34:21.318
STEP: Verifying the service has paired with the endpoint 01/12/23 15:34:21.329
Jan 12 15:34:22.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 15:34:22.333
STEP: create a pod 01/12/23 15:34:22.347
Jan 12 15:34:22.351: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4099" to be "running"
Jan 12 15:34:22.352: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.639607ms
Jan 12 15:34:24.355: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004645807s
Jan 12 15:34:24.355: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/12/23 15:34:24.355
Jan 12 15:34:24.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=webhook-4099 attach --namespace=webhook-4099 to-be-attached-pod -i -c=container1'
Jan 12 15:34:24.426: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:24.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4099" for this suite. 01/12/23 15:34:24.46
STEP: Destroying namespace "webhook-4099-markers" for this suite. 01/12/23 15:34:24.466
------------------------------
 [SLOW TEST] [5.725 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:18.745
    Jan 12 15:34:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:34:18.746
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:18.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:18.767
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:34:18.778
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:34:19.287
    STEP: Deploying the webhook pod 01/12/23 15:34:19.292
    STEP: Wait for the deployment to be ready 01/12/23 15:34:19.306
    Jan 12 15:34:19.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:34:21.318
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:34:21.329
    Jan 12 15:34:22.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 15:34:22.333
    STEP: create a pod 01/12/23 15:34:22.347
    Jan 12 15:34:22.351: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4099" to be "running"
    Jan 12 15:34:22.352: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.639607ms
    Jan 12 15:34:24.355: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004645807s
    Jan 12 15:34:24.355: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/12/23 15:34:24.355
    Jan 12 15:34:24.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=webhook-4099 attach --namespace=webhook-4099 to-be-attached-pod -i -c=container1'
    Jan 12 15:34:24.426: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:24.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4099" for this suite. 01/12/23 15:34:24.46
    STEP: Destroying namespace "webhook-4099-markers" for this suite. 01/12/23 15:34:24.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:24.472
Jan 12 15:34:24.472: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:34:24.472
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:24.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:24.484
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-dcd37d01-3af5-4e38-9470-edc005f37025 01/12/23 15:34:24.486
STEP: Creating a pod to test consume configMaps 01/12/23 15:34:24.489
Jan 12 15:34:24.494: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660" in namespace "configmap-1963" to be "Succeeded or Failed"
Jan 12 15:34:24.496: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79112ms
Jan 12 15:34:26.500: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005454332s
Jan 12 15:34:28.499: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005289513s
STEP: Saw pod success 01/12/23 15:34:28.5
Jan 12 15:34:28.500: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660" satisfied condition "Succeeded or Failed"
Jan 12 15:34:28.501: INFO: Trying to get logs from node worker-1 pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:34:28.506
Jan 12 15:34:28.511: INFO: Waiting for pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 to disappear
Jan 12 15:34:28.513: INFO: Pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1963" for this suite. 01/12/23 15:34:28.515
------------------------------
 [4.049 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:24.472
    Jan 12 15:34:24.472: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:34:24.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:24.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:24.484
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-dcd37d01-3af5-4e38-9470-edc005f37025 01/12/23 15:34:24.486
    STEP: Creating a pod to test consume configMaps 01/12/23 15:34:24.489
    Jan 12 15:34:24.494: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660" in namespace "configmap-1963" to be "Succeeded or Failed"
    Jan 12 15:34:24.496: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79112ms
    Jan 12 15:34:26.500: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005454332s
    Jan 12 15:34:28.499: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005289513s
    STEP: Saw pod success 01/12/23 15:34:28.5
    Jan 12 15:34:28.500: INFO: Pod "pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660" satisfied condition "Succeeded or Failed"
    Jan 12 15:34:28.501: INFO: Trying to get logs from node worker-1 pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:34:28.506
    Jan 12 15:34:28.511: INFO: Waiting for pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 to disappear
    Jan 12 15:34:28.513: INFO: Pod pod-configmaps-c3c82558-141d-478b-9784-2bc9c2f3b660 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1963" for this suite. 01/12/23 15:34:28.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:28.522
Jan 12 15:34:28.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:34:28.523
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:28.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:28.532
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-124dffa7-7fe6-4227-8854-9527c4007a35 01/12/23 15:34:28.535
STEP: Creating a pod to test consume configMaps 01/12/23 15:34:28.539
Jan 12 15:34:28.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864" in namespace "projected-2177" to be "Succeeded or Failed"
Jan 12 15:34:28.545: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839699ms
Jan 12 15:34:30.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005784096s
Jan 12 15:34:32.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005658696s
STEP: Saw pod success 01/12/23 15:34:32.549
Jan 12 15:34:32.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864" satisfied condition "Succeeded or Failed"
Jan 12 15:34:32.551: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:34:32.555
Jan 12 15:34:32.562: INFO: Waiting for pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 to disappear
Jan 12 15:34:32.564: INFO: Pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:32.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2177" for this suite. 01/12/23 15:34:32.566
------------------------------
 [4.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:28.522
    Jan 12 15:34:28.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:34:28.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:28.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:28.532
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-124dffa7-7fe6-4227-8854-9527c4007a35 01/12/23 15:34:28.535
    STEP: Creating a pod to test consume configMaps 01/12/23 15:34:28.539
    Jan 12 15:34:28.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864" in namespace "projected-2177" to be "Succeeded or Failed"
    Jan 12 15:34:28.545: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839699ms
    Jan 12 15:34:30.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005784096s
    Jan 12 15:34:32.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005658696s
    STEP: Saw pod success 01/12/23 15:34:32.549
    Jan 12 15:34:32.549: INFO: Pod "pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864" satisfied condition "Succeeded or Failed"
    Jan 12 15:34:32.551: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:34:32.555
    Jan 12 15:34:32.562: INFO: Waiting for pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 to disappear
    Jan 12 15:34:32.564: INFO: Pod pod-projected-configmaps-c28c6a58-8025-46a4-b23b-363842bff864 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:32.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2177" for this suite. 01/12/23 15:34:32.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:32.57
Jan 12 15:34:32.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 15:34:32.571
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:32.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:32.589
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/12/23 15:34:32.591
Jan 12 15:34:32.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5" in namespace "downward-api-6403" to be "Succeeded or Failed"
Jan 12 15:34:32.600: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.701017ms
Jan 12 15:34:34.602: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432044s
Jan 12 15:34:36.604: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802947s
STEP: Saw pod success 01/12/23 15:34:36.604
Jan 12 15:34:36.604: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5" satisfied condition "Succeeded or Failed"
Jan 12 15:34:36.606: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 container client-container: <nil>
STEP: delete the pod 01/12/23 15:34:36.61
Jan 12 15:34:36.618: INFO: Waiting for pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 to disappear
Jan 12 15:34:36.620: INFO: Pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 15:34:36.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6403" for this suite. 01/12/23 15:34:36.622
------------------------------
 [4.056 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:32.57
    Jan 12 15:34:32.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 15:34:32.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:32.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:32.589
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/12/23 15:34:32.591
    Jan 12 15:34:32.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5" in namespace "downward-api-6403" to be "Succeeded or Failed"
    Jan 12 15:34:32.600: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.701017ms
    Jan 12 15:34:34.602: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00432044s
    Jan 12 15:34:36.604: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802947s
    STEP: Saw pod success 01/12/23 15:34:36.604
    Jan 12 15:34:36.604: INFO: Pod "downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5" satisfied condition "Succeeded or Failed"
    Jan 12 15:34:36.606: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 container client-container: <nil>
    STEP: delete the pod 01/12/23 15:34:36.61
    Jan 12 15:34:36.618: INFO: Waiting for pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 to disappear
    Jan 12 15:34:36.620: INFO: Pod downwardapi-volume-d45971b3-a624-4d38-9a95-0da99f5a0eb5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:34:36.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6403" for this suite. 01/12/23 15:34:36.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:34:36.628
Jan 12 15:34:36.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption 01/12/23 15:34:36.629
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:36.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:36.639
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 15:34:36.650: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 15:35:36.663: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:35:36.664
Jan 12 15:35:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 15:35:36.665
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:35:36.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:35:36.675
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/12/23 15:35:36.677
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 15:35:36.677
Jan 12 15:35:36.682: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9315" to be "running"
Jan 12 15:35:36.684: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971081ms
Jan 12 15:35:38.687: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005074566s
Jan 12 15:35:38.687: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 15:35:38.689
Jan 12 15:35:38.699: INFO: found a healthy node: worker-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan 12 15:35:54.754: INFO: pods created so far: [1 1 1]
Jan 12 15:35:54.754: INFO: length of pods created so far: 3
Jan 12 15:35:56.760: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9315" for this suite. 01/12/23 15:36:03.803
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6527" for this suite. 01/12/23 15:36:03.809
------------------------------
 [SLOW TEST] [87.184 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:34:36.628
    Jan 12 15:34:36.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 15:34:36.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:34:36.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:34:36.639
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 15:34:36.650: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 15:35:36.663: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:35:36.664
    Jan 12 15:35:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 15:35:36.665
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:35:36.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:35:36.675
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/12/23 15:35:36.677
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 15:35:36.677
    Jan 12 15:35:36.682: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-9315" to be "running"
    Jan 12 15:35:36.684: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971081ms
    Jan 12 15:35:38.687: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005074566s
    Jan 12 15:35:38.687: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 15:35:38.689
    Jan 12 15:35:38.699: INFO: found a healthy node: worker-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan 12 15:35:54.754: INFO: pods created so far: [1 1 1]
    Jan 12 15:35:54.754: INFO: length of pods created so far: 3
    Jan 12 15:35:56.760: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:03.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9315" for this suite. 01/12/23 15:36:03.803
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6527" for this suite. 01/12/23 15:36:03.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:03.814
Jan 12 15:36:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename server-version 01/12/23 15:36:03.815
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:03.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:03.828
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/12/23 15:36:03.83
STEP: Confirm major version 01/12/23 15:36:03.831
Jan 12 15:36:03.831: INFO: Major version: 1
STEP: Confirm minor version 01/12/23 15:36:03.831
Jan 12 15:36:03.832: INFO: cleanMinorVersion: 26
Jan 12 15:36:03.832: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-4392" for this suite. 01/12/23 15:36:03.834
------------------------------
 [0.023 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:03.814
    Jan 12 15:36:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename server-version 01/12/23 15:36:03.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:03.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:03.828
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/12/23 15:36:03.83
    STEP: Confirm major version 01/12/23 15:36:03.831
    Jan 12 15:36:03.831: INFO: Major version: 1
    STEP: Confirm minor version 01/12/23 15:36:03.831
    Jan 12 15:36:03.832: INFO: cleanMinorVersion: 26
    Jan 12 15:36:03.832: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-4392" for this suite. 01/12/23 15:36:03.834
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:03.838
Jan 12 15:36:03.839: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:36:03.839
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:03.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:03.848
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7841 01/12/23 15:36:03.851
STEP: changing the ExternalName service to type=NodePort 01/12/23 15:36:03.855
STEP: creating replication controller externalname-service in namespace services-7841 01/12/23 15:36:03.872
I0112 15:36:03.878359      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7841, replica count: 2
I0112 15:36:06.930310      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 15:36:06.930: INFO: Creating new exec pod
Jan 12 15:36:06.934: INFO: Waiting up to 5m0s for pod "execpod9pqw8" in namespace "services-7841" to be "running"
Jan 12 15:36:06.936: INFO: Pod "execpod9pqw8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.995195ms
Jan 12 15:36:08.939: INFO: Pod "execpod9pqw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005131855s
Jan 12 15:36:08.939: INFO: Pod "execpod9pqw8" satisfied condition "running"
Jan 12 15:36:09.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 12 15:36:10.087: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 12 15:36:10.087: INFO: stdout: ""
Jan 12 15:36:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.104.185.95 80'
Jan 12 15:36:10.211: INFO: stderr: "+ nc -v -z -w 2 10.104.185.95 80\nConnection to 10.104.185.95 80 port [tcp/http] succeeded!\n"
Jan 12 15:36:10.211: INFO: stdout: ""
Jan 12 15:36:10.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 32283'
Jan 12 15:36:10.323: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 32283\nConnection to 10.0.42.160 32283 port [tcp/*] succeeded!\n"
Jan 12 15:36:10.323: INFO: stdout: ""
Jan 12 15:36:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 32283'
Jan 12 15:36:10.435: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 32283\nConnection to 10.0.40.50 32283 port [tcp/*] succeeded!\n"
Jan 12 15:36:10.435: INFO: stdout: ""
Jan 12 15:36:10.435: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:10.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7841" for this suite. 01/12/23 15:36:10.454
------------------------------
 [SLOW TEST] [6.619 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:03.838
    Jan 12 15:36:03.839: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:36:03.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:03.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:03.848
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7841 01/12/23 15:36:03.851
    STEP: changing the ExternalName service to type=NodePort 01/12/23 15:36:03.855
    STEP: creating replication controller externalname-service in namespace services-7841 01/12/23 15:36:03.872
    I0112 15:36:03.878359      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7841, replica count: 2
    I0112 15:36:06.930310      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 15:36:06.930: INFO: Creating new exec pod
    Jan 12 15:36:06.934: INFO: Waiting up to 5m0s for pod "execpod9pqw8" in namespace "services-7841" to be "running"
    Jan 12 15:36:06.936: INFO: Pod "execpod9pqw8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.995195ms
    Jan 12 15:36:08.939: INFO: Pod "execpod9pqw8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005131855s
    Jan 12 15:36:08.939: INFO: Pod "execpod9pqw8" satisfied condition "running"
    Jan 12 15:36:09.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 12 15:36:10.087: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 12 15:36:10.087: INFO: stdout: ""
    Jan 12 15:36:10.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.104.185.95 80'
    Jan 12 15:36:10.211: INFO: stderr: "+ nc -v -z -w 2 10.104.185.95 80\nConnection to 10.104.185.95 80 port [tcp/http] succeeded!\n"
    Jan 12 15:36:10.211: INFO: stdout: ""
    Jan 12 15:36:10.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 32283'
    Jan 12 15:36:10.323: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 32283\nConnection to 10.0.42.160 32283 port [tcp/*] succeeded!\n"
    Jan 12 15:36:10.323: INFO: stdout: ""
    Jan 12 15:36:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7841 exec execpod9pqw8 -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 32283'
    Jan 12 15:36:10.435: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 32283\nConnection to 10.0.40.50 32283 port [tcp/*] succeeded!\n"
    Jan 12 15:36:10.435: INFO: stdout: ""
    Jan 12 15:36:10.435: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:10.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7841" for this suite. 01/12/23 15:36:10.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:10.458
Jan 12 15:36:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename containers 01/12/23 15:36:10.459
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:10.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:10.471
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/12/23 15:36:10.474
Jan 12 15:36:10.481: INFO: Waiting up to 5m0s for pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc" in namespace "containers-1949" to be "Succeeded or Failed"
Jan 12 15:36:10.483: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717107ms
Jan 12 15:36:12.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004595695s
Jan 12 15:36:14.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004473983s
STEP: Saw pod success 01/12/23 15:36:14.486
Jan 12 15:36:14.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc" satisfied condition "Succeeded or Failed"
Jan 12 15:36:14.488: INFO: Trying to get logs from node worker-1 pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:36:14.499
Jan 12 15:36:14.507: INFO: Waiting for pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc to disappear
Jan 12 15:36:14.509: INFO: Pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:14.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1949" for this suite. 01/12/23 15:36:14.523
------------------------------
 [4.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:10.458
    Jan 12 15:36:10.459: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename containers 01/12/23 15:36:10.459
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:10.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:10.471
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/12/23 15:36:10.474
    Jan 12 15:36:10.481: INFO: Waiting up to 5m0s for pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc" in namespace "containers-1949" to be "Succeeded or Failed"
    Jan 12 15:36:10.483: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717107ms
    Jan 12 15:36:12.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004595695s
    Jan 12 15:36:14.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004473983s
    STEP: Saw pod success 01/12/23 15:36:14.486
    Jan 12 15:36:14.486: INFO: Pod "client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc" satisfied condition "Succeeded or Failed"
    Jan 12 15:36:14.488: INFO: Trying to get logs from node worker-1 pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:36:14.499
    Jan 12 15:36:14.507: INFO: Waiting for pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc to disappear
    Jan 12 15:36:14.509: INFO: Pod client-containers-fde8c676-ff82-4219-9c30-0b4b8e8948dc no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:14.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1949" for this suite. 01/12/23 15:36:14.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:14.54
Jan 12 15:36:14.540: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename ingressclass 01/12/23 15:36:14.54
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:14.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:14.554
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/12/23 15:36:14.557
STEP: getting /apis/networking.k8s.io 01/12/23 15:36:14.559
STEP: getting /apis/networking.k8s.iov1 01/12/23 15:36:14.56
STEP: creating 01/12/23 15:36:14.561
STEP: getting 01/12/23 15:36:14.572
STEP: listing 01/12/23 15:36:14.574
STEP: watching 01/12/23 15:36:14.576
Jan 12 15:36:14.576: INFO: starting watch
STEP: patching 01/12/23 15:36:14.577
STEP: updating 01/12/23 15:36:14.58
Jan 12 15:36:14.583: INFO: waiting for watch events with expected annotations
Jan 12 15:36:14.583: INFO: saw patched and updated annotations
STEP: deleting 01/12/23 15:36:14.583
STEP: deleting a collection 01/12/23 15:36:14.591
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:14.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-1726" for this suite. 01/12/23 15:36:14.6
------------------------------
 [0.064 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:14.54
    Jan 12 15:36:14.540: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename ingressclass 01/12/23 15:36:14.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:14.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:14.554
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/12/23 15:36:14.557
    STEP: getting /apis/networking.k8s.io 01/12/23 15:36:14.559
    STEP: getting /apis/networking.k8s.iov1 01/12/23 15:36:14.56
    STEP: creating 01/12/23 15:36:14.561
    STEP: getting 01/12/23 15:36:14.572
    STEP: listing 01/12/23 15:36:14.574
    STEP: watching 01/12/23 15:36:14.576
    Jan 12 15:36:14.576: INFO: starting watch
    STEP: patching 01/12/23 15:36:14.577
    STEP: updating 01/12/23 15:36:14.58
    Jan 12 15:36:14.583: INFO: waiting for watch events with expected annotations
    Jan 12 15:36:14.583: INFO: saw patched and updated annotations
    STEP: deleting 01/12/23 15:36:14.583
    STEP: deleting a collection 01/12/23 15:36:14.591
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:14.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-1726" for this suite. 01/12/23 15:36:14.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:14.606
Jan 12 15:36:14.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:36:14.607
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:14.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:14.619
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/12/23 15:36:14.621
Jan 12 15:36:14.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/12/23 15:36:20.68
Jan 12 15:36:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:36:22.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-921" for this suite. 01/12/23 15:36:27.468
------------------------------
 [SLOW TEST] [12.866 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:14.606
    Jan 12 15:36:14.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:36:14.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:14.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:14.619
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/12/23 15:36:14.621
    Jan 12 15:36:14.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/12/23 15:36:20.68
    Jan 12 15:36:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:36:22.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-921" for this suite. 01/12/23 15:36:27.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:27.473
Jan 12 15:36:27.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubelet-test 01/12/23 15:36:27.473
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:27.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:27.485
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 12 15:36:27.492: INFO: Waiting up to 5m0s for pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66" in namespace "kubelet-test-5682" to be "running and ready"
Jan 12 15:36:27.494: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731234ms
Jan 12 15:36:27.494: INFO: The phase of Pod busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:36:29.497: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66": Phase="Running", Reason="", readiness=true. Elapsed: 2.004167735s
Jan 12 15:36:29.497: INFO: The phase of Pod busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66 is Running (Ready = true)
Jan 12 15:36:29.497: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 15:36:29.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5682" for this suite. 01/12/23 15:36:29.505
------------------------------
 [2.038 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:27.473
    Jan 12 15:36:27.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 15:36:27.473
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:27.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:27.485
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 12 15:36:27.492: INFO: Waiting up to 5m0s for pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66" in namespace "kubelet-test-5682" to be "running and ready"
    Jan 12 15:36:27.494: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731234ms
    Jan 12 15:36:27.494: INFO: The phase of Pod busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:36:29.497: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66": Phase="Running", Reason="", readiness=true. Elapsed: 2.004167735s
    Jan 12 15:36:29.497: INFO: The phase of Pod busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66 is Running (Ready = true)
    Jan 12 15:36:29.497: INFO: Pod "busybox-scheduling-212bf5f2-9ef8-4863-af7f-2243f43d9e66" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:36:29.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5682" for this suite. 01/12/23 15:36:29.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:36:29.511
Jan 12 15:36:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename cronjob 01/12/23 15:36:29.512
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:29.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:29.521
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/12/23 15:36:29.523
STEP: Ensuring more than one job is running at a time 01/12/23 15:36:29.528
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/12/23 15:38:01.531
STEP: Removing cronjob 01/12/23 15:38:01.532
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:01.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2024" for this suite. 01/12/23 15:38:01.538
------------------------------
 [SLOW TEST] [92.034 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:36:29.511
    Jan 12 15:36:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename cronjob 01/12/23 15:36:29.512
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:36:29.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:36:29.521
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/12/23 15:36:29.523
    STEP: Ensuring more than one job is running at a time 01/12/23 15:36:29.528
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/12/23 15:38:01.531
    STEP: Removing cronjob 01/12/23 15:38:01.532
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:01.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2024" for this suite. 01/12/23 15:38:01.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:01.546
Jan 12 15:38:01.546: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 15:38:01.547
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:01.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:01.563
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 15:38:01.566
Jan 12 15:38:01.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5775 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 12 15:38:01.636: INFO: stderr: ""
Jan 12 15:38:01.637: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 15:38:01.637
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 12 15:38:01.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5775 delete pods e2e-test-httpd-pod'
Jan 12 15:38:04.075: INFO: stderr: ""
Jan 12 15:38:04.075: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5775" for this suite. 01/12/23 15:38:04.078
------------------------------
 [2.535 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:01.546
    Jan 12 15:38:01.546: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 15:38:01.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:01.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:01.563
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 15:38:01.566
    Jan 12 15:38:01.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5775 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 12 15:38:01.636: INFO: stderr: ""
    Jan 12 15:38:01.637: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 15:38:01.637
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 12 15:38:01.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5775 delete pods e2e-test-httpd-pod'
    Jan 12 15:38:04.075: INFO: stderr: ""
    Jan 12 15:38:04.075: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:04.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5775" for this suite. 01/12/23 15:38:04.078
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:04.082
Jan 12 15:38:04.082: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 15:38:04.083
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:04.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:04.095
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/12/23 15:38:04.097
STEP: Creating a ResourceQuota 01/12/23 15:38:09.1
STEP: Ensuring resource quota status is calculated 01/12/23 15:38:09.108
STEP: Creating a Service 01/12/23 15:38:11.112
STEP: Creating a NodePort Service 01/12/23 15:38:11.131
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/12/23 15:38:11.149
STEP: Ensuring resource quota status captures service creation 01/12/23 15:38:11.169
STEP: Deleting Services 01/12/23 15:38:13.172
STEP: Ensuring resource quota status released usage 01/12/23 15:38:13.197
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:15.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1110" for this suite. 01/12/23 15:38:15.202
------------------------------
 [SLOW TEST] [11.124 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:04.082
    Jan 12 15:38:04.082: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 15:38:04.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:04.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:04.095
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/12/23 15:38:04.097
    STEP: Creating a ResourceQuota 01/12/23 15:38:09.1
    STEP: Ensuring resource quota status is calculated 01/12/23 15:38:09.108
    STEP: Creating a Service 01/12/23 15:38:11.112
    STEP: Creating a NodePort Service 01/12/23 15:38:11.131
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/12/23 15:38:11.149
    STEP: Ensuring resource quota status captures service creation 01/12/23 15:38:11.169
    STEP: Deleting Services 01/12/23 15:38:13.172
    STEP: Ensuring resource quota status released usage 01/12/23 15:38:13.197
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:15.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1110" for this suite. 01/12/23 15:38:15.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:15.208
Jan 12 15:38:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/12/23 15:38:15.208
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:15.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:15.221
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/12/23 15:38:15.223
STEP: Creating hostNetwork=false pod 01/12/23 15:38:15.223
Jan 12 15:38:15.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4608" to be "running and ready"
Jan 12 15:38:15.230: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876698ms
Jan 12 15:38:15.230: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:38:17.234: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00519768s
Jan 12 15:38:17.234: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 12 15:38:17.234: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/12/23 15:38:17.236
Jan 12 15:38:17.240: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4608" to be "running and ready"
Jan 12 15:38:17.243: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025836ms
Jan 12 15:38:17.243: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:38:19.245: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004504634s
Jan 12 15:38:19.245: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 12 15:38:19.245: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/12/23 15:38:19.247
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/12/23 15:38:19.247
Jan 12 15:38:19.247: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.247: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.248: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.248: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 15:38:19.305: INFO: Exec stderr: ""
Jan 12 15:38:19.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.305: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.306: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 15:38:19.361: INFO: Exec stderr: ""
Jan 12 15:38:19.361: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.362: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 15:38:19.413: INFO: Exec stderr: ""
Jan 12 15:38:19.413: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.414: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.414: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 15:38:19.485: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/12/23 15:38:19.485
Jan 12 15:38:19.485: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.486: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.486: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 12 15:38:19.545: INFO: Exec stderr: ""
Jan 12 15:38:19.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.546: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 12 15:38:19.613: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/12/23 15:38:19.613
Jan 12 15:38:19.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.614: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.615: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 15:38:19.676: INFO: Exec stderr: ""
Jan 12 15:38:19.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.676: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.676: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.676: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 12 15:38:19.731: INFO: Exec stderr: ""
Jan 12 15:38:19.731: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.731: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 15:38:19.788: INFO: Exec stderr: ""
Jan 12 15:38:19.788: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:38:19.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:38:19.789: INFO: ExecWithOptions: Clientset creation
Jan 12 15:38:19.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 12 15:38:19.840: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:19.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4608" for this suite. 01/12/23 15:38:19.842
------------------------------
 [4.638 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:15.208
    Jan 12 15:38:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/12/23 15:38:15.208
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:15.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:15.221
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/12/23 15:38:15.223
    STEP: Creating hostNetwork=false pod 01/12/23 15:38:15.223
    Jan 12 15:38:15.228: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4608" to be "running and ready"
    Jan 12 15:38:15.230: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876698ms
    Jan 12 15:38:15.230: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:38:17.234: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00519768s
    Jan 12 15:38:17.234: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 12 15:38:17.234: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/12/23 15:38:17.236
    Jan 12 15:38:17.240: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4608" to be "running and ready"
    Jan 12 15:38:17.243: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025836ms
    Jan 12 15:38:17.243: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:38:19.245: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004504634s
    Jan 12 15:38:19.245: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 12 15:38:19.245: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/12/23 15:38:19.247
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/12/23 15:38:19.247
    Jan 12 15:38:19.247: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.247: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.248: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.248: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 15:38:19.305: INFO: Exec stderr: ""
    Jan 12 15:38:19.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.305: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.306: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 15:38:19.361: INFO: Exec stderr: ""
    Jan 12 15:38:19.361: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.362: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.362: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 15:38:19.413: INFO: Exec stderr: ""
    Jan 12 15:38:19.413: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.414: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.414: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 15:38:19.485: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/12/23 15:38:19.485
    Jan 12 15:38:19.485: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.486: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.486: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 12 15:38:19.545: INFO: Exec stderr: ""
    Jan 12 15:38:19.545: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.546: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 12 15:38:19.613: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/12/23 15:38:19.613
    Jan 12 15:38:19.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.614: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.615: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.615: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 15:38:19.676: INFO: Exec stderr: ""
    Jan 12 15:38:19.676: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.676: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.676: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.676: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 12 15:38:19.731: INFO: Exec stderr: ""
    Jan 12 15:38:19.731: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.731: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.731: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 15:38:19.788: INFO: Exec stderr: ""
    Jan 12 15:38:19.788: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4608 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:38:19.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:38:19.789: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:38:19.789: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4608/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 12 15:38:19.840: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:19.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4608" for this suite. 01/12/23 15:38:19.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:19.848
Jan 12 15:38:19.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:38:19.849
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:19.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:19.864
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/12/23 15:38:19.867
Jan 12 15:38:19.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7" in namespace "projected-3736" to be "Succeeded or Failed"
Jan 12 15:38:19.874: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755028ms
Jan 12 15:38:21.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526099s
Jan 12 15:38:23.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005058332s
STEP: Saw pod success 01/12/23 15:38:23.877
Jan 12 15:38:23.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7" satisfied condition "Succeeded or Failed"
Jan 12 15:38:23.879: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 container client-container: <nil>
STEP: delete the pod 01/12/23 15:38:23.891
Jan 12 15:38:23.900: INFO: Waiting for pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 to disappear
Jan 12 15:38:23.902: INFO: Pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:23.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3736" for this suite. 01/12/23 15:38:23.904
------------------------------
 [4.061 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:19.848
    Jan 12 15:38:19.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:38:19.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:19.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:19.864
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/12/23 15:38:19.867
    Jan 12 15:38:19.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7" in namespace "projected-3736" to be "Succeeded or Failed"
    Jan 12 15:38:19.874: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.755028ms
    Jan 12 15:38:21.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00526099s
    Jan 12 15:38:23.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005058332s
    STEP: Saw pod success 01/12/23 15:38:23.877
    Jan 12 15:38:23.877: INFO: Pod "downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7" satisfied condition "Succeeded or Failed"
    Jan 12 15:38:23.879: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 container client-container: <nil>
    STEP: delete the pod 01/12/23 15:38:23.891
    Jan 12 15:38:23.900: INFO: Waiting for pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 to disappear
    Jan 12 15:38:23.902: INFO: Pod downwardapi-volume-fe56ef21-26f9-4c43-aa51-3fc49dfa07f7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:23.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3736" for this suite. 01/12/23 15:38:23.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:23.91
Jan 12 15:38:23.910: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-runtime 01/12/23 15:38:23.911
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:23.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:23.921
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/12/23 15:38:23.93
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/12/23 15:38:41.977
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/12/23 15:38:41.979
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/12/23 15:38:41.982
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/12/23 15:38:41.982
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/12/23 15:38:41.999
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/12/23 15:38:45.01
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/12/23 15:38:47.017
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/12/23 15:38:47.021
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/12/23 15:38:47.021
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/12/23 15:38:47.036
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/12/23 15:38:48.04
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/12/23 15:38:51.05
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/12/23 15:38:51.053
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/12/23 15:38:51.054
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:51.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3735" for this suite. 01/12/23 15:38:51.071
------------------------------
 [SLOW TEST] [27.165 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:23.91
    Jan 12 15:38:23.910: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-runtime 01/12/23 15:38:23.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:23.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:23.921
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/12/23 15:38:23.93
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/12/23 15:38:41.977
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/12/23 15:38:41.979
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/12/23 15:38:41.982
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/12/23 15:38:41.982
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/12/23 15:38:41.999
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/12/23 15:38:45.01
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/12/23 15:38:47.017
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/12/23 15:38:47.021
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/12/23 15:38:47.021
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/12/23 15:38:47.036
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/12/23 15:38:48.04
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/12/23 15:38:51.05
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/12/23 15:38:51.053
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/12/23 15:38:51.054
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:51.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3735" for this suite. 01/12/23 15:38:51.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:51.078
Jan 12 15:38:51.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:38:51.079
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:51.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:51.088
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/12/23 15:38:51.09
STEP: fetching the ConfigMap 01/12/23 15:38:51.093
STEP: patching the ConfigMap 01/12/23 15:38:51.095
STEP: listing all ConfigMaps in all namespaces with a label selector 01/12/23 15:38:51.098
STEP: deleting the ConfigMap by collection with a label selector 01/12/23 15:38:51.1
STEP: listing all ConfigMaps in test namespace 01/12/23 15:38:51.104
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:51.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8009" for this suite. 01/12/23 15:38:51.108
------------------------------
 [0.033 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:51.078
    Jan 12 15:38:51.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:38:51.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:51.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:51.088
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/12/23 15:38:51.09
    STEP: fetching the ConfigMap 01/12/23 15:38:51.093
    STEP: patching the ConfigMap 01/12/23 15:38:51.095
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/12/23 15:38:51.098
    STEP: deleting the ConfigMap by collection with a label selector 01/12/23 15:38:51.1
    STEP: listing all ConfigMaps in test namespace 01/12/23 15:38:51.104
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:51.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8009" for this suite. 01/12/23 15:38:51.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:51.112
Jan 12 15:38:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 15:38:51.113
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:51.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:51.124
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 12 15:38:51.154: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c6c97dad-4866-4130-8db7-5f4520c4ac49", Controller:(*bool)(0xc003a7322e), BlockOwnerDeletion:(*bool)(0xc003a7322f)}}
Jan 12 15:38:51.163: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5281d796-fd07-450e-867f-2814e8c3c118", Controller:(*bool)(0xc003a7349a), BlockOwnerDeletion:(*bool)(0xc003a7349b)}}
Jan 12 15:38:51.169: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a391d68c-fb1e-43d0-8de6-c9eb23ad5ef9", Controller:(*bool)(0xc003a7370a), BlockOwnerDeletion:(*bool)(0xc003a7370b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1281" for this suite. 01/12/23 15:38:56.182
------------------------------
 [SLOW TEST] [5.079 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:51.112
    Jan 12 15:38:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 15:38:51.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:51.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:51.124
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 12 15:38:51.154: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c6c97dad-4866-4130-8db7-5f4520c4ac49", Controller:(*bool)(0xc003a7322e), BlockOwnerDeletion:(*bool)(0xc003a7322f)}}
    Jan 12 15:38:51.163: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5281d796-fd07-450e-867f-2814e8c3c118", Controller:(*bool)(0xc003a7349a), BlockOwnerDeletion:(*bool)(0xc003a7349b)}}
    Jan 12 15:38:51.169: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a391d68c-fb1e-43d0-8de6-c9eb23ad5ef9", Controller:(*bool)(0xc003a7370a), BlockOwnerDeletion:(*bool)(0xc003a7370b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:56.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1281" for this suite. 01/12/23 15:38:56.182
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:56.191
Jan 12 15:38:56.191: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 15:38:56.192
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:56.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:56.212
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/12/23 15:38:56.215
STEP: getting 01/12/23 15:38:56.23
STEP: listing 01/12/23 15:38:56.234
STEP: deleting 01/12/23 15:38:56.236
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6441" for this suite. 01/12/23 15:38:56.251
------------------------------
 [0.068 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:56.191
    Jan 12 15:38:56.191: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 15:38:56.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:56.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:56.212
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/12/23 15:38:56.215
    STEP: getting 01/12/23 15:38:56.23
    STEP: listing 01/12/23 15:38:56.234
    STEP: deleting 01/12/23 15:38:56.236
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6441" for this suite. 01/12/23 15:38:56.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:56.26
Jan 12 15:38:56.260: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-runtime 01/12/23 15:38:56.261
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:56.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:56.272
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/12/23 15:38:56.275
STEP: wait for the container to reach Succeeded 01/12/23 15:38:56.281
STEP: get the container status 01/12/23 15:38:59.291
STEP: the container should be terminated 01/12/23 15:38:59.293
STEP: the termination message should be set 01/12/23 15:38:59.293
Jan 12 15:38:59.293: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/12/23 15:38:59.293
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 15:38:59.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-645" for this suite. 01/12/23 15:38:59.304
------------------------------
 [3.048 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:56.26
    Jan 12 15:38:56.260: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-runtime 01/12/23 15:38:56.261
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:56.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:56.272
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/12/23 15:38:56.275
    STEP: wait for the container to reach Succeeded 01/12/23 15:38:56.281
    STEP: get the container status 01/12/23 15:38:59.291
    STEP: the container should be terminated 01/12/23 15:38:59.293
    STEP: the termination message should be set 01/12/23 15:38:59.293
    Jan 12 15:38:59.293: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/12/23 15:38:59.293
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:38:59.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-645" for this suite. 01/12/23 15:38:59.304
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:38:59.308
Jan 12 15:38:59.308: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 15:38:59.309
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:59.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:59.32
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 12 15:38:59.323: INFO: Creating simple deployment test-new-deployment
Jan 12 15:38:59.332: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 01/12/23 15:39:01.342
STEP: updating a scale subresource 01/12/23 15:39:01.344
STEP: verifying the deployment Spec.Replicas was modified 01/12/23 15:39:01.348
STEP: Patch a scale subresource 01/12/23 15:39:01.35
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 15:39:01.363: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9008  8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1 6708 3 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-12 15:38:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0061e5588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 15:39:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-12 15:39:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 15:39:01.370: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9008  7d30e7cf-6386-4ced-a587-72dc3839ce5e 6712 2 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1 0xc00424c050 0xc00424c051}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00424c0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:39:01.375: INFO: Pod "test-new-deployment-7f5969cbc7-w55fb" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-w55fb test-new-deployment-7f5969cbc7- deployment-9008  2192763f-5425-4f9e-b283-73f23163c8c6 6713 0 2023-01-12 15:39:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7d30e7cf-6386-4ced-a587-72dc3839ce5e 0xc00424c4c0 0xc00424c4c1}] [] [{kube-controller-manager Update v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d30e7cf-6386-4ced-a587-72dc3839ce5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ln586,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ln586,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 15:39:01.375: INFO: Pod "test-new-deployment-7f5969cbc7-x7c7b" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-x7c7b test-new-deployment-7f5969cbc7- deployment-9008  f8113136-45e1-4431-889b-08902f98ae74 6689 0 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7d30e7cf-6386-4ced-a587-72dc3839ce5e 0xc00424c620 0xc00424c621}] [] [{kube-controller-manager Update v1 2023-01-12 15:38:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d30e7cf-6386-4ced-a587-72dc3839ce5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:39:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4kjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4kjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.103,StartTime:2023-01-12 15:38:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 15:38:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0de6453c5bda8b5db1fe20fcf2359a221eb770c55757c35ec0a34a33db040b33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:01.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9008" for this suite. 01/12/23 15:39:01.38
------------------------------
 [2.078 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:38:59.308
    Jan 12 15:38:59.308: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 15:38:59.309
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:38:59.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:38:59.32
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 12 15:38:59.323: INFO: Creating simple deployment test-new-deployment
    Jan 12 15:38:59.332: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 01/12/23 15:39:01.342
    STEP: updating a scale subresource 01/12/23 15:39:01.344
    STEP: verifying the deployment Spec.Replicas was modified 01/12/23 15:39:01.348
    STEP: Patch a scale subresource 01/12/23 15:39:01.35
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 15:39:01.363: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9008  8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1 6708 3 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-12 15:38:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:39:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0061e5588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 15:39:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-12 15:39:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 15:39:01.370: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9008  7d30e7cf-6386-4ced-a587-72dc3839ce5e 6712 2 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1 0xc00424c050 0xc00424c051}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d2b22e5-97b8-47d1-a12f-5c49a5fd35d1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00424c0d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:39:01.375: INFO: Pod "test-new-deployment-7f5969cbc7-w55fb" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-w55fb test-new-deployment-7f5969cbc7- deployment-9008  2192763f-5425-4f9e-b283-73f23163c8c6 6713 0 2023-01-12 15:39:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7d30e7cf-6386-4ced-a587-72dc3839ce5e 0xc00424c4c0 0xc00424c4c1}] [] [{kube-controller-manager Update v1 2023-01-12 15:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d30e7cf-6386-4ced-a587-72dc3839ce5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ln586,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ln586,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 15:39:01.375: INFO: Pod "test-new-deployment-7f5969cbc7-x7c7b" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-x7c7b test-new-deployment-7f5969cbc7- deployment-9008  f8113136-45e1-4431-889b-08902f98ae74 6689 0 2023-01-12 15:38:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 7d30e7cf-6386-4ced-a587-72dc3839ce5e 0xc00424c620 0xc00424c621}] [] [{kube-controller-manager Update v1 2023-01-12 15:38:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d30e7cf-6386-4ced-a587-72dc3839ce5e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:39:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4kjr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4kjr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:39:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:38:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.103,StartTime:2023-01-12 15:38:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 15:38:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0de6453c5bda8b5db1fe20fcf2359a221eb770c55757c35ec0a34a33db040b33,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:01.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9008" for this suite. 01/12/23 15:39:01.38
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:01.387
Jan 12 15:39:01.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:39:01.388
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:01.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:01.402
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-e096c199-79d8-4600-ba75-936bfedf6b28 01/12/23 15:39:01.405
STEP: Creating a pod to test consume configMaps 01/12/23 15:39:01.407
Jan 12 15:39:01.413: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961" in namespace "configmap-4565" to be "Succeeded or Failed"
Jan 12 15:39:01.416: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.485155ms
Jan 12 15:39:03.419: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005625117s
Jan 12 15:39:05.420: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00638149s
STEP: Saw pod success 01/12/23 15:39:05.42
Jan 12 15:39:05.420: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961" satisfied condition "Succeeded or Failed"
Jan 12 15:39:05.422: INFO: Trying to get logs from node worker-1 pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:39:05.426
Jan 12 15:39:05.437: INFO: Waiting for pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 to disappear
Jan 12 15:39:05.438: INFO: Pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:05.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4565" for this suite. 01/12/23 15:39:05.44
------------------------------
 [4.057 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:01.387
    Jan 12 15:39:01.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:39:01.388
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:01.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:01.402
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-e096c199-79d8-4600-ba75-936bfedf6b28 01/12/23 15:39:01.405
    STEP: Creating a pod to test consume configMaps 01/12/23 15:39:01.407
    Jan 12 15:39:01.413: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961" in namespace "configmap-4565" to be "Succeeded or Failed"
    Jan 12 15:39:01.416: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.485155ms
    Jan 12 15:39:03.419: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005625117s
    Jan 12 15:39:05.420: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00638149s
    STEP: Saw pod success 01/12/23 15:39:05.42
    Jan 12 15:39:05.420: INFO: Pod "pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961" satisfied condition "Succeeded or Failed"
    Jan 12 15:39:05.422: INFO: Trying to get logs from node worker-1 pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:39:05.426
    Jan 12 15:39:05.437: INFO: Waiting for pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 to disappear
    Jan 12 15:39:05.438: INFO: Pod pod-configmaps-8b6772e4-dcb0-444b-a82a-00d24a9c2961 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:05.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4565" for this suite. 01/12/23 15:39:05.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:05.445
Jan 12 15:39:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 15:39:05.446
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:05.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:05.46
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/12/23 15:39:05.463
Jan 12 15:39:05.467: INFO: Waiting up to 5m0s for pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672" in namespace "var-expansion-5226" to be "Succeeded or Failed"
Jan 12 15:39:05.469: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769ms
Jan 12 15:39:07.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004767219s
Jan 12 15:39:09.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818537s
STEP: Saw pod success 01/12/23 15:39:09.472
Jan 12 15:39:09.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672" satisfied condition "Succeeded or Failed"
Jan 12 15:39:09.474: INFO: Trying to get logs from node worker-1 pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 container dapi-container: <nil>
STEP: delete the pod 01/12/23 15:39:09.478
Jan 12 15:39:09.486: INFO: Waiting for pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 to disappear
Jan 12 15:39:09.488: INFO: Pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:09.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5226" for this suite. 01/12/23 15:39:09.49
------------------------------
 [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:05.445
    Jan 12 15:39:05.445: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 15:39:05.446
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:05.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:05.46
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/12/23 15:39:05.463
    Jan 12 15:39:05.467: INFO: Waiting up to 5m0s for pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672" in namespace "var-expansion-5226" to be "Succeeded or Failed"
    Jan 12 15:39:05.469: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769ms
    Jan 12 15:39:07.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004767219s
    Jan 12 15:39:09.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004818537s
    STEP: Saw pod success 01/12/23 15:39:09.472
    Jan 12 15:39:09.472: INFO: Pod "var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672" satisfied condition "Succeeded or Failed"
    Jan 12 15:39:09.474: INFO: Trying to get logs from node worker-1 pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 15:39:09.478
    Jan 12 15:39:09.486: INFO: Waiting for pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 to disappear
    Jan 12 15:39:09.488: INFO: Pod var-expansion-8e7e129a-4235-4367-a0a1-ed0756833672 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:09.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5226" for this suite. 01/12/23 15:39:09.49
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:09.496
Jan 12 15:39:09.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename init-container 01/12/23 15:39:09.497
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:09.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:09.508
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/12/23 15:39:09.511
Jan 12 15:39:09.511: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9864" for this suite. 01/12/23 15:39:13.314
------------------------------
 [3.822 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:09.496
    Jan 12 15:39:09.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename init-container 01/12/23 15:39:09.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:09.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:09.508
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/12/23 15:39:09.511
    Jan 12 15:39:09.511: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9864" for this suite. 01/12/23 15:39:13.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:13.319
Jan 12 15:39:13.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 15:39:13.32
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:13.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:13.331
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 15:39:13.335
Jan 12 15:39:13.341: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4235" to be "running and ready"
Jan 12 15:39:13.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796062ms
Jan 12 15:39:13.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:39:15.347: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005589336s
Jan 12 15:39:15.347: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 15:39:15.347: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/12/23 15:39:15.349
Jan 12 15:39:15.352: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4235" to be "running and ready"
Jan 12 15:39:15.354: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619478ms
Jan 12 15:39:15.354: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:39:17.357: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004398253s
Jan 12 15:39:17.357: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 12 15:39:17.357: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/12/23 15:39:17.358
STEP: delete the pod with lifecycle hook 01/12/23 15:39:17.363
Jan 12 15:39:17.367: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 12 15:39:17.369: INFO: Pod pod-with-poststart-http-hook still exists
Jan 12 15:39:19.370: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 12 15:39:19.372: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:19.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4235" for this suite. 01/12/23 15:39:19.374
------------------------------
 [SLOW TEST] [6.061 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:13.319
    Jan 12 15:39:13.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 15:39:13.32
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:13.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:13.331
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 15:39:13.335
    Jan 12 15:39:13.341: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4235" to be "running and ready"
    Jan 12 15:39:13.343: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796062ms
    Jan 12 15:39:13.343: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:39:15.347: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005589336s
    Jan 12 15:39:15.347: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 15:39:15.347: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/12/23 15:39:15.349
    Jan 12 15:39:15.352: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4235" to be "running and ready"
    Jan 12 15:39:15.354: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619478ms
    Jan 12 15:39:15.354: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:39:17.357: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004398253s
    Jan 12 15:39:17.357: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 12 15:39:17.357: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/12/23 15:39:17.358
    STEP: delete the pod with lifecycle hook 01/12/23 15:39:17.363
    Jan 12 15:39:17.367: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 12 15:39:17.369: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 12 15:39:19.370: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 12 15:39:19.372: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:19.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4235" for this suite. 01/12/23 15:39:19.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:19.383
Jan 12 15:39:19.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:39:19.384
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:19.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:19.393
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:39:19.405
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:39:20.159
STEP: Deploying the webhook pod 01/12/23 15:39:20.166
STEP: Wait for the deployment to be ready 01/12/23 15:39:20.175
Jan 12 15:39:20.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:39:22.187
STEP: Verifying the service has paired with the endpoint 01/12/23 15:39:22.196
Jan 12 15:39:23.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/12/23 15:39:23.198
STEP: Creating a custom resource definition that should be denied by the webhook 01/12/23 15:39:23.215
Jan 12 15:39:23.215: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:39:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5693" for this suite. 01/12/23 15:39:23.255
STEP: Destroying namespace "webhook-5693-markers" for this suite. 01/12/23 15:39:23.258
------------------------------
 [3.882 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:19.383
    Jan 12 15:39:19.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:39:19.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:19.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:19.393
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:39:19.405
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:39:20.159
    STEP: Deploying the webhook pod 01/12/23 15:39:20.166
    STEP: Wait for the deployment to be ready 01/12/23 15:39:20.175
    Jan 12 15:39:20.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:39:22.187
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:39:22.196
    Jan 12 15:39:23.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/12/23 15:39:23.198
    STEP: Creating a custom resource definition that should be denied by the webhook 01/12/23 15:39:23.215
    Jan 12 15:39:23.215: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:39:23.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5693" for this suite. 01/12/23 15:39:23.255
    STEP: Destroying namespace "webhook-5693-markers" for this suite. 01/12/23 15:39:23.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:39:23.265
Jan 12 15:39:23.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 15:39:23.266
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:23.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:23.275
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:23.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8205" for this suite. 01/12/23 15:40:23.29
------------------------------
 [SLOW TEST] [60.030 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:39:23.265
    Jan 12 15:39:23.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 15:39:23.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:39:23.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:39:23.275
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:23.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8205" for this suite. 01/12/23 15:40:23.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:23.297
Jan 12 15:40:23.297: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 15:40:23.298
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:23.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:23.308
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/12/23 15:40:23.31
Jan 12 15:40:23.317: INFO: Waiting up to 5m0s for pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1" in namespace "var-expansion-9138" to be "Succeeded or Failed"
Jan 12 15:40:23.319: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767157ms
Jan 12 15:40:25.323: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005621724s
Jan 12 15:40:27.324: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006225024s
STEP: Saw pod success 01/12/23 15:40:27.324
Jan 12 15:40:27.324: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1" satisfied condition "Succeeded or Failed"
Jan 12 15:40:27.326: INFO: Trying to get logs from node worker-1 pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 container dapi-container: <nil>
STEP: delete the pod 01/12/23 15:40:27.33
Jan 12 15:40:27.340: INFO: Waiting for pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 to disappear
Jan 12 15:40:27.342: INFO: Pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:27.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9138" for this suite. 01/12/23 15:40:27.344
------------------------------
 [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:23.297
    Jan 12 15:40:23.297: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 15:40:23.298
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:23.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:23.308
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/12/23 15:40:23.31
    Jan 12 15:40:23.317: INFO: Waiting up to 5m0s for pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1" in namespace "var-expansion-9138" to be "Succeeded or Failed"
    Jan 12 15:40:23.319: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.767157ms
    Jan 12 15:40:25.323: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005621724s
    Jan 12 15:40:27.324: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006225024s
    STEP: Saw pod success 01/12/23 15:40:27.324
    Jan 12 15:40:27.324: INFO: Pod "var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1" satisfied condition "Succeeded or Failed"
    Jan 12 15:40:27.326: INFO: Trying to get logs from node worker-1 pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 15:40:27.33
    Jan 12 15:40:27.340: INFO: Waiting for pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 to disappear
    Jan 12 15:40:27.342: INFO: Pod var-expansion-66ed5ea7-fdb4-4a97-8123-583cb24e1bd1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:27.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9138" for this suite. 01/12/23 15:40:27.344
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:27.349
Jan 12 15:40:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 15:40:27.349
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:27.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:27.36
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/12/23 15:40:27.362
Jan 12 15:40:27.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 create -f -'
Jan 12 15:40:27.973: INFO: stderr: ""
Jan 12 15:40:27.973: INFO: stdout: "pod/pause created\n"
Jan 12 15:40:27.973: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 12 15:40:27.973: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1822" to be "running and ready"
Jan 12 15:40:27.975: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099889ms
Jan 12 15:40:27.975: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-1' to be 'Running' but was 'Pending'
Jan 12 15:40:29.978: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005582387s
Jan 12 15:40:29.978: INFO: Pod "pause" satisfied condition "running and ready"
Jan 12 15:40:29.978: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/12/23 15:40:29.978
Jan 12 15:40:29.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 label pods pause testing-label=testing-label-value'
Jan 12 15:40:30.044: INFO: stderr: ""
Jan 12 15:40:30.044: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/12/23 15:40:30.044
Jan 12 15:40:30.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pod pause -L testing-label'
Jan 12 15:40:30.108: INFO: stderr: ""
Jan 12 15:40:30.108: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/12/23 15:40:30.108
Jan 12 15:40:30.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 label pods pause testing-label-'
Jan 12 15:40:30.185: INFO: stderr: ""
Jan 12 15:40:30.185: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/12/23 15:40:30.185
Jan 12 15:40:30.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pod pause -L testing-label'
Jan 12 15:40:30.250: INFO: stderr: ""
Jan 12 15:40:30.250: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/12/23 15:40:30.25
Jan 12 15:40:30.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 delete --grace-period=0 --force -f -'
Jan 12 15:40:30.318: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 15:40:30.318: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 12 15:40:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get rc,svc -l name=pause --no-headers'
Jan 12 15:40:30.390: INFO: stderr: "No resources found in kubectl-1822 namespace.\n"
Jan 12 15:40:30.390: INFO: stdout: ""
Jan 12 15:40:30.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 15:40:30.452: INFO: stderr: ""
Jan 12 15:40:30.452: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:30.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1822" for this suite. 01/12/23 15:40:30.454
------------------------------
 [3.109 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:27.349
    Jan 12 15:40:27.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 15:40:27.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:27.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:27.36
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/12/23 15:40:27.362
    Jan 12 15:40:27.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 create -f -'
    Jan 12 15:40:27.973: INFO: stderr: ""
    Jan 12 15:40:27.973: INFO: stdout: "pod/pause created\n"
    Jan 12 15:40:27.973: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 12 15:40:27.973: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1822" to be "running and ready"
    Jan 12 15:40:27.975: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099889ms
    Jan 12 15:40:27.975: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'worker-1' to be 'Running' but was 'Pending'
    Jan 12 15:40:29.978: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005582387s
    Jan 12 15:40:29.978: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 12 15:40:29.978: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/12/23 15:40:29.978
    Jan 12 15:40:29.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 label pods pause testing-label=testing-label-value'
    Jan 12 15:40:30.044: INFO: stderr: ""
    Jan 12 15:40:30.044: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/12/23 15:40:30.044
    Jan 12 15:40:30.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pod pause -L testing-label'
    Jan 12 15:40:30.108: INFO: stderr: ""
    Jan 12 15:40:30.108: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/12/23 15:40:30.108
    Jan 12 15:40:30.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 label pods pause testing-label-'
    Jan 12 15:40:30.185: INFO: stderr: ""
    Jan 12 15:40:30.185: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/12/23 15:40:30.185
    Jan 12 15:40:30.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pod pause -L testing-label'
    Jan 12 15:40:30.250: INFO: stderr: ""
    Jan 12 15:40:30.250: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/12/23 15:40:30.25
    Jan 12 15:40:30.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 delete --grace-period=0 --force -f -'
    Jan 12 15:40:30.318: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 15:40:30.318: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 12 15:40:30.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get rc,svc -l name=pause --no-headers'
    Jan 12 15:40:30.390: INFO: stderr: "No resources found in kubectl-1822 namespace.\n"
    Jan 12 15:40:30.390: INFO: stdout: ""
    Jan 12 15:40:30.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-1822 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 15:40:30.452: INFO: stderr: ""
    Jan 12 15:40:30.452: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:30.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1822" for this suite. 01/12/23 15:40:30.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:30.46
Jan 12 15:40:30.460: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 15:40:30.461
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:30.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:30.472
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 12 15:40:30.475: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/12/23 15:40:31.482
STEP: Checking rc "condition-test" has the desired failure condition set 01/12/23 15:40:31.487
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/12/23 15:40:32.496
Jan 12 15:40:32.523: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/12/23 15:40:32.523
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:33.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3589" for this suite. 01/12/23 15:40:33.53
------------------------------
 [3.074 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:30.46
    Jan 12 15:40:30.460: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 15:40:30.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:30.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:30.472
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 12 15:40:30.475: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/12/23 15:40:31.482
    STEP: Checking rc "condition-test" has the desired failure condition set 01/12/23 15:40:31.487
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/12/23 15:40:32.496
    Jan 12 15:40:32.523: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/12/23 15:40:32.523
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:33.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3589" for this suite. 01/12/23 15:40:33.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:33.535
Jan 12 15:40:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:40:33.536
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:33.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:33.545
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/12/23 15:40:33.548
Jan 12 15:40:33.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d" in namespace "projected-7475" to be "Succeeded or Failed"
Jan 12 15:40:33.555: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.739227ms
Jan 12 15:40:35.557: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004137689s
Jan 12 15:40:37.557: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004342637s
STEP: Saw pod success 01/12/23 15:40:37.557
Jan 12 15:40:37.558: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d" satisfied condition "Succeeded or Failed"
Jan 12 15:40:37.559: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d container client-container: <nil>
STEP: delete the pod 01/12/23 15:40:37.563
Jan 12 15:40:37.572: INFO: Waiting for pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d to disappear
Jan 12 15:40:37.574: INFO: Pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:37.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7475" for this suite. 01/12/23 15:40:37.576
------------------------------
 [4.048 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:33.535
    Jan 12 15:40:33.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:40:33.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:33.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:33.545
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/12/23 15:40:33.548
    Jan 12 15:40:33.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d" in namespace "projected-7475" to be "Succeeded or Failed"
    Jan 12 15:40:33.555: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.739227ms
    Jan 12 15:40:35.557: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004137689s
    Jan 12 15:40:37.557: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004342637s
    STEP: Saw pod success 01/12/23 15:40:37.557
    Jan 12 15:40:37.558: INFO: Pod "downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d" satisfied condition "Succeeded or Failed"
    Jan 12 15:40:37.559: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d container client-container: <nil>
    STEP: delete the pod 01/12/23 15:40:37.563
    Jan 12 15:40:37.572: INFO: Waiting for pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d to disappear
    Jan 12 15:40:37.574: INFO: Pod downwardapi-volume-525763aa-2334-43f0-b5a0-cbd87d34771d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:37.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7475" for this suite. 01/12/23 15:40:37.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:37.584
Jan 12 15:40:37.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 15:40:37.584
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:37.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:37.594
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 15:40:37.604
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:40:38.234
STEP: Deploying the webhook pod 01/12/23 15:40:38.239
STEP: Wait for the deployment to be ready 01/12/23 15:40:38.247
Jan 12 15:40:38.253: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:40:40.259
STEP: Verifying the service has paired with the endpoint 01/12/23 15:40:40.272
Jan 12 15:40:41.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/12/23 15:40:41.276
STEP: create a pod that should be updated by the webhook 01/12/23 15:40:41.292
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:41.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6264" for this suite. 01/12/23 15:40:41.354
STEP: Destroying namespace "webhook-6264-markers" for this suite. 01/12/23 15:40:41.36
------------------------------
 [3.781 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:37.584
    Jan 12 15:40:37.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 15:40:37.584
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:37.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:37.594
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 15:40:37.604
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 15:40:38.234
    STEP: Deploying the webhook pod 01/12/23 15:40:38.239
    STEP: Wait for the deployment to be ready 01/12/23 15:40:38.247
    Jan 12 15:40:38.253: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:40:40.259
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:40:40.272
    Jan 12 15:40:41.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/12/23 15:40:41.276
    STEP: create a pod that should be updated by the webhook 01/12/23 15:40:41.292
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:41.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6264" for this suite. 01/12/23 15:40:41.354
    STEP: Destroying namespace "webhook-6264-markers" for this suite. 01/12/23 15:40:41.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:41.366
Jan 12 15:40:41.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:40:41.367
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:41.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:41.379
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8620 01/12/23 15:40:41.381
STEP: changing the ExternalName service to type=ClusterIP 01/12/23 15:40:41.384
STEP: creating replication controller externalname-service in namespace services-8620 01/12/23 15:40:41.395
I0112 15:40:41.399985      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8620, replica count: 2
I0112 15:40:44.451621      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 15:40:44.451: INFO: Creating new exec pod
Jan 12 15:40:44.455: INFO: Waiting up to 5m0s for pod "execpodxggcs" in namespace "services-8620" to be "running"
Jan 12 15:40:44.458: INFO: Pod "execpodxggcs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041726ms
Jan 12 15:40:46.460: INFO: Pod "execpodxggcs": Phase="Running", Reason="", readiness=true. Elapsed: 2.004871828s
Jan 12 15:40:46.460: INFO: Pod "execpodxggcs" satisfied condition "running"
Jan 12 15:40:47.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-8620 exec execpodxggcs -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 12 15:40:47.591: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 12 15:40:47.591: INFO: stdout: ""
Jan 12 15:40:47.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-8620 exec execpodxggcs -- /bin/sh -x -c nc -v -z -w 2 10.98.116.54 80'
Jan 12 15:40:47.706: INFO: stderr: "+ nc -v -z -w 2 10.98.116.54 80\nConnection to 10.98.116.54 80 port [tcp/http] succeeded!\n"
Jan 12 15:40:47.706: INFO: stdout: ""
Jan 12 15:40:47.706: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8620" for this suite. 01/12/23 15:40:47.722
------------------------------
 [SLOW TEST] [6.359 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:41.366
    Jan 12 15:40:41.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:40:41.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:41.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:41.379
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8620 01/12/23 15:40:41.381
    STEP: changing the ExternalName service to type=ClusterIP 01/12/23 15:40:41.384
    STEP: creating replication controller externalname-service in namespace services-8620 01/12/23 15:40:41.395
    I0112 15:40:41.399985      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8620, replica count: 2
    I0112 15:40:44.451621      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 15:40:44.451: INFO: Creating new exec pod
    Jan 12 15:40:44.455: INFO: Waiting up to 5m0s for pod "execpodxggcs" in namespace "services-8620" to be "running"
    Jan 12 15:40:44.458: INFO: Pod "execpodxggcs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041726ms
    Jan 12 15:40:46.460: INFO: Pod "execpodxggcs": Phase="Running", Reason="", readiness=true. Elapsed: 2.004871828s
    Jan 12 15:40:46.460: INFO: Pod "execpodxggcs" satisfied condition "running"
    Jan 12 15:40:47.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-8620 exec execpodxggcs -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 12 15:40:47.591: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 12 15:40:47.591: INFO: stdout: ""
    Jan 12 15:40:47.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-8620 exec execpodxggcs -- /bin/sh -x -c nc -v -z -w 2 10.98.116.54 80'
    Jan 12 15:40:47.706: INFO: stderr: "+ nc -v -z -w 2 10.98.116.54 80\nConnection to 10.98.116.54 80 port [tcp/http] succeeded!\n"
    Jan 12 15:40:47.706: INFO: stdout: ""
    Jan 12 15:40:47.706: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8620" for this suite. 01/12/23 15:40:47.722
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:47.725
Jan 12 15:40:47.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:40:47.726
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:47.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:47.741
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-a16ed839-e713-43a9-844a-1eaaf57a73f2 01/12/23 15:40:47.743
STEP: Creating a pod to test consume configMaps 01/12/23 15:40:47.746
Jan 12 15:40:47.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e" in namespace "configmap-1056" to be "Succeeded or Failed"
Jan 12 15:40:47.753: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.603303ms
Jan 12 15:40:49.756: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004071419s
Jan 12 15:40:51.757: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004928954s
STEP: Saw pod success 01/12/23 15:40:51.757
Jan 12 15:40:51.757: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e" satisfied condition "Succeeded or Failed"
Jan 12 15:40:51.758: INFO: Trying to get logs from node worker-0 pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e container agnhost-container: <nil>
STEP: delete the pod 01/12/23 15:40:51.77
Jan 12 15:40:51.778: INFO: Waiting for pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e to disappear
Jan 12 15:40:51.781: INFO: Pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:51.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1056" for this suite. 01/12/23 15:40:51.783
------------------------------
 [4.061 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:47.725
    Jan 12 15:40:47.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:40:47.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:47.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:47.741
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-a16ed839-e713-43a9-844a-1eaaf57a73f2 01/12/23 15:40:47.743
    STEP: Creating a pod to test consume configMaps 01/12/23 15:40:47.746
    Jan 12 15:40:47.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e" in namespace "configmap-1056" to be "Succeeded or Failed"
    Jan 12 15:40:47.753: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.603303ms
    Jan 12 15:40:49.756: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004071419s
    Jan 12 15:40:51.757: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004928954s
    STEP: Saw pod success 01/12/23 15:40:51.757
    Jan 12 15:40:51.757: INFO: Pod "pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e" satisfied condition "Succeeded or Failed"
    Jan 12 15:40:51.758: INFO: Trying to get logs from node worker-0 pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 15:40:51.77
    Jan 12 15:40:51.778: INFO: Waiting for pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e to disappear
    Jan 12 15:40:51.781: INFO: Pod pod-configmaps-761e87d6-3c00-4923-a001-d16747ba260e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:51.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1056" for this suite. 01/12/23 15:40:51.783
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:51.787
Jan 12 15:40:51.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 15:40:51.788
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:51.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:51.798
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/12/23 15:40:51.8
STEP: submitting the pod to kubernetes 01/12/23 15:40:51.801
Jan 12 15:40:51.806: INFO: Waiting up to 5m0s for pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" in namespace "pods-3774" to be "running and ready"
Jan 12 15:40:51.808: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742231ms
Jan 12 15:40:51.808: INFO: The phase of Pod pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:40:53.811: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Running", Reason="", readiness=true. Elapsed: 2.004968952s
Jan 12 15:40:53.811: INFO: The phase of Pod pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022 is Running (Ready = true)
Jan 12 15:40:53.811: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/12/23 15:40:53.813
STEP: updating the pod 01/12/23 15:40:53.815
Jan 12 15:40:54.328: INFO: Successfully updated pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022"
Jan 12 15:40:54.328: INFO: Waiting up to 5m0s for pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" in namespace "pods-3774" to be "running"
Jan 12 15:40:54.330: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Running", Reason="", readiness=true. Elapsed: 1.68838ms
Jan 12 15:40:54.330: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/12/23 15:40:54.33
Jan 12 15:40:54.331: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3774" for this suite. 01/12/23 15:40:54.334
------------------------------
 [2.552 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:51.787
    Jan 12 15:40:51.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 15:40:51.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:51.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:51.798
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/12/23 15:40:51.8
    STEP: submitting the pod to kubernetes 01/12/23 15:40:51.801
    Jan 12 15:40:51.806: INFO: Waiting up to 5m0s for pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" in namespace "pods-3774" to be "running and ready"
    Jan 12 15:40:51.808: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Pending", Reason="", readiness=false. Elapsed: 1.742231ms
    Jan 12 15:40:51.808: INFO: The phase of Pod pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:40:53.811: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Running", Reason="", readiness=true. Elapsed: 2.004968952s
    Jan 12 15:40:53.811: INFO: The phase of Pod pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022 is Running (Ready = true)
    Jan 12 15:40:53.811: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/12/23 15:40:53.813
    STEP: updating the pod 01/12/23 15:40:53.815
    Jan 12 15:40:54.328: INFO: Successfully updated pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022"
    Jan 12 15:40:54.328: INFO: Waiting up to 5m0s for pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" in namespace "pods-3774" to be "running"
    Jan 12 15:40:54.330: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022": Phase="Running", Reason="", readiness=true. Elapsed: 1.68838ms
    Jan 12 15:40:54.330: INFO: Pod "pod-update-03b2df35-d22a-43b2-9558-38aa2bd28022" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/12/23 15:40:54.33
    Jan 12 15:40:54.331: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3774" for this suite. 01/12/23 15:40:54.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:54.341
Jan 12 15:40:54.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 15:40:54.342
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:54.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:54.355
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-ppx24" 01/12/23 15:40:54.357
Jan 12 15:40:54.362: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
Jan 12 15:40:55.364: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
Jan 12 15:40:55.366: INFO: Found 1 replicas for "e2e-rc-ppx24" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-ppx24" 01/12/23 15:40:55.366
STEP: Updating a scale subresource 01/12/23 15:40:55.368
STEP: Verifying replicas where modified for replication controller "e2e-rc-ppx24" 01/12/23 15:40:55.372
Jan 12 15:40:55.373: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
Jan 12 15:40:56.374: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
Jan 12 15:40:56.377: INFO: Found 2 replicas for "e2e-rc-ppx24" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6772" for this suite. 01/12/23 15:40:56.379
------------------------------
 [2.042 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:54.341
    Jan 12 15:40:54.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 15:40:54.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:54.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:54.355
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-ppx24" 01/12/23 15:40:54.357
    Jan 12 15:40:54.362: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
    Jan 12 15:40:55.364: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
    Jan 12 15:40:55.366: INFO: Found 1 replicas for "e2e-rc-ppx24" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-ppx24" 01/12/23 15:40:55.366
    STEP: Updating a scale subresource 01/12/23 15:40:55.368
    STEP: Verifying replicas where modified for replication controller "e2e-rc-ppx24" 01/12/23 15:40:55.372
    Jan 12 15:40:55.373: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
    Jan 12 15:40:56.374: INFO: Get Replication Controller "e2e-rc-ppx24" to confirm replicas
    Jan 12 15:40:56.377: INFO: Found 2 replicas for "e2e-rc-ppx24" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6772" for this suite. 01/12/23 15:40:56.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:56.386
Jan 12 15:40:56.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 15:40:56.387
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:56.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:56.398
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/12/23 15:40:56.4
STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 15:40:56.404
STEP: delete the deployment 01/12/23 15:40:56.908
STEP: wait for all rs to be garbage collected 01/12/23 15:40:56.912
STEP: expected 0 rs, got 1 rs 01/12/23 15:40:56.916
STEP: expected 0 pods, got 2 pods 01/12/23 15:40:56.919
STEP: Gathering metrics 01/12/23 15:40:57.426
W0112 15:40:57.429847      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 15:40:57.429: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 15:40:57.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9570" for this suite. 01/12/23 15:40:57.432
------------------------------
 [1.049 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:56.386
    Jan 12 15:40:56.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 15:40:56.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:56.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:56.398
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/12/23 15:40:56.4
    STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 15:40:56.404
    STEP: delete the deployment 01/12/23 15:40:56.908
    STEP: wait for all rs to be garbage collected 01/12/23 15:40:56.912
    STEP: expected 0 rs, got 1 rs 01/12/23 15:40:56.916
    STEP: expected 0 pods, got 2 pods 01/12/23 15:40:56.919
    STEP: Gathering metrics 01/12/23 15:40:57.426
    W0112 15:40:57.429847      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 15:40:57.429: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:40:57.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9570" for this suite. 01/12/23 15:40:57.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:40:57.436
Jan 12 15:40:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename init-container 01/12/23 15:40:57.436
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:57.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:57.445
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/12/23 15:40:57.447
Jan 12 15:40:57.447: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:00.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1019" for this suite. 01/12/23 15:41:00.59
------------------------------
 [3.159 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:40:57.436
    Jan 12 15:40:57.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename init-container 01/12/23 15:40:57.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:40:57.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:40:57.445
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/12/23 15:40:57.447
    Jan 12 15:40:57.447: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:00.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1019" for this suite. 01/12/23 15:41:00.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:00.595
Jan 12 15:41:00.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename limitrange 01/12/23 15:41:00.596
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:00.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:00.605
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-7fchz" in namespace "limitrange-6400" 01/12/23 15:41:00.607
STEP: Creating another limitRange in another namespace 01/12/23 15:41:00.613
Jan 12 15:41:00.620: INFO: Namespace "e2e-limitrange-7fchz-8061" created
Jan 12 15:41:00.620: INFO: Creating LimitRange "e2e-limitrange-7fchz" in namespace "e2e-limitrange-7fchz-8061"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-7fchz" 01/12/23 15:41:00.624
Jan 12 15:41:00.626: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-7fchz" in "limitrange-6400" namespace 01/12/23 15:41:00.626
Jan 12 15:41:00.630: INFO: LimitRange "e2e-limitrange-7fchz" has been patched
STEP: Delete LimitRange "e2e-limitrange-7fchz" by Collection with labelSelector: "e2e-limitrange-7fchz=patched" 01/12/23 15:41:00.63
STEP: Confirm that the limitRange "e2e-limitrange-7fchz" has been deleted 01/12/23 15:41:00.635
Jan 12 15:41:00.635: INFO: Requesting list of LimitRange to confirm quantity
Jan 12 15:41:00.636: INFO: Found 0 LimitRange with label "e2e-limitrange-7fchz=patched"
Jan 12 15:41:00.636: INFO: LimitRange "e2e-limitrange-7fchz" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-7fchz" 01/12/23 15:41:00.636
Jan 12 15:41:00.638: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6400" for this suite. 01/12/23 15:41:00.64
STEP: Destroying namespace "e2e-limitrange-7fchz-8061" for this suite. 01/12/23 15:41:00.643
------------------------------
 [0.051 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:00.595
    Jan 12 15:41:00.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename limitrange 01/12/23 15:41:00.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:00.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:00.605
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-7fchz" in namespace "limitrange-6400" 01/12/23 15:41:00.607
    STEP: Creating another limitRange in another namespace 01/12/23 15:41:00.613
    Jan 12 15:41:00.620: INFO: Namespace "e2e-limitrange-7fchz-8061" created
    Jan 12 15:41:00.620: INFO: Creating LimitRange "e2e-limitrange-7fchz" in namespace "e2e-limitrange-7fchz-8061"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-7fchz" 01/12/23 15:41:00.624
    Jan 12 15:41:00.626: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-7fchz" in "limitrange-6400" namespace 01/12/23 15:41:00.626
    Jan 12 15:41:00.630: INFO: LimitRange "e2e-limitrange-7fchz" has been patched
    STEP: Delete LimitRange "e2e-limitrange-7fchz" by Collection with labelSelector: "e2e-limitrange-7fchz=patched" 01/12/23 15:41:00.63
    STEP: Confirm that the limitRange "e2e-limitrange-7fchz" has been deleted 01/12/23 15:41:00.635
    Jan 12 15:41:00.635: INFO: Requesting list of LimitRange to confirm quantity
    Jan 12 15:41:00.636: INFO: Found 0 LimitRange with label "e2e-limitrange-7fchz=patched"
    Jan 12 15:41:00.636: INFO: LimitRange "e2e-limitrange-7fchz" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-7fchz" 01/12/23 15:41:00.636
    Jan 12 15:41:00.638: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:00.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6400" for this suite. 01/12/23 15:41:00.64
    STEP: Destroying namespace "e2e-limitrange-7fchz-8061" for this suite. 01/12/23 15:41:00.643
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:00.646
Jan 12 15:41:00.647: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 15:41:00.647
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:00.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:00.665
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/12/23 15:41:00.669
Jan 12 15:41:00.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 create -f -'
Jan 12 15:41:00.858: INFO: stderr: ""
Jan 12 15:41:00.858: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:00.858
Jan 12 15:41:00.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 15:41:00.929: INFO: stderr: ""
Jan 12 15:41:00.929: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
Jan 12 15:41:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:00.990: INFO: stderr: ""
Jan 12 15:41:00.990: INFO: stdout: ""
Jan 12 15:41:00.990: INFO: update-demo-nautilus-m6lgn is created but not running
Jan 12 15:41:05.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 15:41:06.050: INFO: stderr: ""
Jan 12 15:41:06.050: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
Jan 12 15:41:06.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:06.106: INFO: stderr: ""
Jan 12 15:41:06.106: INFO: stdout: "true"
Jan 12 15:41:06.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 15:41:06.161: INFO: stderr: ""
Jan 12 15:41:06.161: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 15:41:06.161: INFO: validating pod update-demo-nautilus-m6lgn
Jan 12 15:41:06.166: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 15:41:06.167: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 15:41:06.167: INFO: update-demo-nautilus-m6lgn is verified up and running
Jan 12 15:41:06.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:06.223: INFO: stderr: ""
Jan 12 15:41:06.223: INFO: stdout: "true"
Jan 12 15:41:06.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 15:41:06.279: INFO: stderr: ""
Jan 12 15:41:06.279: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 15:41:06.279: INFO: validating pod update-demo-nautilus-rgx56
Jan 12 15:41:06.284: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 15:41:06.284: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 15:41:06.284: INFO: update-demo-nautilus-rgx56 is verified up and running
STEP: scaling down the replication controller 01/12/23 15:41:06.284
Jan 12 15:41:06.285: INFO: scanned /root for discovery docs: <nil>
Jan 12 15:41:06.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 12 15:41:07.356: INFO: stderr: ""
Jan 12 15:41:07.357: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:07.357
Jan 12 15:41:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 15:41:07.424: INFO: stderr: ""
Jan 12 15:41:07.424: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/12/23 15:41:07.424
Jan 12 15:41:12.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 15:41:12.483: INFO: stderr: ""
Jan 12 15:41:12.483: INFO: stdout: "update-demo-nautilus-rgx56 "
Jan 12 15:41:12.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:12.539: INFO: stderr: ""
Jan 12 15:41:12.539: INFO: stdout: "true"
Jan 12 15:41:12.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 15:41:12.597: INFO: stderr: ""
Jan 12 15:41:12.597: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 15:41:12.597: INFO: validating pod update-demo-nautilus-rgx56
Jan 12 15:41:12.600: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 15:41:12.600: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 15:41:12.600: INFO: update-demo-nautilus-rgx56 is verified up and running
STEP: scaling up the replication controller 01/12/23 15:41:12.6
Jan 12 15:41:12.601: INFO: scanned /root for discovery docs: <nil>
Jan 12 15:41:12.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 12 15:41:13.674: INFO: stderr: ""
Jan 12 15:41:13.674: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:13.674
Jan 12 15:41:13.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 15:41:13.732: INFO: stderr: ""
Jan 12 15:41:13.732: INFO: stdout: "update-demo-nautilus-rgx56 update-demo-nautilus-zjwqd "
Jan 12 15:41:13.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:13.791: INFO: stderr: ""
Jan 12 15:41:13.791: INFO: stdout: "true"
Jan 12 15:41:13.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 15:41:13.847: INFO: stderr: ""
Jan 12 15:41:13.847: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 15:41:13.847: INFO: validating pod update-demo-nautilus-rgx56
Jan 12 15:41:13.850: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 15:41:13.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 15:41:13.850: INFO: update-demo-nautilus-rgx56 is verified up and running
Jan 12 15:41:13.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-zjwqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 15:41:13.907: INFO: stderr: ""
Jan 12 15:41:13.907: INFO: stdout: "true"
Jan 12 15:41:13.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-zjwqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 15:41:13.962: INFO: stderr: ""
Jan 12 15:41:13.963: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 15:41:13.963: INFO: validating pod update-demo-nautilus-zjwqd
Jan 12 15:41:13.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 15:41:13.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 15:41:13.968: INFO: update-demo-nautilus-zjwqd is verified up and running
STEP: using delete to clean up resources 01/12/23 15:41:13.968
Jan 12 15:41:13.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 delete --grace-period=0 --force -f -'
Jan 12 15:41:14.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 15:41:14.028: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 12 15:41:14.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get rc,svc -l name=update-demo --no-headers'
Jan 12 15:41:14.089: INFO: stderr: "No resources found in kubectl-945 namespace.\n"
Jan 12 15:41:14.089: INFO: stdout: ""
Jan 12 15:41:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 15:41:14.148: INFO: stderr: ""
Jan 12 15:41:14.148: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:14.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-945" for this suite. 01/12/23 15:41:14.15
------------------------------
 [SLOW TEST] [13.507 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:00.646
    Jan 12 15:41:00.647: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 15:41:00.647
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:00.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:00.665
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/12/23 15:41:00.669
    Jan 12 15:41:00.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 create -f -'
    Jan 12 15:41:00.858: INFO: stderr: ""
    Jan 12 15:41:00.858: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:00.858
    Jan 12 15:41:00.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 15:41:00.929: INFO: stderr: ""
    Jan 12 15:41:00.929: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
    Jan 12 15:41:00.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:00.990: INFO: stderr: ""
    Jan 12 15:41:00.990: INFO: stdout: ""
    Jan 12 15:41:00.990: INFO: update-demo-nautilus-m6lgn is created but not running
    Jan 12 15:41:05.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 15:41:06.050: INFO: stderr: ""
    Jan 12 15:41:06.050: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
    Jan 12 15:41:06.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:06.106: INFO: stderr: ""
    Jan 12 15:41:06.106: INFO: stdout: "true"
    Jan 12 15:41:06.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-m6lgn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 15:41:06.161: INFO: stderr: ""
    Jan 12 15:41:06.161: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 15:41:06.161: INFO: validating pod update-demo-nautilus-m6lgn
    Jan 12 15:41:06.166: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 15:41:06.167: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 15:41:06.167: INFO: update-demo-nautilus-m6lgn is verified up and running
    Jan 12 15:41:06.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:06.223: INFO: stderr: ""
    Jan 12 15:41:06.223: INFO: stdout: "true"
    Jan 12 15:41:06.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 15:41:06.279: INFO: stderr: ""
    Jan 12 15:41:06.279: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 15:41:06.279: INFO: validating pod update-demo-nautilus-rgx56
    Jan 12 15:41:06.284: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 15:41:06.284: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 15:41:06.284: INFO: update-demo-nautilus-rgx56 is verified up and running
    STEP: scaling down the replication controller 01/12/23 15:41:06.284
    Jan 12 15:41:06.285: INFO: scanned /root for discovery docs: <nil>
    Jan 12 15:41:06.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 12 15:41:07.356: INFO: stderr: ""
    Jan 12 15:41:07.357: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:07.357
    Jan 12 15:41:07.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 15:41:07.424: INFO: stderr: ""
    Jan 12 15:41:07.424: INFO: stdout: "update-demo-nautilus-m6lgn update-demo-nautilus-rgx56 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/12/23 15:41:07.424
    Jan 12 15:41:12.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 15:41:12.483: INFO: stderr: ""
    Jan 12 15:41:12.483: INFO: stdout: "update-demo-nautilus-rgx56 "
    Jan 12 15:41:12.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:12.539: INFO: stderr: ""
    Jan 12 15:41:12.539: INFO: stdout: "true"
    Jan 12 15:41:12.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 15:41:12.597: INFO: stderr: ""
    Jan 12 15:41:12.597: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 15:41:12.597: INFO: validating pod update-demo-nautilus-rgx56
    Jan 12 15:41:12.600: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 15:41:12.600: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 15:41:12.600: INFO: update-demo-nautilus-rgx56 is verified up and running
    STEP: scaling up the replication controller 01/12/23 15:41:12.6
    Jan 12 15:41:12.601: INFO: scanned /root for discovery docs: <nil>
    Jan 12 15:41:12.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 12 15:41:13.674: INFO: stderr: ""
    Jan 12 15:41:13.674: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 15:41:13.674
    Jan 12 15:41:13.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 15:41:13.732: INFO: stderr: ""
    Jan 12 15:41:13.732: INFO: stdout: "update-demo-nautilus-rgx56 update-demo-nautilus-zjwqd "
    Jan 12 15:41:13.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:13.791: INFO: stderr: ""
    Jan 12 15:41:13.791: INFO: stdout: "true"
    Jan 12 15:41:13.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-rgx56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 15:41:13.847: INFO: stderr: ""
    Jan 12 15:41:13.847: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 15:41:13.847: INFO: validating pod update-demo-nautilus-rgx56
    Jan 12 15:41:13.850: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 15:41:13.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 15:41:13.850: INFO: update-demo-nautilus-rgx56 is verified up and running
    Jan 12 15:41:13.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-zjwqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 15:41:13.907: INFO: stderr: ""
    Jan 12 15:41:13.907: INFO: stdout: "true"
    Jan 12 15:41:13.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods update-demo-nautilus-zjwqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 15:41:13.962: INFO: stderr: ""
    Jan 12 15:41:13.963: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 15:41:13.963: INFO: validating pod update-demo-nautilus-zjwqd
    Jan 12 15:41:13.968: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 15:41:13.968: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 15:41:13.968: INFO: update-demo-nautilus-zjwqd is verified up and running
    STEP: using delete to clean up resources 01/12/23 15:41:13.968
    Jan 12 15:41:13.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 delete --grace-period=0 --force -f -'
    Jan 12 15:41:14.028: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 15:41:14.028: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 12 15:41:14.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get rc,svc -l name=update-demo --no-headers'
    Jan 12 15:41:14.089: INFO: stderr: "No resources found in kubectl-945 namespace.\n"
    Jan 12 15:41:14.089: INFO: stdout: ""
    Jan 12 15:41:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-945 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 15:41:14.148: INFO: stderr: ""
    Jan 12 15:41:14.148: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:14.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-945" for this suite. 01/12/23 15:41:14.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:14.155
Jan 12 15:41:14.155: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 15:41:14.156
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:14.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:14.167
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-805" 01/12/23 15:41:14.169
Jan 12 15:41:14.176: INFO: Namespace "namespaces-805" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"40cab34b-98f1-488c-8c7d-dad3d0da3836", "kubernetes.io/metadata.name":"namespaces-805", "namespaces-805":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:14.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-805" for this suite. 01/12/23 15:41:14.178
------------------------------
 [0.028 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:14.155
    Jan 12 15:41:14.155: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 15:41:14.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:14.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:14.167
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-805" 01/12/23 15:41:14.169
    Jan 12 15:41:14.176: INFO: Namespace "namespaces-805" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"40cab34b-98f1-488c-8c7d-dad3d0da3836", "kubernetes.io/metadata.name":"namespaces-805", "namespaces-805":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:14.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-805" for this suite. 01/12/23 15:41:14.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:14.183
Jan 12 15:41:14.183: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 15:41:14.184
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:14.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:14.194
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/12/23 15:41:14.196
Jan 12 15:41:14.201: INFO: Waiting up to 5m0s for pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5" in namespace "downward-api-2398" to be "Succeeded or Failed"
Jan 12 15:41:14.202: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727753ms
Jan 12 15:41:16.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005367538s
Jan 12 15:41:18.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005535408s
STEP: Saw pod success 01/12/23 15:41:18.206
Jan 12 15:41:18.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5" satisfied condition "Succeeded or Failed"
Jan 12 15:41:18.208: INFO: Trying to get logs from node worker-1 pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 container dapi-container: <nil>
STEP: delete the pod 01/12/23 15:41:18.214
Jan 12 15:41:18.223: INFO: Waiting for pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 to disappear
Jan 12 15:41:18.225: INFO: Pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:18.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2398" for this suite. 01/12/23 15:41:18.227
------------------------------
 [4.047 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:14.183
    Jan 12 15:41:14.183: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 15:41:14.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:14.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:14.194
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/12/23 15:41:14.196
    Jan 12 15:41:14.201: INFO: Waiting up to 5m0s for pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5" in namespace "downward-api-2398" to be "Succeeded or Failed"
    Jan 12 15:41:14.202: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727753ms
    Jan 12 15:41:16.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005367538s
    Jan 12 15:41:18.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005535408s
    STEP: Saw pod success 01/12/23 15:41:18.206
    Jan 12 15:41:18.206: INFO: Pod "downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5" satisfied condition "Succeeded or Failed"
    Jan 12 15:41:18.208: INFO: Trying to get logs from node worker-1 pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 15:41:18.214
    Jan 12 15:41:18.223: INFO: Waiting for pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 to disappear
    Jan 12 15:41:18.225: INFO: Pod downward-api-f1f4c812-d98f-4b9e-83c6-7278240865b5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:18.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2398" for this suite. 01/12/23 15:41:18.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:18.232
Jan 12 15:41:18.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 15:41:18.233
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:18.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:18.243
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-f7a793f5-5fa2-4527-903f-bbe12ab4ac9d 01/12/23 15:41:18.245
STEP: Creating a pod to test consume secrets 01/12/23 15:41:18.248
Jan 12 15:41:18.253: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7" in namespace "secrets-1121" to be "Succeeded or Failed"
Jan 12 15:41:18.256: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198299ms
Jan 12 15:41:20.259: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005760592s
Jan 12 15:41:22.260: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006366295s
STEP: Saw pod success 01/12/23 15:41:22.26
Jan 12 15:41:22.260: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7" satisfied condition "Succeeded or Failed"
Jan 12 15:41:22.261: INFO: Trying to get logs from node worker-1 pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 15:41:22.266
Jan 12 15:41:22.275: INFO: Waiting for pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 to disappear
Jan 12 15:41:22.276: INFO: Pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:22.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1121" for this suite. 01/12/23 15:41:22.279
------------------------------
 [4.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:18.232
    Jan 12 15:41:18.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 15:41:18.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:18.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:18.243
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-f7a793f5-5fa2-4527-903f-bbe12ab4ac9d 01/12/23 15:41:18.245
    STEP: Creating a pod to test consume secrets 01/12/23 15:41:18.248
    Jan 12 15:41:18.253: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7" in namespace "secrets-1121" to be "Succeeded or Failed"
    Jan 12 15:41:18.256: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198299ms
    Jan 12 15:41:20.259: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005760592s
    Jan 12 15:41:22.260: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006366295s
    STEP: Saw pod success 01/12/23 15:41:22.26
    Jan 12 15:41:22.260: INFO: Pod "pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7" satisfied condition "Succeeded or Failed"
    Jan 12 15:41:22.261: INFO: Trying to get logs from node worker-1 pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:41:22.266
    Jan 12 15:41:22.275: INFO: Waiting for pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 to disappear
    Jan 12 15:41:22.276: INFO: Pod pod-secrets-2dc7c12a-d51f-4673-9ce4-22312334a6e7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:22.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1121" for this suite. 01/12/23 15:41:22.279
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:22.284
Jan 12 15:41:22.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename certificates 01/12/23 15:41:22.285
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.296
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/12/23 15:41:22.829
STEP: getting /apis/certificates.k8s.io 01/12/23 15:41:22.832
STEP: getting /apis/certificates.k8s.io/v1 01/12/23 15:41:22.833
STEP: creating 01/12/23 15:41:22.834
STEP: getting 01/12/23 15:41:22.844
STEP: listing 01/12/23 15:41:22.846
STEP: watching 01/12/23 15:41:22.848
Jan 12 15:41:22.848: INFO: starting watch
STEP: patching 01/12/23 15:41:22.849
STEP: updating 01/12/23 15:41:22.854
Jan 12 15:41:22.858: INFO: waiting for watch events with expected annotations
Jan 12 15:41:22.858: INFO: saw patched and updated annotations
STEP: getting /approval 01/12/23 15:41:22.858
STEP: patching /approval 01/12/23 15:41:22.86
STEP: updating /approval 01/12/23 15:41:22.865
STEP: getting /status 01/12/23 15:41:22.869
STEP: patching /status 01/12/23 15:41:22.871
STEP: updating /status 01/12/23 15:41:22.877
STEP: deleting 01/12/23 15:41:22.883
STEP: deleting a collection 01/12/23 15:41:22.889
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-8923" for this suite. 01/12/23 15:41:22.899
------------------------------
 [0.620 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:22.284
    Jan 12 15:41:22.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename certificates 01/12/23 15:41:22.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.296
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/12/23 15:41:22.829
    STEP: getting /apis/certificates.k8s.io 01/12/23 15:41:22.832
    STEP: getting /apis/certificates.k8s.io/v1 01/12/23 15:41:22.833
    STEP: creating 01/12/23 15:41:22.834
    STEP: getting 01/12/23 15:41:22.844
    STEP: listing 01/12/23 15:41:22.846
    STEP: watching 01/12/23 15:41:22.848
    Jan 12 15:41:22.848: INFO: starting watch
    STEP: patching 01/12/23 15:41:22.849
    STEP: updating 01/12/23 15:41:22.854
    Jan 12 15:41:22.858: INFO: waiting for watch events with expected annotations
    Jan 12 15:41:22.858: INFO: saw patched and updated annotations
    STEP: getting /approval 01/12/23 15:41:22.858
    STEP: patching /approval 01/12/23 15:41:22.86
    STEP: updating /approval 01/12/23 15:41:22.865
    STEP: getting /status 01/12/23 15:41:22.869
    STEP: patching /status 01/12/23 15:41:22.871
    STEP: updating /status 01/12/23 15:41:22.877
    STEP: deleting 01/12/23 15:41:22.883
    STEP: deleting a collection 01/12/23 15:41:22.889
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-8923" for this suite. 01/12/23 15:41:22.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:22.906
Jan 12 15:41:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename endpointslice 01/12/23 15:41:22.906
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.916
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/12/23 15:41:22.918
STEP: getting /apis/discovery.k8s.io 01/12/23 15:41:22.92
STEP: getting /apis/discovery.k8s.iov1 01/12/23 15:41:22.921
STEP: creating 01/12/23 15:41:22.922
STEP: getting 01/12/23 15:41:22.932
STEP: listing 01/12/23 15:41:22.934
STEP: watching 01/12/23 15:41:22.935
Jan 12 15:41:22.935: INFO: starting watch
STEP: cluster-wide listing 01/12/23 15:41:22.936
STEP: cluster-wide watching 01/12/23 15:41:22.938
Jan 12 15:41:22.938: INFO: starting watch
STEP: patching 01/12/23 15:41:22.939
STEP: updating 01/12/23 15:41:22.942
Jan 12 15:41:22.947: INFO: waiting for watch events with expected annotations
Jan 12 15:41:22.947: INFO: saw patched and updated annotations
STEP: deleting 01/12/23 15:41:22.947
STEP: deleting a collection 01/12/23 15:41:22.953
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:22.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7003" for this suite. 01/12/23 15:41:22.964
------------------------------
 [0.062 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:22.906
    Jan 12 15:41:22.906: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename endpointslice 01/12/23 15:41:22.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.916
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/12/23 15:41:22.918
    STEP: getting /apis/discovery.k8s.io 01/12/23 15:41:22.92
    STEP: getting /apis/discovery.k8s.iov1 01/12/23 15:41:22.921
    STEP: creating 01/12/23 15:41:22.922
    STEP: getting 01/12/23 15:41:22.932
    STEP: listing 01/12/23 15:41:22.934
    STEP: watching 01/12/23 15:41:22.935
    Jan 12 15:41:22.935: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 15:41:22.936
    STEP: cluster-wide watching 01/12/23 15:41:22.938
    Jan 12 15:41:22.938: INFO: starting watch
    STEP: patching 01/12/23 15:41:22.939
    STEP: updating 01/12/23 15:41:22.942
    Jan 12 15:41:22.947: INFO: waiting for watch events with expected annotations
    Jan 12 15:41:22.947: INFO: saw patched and updated annotations
    STEP: deleting 01/12/23 15:41:22.947
    STEP: deleting a collection 01/12/23 15:41:22.953
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:22.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7003" for this suite. 01/12/23 15:41:22.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:22.971
Jan 12 15:41:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 15:41:22.971
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.98
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/12/23 15:41:22.982
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.99
STEP: Creating a service in the namespace 01/12/23 15:41:22.992
STEP: Deleting the namespace 01/12/23 15:41:23
STEP: Waiting for the namespace to be removed. 01/12/23 15:41:23.006
STEP: Recreating the namespace 01/12/23 15:41:29.009
STEP: Verifying there is no service in the namespace 01/12/23 15:41:29.018
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4504" for this suite. 01/12/23 15:41:29.022
STEP: Destroying namespace "nsdeletetest-9426" for this suite. 01/12/23 15:41:29.025
Jan 12 15:41:29.027: INFO: Namespace nsdeletetest-9426 was already deleted
STEP: Destroying namespace "nsdeletetest-495" for this suite. 01/12/23 15:41:29.027
------------------------------
 [SLOW TEST] [6.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:22.971
    Jan 12 15:41:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 15:41:22.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:22.98
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/12/23 15:41:22.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:22.99
    STEP: Creating a service in the namespace 01/12/23 15:41:22.992
    STEP: Deleting the namespace 01/12/23 15:41:23
    STEP: Waiting for the namespace to be removed. 01/12/23 15:41:23.006
    STEP: Recreating the namespace 01/12/23 15:41:29.009
    STEP: Verifying there is no service in the namespace 01/12/23 15:41:29.018
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4504" for this suite. 01/12/23 15:41:29.022
    STEP: Destroying namespace "nsdeletetest-9426" for this suite. 01/12/23 15:41:29.025
    Jan 12 15:41:29.027: INFO: Namespace nsdeletetest-9426 was already deleted
    STEP: Destroying namespace "nsdeletetest-495" for this suite. 01/12/23 15:41:29.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:29.032
Jan 12 15:41:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:41:29.033
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:29.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:29.042
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-2ccc4092-a466-424b-af6f-b9b39d1627d8 01/12/23 15:41:29.044
STEP: Creating a pod to test consume secrets 01/12/23 15:41:29.047
Jan 12 15:41:29.052: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7" in namespace "projected-825" to be "Succeeded or Failed"
Jan 12 15:41:29.054: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.835278ms
Jan 12 15:41:31.056: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00452269s
Jan 12 15:41:33.057: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004728133s
STEP: Saw pod success 01/12/23 15:41:33.057
Jan 12 15:41:33.057: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7" satisfied condition "Succeeded or Failed"
Jan 12 15:41:33.058: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 15:41:33.063
Jan 12 15:41:33.071: INFO: Waiting for pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 to disappear
Jan 12 15:41:33.073: INFO: Pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 15:41:33.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-825" for this suite. 01/12/23 15:41:33.075
------------------------------
 [4.047 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:29.032
    Jan 12 15:41:29.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:41:29.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:29.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:29.042
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-2ccc4092-a466-424b-af6f-b9b39d1627d8 01/12/23 15:41:29.044
    STEP: Creating a pod to test consume secrets 01/12/23 15:41:29.047
    Jan 12 15:41:29.052: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7" in namespace "projected-825" to be "Succeeded or Failed"
    Jan 12 15:41:29.054: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.835278ms
    Jan 12 15:41:31.056: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00452269s
    Jan 12 15:41:33.057: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004728133s
    STEP: Saw pod success 01/12/23 15:41:33.057
    Jan 12 15:41:33.057: INFO: Pod "pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7" satisfied condition "Succeeded or Failed"
    Jan 12 15:41:33.058: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:41:33.063
    Jan 12 15:41:33.071: INFO: Waiting for pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 to disappear
    Jan 12 15:41:33.073: INFO: Pod pod-projected-secrets-595c013b-6cd4-46b6-b581-1d02581a35d7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:41:33.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-825" for this suite. 01/12/23 15:41:33.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:41:33.081
Jan 12 15:41:33.081: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption 01/12/23 15:41:33.081
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:33.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:33.093
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 15:41:33.104: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 15:42:33.116: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/12/23 15:42:33.118
Jan 12 15:42:33.131: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 12 15:42:33.136: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 12 15:42:33.152: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 12 15:42:33.160: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/12/23 15:42:33.16
Jan 12 15:42:33.160: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-952" to be "running"
Jan 12 15:42:33.162: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990279ms
Jan 12 15:42:35.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004705051s
Jan 12 15:42:37.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005149791s
Jan 12 15:42:37.165: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 12 15:42:37.165: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
Jan 12 15:42:37.167: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.634445ms
Jan 12 15:42:37.167: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 15:42:37.167: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
Jan 12 15:42:37.168: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.7448ms
Jan 12 15:42:37.168: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 15:42:37.168: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
Jan 12 15:42:37.170: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.553524ms
Jan 12 15:42:37.170: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/12/23 15:42:37.17
Jan 12 15:42:37.174: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-952" to be "running"
Jan 12 15:42:37.176: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926731ms
Jan 12 15:42:39.178: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004777289s
Jan 12 15:42:41.178: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004684572s
Jan 12 15:42:43.180: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005875412s
Jan 12 15:42:43.180: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:42:43.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-952" for this suite. 01/12/23 15:42:43.208
------------------------------
 [SLOW TEST] [70.131 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:41:33.081
    Jan 12 15:41:33.081: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 15:41:33.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:41:33.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:41:33.093
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 15:41:33.104: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 15:42:33.116: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/12/23 15:42:33.118
    Jan 12 15:42:33.131: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 12 15:42:33.136: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 12 15:42:33.152: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 12 15:42:33.160: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/12/23 15:42:33.16
    Jan 12 15:42:33.160: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-952" to be "running"
    Jan 12 15:42:33.162: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990279ms
    Jan 12 15:42:35.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004705051s
    Jan 12 15:42:37.165: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.005149791s
    Jan 12 15:42:37.165: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 12 15:42:37.165: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
    Jan 12 15:42:37.167: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.634445ms
    Jan 12 15:42:37.167: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 15:42:37.167: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
    Jan 12 15:42:37.168: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.7448ms
    Jan 12 15:42:37.168: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 15:42:37.168: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-952" to be "running"
    Jan 12 15:42:37.170: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.553524ms
    Jan 12 15:42:37.170: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/12/23 15:42:37.17
    Jan 12 15:42:37.174: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-952" to be "running"
    Jan 12 15:42:37.176: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926731ms
    Jan 12 15:42:39.178: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004777289s
    Jan 12 15:42:41.178: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004684572s
    Jan 12 15:42:43.180: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005875412s
    Jan 12 15:42:43.180: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:42:43.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-952" for this suite. 01/12/23 15:42:43.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:42:43.213
Jan 12 15:42:43.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 15:42:43.214
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:43.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:43.226
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/12/23 15:42:43.228
STEP: Ensure pods equal to parallelism count is attached to the job 01/12/23 15:42:43.233
STEP: patching /status 01/12/23 15:42:45.237
STEP: updating /status 01/12/23 15:42:45.243
STEP: get /status 01/12/23 15:42:45.266
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 15:42:45.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4882" for this suite. 01/12/23 15:42:45.272
------------------------------
 [2.063 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:42:43.213
    Jan 12 15:42:43.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 15:42:43.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:43.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:43.226
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/12/23 15:42:43.228
    STEP: Ensure pods equal to parallelism count is attached to the job 01/12/23 15:42:43.233
    STEP: patching /status 01/12/23 15:42:45.237
    STEP: updating /status 01/12/23 15:42:45.243
    STEP: get /status 01/12/23 15:42:45.266
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:42:45.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4882" for this suite. 01/12/23 15:42:45.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:42:45.277
Jan 12 15:42:45.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 15:42:45.278
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:45.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:45.289
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 15:42:45.291
Jan 12 15:42:45.298: INFO: Waiting up to 5m0s for pod "pod-5ac674fb-e7a8-4273-8012-970617d21164" in namespace "emptydir-1529" to be "Succeeded or Failed"
Jan 12 15:42:45.300: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848064ms
Jan 12 15:42:47.303: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895916s
Jan 12 15:42:49.302: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004011497s
STEP: Saw pod success 01/12/23 15:42:49.302
Jan 12 15:42:49.302: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164" satisfied condition "Succeeded or Failed"
Jan 12 15:42:49.304: INFO: Trying to get logs from node worker-1 pod pod-5ac674fb-e7a8-4273-8012-970617d21164 container test-container: <nil>
STEP: delete the pod 01/12/23 15:42:49.309
Jan 12 15:42:49.316: INFO: Waiting for pod pod-5ac674fb-e7a8-4273-8012-970617d21164 to disappear
Jan 12 15:42:49.318: INFO: Pod pod-5ac674fb-e7a8-4273-8012-970617d21164 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 15:42:49.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1529" for this suite. 01/12/23 15:42:49.321
------------------------------
 [4.050 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:42:45.277
    Jan 12 15:42:45.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 15:42:45.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:45.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:45.289
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 15:42:45.291
    Jan 12 15:42:45.298: INFO: Waiting up to 5m0s for pod "pod-5ac674fb-e7a8-4273-8012-970617d21164" in namespace "emptydir-1529" to be "Succeeded or Failed"
    Jan 12 15:42:45.300: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.848064ms
    Jan 12 15:42:47.303: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895916s
    Jan 12 15:42:49.302: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004011497s
    STEP: Saw pod success 01/12/23 15:42:49.302
    Jan 12 15:42:49.302: INFO: Pod "pod-5ac674fb-e7a8-4273-8012-970617d21164" satisfied condition "Succeeded or Failed"
    Jan 12 15:42:49.304: INFO: Trying to get logs from node worker-1 pod pod-5ac674fb-e7a8-4273-8012-970617d21164 container test-container: <nil>
    STEP: delete the pod 01/12/23 15:42:49.309
    Jan 12 15:42:49.316: INFO: Waiting for pod pod-5ac674fb-e7a8-4273-8012-970617d21164 to disappear
    Jan 12 15:42:49.318: INFO: Pod pod-5ac674fb-e7a8-4273-8012-970617d21164 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:42:49.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1529" for this suite. 01/12/23 15:42:49.321
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:42:49.329
Jan 12 15:42:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 15:42:49.33
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:49.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:49.341
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/12/23 15:42:49.344
STEP: Ensuring ResourceQuota status is calculated 01/12/23 15:42:49.349
STEP: Creating a ResourceQuota with not terminating scope 01/12/23 15:42:51.353
STEP: Ensuring ResourceQuota status is calculated 01/12/23 15:42:51.356
STEP: Creating a long running pod 01/12/23 15:42:53.358
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/12/23 15:42:53.367
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/12/23 15:42:55.369
STEP: Deleting the pod 01/12/23 15:42:57.372
STEP: Ensuring resource quota status released the pod usage 01/12/23 15:42:57.38
STEP: Creating a terminating pod 01/12/23 15:42:59.383
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/12/23 15:42:59.391
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/12/23 15:43:01.395
STEP: Deleting the pod 01/12/23 15:43:03.398
STEP: Ensuring resource quota status released the pod usage 01/12/23 15:43:03.408
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 15:43:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1049" for this suite. 01/12/23 15:43:05.413
------------------------------
 [SLOW TEST] [16.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:42:49.329
    Jan 12 15:42:49.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 15:42:49.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:42:49.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:42:49.341
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/12/23 15:42:49.344
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 15:42:49.349
    STEP: Creating a ResourceQuota with not terminating scope 01/12/23 15:42:51.353
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 15:42:51.356
    STEP: Creating a long running pod 01/12/23 15:42:53.358
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/12/23 15:42:53.367
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/12/23 15:42:55.369
    STEP: Deleting the pod 01/12/23 15:42:57.372
    STEP: Ensuring resource quota status released the pod usage 01/12/23 15:42:57.38
    STEP: Creating a terminating pod 01/12/23 15:42:59.383
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/12/23 15:42:59.391
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/12/23 15:43:01.395
    STEP: Deleting the pod 01/12/23 15:43:03.398
    STEP: Ensuring resource quota status released the pod usage 01/12/23 15:43:03.408
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:43:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1049" for this suite. 01/12/23 15:43:05.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:43:05.42
Jan 12 15:43:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 15:43:05.42
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:05.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:05.429
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/12/23 15:43:05.432
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_udp@PTR;check="$$(dig +tcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_tcp@PTR;sleep 1; done
 01/12/23 15:43:05.446
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_udp@PTR;check="$$(dig +tcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_tcp@PTR;sleep 1; done
 01/12/23 15:43:05.447
STEP: creating a pod to probe DNS 01/12/23 15:43:05.447
STEP: submitting the pod to kubernetes 01/12/23 15:43:05.448
Jan 12 15:43:05.457: INFO: Waiting up to 15m0s for pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9" in namespace "dns-3241" to be "running"
Jan 12 15:43:05.459: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.801199ms
Jan 12 15:43:07.462: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.0049986s
Jan 12 15:43:07.462: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9" satisfied condition "running"
STEP: retrieving the pod 01/12/23 15:43:07.462
STEP: looking for the results for each expected name from probers 01/12/23 15:43:07.465
Jan 12 15:43:07.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.478: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.497: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:07.507: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:12.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:12.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:12.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:12.540: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:12.551: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:17.516: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:17.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:17.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:17.540: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:17.550: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:22.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:22.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:22.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:22.541: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:22.551: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:27.516: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:27.519: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:27.537: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:27.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:27.550: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:32.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:32.520: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:32.536: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:32.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
Jan 12 15:43:32.548: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

Jan 12 15:43:37.546: INFO: DNS probes using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 succeeded

STEP: deleting the pod 01/12/23 15:43:37.546
STEP: deleting the test service 01/12/23 15:43:37.561
STEP: deleting the test headless service 01/12/23 15:43:37.593
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 15:43:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3241" for this suite. 01/12/23 15:43:37.604
------------------------------
 [SLOW TEST] [32.188 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:43:05.42
    Jan 12 15:43:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 15:43:05.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:05.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:05.429
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/12/23 15:43:05.432
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_udp@PTR;check="$$(dig +tcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_tcp@PTR;sleep 1; done
     01/12/23 15:43:05.446
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3241.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3241.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3241.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_udp@PTR;check="$$(dig +tcp +noall +answer +search 65.70.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.70.65_tcp@PTR;sleep 1; done
     01/12/23 15:43:05.447
    STEP: creating a pod to probe DNS 01/12/23 15:43:05.447
    STEP: submitting the pod to kubernetes 01/12/23 15:43:05.448
    Jan 12 15:43:05.457: INFO: Waiting up to 15m0s for pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9" in namespace "dns-3241" to be "running"
    Jan 12 15:43:05.459: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.801199ms
    Jan 12 15:43:07.462: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.0049986s
    Jan 12 15:43:07.462: INFO: Pod "dns-test-eada351d-b9fe-4310-bab8-d366072a38e9" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 15:43:07.462
    STEP: looking for the results for each expected name from probers 01/12/23 15:43:07.465
    Jan 12 15:43:07.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.478: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.495: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.497: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:07.507: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_tcp@dns-test-service.dns-3241.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:12.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:12.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:12.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:12.540: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:12.551: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:17.516: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:17.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:17.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:17.540: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:17.550: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:22.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:22.521: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:22.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:22.541: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:22.551: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:27.516: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:27.519: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:27.537: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:27.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:27.550: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:32.518: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:32.520: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:32.536: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:32.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local from pod dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9: the server could not find the requested resource (get pods dns-test-eada351d-b9fe-4310-bab8-d366072a38e9)
    Jan 12 15:43:32.548: INFO: Lookups using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3241.svc.cluster.local]

    Jan 12 15:43:37.546: INFO: DNS probes using dns-3241/dns-test-eada351d-b9fe-4310-bab8-d366072a38e9 succeeded

    STEP: deleting the pod 01/12/23 15:43:37.546
    STEP: deleting the test service 01/12/23 15:43:37.561
    STEP: deleting the test headless service 01/12/23 15:43:37.593
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:43:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3241" for this suite. 01/12/23 15:43:37.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:43:37.61
Jan 12 15:43:37.610: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 15:43:37.611
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:37.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:37.622
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 15:43:37.632
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 15:43:37.635
Jan 12 15:43:37.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 15:43:37.641: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:38.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 15:43:38.645: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:39.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 15:43:39.646: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/12/23 15:43:39.647
Jan 12 15:43:39.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 15:43:39.658: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:40.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 15:43:40.663: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:41.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 15:43:41.663: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:42.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 15:43:42.664: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:43:43.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 15:43:43.663: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 15:43:43.665
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1443, will wait for the garbage collector to delete the pods 01/12/23 15:43:43.665
Jan 12 15:43:43.721: INFO: Deleting DaemonSet.extensions daemon-set took: 4.122276ms
Jan 12 15:43:43.822: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788216ms
Jan 12 15:43:45.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 15:43:45.824: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 15:43:45.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8666"},"items":null}

Jan 12 15:43:45.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8666"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:43:45.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1443" for this suite. 01/12/23 15:43:45.836
------------------------------
 [SLOW TEST] [8.231 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:43:37.61
    Jan 12 15:43:37.610: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 15:43:37.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:37.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:37.622
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 15:43:37.632
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 15:43:37.635
    Jan 12 15:43:37.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 15:43:37.641: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:38.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 15:43:38.645: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:39.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 15:43:39.646: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/12/23 15:43:39.647
    Jan 12 15:43:39.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 15:43:39.658: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:40.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 15:43:40.663: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:41.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 15:43:41.663: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:42.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 15:43:42.664: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:43:43.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 15:43:43.663: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 15:43:43.665
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1443, will wait for the garbage collector to delete the pods 01/12/23 15:43:43.665
    Jan 12 15:43:43.721: INFO: Deleting DaemonSet.extensions daemon-set took: 4.122276ms
    Jan 12 15:43:43.822: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788216ms
    Jan 12 15:43:45.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 15:43:45.824: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 15:43:45.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8666"},"items":null}

    Jan 12 15:43:45.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8666"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:43:45.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1443" for this suite. 01/12/23 15:43:45.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:43:45.842
Jan 12 15:43:45.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 15:43:45.843
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:45.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:45.852
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 in namespace container-probe-1224 01/12/23 15:43:45.855
Jan 12 15:43:45.861: INFO: Waiting up to 5m0s for pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647" in namespace "container-probe-1224" to be "not pending"
Jan 12 15:43:45.863: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799604ms
Jan 12 15:43:47.866: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647": Phase="Running", Reason="", readiness=true. Elapsed: 2.004684254s
Jan 12 15:43:47.866: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647" satisfied condition "not pending"
Jan 12 15:43:47.866: INFO: Started pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 in namespace container-probe-1224
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:43:47.866
Jan 12 15:43:47.868: INFO: Initial restart count of pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 is 0
Jan 12 15:44:37.954: INFO: Restart count of pod container-probe-1224/busybox-9554c5d6-805b-4220-ad86-70e115acb647 is now 1 (50.086447224s elapsed)
STEP: deleting the pod 01/12/23 15:44:37.954
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 15:44:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1224" for this suite. 01/12/23 15:44:37.97
------------------------------
 [SLOW TEST] [52.132 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:43:45.842
    Jan 12 15:43:45.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 15:43:45.843
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:43:45.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:43:45.852
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 in namespace container-probe-1224 01/12/23 15:43:45.855
    Jan 12 15:43:45.861: INFO: Waiting up to 5m0s for pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647" in namespace "container-probe-1224" to be "not pending"
    Jan 12 15:43:45.863: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799604ms
    Jan 12 15:43:47.866: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647": Phase="Running", Reason="", readiness=true. Elapsed: 2.004684254s
    Jan 12 15:43:47.866: INFO: Pod "busybox-9554c5d6-805b-4220-ad86-70e115acb647" satisfied condition "not pending"
    Jan 12 15:43:47.866: INFO: Started pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 in namespace container-probe-1224
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:43:47.866
    Jan 12 15:43:47.868: INFO: Initial restart count of pod busybox-9554c5d6-805b-4220-ad86-70e115acb647 is 0
    Jan 12 15:44:37.954: INFO: Restart count of pod container-probe-1224/busybox-9554c5d6-805b-4220-ad86-70e115acb647 is now 1 (50.086447224s elapsed)
    STEP: deleting the pod 01/12/23 15:44:37.954
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:44:37.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1224" for this suite. 01/12/23 15:44:37.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:44:37.978
Jan 12 15:44:37.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 15:44:37.979
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:37.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:37.99
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 15:44:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7260" for this suite. 01/12/23 15:44:38.021
------------------------------
 [0.048 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:44:37.978
    Jan 12 15:44:37.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 15:44:37.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:37.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:37.99
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:44:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7260" for this suite. 01/12/23 15:44:38.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:44:38.027
Jan 12 15:44:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 15:44:38.028
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:38.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:38.038
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 12 15:44:38.040: INFO: Creating ReplicaSet my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583
Jan 12 15:44:38.048: INFO: Pod name my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Found 0 pods out of 1
Jan 12 15:44:43.052: INFO: Pod name my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Found 1 pods out of 1
Jan 12 15:44:43.052: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583" is running
Jan 12 15:44:43.052: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" in namespace "replicaset-5368" to be "running"
Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.408391ms
Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" satisfied condition "running"
Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:38 +0000 UTC Reason: Message:}])
Jan 12 15:44:43.054: INFO: Trying to dial the pod
Jan 12 15:44:48.062: INFO: Controller my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Got expected result from replica 1 [my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc]: "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 15:44:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5368" for this suite. 01/12/23 15:44:48.065
------------------------------
 [SLOW TEST] [10.040 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:44:38.027
    Jan 12 15:44:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 15:44:38.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:38.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:38.038
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 12 15:44:38.040: INFO: Creating ReplicaSet my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583
    Jan 12 15:44:38.048: INFO: Pod name my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Found 0 pods out of 1
    Jan 12 15:44:43.052: INFO: Pod name my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Found 1 pods out of 1
    Jan 12 15:44:43.052: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583" is running
    Jan 12 15:44:43.052: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" in namespace "replicaset-5368" to be "running"
    Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.408391ms
    Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" satisfied condition "running"
    Jan 12 15:44:43.054: INFO: Pod "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 15:44:38 +0000 UTC Reason: Message:}])
    Jan 12 15:44:43.054: INFO: Trying to dial the pod
    Jan 12 15:44:48.062: INFO: Controller my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583: Got expected result from replica 1 [my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc]: "my-hostname-basic-f1e6a2ea-084c-48bf-ab30-356eb8d19583-swtbc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:44:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5368" for this suite. 01/12/23 15:44:48.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:44:48.07
Jan 12 15:44:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-webhook 01/12/23 15:44:48.07
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:48.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:48.082
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/12/23 15:44:48.084
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 15:44:48.518
STEP: Deploying the custom resource conversion webhook pod 01/12/23 15:44:48.523
STEP: Wait for the deployment to be ready 01/12/23 15:44:48.533
Jan 12 15:44:48.537: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 15:44:50.544
STEP: Verifying the service has paired with the endpoint 01/12/23 15:44:50.553
Jan 12 15:44:51.554: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 12 15:44:51.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Creating a v1 custom resource 01/12/23 15:44:54.14
STEP: Create a v2 custom resource 01/12/23 15:44:54.156
STEP: List CRs in v1 01/12/23 15:44:54.228
STEP: List CRs in v2 01/12/23 15:44:54.237
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:44:54.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5536" for this suite. 01/12/23 15:44:54.787
------------------------------
 [SLOW TEST] [6.724 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:44:48.07
    Jan 12 15:44:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-webhook 01/12/23 15:44:48.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:48.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:48.082
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/12/23 15:44:48.084
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 15:44:48.518
    STEP: Deploying the custom resource conversion webhook pod 01/12/23 15:44:48.523
    STEP: Wait for the deployment to be ready 01/12/23 15:44:48.533
    Jan 12 15:44:48.537: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 15:44:50.544
    STEP: Verifying the service has paired with the endpoint 01/12/23 15:44:50.553
    Jan 12 15:44:51.554: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 12 15:44:51.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Creating a v1 custom resource 01/12/23 15:44:54.14
    STEP: Create a v2 custom resource 01/12/23 15:44:54.156
    STEP: List CRs in v1 01/12/23 15:44:54.228
    STEP: List CRs in v2 01/12/23 15:44:54.237
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:44:54.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5536" for this suite. 01/12/23 15:44:54.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:44:54.795
Jan 12 15:44:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:44:54.796
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:54.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:54.806
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9042 01/12/23 15:44:54.808
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 15:44:54.817
STEP: creating service externalsvc in namespace services-9042 01/12/23 15:44:54.818
STEP: creating replication controller externalsvc in namespace services-9042 01/12/23 15:44:54.827
I0112 15:44:54.833581      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9042, replica count: 2
I0112 15:44:57.885498      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/12/23 15:44:57.888
Jan 12 15:44:57.898: INFO: Creating new exec pod
Jan 12 15:44:57.903: INFO: Waiting up to 5m0s for pod "execpodjk7f2" in namespace "services-9042" to be "running"
Jan 12 15:44:57.905: INFO: Pod "execpodjk7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.928001ms
Jan 12 15:44:59.908: INFO: Pod "execpodjk7f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004431917s
Jan 12 15:44:59.908: INFO: Pod "execpodjk7f2" satisfied condition "running"
Jan 12 15:44:59.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9042 exec execpodjk7f2 -- /bin/sh -x -c nslookup clusterip-service.services-9042.svc.cluster.local'
Jan 12 15:45:00.060: INFO: stderr: "+ nslookup clusterip-service.services-9042.svc.cluster.local\n"
Jan 12 15:45:00.060: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9042.svc.cluster.local\tcanonical name = externalsvc.services-9042.svc.cluster.local.\nName:\texternalsvc.services-9042.svc.cluster.local\nAddress: 10.104.68.241\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9042, will wait for the garbage collector to delete the pods 01/12/23 15:45:00.06
Jan 12 15:45:00.116: INFO: Deleting ReplicationController externalsvc took: 3.239081ms
Jan 12 15:45:00.217: INFO: Terminating ReplicationController externalsvc pods took: 100.59885ms
Jan 12 15:45:02.233: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:45:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9042" for this suite. 01/12/23 15:45:02.243
------------------------------
 [SLOW TEST] [7.455 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:44:54.795
    Jan 12 15:44:54.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:44:54.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:44:54.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:44:54.806
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9042 01/12/23 15:44:54.808
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 15:44:54.817
    STEP: creating service externalsvc in namespace services-9042 01/12/23 15:44:54.818
    STEP: creating replication controller externalsvc in namespace services-9042 01/12/23 15:44:54.827
    I0112 15:44:54.833581      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9042, replica count: 2
    I0112 15:44:57.885498      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/12/23 15:44:57.888
    Jan 12 15:44:57.898: INFO: Creating new exec pod
    Jan 12 15:44:57.903: INFO: Waiting up to 5m0s for pod "execpodjk7f2" in namespace "services-9042" to be "running"
    Jan 12 15:44:57.905: INFO: Pod "execpodjk7f2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.928001ms
    Jan 12 15:44:59.908: INFO: Pod "execpodjk7f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004431917s
    Jan 12 15:44:59.908: INFO: Pod "execpodjk7f2" satisfied condition "running"
    Jan 12 15:44:59.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-9042 exec execpodjk7f2 -- /bin/sh -x -c nslookup clusterip-service.services-9042.svc.cluster.local'
    Jan 12 15:45:00.060: INFO: stderr: "+ nslookup clusterip-service.services-9042.svc.cluster.local\n"
    Jan 12 15:45:00.060: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9042.svc.cluster.local\tcanonical name = externalsvc.services-9042.svc.cluster.local.\nName:\texternalsvc.services-9042.svc.cluster.local\nAddress: 10.104.68.241\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9042, will wait for the garbage collector to delete the pods 01/12/23 15:45:00.06
    Jan 12 15:45:00.116: INFO: Deleting ReplicationController externalsvc took: 3.239081ms
    Jan 12 15:45:00.217: INFO: Terminating ReplicationController externalsvc pods took: 100.59885ms
    Jan 12 15:45:02.233: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:45:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9042" for this suite. 01/12/23 15:45:02.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:45:02.251
Jan 12 15:45:02.251: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:45:02.252
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:02.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:02.263
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/12/23 15:45:02.267
STEP: waiting for available Endpoint 01/12/23 15:45:02.271
STEP: listing all Endpoints 01/12/23 15:45:02.272
STEP: updating the Endpoint 01/12/23 15:45:02.273
STEP: fetching the Endpoint 01/12/23 15:45:02.277
STEP: patching the Endpoint 01/12/23 15:45:02.279
STEP: fetching the Endpoint 01/12/23 15:45:02.285
STEP: deleting the Endpoint by Collection 01/12/23 15:45:02.286
STEP: waiting for Endpoint deletion 01/12/23 15:45:02.29
STEP: fetching the Endpoint 01/12/23 15:45:02.291
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:45:02.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5582" for this suite. 01/12/23 15:45:02.295
------------------------------
 [0.047 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:45:02.251
    Jan 12 15:45:02.251: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:45:02.252
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:02.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:02.263
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/12/23 15:45:02.267
    STEP: waiting for available Endpoint 01/12/23 15:45:02.271
    STEP: listing all Endpoints 01/12/23 15:45:02.272
    STEP: updating the Endpoint 01/12/23 15:45:02.273
    STEP: fetching the Endpoint 01/12/23 15:45:02.277
    STEP: patching the Endpoint 01/12/23 15:45:02.279
    STEP: fetching the Endpoint 01/12/23 15:45:02.285
    STEP: deleting the Endpoint by Collection 01/12/23 15:45:02.286
    STEP: waiting for Endpoint deletion 01/12/23 15:45:02.29
    STEP: fetching the Endpoint 01/12/23 15:45:02.291
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:45:02.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5582" for this suite. 01/12/23 15:45:02.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:45:02.299
Jan 12 15:45:02.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 15:45:02.3
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:02.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:02.31
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/12/23 15:45:02.312
Jan 12 15:45:02.317: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 15:45:07.320: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 15:45:07.32
STEP: getting scale subresource 01/12/23 15:45:07.32
STEP: updating a scale subresource 01/12/23 15:45:07.323
STEP: verifying the replicaset Spec.Replicas was modified 01/12/23 15:45:07.33
STEP: Patch a scale subresource 01/12/23 15:45:07.332
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 15:45:07.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7197" for this suite. 01/12/23 15:45:07.349
------------------------------
 [SLOW TEST] [5.060 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:45:02.299
    Jan 12 15:45:02.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 15:45:02.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:02.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:02.31
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/12/23 15:45:02.312
    Jan 12 15:45:02.317: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 15:45:07.320: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 15:45:07.32
    STEP: getting scale subresource 01/12/23 15:45:07.32
    STEP: updating a scale subresource 01/12/23 15:45:07.323
    STEP: verifying the replicaset Spec.Replicas was modified 01/12/23 15:45:07.33
    STEP: Patch a scale subresource 01/12/23 15:45:07.332
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:45:07.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7197" for this suite. 01/12/23 15:45:07.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:45:07.36
Jan 12 15:45:07.360: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename cronjob 01/12/23 15:45:07.361
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:07.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:07.382
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/12/23 15:45:07.384
STEP: Ensuring no jobs are scheduled 01/12/23 15:45:07.39
STEP: Ensuring no job exists by listing jobs explicitly 01/12/23 15:50:07.397
STEP: Removing cronjob 01/12/23 15:50:07.399
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7690" for this suite. 01/12/23 15:50:07.405
------------------------------
 [SLOW TEST] [300.048 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:45:07.36
    Jan 12 15:45:07.360: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename cronjob 01/12/23 15:45:07.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:45:07.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:45:07.382
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/12/23 15:45:07.384
    STEP: Ensuring no jobs are scheduled 01/12/23 15:45:07.39
    STEP: Ensuring no job exists by listing jobs explicitly 01/12/23 15:50:07.397
    STEP: Removing cronjob 01/12/23 15:50:07.399
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7690" for this suite. 01/12/23 15:50:07.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:07.411
Jan 12 15:50:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:50:07.412
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:07.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:07.425
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/12/23 15:50:07.427
Jan 12 15:50:07.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: mark a version not serverd 01/12/23 15:50:10.88
STEP: check the unserved version gets removed 01/12/23 15:50:10.893
STEP: check the other version is not changed 01/12/23 15:50:11.78
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:14.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9926" for this suite. 01/12/23 15:50:14.496
------------------------------
 [SLOW TEST] [7.089 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:07.411
    Jan 12 15:50:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 15:50:07.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:07.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:07.425
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/12/23 15:50:07.427
    Jan 12 15:50:07.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: mark a version not serverd 01/12/23 15:50:10.88
    STEP: check the unserved version gets removed 01/12/23 15:50:10.893
    STEP: check the other version is not changed 01/12/23 15:50:11.78
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:14.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9926" for this suite. 01/12/23 15:50:14.496
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:14.5
Jan 12 15:50:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 15:50:14.501
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:14.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:14.51
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/12/23 15:50:14.515
STEP: delete the rc 01/12/23 15:50:19.521
STEP: wait for the rc to be deleted 01/12/23 15:50:19.53
Jan 12 15:50:20.538: INFO: 80 pods remaining
Jan 12 15:50:20.538: INFO: 80 pods has nil DeletionTimestamp
Jan 12 15:50:20.538: INFO: 
Jan 12 15:50:21.544: INFO: 71 pods remaining
Jan 12 15:50:21.544: INFO: 71 pods has nil DeletionTimestamp
Jan 12 15:50:21.544: INFO: 
Jan 12 15:50:22.538: INFO: 60 pods remaining
Jan 12 15:50:22.538: INFO: 60 pods has nil DeletionTimestamp
Jan 12 15:50:22.538: INFO: 
Jan 12 15:50:23.540: INFO: 40 pods remaining
Jan 12 15:50:23.540: INFO: 40 pods has nil DeletionTimestamp
Jan 12 15:50:23.540: INFO: 
Jan 12 15:50:24.548: INFO: 31 pods remaining
Jan 12 15:50:24.548: INFO: 31 pods has nil DeletionTimestamp
Jan 12 15:50:24.548: INFO: 
Jan 12 15:50:25.536: INFO: 20 pods remaining
Jan 12 15:50:25.536: INFO: 20 pods has nil DeletionTimestamp
Jan 12 15:50:25.536: INFO: 
STEP: Gathering metrics 01/12/23 15:50:26.535
W0112 15:50:26.539212      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 15:50:26.539: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9846" for this suite. 01/12/23 15:50:26.541
------------------------------
 [SLOW TEST] [12.044 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:14.5
    Jan 12 15:50:14.500: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 15:50:14.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:14.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:14.51
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/12/23 15:50:14.515
    STEP: delete the rc 01/12/23 15:50:19.521
    STEP: wait for the rc to be deleted 01/12/23 15:50:19.53
    Jan 12 15:50:20.538: INFO: 80 pods remaining
    Jan 12 15:50:20.538: INFO: 80 pods has nil DeletionTimestamp
    Jan 12 15:50:20.538: INFO: 
    Jan 12 15:50:21.544: INFO: 71 pods remaining
    Jan 12 15:50:21.544: INFO: 71 pods has nil DeletionTimestamp
    Jan 12 15:50:21.544: INFO: 
    Jan 12 15:50:22.538: INFO: 60 pods remaining
    Jan 12 15:50:22.538: INFO: 60 pods has nil DeletionTimestamp
    Jan 12 15:50:22.538: INFO: 
    Jan 12 15:50:23.540: INFO: 40 pods remaining
    Jan 12 15:50:23.540: INFO: 40 pods has nil DeletionTimestamp
    Jan 12 15:50:23.540: INFO: 
    Jan 12 15:50:24.548: INFO: 31 pods remaining
    Jan 12 15:50:24.548: INFO: 31 pods has nil DeletionTimestamp
    Jan 12 15:50:24.548: INFO: 
    Jan 12 15:50:25.536: INFO: 20 pods remaining
    Jan 12 15:50:25.536: INFO: 20 pods has nil DeletionTimestamp
    Jan 12 15:50:25.536: INFO: 
    STEP: Gathering metrics 01/12/23 15:50:26.535
    W0112 15:50:26.539212      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 15:50:26.539: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:26.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9846" for this suite. 01/12/23 15:50:26.541
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:26.545
Jan 12 15:50:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 15:50:26.546
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:26.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:26.557
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 12 15:50:26.564: INFO: Waiting up to 5m0s for pod "server-envvars-f0980587-44fa-437e-962c-f61530421880" in namespace "pods-3682" to be "running and ready"
Jan 12 15:50:26.566: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 1.724946ms
Jan 12 15:50:26.566: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:28.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005026636s
Jan 12 15:50:28.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:30.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005100151s
Jan 12 15:50:30.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:32.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005022119s
Jan 12 15:50:32.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:34.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004639917s
Jan 12 15:50:34.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:36.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003958061s
Jan 12 15:50:36.568: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:38.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Running", Reason="", readiness=true. Elapsed: 12.003868011s
Jan 12 15:50:38.568: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Running (Ready = true)
Jan 12 15:50:38.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880" satisfied condition "running and ready"
Jan 12 15:50:38.590: INFO: Waiting up to 5m0s for pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6" in namespace "pods-3682" to be "Succeeded or Failed"
Jan 12 15:50:38.591: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723479ms
Jan 12 15:50:40.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005450741s
Jan 12 15:50:42.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005276609s
STEP: Saw pod success 01/12/23 15:50:42.595
Jan 12 15:50:42.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6" satisfied condition "Succeeded or Failed"
Jan 12 15:50:42.597: INFO: Trying to get logs from node worker-1 pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 container env3cont: <nil>
STEP: delete the pod 01/12/23 15:50:42.61
Jan 12 15:50:42.618: INFO: Waiting for pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 to disappear
Jan 12 15:50:42.619: INFO: Pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:42.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3682" for this suite. 01/12/23 15:50:42.622
------------------------------
 [SLOW TEST] [16.082 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:26.545
    Jan 12 15:50:26.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 15:50:26.546
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:26.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:26.557
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 12 15:50:26.564: INFO: Waiting up to 5m0s for pod "server-envvars-f0980587-44fa-437e-962c-f61530421880" in namespace "pods-3682" to be "running and ready"
    Jan 12 15:50:26.566: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 1.724946ms
    Jan 12 15:50:26.566: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:28.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005026636s
    Jan 12 15:50:28.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:30.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005100151s
    Jan 12 15:50:30.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:32.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005022119s
    Jan 12 15:50:32.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:34.569: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004639917s
    Jan 12 15:50:34.569: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:36.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003958061s
    Jan 12 15:50:36.568: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:38.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880": Phase="Running", Reason="", readiness=true. Elapsed: 12.003868011s
    Jan 12 15:50:38.568: INFO: The phase of Pod server-envvars-f0980587-44fa-437e-962c-f61530421880 is Running (Ready = true)
    Jan 12 15:50:38.568: INFO: Pod "server-envvars-f0980587-44fa-437e-962c-f61530421880" satisfied condition "running and ready"
    Jan 12 15:50:38.590: INFO: Waiting up to 5m0s for pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6" in namespace "pods-3682" to be "Succeeded or Failed"
    Jan 12 15:50:38.591: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.723479ms
    Jan 12 15:50:40.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005450741s
    Jan 12 15:50:42.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005276609s
    STEP: Saw pod success 01/12/23 15:50:42.595
    Jan 12 15:50:42.595: INFO: Pod "client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6" satisfied condition "Succeeded or Failed"
    Jan 12 15:50:42.597: INFO: Trying to get logs from node worker-1 pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 container env3cont: <nil>
    STEP: delete the pod 01/12/23 15:50:42.61
    Jan 12 15:50:42.618: INFO: Waiting for pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 to disappear
    Jan 12 15:50:42.619: INFO: Pod client-envvars-9f7eb9d5-7c21-4fc7-9409-1c61b52545c6 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:42.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3682" for this suite. 01/12/23 15:50:42.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:42.629
Jan 12 15:50:42.629: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 15:50:42.63
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:42.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:42.645
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/12/23 15:50:42.647
Jan 12 15:50:42.654: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1" in namespace "emptydir-2366" to be "running"
Jan 12 15:50:42.656: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950515ms
Jan 12 15:50:44.658: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1": Phase="Running", Reason="", readiness=false. Elapsed: 2.004304309s
Jan 12 15:50:44.658: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/12/23 15:50:44.658
Jan 12 15:50:44.658: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2366 PodName:pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 15:50:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 15:50:44.659: INFO: ExecWithOptions: Clientset creation
Jan 12 15:50:44.659: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-2366/pods/pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 12 15:50:44.713: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:44.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2366" for this suite. 01/12/23 15:50:44.715
------------------------------
 [2.090 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:42.629
    Jan 12 15:50:42.629: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 15:50:42.63
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:42.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:42.645
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/12/23 15:50:42.647
    Jan 12 15:50:42.654: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1" in namespace "emptydir-2366" to be "running"
    Jan 12 15:50:42.656: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950515ms
    Jan 12 15:50:44.658: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1": Phase="Running", Reason="", readiness=false. Elapsed: 2.004304309s
    Jan 12 15:50:44.658: INFO: Pod "pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/12/23 15:50:44.658
    Jan 12 15:50:44.658: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2366 PodName:pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 15:50:44.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 15:50:44.659: INFO: ExecWithOptions: Clientset creation
    Jan 12 15:50:44.659: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-2366/pods/pod-sharedvolume-9f4293b6-5926-494e-a280-192f7ecc11c1/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 12 15:50:44.713: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:44.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2366" for this suite. 01/12/23 15:50:44.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:44.72
Jan 12 15:50:44.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 15:50:44.721
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:44.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:44.73
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/12/23 15:50:44.732
STEP: submitting the pod to kubernetes 01/12/23 15:50:44.732
Jan 12 15:50:44.736: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" in namespace "pods-4942" to be "running and ready"
Jan 12 15:50:44.738: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 1.805237ms
Jan 12 15:50:44.738: INFO: The phase of Pod pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:50:46.741: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=true. Elapsed: 2.00457551s
Jan 12 15:50:46.741: INFO: The phase of Pod pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739 is Running (Ready = true)
Jan 12 15:50:46.741: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/12/23 15:50:46.743
STEP: updating the pod 01/12/23 15:50:46.745
Jan 12 15:50:47.253: INFO: Successfully updated pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739"
Jan 12 15:50:47.253: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" in namespace "pods-4942" to be "terminated with reason DeadlineExceeded"
Jan 12 15:50:47.255: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=true. Elapsed: 1.66732ms
Jan 12 15:50:49.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=false. Elapsed: 2.003651266s
Jan 12 15:50:51.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.004006804s
Jan 12 15:50:51.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:51.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4942" for this suite. 01/12/23 15:50:51.259
------------------------------
 [SLOW TEST] [6.543 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:44.72
    Jan 12 15:50:44.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 15:50:44.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:44.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:44.73
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/12/23 15:50:44.732
    STEP: submitting the pod to kubernetes 01/12/23 15:50:44.732
    Jan 12 15:50:44.736: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" in namespace "pods-4942" to be "running and ready"
    Jan 12 15:50:44.738: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Pending", Reason="", readiness=false. Elapsed: 1.805237ms
    Jan 12 15:50:44.738: INFO: The phase of Pod pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:50:46.741: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=true. Elapsed: 2.00457551s
    Jan 12 15:50:46.741: INFO: The phase of Pod pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739 is Running (Ready = true)
    Jan 12 15:50:46.741: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/12/23 15:50:46.743
    STEP: updating the pod 01/12/23 15:50:46.745
    Jan 12 15:50:47.253: INFO: Successfully updated pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739"
    Jan 12 15:50:47.253: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" in namespace "pods-4942" to be "terminated with reason DeadlineExceeded"
    Jan 12 15:50:47.255: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=true. Elapsed: 1.66732ms
    Jan 12 15:50:49.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Running", Reason="", readiness=false. Elapsed: 2.003651266s
    Jan 12 15:50:51.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.004006804s
    Jan 12 15:50:51.257: INFO: Pod "pod-update-activedeadlineseconds-86f9abe8-d9ea-4fa0-8ba7-89c0f2a1a739" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:51.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4942" for this suite. 01/12/23 15:50:51.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:51.268
Jan 12 15:50:51.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:50:51.269
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:51.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:51.279
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/12/23 15:50:51.282
Jan 12 15:50:51.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332" in namespace "projected-8066" to be "Succeeded or Failed"
Jan 12 15:50:51.289: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893636ms
Jan 12 15:50:53.292: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798322s
Jan 12 15:50:55.291: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003933526s
STEP: Saw pod success 01/12/23 15:50:55.291
Jan 12 15:50:55.291: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332" satisfied condition "Succeeded or Failed"
Jan 12 15:50:55.292: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 container client-container: <nil>
STEP: delete the pod 01/12/23 15:50:55.297
Jan 12 15:50:55.305: INFO: Waiting for pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 to disappear
Jan 12 15:50:55.307: INFO: Pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:55.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8066" for this suite. 01/12/23 15:50:55.309
------------------------------
 [4.044 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:51.268
    Jan 12 15:50:51.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:50:51.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:51.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:51.279
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/12/23 15:50:51.282
    Jan 12 15:50:51.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332" in namespace "projected-8066" to be "Succeeded or Failed"
    Jan 12 15:50:51.289: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893636ms
    Jan 12 15:50:53.292: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004798322s
    Jan 12 15:50:55.291: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003933526s
    STEP: Saw pod success 01/12/23 15:50:55.291
    Jan 12 15:50:55.291: INFO: Pod "downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332" satisfied condition "Succeeded or Failed"
    Jan 12 15:50:55.292: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 container client-container: <nil>
    STEP: delete the pod 01/12/23 15:50:55.297
    Jan 12 15:50:55.305: INFO: Waiting for pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 to disappear
    Jan 12 15:50:55.307: INFO: Pod downwardapi-volume-7ec71e4d-a978-45b0-a808-273a22146332 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:55.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8066" for this suite. 01/12/23 15:50:55.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:55.313
Jan 12 15:50:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 15:50:55.314
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:55.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:55.326
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 15:50:55.337
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 15:50:55.34
Jan 12 15:50:55.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 15:50:55.346: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 15:50:56.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 15:50:56.350: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/12/23 15:50:56.352
STEP: DeleteCollection of the DaemonSets 01/12/23 15:50:56.354
STEP: Verify that ReplicaSets have been deleted 01/12/23 15:50:56.358
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 12 15:50:56.370: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11013"},"items":null}

Jan 12 15:50:56.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11014"},"items":[{"metadata":{"name":"daemon-set-jrsh7","generateName":"daemon-set-","namespace":"daemonsets-3527","uid":"4cb661c4-fe8f-4e0f-b9c8-cd8c0f6800c2","resourceVersion":"11011","creationTimestamp":"2023-01-12T15:50:55Z","deletionTimestamp":"2023-01-12T15:51:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e13b4cf6-6484-4982-9039-57d942f442c1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e13b4cf6-6484-4982-9039-57d942f442c1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tlpzh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tlpzh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"}],"hostIP":"10.0.40.50","podIP":"10.244.1.176","podIPs":[{"ip":"10.244.1.176"}],"startTime":"2023-01-12T15:50:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T15:50:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6dde6564b91e44a22ac12ca66dc42f193a6abb0758570fe1bc0926d991cb615d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z6d6s","generateName":"daemon-set-","namespace":"daemonsets-3527","uid":"335c949f-f768-43b1-8e77-1305b91173c2","resourceVersion":"11012","creationTimestamp":"2023-01-12T15:50:55Z","deletionTimestamp":"2023-01-12T15:51:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e13b4cf6-6484-4982-9039-57d942f442c1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e13b4cf6-6484-4982-9039-57d942f442c1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-brpqh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-brpqh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"}],"hostIP":"10.0.42.160","podIP":"10.244.0.128","podIPs":[{"ip":"10.244.0.128"}],"startTime":"2023-01-12T15:50:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T15:50:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://929252f39a2f0d09dedea9120fea74c806c269d85916245f7e32e2f5e14f475d","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:50:56.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3527" for this suite. 01/12/23 15:50:56.384
------------------------------
 [1.076 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:55.313
    Jan 12 15:50:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 15:50:55.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:55.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:55.326
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 15:50:55.337
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 15:50:55.34
    Jan 12 15:50:55.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 15:50:55.346: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 15:50:56.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 15:50:56.350: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/12/23 15:50:56.352
    STEP: DeleteCollection of the DaemonSets 01/12/23 15:50:56.354
    STEP: Verify that ReplicaSets have been deleted 01/12/23 15:50:56.358
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 12 15:50:56.370: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11013"},"items":null}

    Jan 12 15:50:56.375: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11014"},"items":[{"metadata":{"name":"daemon-set-jrsh7","generateName":"daemon-set-","namespace":"daemonsets-3527","uid":"4cb661c4-fe8f-4e0f-b9c8-cd8c0f6800c2","resourceVersion":"11011","creationTimestamp":"2023-01-12T15:50:55Z","deletionTimestamp":"2023-01-12T15:51:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e13b4cf6-6484-4982-9039-57d942f442c1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e13b4cf6-6484-4982-9039-57d942f442c1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.176\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-tlpzh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-tlpzh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"}],"hostIP":"10.0.40.50","podIP":"10.244.1.176","podIPs":[{"ip":"10.244.1.176"}],"startTime":"2023-01-12T15:50:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T15:50:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6dde6564b91e44a22ac12ca66dc42f193a6abb0758570fe1bc0926d991cb615d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-z6d6s","generateName":"daemon-set-","namespace":"daemonsets-3527","uid":"335c949f-f768-43b1-8e77-1305b91173c2","resourceVersion":"11012","creationTimestamp":"2023-01-12T15:50:55Z","deletionTimestamp":"2023-01-12T15:51:26Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e13b4cf6-6484-4982-9039-57d942f442c1","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e13b4cf6-6484-4982-9039-57d942f442c1\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-12T15:50:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-brpqh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-brpqh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-12T15:50:55Z"}],"hostIP":"10.0.42.160","podIP":"10.244.0.128","podIPs":[{"ip":"10.244.0.128"}],"startTime":"2023-01-12T15:50:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-12T15:50:55Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://929252f39a2f0d09dedea9120fea74c806c269d85916245f7e32e2f5e14f475d","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:50:56.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3527" for this suite. 01/12/23 15:50:56.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:50:56.392
Jan 12 15:50:56.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:50:56.393
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:56.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:56.405
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5608/configmap-test-ca17becf-8d6c-43fd-94d4-5bdbeb004d77 01/12/23 15:50:56.407
STEP: Creating a pod to test consume configMaps 01/12/23 15:50:56.41
Jan 12 15:50:56.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1" in namespace "configmap-5608" to be "Succeeded or Failed"
Jan 12 15:50:56.418: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799311ms
Jan 12 15:50:58.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473103s
Jan 12 15:51:00.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004935311s
STEP: Saw pod success 01/12/23 15:51:00.421
Jan 12 15:51:00.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1" satisfied condition "Succeeded or Failed"
Jan 12 15:51:00.423: INFO: Trying to get logs from node worker-1 pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 container env-test: <nil>
STEP: delete the pod 01/12/23 15:51:00.428
Jan 12 15:51:00.436: INFO: Waiting for pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 to disappear
Jan 12 15:51:00.437: INFO: Pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:51:00.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5608" for this suite. 01/12/23 15:51:00.44
------------------------------
 [4.053 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:50:56.392
    Jan 12 15:50:56.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:50:56.393
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:50:56.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:50:56.405
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5608/configmap-test-ca17becf-8d6c-43fd-94d4-5bdbeb004d77 01/12/23 15:50:56.407
    STEP: Creating a pod to test consume configMaps 01/12/23 15:50:56.41
    Jan 12 15:50:56.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1" in namespace "configmap-5608" to be "Succeeded or Failed"
    Jan 12 15:50:56.418: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799311ms
    Jan 12 15:50:58.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00473103s
    Jan 12 15:51:00.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004935311s
    STEP: Saw pod success 01/12/23 15:51:00.421
    Jan 12 15:51:00.421: INFO: Pod "pod-configmaps-af49ae08-388b-4798-a578-f633022104a1" satisfied condition "Succeeded or Failed"
    Jan 12 15:51:00.423: INFO: Trying to get logs from node worker-1 pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 container env-test: <nil>
    STEP: delete the pod 01/12/23 15:51:00.428
    Jan 12 15:51:00.436: INFO: Waiting for pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 to disappear
    Jan 12 15:51:00.437: INFO: Pod pod-configmaps-af49ae08-388b-4798-a578-f633022104a1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:51:00.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5608" for this suite. 01/12/23 15:51:00.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:51:00.446
Jan 12 15:51:00.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 15:51:00.447
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:00.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:00.459
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-fea57352-eb09-482f-a41e-39b6749a5180 01/12/23 15:51:00.461
STEP: Creating a pod to test consume secrets 01/12/23 15:51:00.464
Jan 12 15:51:00.472: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc" in namespace "projected-8561" to be "Succeeded or Failed"
Jan 12 15:51:00.474: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638085ms
Jan 12 15:51:02.477: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005216858s
Jan 12 15:51:04.476: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003917027s
STEP: Saw pod success 01/12/23 15:51:04.476
Jan 12 15:51:04.476: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc" satisfied condition "Succeeded or Failed"
Jan 12 15:51:04.478: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 15:51:04.482
Jan 12 15:51:04.491: INFO: Waiting for pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc to disappear
Jan 12 15:51:04.493: INFO: Pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 15:51:04.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8561" for this suite. 01/12/23 15:51:04.496
------------------------------
 [4.053 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:51:00.446
    Jan 12 15:51:00.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 15:51:00.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:00.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:00.459
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-fea57352-eb09-482f-a41e-39b6749a5180 01/12/23 15:51:00.461
    STEP: Creating a pod to test consume secrets 01/12/23 15:51:00.464
    Jan 12 15:51:00.472: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc" in namespace "projected-8561" to be "Succeeded or Failed"
    Jan 12 15:51:00.474: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.638085ms
    Jan 12 15:51:02.477: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005216858s
    Jan 12 15:51:04.476: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003917027s
    STEP: Saw pod success 01/12/23 15:51:04.476
    Jan 12 15:51:04.476: INFO: Pod "pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc" satisfied condition "Succeeded or Failed"
    Jan 12 15:51:04.478: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 15:51:04.482
    Jan 12 15:51:04.491: INFO: Waiting for pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc to disappear
    Jan 12 15:51:04.493: INFO: Pod pod-projected-secrets-c8d22081-8586-4213-85da-c70e96deebcc no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:51:04.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8561" for this suite. 01/12/23 15:51:04.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:51:04.5
Jan 12 15:51:04.501: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 15:51:04.501
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:04.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:04.51
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/12/23 15:51:04.513
STEP: setting up watch 01/12/23 15:51:04.513
STEP: submitting the pod to kubernetes 01/12/23 15:51:04.616
STEP: verifying the pod is in kubernetes 01/12/23 15:51:04.623
STEP: verifying pod creation was observed 01/12/23 15:51:04.625
Jan 12 15:51:04.625: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2" in namespace "pods-6304" to be "running"
Jan 12 15:51:04.627: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.784997ms
Jan 12 15:51:06.629: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004411547s
Jan 12 15:51:06.629: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 15:51:06.631
STEP: verifying pod deletion was observed 01/12/23 15:51:06.635
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 15:51:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6304" for this suite. 01/12/23 15:51:08.286
------------------------------
 [3.791 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:51:04.5
    Jan 12 15:51:04.501: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 15:51:04.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:04.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:04.51
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/12/23 15:51:04.513
    STEP: setting up watch 01/12/23 15:51:04.513
    STEP: submitting the pod to kubernetes 01/12/23 15:51:04.616
    STEP: verifying the pod is in kubernetes 01/12/23 15:51:04.623
    STEP: verifying pod creation was observed 01/12/23 15:51:04.625
    Jan 12 15:51:04.625: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2" in namespace "pods-6304" to be "running"
    Jan 12 15:51:04.627: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.784997ms
    Jan 12 15:51:06.629: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004411547s
    Jan 12 15:51:06.629: INFO: Pod "pod-submit-remove-5a838e64-4437-4aa5-92a8-13e89c4c4fb2" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 15:51:06.631
    STEP: verifying pod deletion was observed 01/12/23 15:51:06.635
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:51:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6304" for this suite. 01/12/23 15:51:08.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:51:08.292
Jan 12 15:51:08.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename taint-multiple-pods 01/12/23 15:51:08.293
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:08.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:08.303
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 12 15:51:08.306: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 15:52:08.318: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 12 15:52:08.319: INFO: Starting informer...
STEP: Starting pods... 01/12/23 15:52:08.32
Jan 12 15:52:08.533: INFO: Pod1 is running on worker-1. Tainting Node
Jan 12 15:52:08.739: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7611" to be "running"
Jan 12 15:52:08.741: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.908178ms
Jan 12 15:52:10.744: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005112059s
Jan 12 15:52:10.744: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 12 15:52:10.744: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7611" to be "running"
Jan 12 15:52:10.746: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.62264ms
Jan 12 15:52:10.746: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 12 15:52:10.746: INFO: Pod2 is running on worker-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/12/23 15:52:10.746
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 15:52:10.754
STEP: Waiting for Pod1 and Pod2 to be deleted 01/12/23 15:52:10.758
Jan 12 15:52:16.424: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 12 15:52:36.463: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 15:52:36.473
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 15:52:36.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7611" for this suite. 01/12/23 15:52:36.477
------------------------------
 [SLOW TEST] [88.189 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:51:08.292
    Jan 12 15:51:08.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename taint-multiple-pods 01/12/23 15:51:08.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:51:08.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:51:08.303
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 12 15:51:08.306: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 15:52:08.318: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 12 15:52:08.319: INFO: Starting informer...
    STEP: Starting pods... 01/12/23 15:52:08.32
    Jan 12 15:52:08.533: INFO: Pod1 is running on worker-1. Tainting Node
    Jan 12 15:52:08.739: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7611" to be "running"
    Jan 12 15:52:08.741: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.908178ms
    Jan 12 15:52:10.744: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005112059s
    Jan 12 15:52:10.744: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 12 15:52:10.744: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7611" to be "running"
    Jan 12 15:52:10.746: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.62264ms
    Jan 12 15:52:10.746: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 12 15:52:10.746: INFO: Pod2 is running on worker-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/12/23 15:52:10.746
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 15:52:10.754
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/12/23 15:52:10.758
    Jan 12 15:52:16.424: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 12 15:52:36.463: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 15:52:36.473
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:52:36.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7611" for this suite. 01/12/23 15:52:36.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:52:36.482
Jan 12 15:52:36.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 15:52:36.483
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:36.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:36.495
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/12/23 15:52:36.497
Jan 12 15:52:36.497: INFO: Creating e2e-svc-a-vdg66
Jan 12 15:52:36.506: INFO: Creating e2e-svc-b-498cr
Jan 12 15:52:36.515: INFO: Creating e2e-svc-c-nz2st
STEP: deleting service collection 01/12/23 15:52:36.532
Jan 12 15:52:36.558: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 15:52:36.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2001" for this suite. 01/12/23 15:52:36.561
------------------------------
 [0.083 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:52:36.482
    Jan 12 15:52:36.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 15:52:36.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:36.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:36.495
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/12/23 15:52:36.497
    Jan 12 15:52:36.497: INFO: Creating e2e-svc-a-vdg66
    Jan 12 15:52:36.506: INFO: Creating e2e-svc-b-498cr
    Jan 12 15:52:36.515: INFO: Creating e2e-svc-c-nz2st
    STEP: deleting service collection 01/12/23 15:52:36.532
    Jan 12 15:52:36.558: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:52:36.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2001" for this suite. 01/12/23 15:52:36.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:52:36.568
Jan 12 15:52:36.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 15:52:36.568
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:36.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:36.578
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 12 15:52:36.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 create -f -'
Jan 12 15:52:37.174: INFO: stderr: ""
Jan 12 15:52:37.174: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 12 15:52:37.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 create -f -'
Jan 12 15:52:37.395: INFO: stderr: ""
Jan 12 15:52:37.395: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 15:52:37.395
Jan 12 15:52:38.397: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 15:52:38.397: INFO: Found 0 / 1
Jan 12 15:52:39.397: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 15:52:39.397: INFO: Found 1 / 1
Jan 12 15:52:39.397: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 12 15:52:39.399: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 15:52:39.399: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 15:52:39.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe pod agnhost-primary-lrncp'
Jan 12 15:52:39.462: INFO: stderr: ""
Jan 12 15:52:39.462: INFO: stdout: "Name:             agnhost-primary-lrncp\nNamespace:        kubectl-2564\nPriority:         0\nService Account:  default\nNode:             worker-1/10.0.40.50\nStart Time:       Thu, 12 Jan 2023 15:52:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.182\nIPs:\n  IP:           10.244.1.182\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d566982991deeaf0430d76a528c0e9b9c242aaaf65e8e166dbc70f62b597b15b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Jan 2023 15:52:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5kcbk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5kcbk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2564/agnhost-primary-lrncp to worker-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 12 15:52:39.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe rc agnhost-primary'
Jan 12 15:52:39.528: INFO: stderr: ""
Jan 12 15:52:39.528: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2564\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lrncp\n"
Jan 12 15:52:39.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe service agnhost-primary'
Jan 12 15:52:39.590: INFO: stderr: ""
Jan 12 15:52:39.590: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2564\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.97.39.168\nIPs:               10.97.39.168\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.182:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 12 15:52:39.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe node worker-0'
Jan 12 15:52:39.670: INFO: stderr: ""
Jan 12 15:52:39.670: INFO: stdout: "Name:               worker-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker-0\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 12 Jan 2023 15:23:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  worker-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 12 Jan 2023 15:52:36 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:17 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.42.160\n  Hostname:    worker-0\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7621620Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      46651590989\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7519220Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 5e430cfeeacb454495260306edf43a0a\n  System UUID:                ec231a70-51e3-6656-1b4c-aa2a198611da\n  Boot ID:                    f6a0402e-5160-4c8b-b40a-40b6df3b4901\n  Kernel Version:             5.15.0-1026-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.0+k0s\n  Kube-Proxy Version:         v1.26.0+k0s\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-9864b985-9m6mg                                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 konnectivity-agent-n82db                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-stxrn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-router-rtpgj                                          250m (6%)     0 (0%)      16Mi (0%)        0 (0%)         29m\n  kube-system                 metrics-server-7446cc488c-tsnl2                            10m (0%)      0 (0%)      30M (0%)         0 (0%)         29m\n  sonobuoy                    sonobuoy-e2e-job-829dc73999d04348                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests        Limits\n  --------               --------        ------\n  cpu                    360m (9%)       0 (0%)\n  memory                 120177536 (1%)  170Mi (2%)\n  ephemeral-storage      0 (0%)          0 (0%)\n  hugepages-1Gi          0 (0%)          0 (0%)\n  hugepages-2Mi          0 (0%)          0 (0%)\n  scheduling.k8s.io/foo  0               0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 29m                kube-proxy       \n  Normal   Starting                 29m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      29m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  29m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           29m                node-controller  Node worker-0 event: Registered Node worker-0 in Controller\n  Normal   NodeReady                29m                kubelet          Node worker-0 status is now: NodeReady\n"
Jan 12 15:52:39.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe namespace kubectl-2564'
Jan 12 15:52:39.732: INFO: stderr: ""
Jan 12 15:52:39.732: INFO: stdout: "Name:         kubectl-2564\nLabels:       e2e-framework=kubectl\n              e2e-run=40cab34b-98f1-488c-8c7d-dad3d0da3836\n              kubernetes.io/metadata.name=kubectl-2564\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 15:52:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2564" for this suite. 01/12/23 15:52:39.734
------------------------------
 [3.170 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:52:36.568
    Jan 12 15:52:36.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 15:52:36.568
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:36.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:36.578
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 12 15:52:36.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 create -f -'
    Jan 12 15:52:37.174: INFO: stderr: ""
    Jan 12 15:52:37.174: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 12 15:52:37.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 create -f -'
    Jan 12 15:52:37.395: INFO: stderr: ""
    Jan 12 15:52:37.395: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 15:52:37.395
    Jan 12 15:52:38.397: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 15:52:38.397: INFO: Found 0 / 1
    Jan 12 15:52:39.397: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 15:52:39.397: INFO: Found 1 / 1
    Jan 12 15:52:39.397: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 12 15:52:39.399: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 15:52:39.399: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 15:52:39.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe pod agnhost-primary-lrncp'
    Jan 12 15:52:39.462: INFO: stderr: ""
    Jan 12 15:52:39.462: INFO: stdout: "Name:             agnhost-primary-lrncp\nNamespace:        kubectl-2564\nPriority:         0\nService Account:  default\nNode:             worker-1/10.0.40.50\nStart Time:       Thu, 12 Jan 2023 15:52:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.182\nIPs:\n  IP:           10.244.1.182\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://d566982991deeaf0430d76a528c0e9b9c242aaaf65e8e166dbc70f62b597b15b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 12 Jan 2023 15:52:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5kcbk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5kcbk:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2564/agnhost-primary-lrncp to worker-1\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 12 15:52:39.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe rc agnhost-primary'
    Jan 12 15:52:39.528: INFO: stderr: ""
    Jan 12 15:52:39.528: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2564\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lrncp\n"
    Jan 12 15:52:39.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe service agnhost-primary'
    Jan 12 15:52:39.590: INFO: stderr: ""
    Jan 12 15:52:39.590: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2564\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.97.39.168\nIPs:               10.97.39.168\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.182:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 12 15:52:39.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe node worker-0'
    Jan 12 15:52:39.670: INFO: stderr: ""
    Jan 12 15:52:39.670: INFO: stdout: "Name:               worker-0\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker-0\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 12 Jan 2023 15:23:00 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  worker-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 12 Jan 2023 15:52:36 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 12 Jan 2023 15:47:41 +0000   Thu, 12 Jan 2023 15:23:17 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.42.160\n  Hostname:    worker-0\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      50620216Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7621620Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      46651590989\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7519220Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 5e430cfeeacb454495260306edf43a0a\n  System UUID:                ec231a70-51e3-6656-1b4c-aa2a198611da\n  Boot ID:                    f6a0402e-5160-4c8b-b40a-40b6df3b4901\n  Kernel Version:             5.15.0-1026-aws\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.26.0+k0s\n  Kube-Proxy Version:         v1.26.0+k0s\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-9864b985-9m6mg                                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 konnectivity-agent-n82db                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-stxrn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-router-rtpgj                                          250m (6%)     0 (0%)      16Mi (0%)        0 (0%)         29m\n  kube-system                 metrics-server-7446cc488c-tsnl2                            10m (0%)      0 (0%)      30M (0%)         0 (0%)         29m\n  sonobuoy                    sonobuoy-e2e-job-829dc73999d04348                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests        Limits\n  --------               --------        ------\n  cpu                    360m (9%)       0 (0%)\n  memory                 120177536 (1%)  170Mi (2%)\n  ephemeral-storage      0 (0%)          0 (0%)\n  hugepages-1Gi          0 (0%)          0 (0%)\n  hugepages-2Mi          0 (0%)          0 (0%)\n  scheduling.k8s.io/foo  0               0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 29m                kube-proxy       \n  Normal   Starting                 29m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      29m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m (x2 over 29m)  kubelet          Node worker-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  29m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           29m                node-controller  Node worker-0 event: Registered Node worker-0 in Controller\n  Normal   NodeReady                29m                kubelet          Node worker-0 status is now: NodeReady\n"
    Jan 12 15:52:39.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-2564 describe namespace kubectl-2564'
    Jan 12 15:52:39.732: INFO: stderr: ""
    Jan 12 15:52:39.732: INFO: stdout: "Name:         kubectl-2564\nLabels:       e2e-framework=kubectl\n              e2e-run=40cab34b-98f1-488c-8c7d-dad3d0da3836\n              kubernetes.io/metadata.name=kubectl-2564\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:52:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2564" for this suite. 01/12/23 15:52:39.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:52:39.739
Jan 12 15:52:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 15:52:39.74
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:39.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:39.75
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 12 15:52:39.758: INFO: Waiting up to 2m0s for pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" in namespace "var-expansion-5995" to be "container 0 failed with reason CreateContainerConfigError"
Jan 12 15:52:39.760: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26": Phase="Pending", Reason="", readiness=false. Elapsed: 1.988421ms
Jan 12 15:52:41.763: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004589953s
Jan 12 15:52:41.763: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 12 15:52:41.763: INFO: Deleting pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" in namespace "var-expansion-5995"
Jan 12 15:52:41.766: INFO: Wait up to 5m0s for pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 15:52:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5995" for this suite. 01/12/23 15:52:43.773
------------------------------
 [4.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:52:39.739
    Jan 12 15:52:39.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 15:52:39.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:39.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:39.75
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 12 15:52:39.758: INFO: Waiting up to 2m0s for pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" in namespace "var-expansion-5995" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 12 15:52:39.760: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26": Phase="Pending", Reason="", readiness=false. Elapsed: 1.988421ms
    Jan 12 15:52:41.763: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004589953s
    Jan 12 15:52:41.763: INFO: Pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 12 15:52:41.763: INFO: Deleting pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" in namespace "var-expansion-5995"
    Jan 12 15:52:41.766: INFO: Wait up to 5m0s for pod "var-expansion-3923889f-52ea-4273-b3c0-51a3c3855b26" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:52:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5995" for this suite. 01/12/23 15:52:43.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:52:43.782
Jan 12 15:52:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 15:52:43.783
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:43.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:43.794
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/12/23 15:52:43.796
STEP: submitting the pod to kubernetes 01/12/23 15:52:43.796
STEP: verifying QOS class is set on the pod 01/12/23 15:52:43.802
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 12 15:52:43.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5149" for this suite. 01/12/23 15:52:43.807
------------------------------
 [0.028 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:52:43.782
    Jan 12 15:52:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 15:52:43.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:43.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:43.794
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/12/23 15:52:43.796
    STEP: submitting the pod to kubernetes 01/12/23 15:52:43.796
    STEP: verifying QOS class is set on the pod 01/12/23 15:52:43.802
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:52:43.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5149" for this suite. 01/12/23 15:52:43.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:52:43.811
Jan 12 15:52:43.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 15:52:43.812
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:43.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:43.825
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 12 15:52:43.832: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 12 15:52:48.836: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 15:52:48.836
Jan 12 15:52:48.836: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 12 15:52:50.840: INFO: Creating deployment "test-rollover-deployment"
Jan 12 15:52:50.845: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 12 15:52:52.850: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 12 15:52:52.853: INFO: Ensure that both replica sets have 1 created replica
Jan 12 15:52:52.857: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 12 15:52:52.863: INFO: Updating deployment test-rollover-deployment
Jan 12 15:52:52.863: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 12 15:52:54.868: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 12 15:52:54.872: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 12 15:52:54.875: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 15:52:54.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:52:56.880: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 15:52:56.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:52:58.880: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 15:52:58.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:53:00.880: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 15:53:00.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:53:02.881: INFO: all replica sets need to contain the pod-template-hash label
Jan 12 15:53:02.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 15:53:04.880: INFO: 
Jan 12 15:53:04.880: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 15:53:04.885: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1540  137b15c1-0028-416a-9b0f-69fe6c015c67 11622 2 2023-01-12 15:52:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a70be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 15:52:50 +0000 UTC,LastTransitionTime:2023-01-12 15:52:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-12 15:53:04 +0000 UTC,LastTransitionTime:2023-01-12 15:52:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 15:53:04.887: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-1540  af7cdd29-5d98-42ea-9fe5-846d5f10a7c5 11612 2 2023-01-12 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc0049433c7 0xc0049433c8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004943478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:53:04.887: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 12 15:53:04.888: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1540  79795d7b-4599-4b90-a20c-b820d64e5b54 11621 2 2023-01-12 15:52:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc004943297 0xc004943298}] [] [{e2e.test Update apps/v1 2023-01-12 15:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004943358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:53:04.888: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-1540  f629e702-d7ca-4123-bfad-b935f950da35 11577 2 2023-01-12 15:52:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc0049434e7 0xc0049434e8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004943598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 15:53:04.890: INFO: Pod "test-rollover-deployment-6c6df9974f-85rzt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-85rzt test-rollover-deployment-6c6df9974f- deployment-1540  069dcf89-2d38-4706-ae4e-6995389b17f6 11589 0 2023-01-12 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f af7cdd29-5d98-42ea-9fe5-846d5f10a7c5 0xc004aaa5e7 0xc004aaa5e8}] [] [{kube-controller-manager Update v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af7cdd29-5d98-42ea-9fe5-846d5f10a7c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:52:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68zzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68zzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.130,StartTime:2023-01-12 15:52:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 15:52:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3eb2638f3894acb853e40322aaa2557bbb9f29f9fa5cd913c36a43fbf3ed1f62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:04.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1540" for this suite. 01/12/23 15:53:04.892
------------------------------
 [SLOW TEST] [21.084 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:52:43.811
    Jan 12 15:52:43.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 15:52:43.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:52:43.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:52:43.825
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 12 15:52:43.832: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 12 15:52:48.836: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 15:52:48.836
    Jan 12 15:52:48.836: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 12 15:52:50.840: INFO: Creating deployment "test-rollover-deployment"
    Jan 12 15:52:50.845: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 12 15:52:52.850: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 12 15:52:52.853: INFO: Ensure that both replica sets have 1 created replica
    Jan 12 15:52:52.857: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 12 15:52:52.863: INFO: Updating deployment test-rollover-deployment
    Jan 12 15:52:52.863: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 12 15:52:54.868: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 12 15:52:54.872: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 12 15:52:54.875: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 15:52:54.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:52:56.880: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 15:52:56.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:52:58.880: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 15:52:58.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:53:00.880: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 15:53:00.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:53:02.881: INFO: all replica sets need to contain the pod-template-hash label
    Jan 12 15:53:02.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 15, 52, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 15, 52, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 15:53:04.880: INFO: 
    Jan 12 15:53:04.880: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 15:53:04.885: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1540  137b15c1-0028-416a-9b0f-69fe6c015c67 11622 2 2023-01-12 15:52:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a70be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 15:52:50 +0000 UTC,LastTransitionTime:2023-01-12 15:52:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-12 15:53:04 +0000 UTC,LastTransitionTime:2023-01-12 15:52:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 15:53:04.887: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-1540  af7cdd29-5d98-42ea-9fe5-846d5f10a7c5 11612 2 2023-01-12 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc0049433c7 0xc0049433c8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004943478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:53:04.887: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 12 15:53:04.888: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1540  79795d7b-4599-4b90-a20c-b820d64e5b54 11621 2 2023-01-12 15:52:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc004943297 0xc004943298}] [] [{e2e.test Update apps/v1 2023-01-12 15:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:53:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004943358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:53:04.888: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-1540  f629e702-d7ca-4123-bfad-b935f950da35 11577 2 2023-01-12 15:52:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 137b15c1-0028-416a-9b0f-69fe6c015c67 0xc0049434e7 0xc0049434e8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"137b15c1-0028-416a-9b0f-69fe6c015c67\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004943598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 15:53:04.890: INFO: Pod "test-rollover-deployment-6c6df9974f-85rzt" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-85rzt test-rollover-deployment-6c6df9974f- deployment-1540  069dcf89-2d38-4706-ae4e-6995389b17f6 11589 0 2023-01-12 15:52:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f af7cdd29-5d98-42ea-9fe5-846d5f10a7c5 0xc004aaa5e7 0xc004aaa5e8}] [] [{kube-controller-manager Update v1 2023-01-12 15:52:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af7cdd29-5d98-42ea-9fe5-846d5f10a7c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 15:52:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68zzx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68zzx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 15:52:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.130,StartTime:2023-01-12 15:52:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 15:52:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3eb2638f3894acb853e40322aaa2557bbb9f29f9fa5cd913c36a43fbf3ed1f62,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:04.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1540" for this suite. 01/12/23 15:53:04.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:04.898
Jan 12 15:53:04.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename subpath 01/12/23 15:53:04.899
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:04.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:04.909
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 15:53:04.912
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-9xp5 01/12/23 15:53:04.92
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 15:53:04.92
Jan 12 15:53:04.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9xp5" in namespace "subpath-3585" to be "Succeeded or Failed"
Jan 12 15:53:04.927: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759101ms
Jan 12 15:53:06.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00492502s
Jan 12 15:53:08.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 4.005661207s
Jan 12 15:53:10.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 6.004118657s
Jan 12 15:53:12.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005949101s
Jan 12 15:53:14.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004364434s
Jan 12 15:53:16.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 12.004325164s
Jan 12 15:53:18.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005181837s
Jan 12 15:53:20.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 16.00525537s
Jan 12 15:53:22.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 18.005474046s
Jan 12 15:53:24.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 20.004840146s
Jan 12 15:53:26.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=false. Elapsed: 22.006166286s
Jan 12 15:53:28.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005114916s
STEP: Saw pod success 01/12/23 15:53:28.93
Jan 12 15:53:28.930: INFO: Pod "pod-subpath-test-configmap-9xp5" satisfied condition "Succeeded or Failed"
Jan 12 15:53:28.932: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-9xp5 container test-container-subpath-configmap-9xp5: <nil>
STEP: delete the pod 01/12/23 15:53:28.943
Jan 12 15:53:28.951: INFO: Waiting for pod pod-subpath-test-configmap-9xp5 to disappear
Jan 12 15:53:28.953: INFO: Pod pod-subpath-test-configmap-9xp5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9xp5 01/12/23 15:53:28.953
Jan 12 15:53:28.953: INFO: Deleting pod "pod-subpath-test-configmap-9xp5" in namespace "subpath-3585"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:28.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3585" for this suite. 01/12/23 15:53:28.957
------------------------------
 [SLOW TEST] [24.063 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:04.898
    Jan 12 15:53:04.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename subpath 01/12/23 15:53:04.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:04.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:04.909
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 15:53:04.912
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-9xp5 01/12/23 15:53:04.92
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 15:53:04.92
    Jan 12 15:53:04.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9xp5" in namespace "subpath-3585" to be "Succeeded or Failed"
    Jan 12 15:53:04.927: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.759101ms
    Jan 12 15:53:06.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 2.00492502s
    Jan 12 15:53:08.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 4.005661207s
    Jan 12 15:53:10.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 6.004118657s
    Jan 12 15:53:12.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005949101s
    Jan 12 15:53:14.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 10.004364434s
    Jan 12 15:53:16.929: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 12.004325164s
    Jan 12 15:53:18.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 14.005181837s
    Jan 12 15:53:20.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 16.00525537s
    Jan 12 15:53:22.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 18.005474046s
    Jan 12 15:53:24.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=true. Elapsed: 20.004840146s
    Jan 12 15:53:26.931: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Running", Reason="", readiness=false. Elapsed: 22.006166286s
    Jan 12 15:53:28.930: INFO: Pod "pod-subpath-test-configmap-9xp5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005114916s
    STEP: Saw pod success 01/12/23 15:53:28.93
    Jan 12 15:53:28.930: INFO: Pod "pod-subpath-test-configmap-9xp5" satisfied condition "Succeeded or Failed"
    Jan 12 15:53:28.932: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-9xp5 container test-container-subpath-configmap-9xp5: <nil>
    STEP: delete the pod 01/12/23 15:53:28.943
    Jan 12 15:53:28.951: INFO: Waiting for pod pod-subpath-test-configmap-9xp5 to disappear
    Jan 12 15:53:28.953: INFO: Pod pod-subpath-test-configmap-9xp5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-9xp5 01/12/23 15:53:28.953
    Jan 12 15:53:28.953: INFO: Deleting pod "pod-subpath-test-configmap-9xp5" in namespace "subpath-3585"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:28.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3585" for this suite. 01/12/23 15:53:28.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:28.962
Jan 12 15:53:28.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename limitrange 01/12/23 15:53:28.962
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:28.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:28.975
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/12/23 15:53:28.978
STEP: Setting up watch 01/12/23 15:53:28.978
STEP: Submitting a LimitRange 01/12/23 15:53:29.08
STEP: Verifying LimitRange creation was observed 01/12/23 15:53:29.083
STEP: Fetching the LimitRange to ensure it has proper values 01/12/23 15:53:29.083
Jan 12 15:53:29.085: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 12 15:53:29.085: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/12/23 15:53:29.085
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/12/23 15:53:29.088
Jan 12 15:53:29.090: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 12 15:53:29.090: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/12/23 15:53:29.09
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/12/23 15:53:29.096
Jan 12 15:53:29.098: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 12 15:53:29.098: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/12/23 15:53:29.098
STEP: Failing to create a Pod with more than max resources 01/12/23 15:53:29.1
STEP: Updating a LimitRange 01/12/23 15:53:29.102
STEP: Verifying LimitRange updating is effective 01/12/23 15:53:29.105
STEP: Creating a Pod with less than former min resources 01/12/23 15:53:31.109
STEP: Failing to create a Pod with more than max resources 01/12/23 15:53:31.114
STEP: Deleting a LimitRange 01/12/23 15:53:31.116
STEP: Verifying the LimitRange was deleted 01/12/23 15:53:31.121
Jan 12 15:53:36.126: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/12/23 15:53:36.126
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:36.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-244" for this suite. 01/12/23 15:53:36.136
------------------------------
 [SLOW TEST] [7.180 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:28.962
    Jan 12 15:53:28.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename limitrange 01/12/23 15:53:28.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:28.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:28.975
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/12/23 15:53:28.978
    STEP: Setting up watch 01/12/23 15:53:28.978
    STEP: Submitting a LimitRange 01/12/23 15:53:29.08
    STEP: Verifying LimitRange creation was observed 01/12/23 15:53:29.083
    STEP: Fetching the LimitRange to ensure it has proper values 01/12/23 15:53:29.083
    Jan 12 15:53:29.085: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 12 15:53:29.085: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/12/23 15:53:29.085
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/12/23 15:53:29.088
    Jan 12 15:53:29.090: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 12 15:53:29.090: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/12/23 15:53:29.09
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/12/23 15:53:29.096
    Jan 12 15:53:29.098: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 12 15:53:29.098: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/12/23 15:53:29.098
    STEP: Failing to create a Pod with more than max resources 01/12/23 15:53:29.1
    STEP: Updating a LimitRange 01/12/23 15:53:29.102
    STEP: Verifying LimitRange updating is effective 01/12/23 15:53:29.105
    STEP: Creating a Pod with less than former min resources 01/12/23 15:53:31.109
    STEP: Failing to create a Pod with more than max resources 01/12/23 15:53:31.114
    STEP: Deleting a LimitRange 01/12/23 15:53:31.116
    STEP: Verifying the LimitRange was deleted 01/12/23 15:53:31.121
    Jan 12 15:53:36.126: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/12/23 15:53:36.126
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:36.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-244" for this suite. 01/12/23 15:53:36.136
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:36.142
Jan 12 15:53:36.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 15:53:36.143
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:36.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:36.153
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/12/23 15:53:36.156
Jan 12 15:53:36.163: INFO: Waiting up to 5m0s for pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9" in namespace "downward-api-9257" to be "running and ready"
Jan 12 15:53:36.167: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26602ms
Jan 12 15:53:36.167: INFO: The phase of Pod labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 15:53:38.170: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00744011s
Jan 12 15:53:38.170: INFO: The phase of Pod labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9 is Running (Ready = true)
Jan 12 15:53:38.170: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9" satisfied condition "running and ready"
Jan 12 15:53:38.692: INFO: Successfully updated pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:42.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9257" for this suite. 01/12/23 15:53:42.71
------------------------------
 [SLOW TEST] [6.572 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:36.142
    Jan 12 15:53:36.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 15:53:36.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:36.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:36.153
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/12/23 15:53:36.156
    Jan 12 15:53:36.163: INFO: Waiting up to 5m0s for pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9" in namespace "downward-api-9257" to be "running and ready"
    Jan 12 15:53:36.167: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26602ms
    Jan 12 15:53:36.167: INFO: The phase of Pod labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 15:53:38.170: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00744011s
    Jan 12 15:53:38.170: INFO: The phase of Pod labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9 is Running (Ready = true)
    Jan 12 15:53:38.170: INFO: Pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9" satisfied condition "running and ready"
    Jan 12 15:53:38.692: INFO: Successfully updated pod "labelsupdate2b05bc82-a3bb-49c5-b917-552e1a1111e9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:42.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9257" for this suite. 01/12/23 15:53:42.71
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:42.714
Jan 12 15:53:42.714: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename cronjob 01/12/23 15:53:42.715
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.728
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/12/23 15:53:42.731
STEP: creating 01/12/23 15:53:42.731
STEP: getting 01/12/23 15:53:42.734
STEP: listing 01/12/23 15:53:42.735
STEP: watching 01/12/23 15:53:42.737
Jan 12 15:53:42.737: INFO: starting watch
STEP: cluster-wide listing 01/12/23 15:53:42.738
STEP: cluster-wide watching 01/12/23 15:53:42.74
Jan 12 15:53:42.740: INFO: starting watch
STEP: patching 01/12/23 15:53:42.741
STEP: updating 01/12/23 15:53:42.747
Jan 12 15:53:42.752: INFO: waiting for watch events with expected annotations
Jan 12 15:53:42.752: INFO: saw patched and updated annotations
STEP: patching /status 01/12/23 15:53:42.752
STEP: updating /status 01/12/23 15:53:42.756
STEP: get /status 01/12/23 15:53:42.76
STEP: deleting 01/12/23 15:53:42.762
STEP: deleting a collection 01/12/23 15:53:42.771
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:42.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4418" for this suite. 01/12/23 15:53:42.779
------------------------------
 [0.072 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:42.714
    Jan 12 15:53:42.714: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename cronjob 01/12/23 15:53:42.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.728
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/12/23 15:53:42.731
    STEP: creating 01/12/23 15:53:42.731
    STEP: getting 01/12/23 15:53:42.734
    STEP: listing 01/12/23 15:53:42.735
    STEP: watching 01/12/23 15:53:42.737
    Jan 12 15:53:42.737: INFO: starting watch
    STEP: cluster-wide listing 01/12/23 15:53:42.738
    STEP: cluster-wide watching 01/12/23 15:53:42.74
    Jan 12 15:53:42.740: INFO: starting watch
    STEP: patching 01/12/23 15:53:42.741
    STEP: updating 01/12/23 15:53:42.747
    Jan 12 15:53:42.752: INFO: waiting for watch events with expected annotations
    Jan 12 15:53:42.752: INFO: saw patched and updated annotations
    STEP: patching /status 01/12/23 15:53:42.752
    STEP: updating /status 01/12/23 15:53:42.756
    STEP: get /status 01/12/23 15:53:42.76
    STEP: deleting 01/12/23 15:53:42.762
    STEP: deleting a collection 01/12/23 15:53:42.771
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:42.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4418" for this suite. 01/12/23 15:53:42.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:42.788
Jan 12 15:53:42.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename events 01/12/23 15:53:42.789
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.798
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/12/23 15:53:42.8
STEP: listing all events in all namespaces 01/12/23 15:53:42.803
STEP: patching the test event 01/12/23 15:53:42.807
STEP: fetching the test event 01/12/23 15:53:42.811
STEP: updating the test event 01/12/23 15:53:42.813
STEP: getting the test event 01/12/23 15:53:42.821
STEP: deleting the test event 01/12/23 15:53:42.822
STEP: listing all events in all namespaces 01/12/23 15:53:42.826
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:42.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5404" for this suite. 01/12/23 15:53:42.832
------------------------------
 [0.049 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:42.788
    Jan 12 15:53:42.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename events 01/12/23 15:53:42.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.798
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/12/23 15:53:42.8
    STEP: listing all events in all namespaces 01/12/23 15:53:42.803
    STEP: patching the test event 01/12/23 15:53:42.807
    STEP: fetching the test event 01/12/23 15:53:42.811
    STEP: updating the test event 01/12/23 15:53:42.813
    STEP: getting the test event 01/12/23 15:53:42.821
    STEP: deleting the test event 01/12/23 15:53:42.822
    STEP: listing all events in all namespaces 01/12/23 15:53:42.826
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:42.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5404" for this suite. 01/12/23 15:53:42.832
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:42.837
Jan 12 15:53:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 15:53:42.838
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.847
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-a739dfe9-eb27-4dea-a437-307b7533a8a7 01/12/23 15:53:42.851
STEP: Creating the pod 01/12/23 15:53:42.853
Jan 12 15:53:42.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb" in namespace "configmap-3954" to be "running"
Jan 12 15:53:42.861: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748241ms
Jan 12 15:53:44.865: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb": Phase="Running", Reason="", readiness=false. Elapsed: 2.005201484s
Jan 12 15:53:44.865: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb" satisfied condition "running"
STEP: Waiting for pod with text data 01/12/23 15:53:44.865
STEP: Waiting for pod with binary data 01/12/23 15:53:44.869
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 15:53:44.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3954" for this suite. 01/12/23 15:53:44.875
------------------------------
 [2.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:42.837
    Jan 12 15:53:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 15:53:42.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:42.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:42.847
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-a739dfe9-eb27-4dea-a437-307b7533a8a7 01/12/23 15:53:42.851
    STEP: Creating the pod 01/12/23 15:53:42.853
    Jan 12 15:53:42.859: INFO: Waiting up to 5m0s for pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb" in namespace "configmap-3954" to be "running"
    Jan 12 15:53:42.861: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748241ms
    Jan 12 15:53:44.865: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb": Phase="Running", Reason="", readiness=false. Elapsed: 2.005201484s
    Jan 12 15:53:44.865: INFO: Pod "pod-configmaps-e96b9547-f300-4ae5-9ad5-6bac290a99cb" satisfied condition "running"
    STEP: Waiting for pod with text data 01/12/23 15:53:44.865
    STEP: Waiting for pod with binary data 01/12/23 15:53:44.869
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:53:44.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3954" for this suite. 01/12/23 15:53:44.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:53:44.88
Jan 12 15:53:44.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 15:53:44.881
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:44.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:44.892
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-e14ac365-82db-43b8-9464-3408627748eb in namespace container-probe-6123 01/12/23 15:53:44.895
Jan 12 15:53:44.902: INFO: Waiting up to 5m0s for pod "liveness-e14ac365-82db-43b8-9464-3408627748eb" in namespace "container-probe-6123" to be "not pending"
Jan 12 15:53:44.905: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815365ms
Jan 12 15:53:46.908: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006329636s
Jan 12 15:53:46.908: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb" satisfied condition "not pending"
Jan 12 15:53:46.908: INFO: Started pod liveness-e14ac365-82db-43b8-9464-3408627748eb in namespace container-probe-6123
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:53:46.908
Jan 12 15:53:46.910: INFO: Initial restart count of pod liveness-e14ac365-82db-43b8-9464-3408627748eb is 0
STEP: deleting the pod 01/12/23 15:57:47.297
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 15:57:47.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6123" for this suite. 01/12/23 15:57:47.309
------------------------------
 [SLOW TEST] [242.432 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:53:44.88
    Jan 12 15:53:44.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 15:53:44.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:53:44.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:53:44.892
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-e14ac365-82db-43b8-9464-3408627748eb in namespace container-probe-6123 01/12/23 15:53:44.895
    Jan 12 15:53:44.902: INFO: Waiting up to 5m0s for pod "liveness-e14ac365-82db-43b8-9464-3408627748eb" in namespace "container-probe-6123" to be "not pending"
    Jan 12 15:53:44.905: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815365ms
    Jan 12 15:53:46.908: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006329636s
    Jan 12 15:53:46.908: INFO: Pod "liveness-e14ac365-82db-43b8-9464-3408627748eb" satisfied condition "not pending"
    Jan 12 15:53:46.908: INFO: Started pod liveness-e14ac365-82db-43b8-9464-3408627748eb in namespace container-probe-6123
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:53:46.908
    Jan 12 15:53:46.910: INFO: Initial restart count of pod liveness-e14ac365-82db-43b8-9464-3408627748eb is 0
    STEP: deleting the pod 01/12/23 15:57:47.297
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:57:47.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6123" for this suite. 01/12/23 15:57:47.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:57:47.316
Jan 12 15:57:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 15:57:47.318
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:47.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:47.327
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/12/23 15:57:47.333
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/12/23 15:57:47.334
STEP: creating a pod to probe DNS 01/12/23 15:57:47.334
STEP: submitting the pod to kubernetes 01/12/23 15:57:47.334
Jan 12 15:57:47.340: INFO: Waiting up to 15m0s for pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8" in namespace "dns-4929" to be "running"
Jan 12 15:57:47.342: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.75167ms
Jan 12 15:57:49.345: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004276126s
Jan 12 15:57:49.345: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8" satisfied condition "running"
STEP: retrieving the pod 01/12/23 15:57:49.345
STEP: looking for the results for each expected name from probers 01/12/23 15:57:49.346
Jan 12 15:57:49.358: INFO: DNS probes using dns-4929/dns-test-adce14d8-e93b-430a-81cd-511cb79651e8 succeeded

STEP: deleting the pod 01/12/23 15:57:49.358
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 15:57:49.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4929" for this suite. 01/12/23 15:57:49.367
------------------------------
 [2.054 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:57:47.316
    Jan 12 15:57:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 15:57:47.318
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:47.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:47.327
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/12/23 15:57:47.333
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/12/23 15:57:47.334
    STEP: creating a pod to probe DNS 01/12/23 15:57:47.334
    STEP: submitting the pod to kubernetes 01/12/23 15:57:47.334
    Jan 12 15:57:47.340: INFO: Waiting up to 15m0s for pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8" in namespace "dns-4929" to be "running"
    Jan 12 15:57:47.342: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.75167ms
    Jan 12 15:57:49.345: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004276126s
    Jan 12 15:57:49.345: INFO: Pod "dns-test-adce14d8-e93b-430a-81cd-511cb79651e8" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 15:57:49.345
    STEP: looking for the results for each expected name from probers 01/12/23 15:57:49.346
    Jan 12 15:57:49.358: INFO: DNS probes using dns-4929/dns-test-adce14d8-e93b-430a-81cd-511cb79651e8 succeeded

    STEP: deleting the pod 01/12/23 15:57:49.358
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:57:49.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4929" for this suite. 01/12/23 15:57:49.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:57:49.372
Jan 12 15:57:49.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 15:57:49.373
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:49.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:49.384
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 12 15:57:49.387: INFO: Got root ca configmap in namespace "svcaccounts-5627"
Jan 12 15:57:49.391: INFO: Deleted root ca configmap in namespace "svcaccounts-5627"
STEP: waiting for a new root ca configmap created 01/12/23 15:57:49.892
Jan 12 15:57:49.894: INFO: Recreated root ca configmap in namespace "svcaccounts-5627"
Jan 12 15:57:49.897: INFO: Updated root ca configmap in namespace "svcaccounts-5627"
STEP: waiting for the root ca configmap reconciled 01/12/23 15:57:50.398
Jan 12 15:57:50.400: INFO: Reconciled root ca configmap in namespace "svcaccounts-5627"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 15:57:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5627" for this suite. 01/12/23 15:57:50.402
------------------------------
 [1.034 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:57:49.372
    Jan 12 15:57:49.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 15:57:49.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:49.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:49.384
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 12 15:57:49.387: INFO: Got root ca configmap in namespace "svcaccounts-5627"
    Jan 12 15:57:49.391: INFO: Deleted root ca configmap in namespace "svcaccounts-5627"
    STEP: waiting for a new root ca configmap created 01/12/23 15:57:49.892
    Jan 12 15:57:49.894: INFO: Recreated root ca configmap in namespace "svcaccounts-5627"
    Jan 12 15:57:49.897: INFO: Updated root ca configmap in namespace "svcaccounts-5627"
    STEP: waiting for the root ca configmap reconciled 01/12/23 15:57:50.398
    Jan 12 15:57:50.400: INFO: Reconciled root ca configmap in namespace "svcaccounts-5627"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 15:57:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5627" for this suite. 01/12/23 15:57:50.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 15:57:50.406
Jan 12 15:57:50.406: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 15:57:50.407
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:50.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:50.417
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 in namespace container-probe-2034 01/12/23 15:57:50.419
Jan 12 15:57:50.423: INFO: Waiting up to 5m0s for pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8" in namespace "container-probe-2034" to be "not pending"
Jan 12 15:57:50.425: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78852ms
Jan 12 15:57:52.428: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004825952s
Jan 12 15:57:52.428: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8" satisfied condition "not pending"
Jan 12 15:57:52.428: INFO: Started pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 in namespace container-probe-2034
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:57:52.428
Jan 12 15:57:52.430: INFO: Initial restart count of pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 is 0
STEP: deleting the pod 01/12/23 16:01:52.804
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 16:01:52.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2034" for this suite. 01/12/23 16:01:52.818
------------------------------
 [SLOW TEST] [242.416 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 15:57:50.406
    Jan 12 15:57:50.406: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 15:57:50.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 15:57:50.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 15:57:50.417
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 in namespace container-probe-2034 01/12/23 15:57:50.419
    Jan 12 15:57:50.423: INFO: Waiting up to 5m0s for pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8" in namespace "container-probe-2034" to be "not pending"
    Jan 12 15:57:50.425: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78852ms
    Jan 12 15:57:52.428: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.004825952s
    Jan 12 15:57:52.428: INFO: Pod "busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8" satisfied condition "not pending"
    Jan 12 15:57:52.428: INFO: Started pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 in namespace container-probe-2034
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 15:57:52.428
    Jan 12 15:57:52.430: INFO: Initial restart count of pod busybox-4d85f8bf-4971-4412-887f-c2f7c52f5cf8 is 0
    STEP: deleting the pod 01/12/23 16:01:52.804
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:01:52.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2034" for this suite. 01/12/23 16:01:52.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:01:52.823
Jan 12 16:01:52.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 16:01:52.824
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:52.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:52.836
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/12/23 16:01:52.838
STEP: getting 01/12/23 16:01:52.85
STEP: listing in namespace 01/12/23 16:01:52.855
STEP: patching 01/12/23 16:01:52.858
STEP: deleting 01/12/23 16:01:52.864
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:01:52.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2166" for this suite. 01/12/23 16:01:52.874
------------------------------
 [0.055 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:01:52.823
    Jan 12 16:01:52.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename csiinlinevolumes 01/12/23 16:01:52.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:52.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:52.836
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/12/23 16:01:52.838
    STEP: getting 01/12/23 16:01:52.85
    STEP: listing in namespace 01/12/23 16:01:52.855
    STEP: patching 01/12/23 16:01:52.858
    STEP: deleting 01/12/23 16:01:52.864
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:01:52.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2166" for this suite. 01/12/23 16:01:52.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:01:52.879
Jan 12 16:01:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename discovery 01/12/23 16:01:52.88
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:52.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:52.89
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/12/23 16:01:52.893
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 12 16:01:53.308: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 12 16:01:53.309: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 12 16:01:53.309: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 12 16:01:53.309: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 12 16:01:53.309: INFO: Checking APIGroup: apps
Jan 12 16:01:53.309: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 12 16:01:53.309: INFO: Versions found [{apps/v1 v1}]
Jan 12 16:01:53.309: INFO: apps/v1 matches apps/v1
Jan 12 16:01:53.309: INFO: Checking APIGroup: events.k8s.io
Jan 12 16:01:53.310: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 12 16:01:53.310: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 12 16:01:53.310: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 12 16:01:53.310: INFO: Checking APIGroup: authentication.k8s.io
Jan 12 16:01:53.311: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 12 16:01:53.311: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 12 16:01:53.311: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 12 16:01:53.311: INFO: Checking APIGroup: authorization.k8s.io
Jan 12 16:01:53.312: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 12 16:01:53.312: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 12 16:01:53.312: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 12 16:01:53.312: INFO: Checking APIGroup: autoscaling
Jan 12 16:01:53.313: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 12 16:01:53.313: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 12 16:01:53.313: INFO: autoscaling/v2 matches autoscaling/v2
Jan 12 16:01:53.313: INFO: Checking APIGroup: batch
Jan 12 16:01:53.314: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 12 16:01:53.314: INFO: Versions found [{batch/v1 v1}]
Jan 12 16:01:53.314: INFO: batch/v1 matches batch/v1
Jan 12 16:01:53.314: INFO: Checking APIGroup: certificates.k8s.io
Jan 12 16:01:53.315: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 12 16:01:53.315: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 12 16:01:53.315: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 12 16:01:53.315: INFO: Checking APIGroup: networking.k8s.io
Jan 12 16:01:53.315: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 12 16:01:53.315: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 12 16:01:53.315: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 12 16:01:53.315: INFO: Checking APIGroup: policy
Jan 12 16:01:53.316: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 12 16:01:53.316: INFO: Versions found [{policy/v1 v1}]
Jan 12 16:01:53.316: INFO: policy/v1 matches policy/v1
Jan 12 16:01:53.316: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 12 16:01:53.317: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 12 16:01:53.317: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 12 16:01:53.317: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 12 16:01:53.317: INFO: Checking APIGroup: storage.k8s.io
Jan 12 16:01:53.318: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 12 16:01:53.318: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 12 16:01:53.318: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 12 16:01:53.318: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 12 16:01:53.318: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 12 16:01:53.318: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 12 16:01:53.318: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 12 16:01:53.318: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 12 16:01:53.319: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 12 16:01:53.319: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 12 16:01:53.319: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 12 16:01:53.319: INFO: Checking APIGroup: scheduling.k8s.io
Jan 12 16:01:53.320: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 12 16:01:53.320: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 12 16:01:53.320: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 12 16:01:53.320: INFO: Checking APIGroup: coordination.k8s.io
Jan 12 16:01:53.321: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 12 16:01:53.321: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 12 16:01:53.321: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 12 16:01:53.321: INFO: Checking APIGroup: node.k8s.io
Jan 12 16:01:53.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 12 16:01:53.321: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 12 16:01:53.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 12 16:01:53.321: INFO: Checking APIGroup: discovery.k8s.io
Jan 12 16:01:53.322: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 12 16:01:53.322: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 12 16:01:53.322: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 12 16:01:53.322: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 12 16:01:53.323: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 12 16:01:53.323: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 12 16:01:53.323: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 12 16:01:53.323: INFO: Checking APIGroup: helm.k0sproject.io
Jan 12 16:01:53.324: INFO: PreferredVersion.GroupVersion: helm.k0sproject.io/v1beta1
Jan 12 16:01:53.324: INFO: Versions found [{helm.k0sproject.io/v1beta1 v1beta1}]
Jan 12 16:01:53.324: INFO: helm.k0sproject.io/v1beta1 matches helm.k0sproject.io/v1beta1
Jan 12 16:01:53.324: INFO: Checking APIGroup: autopilot.k0sproject.io
Jan 12 16:01:53.325: INFO: PreferredVersion.GroupVersion: autopilot.k0sproject.io/v1beta2
Jan 12 16:01:53.325: INFO: Versions found [{autopilot.k0sproject.io/v1beta2 v1beta2}]
Jan 12 16:01:53.325: INFO: autopilot.k0sproject.io/v1beta2 matches autopilot.k0sproject.io/v1beta2
Jan 12 16:01:53.325: INFO: Checking APIGroup: metrics.k8s.io
Jan 12 16:01:53.326: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 12 16:01:53.326: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 12 16:01:53.326: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 12 16:01:53.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-7760" for this suite. 01/12/23 16:01:53.328
------------------------------
 [0.453 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:01:52.879
    Jan 12 16:01:52.879: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename discovery 01/12/23 16:01:52.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:52.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:52.89
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/12/23 16:01:52.893
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 12 16:01:53.308: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 12 16:01:53.309: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 12 16:01:53.309: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 12 16:01:53.309: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 12 16:01:53.309: INFO: Checking APIGroup: apps
    Jan 12 16:01:53.309: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 12 16:01:53.309: INFO: Versions found [{apps/v1 v1}]
    Jan 12 16:01:53.309: INFO: apps/v1 matches apps/v1
    Jan 12 16:01:53.309: INFO: Checking APIGroup: events.k8s.io
    Jan 12 16:01:53.310: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 12 16:01:53.310: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 12 16:01:53.310: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 12 16:01:53.310: INFO: Checking APIGroup: authentication.k8s.io
    Jan 12 16:01:53.311: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 12 16:01:53.311: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 12 16:01:53.311: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 12 16:01:53.311: INFO: Checking APIGroup: authorization.k8s.io
    Jan 12 16:01:53.312: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 12 16:01:53.312: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 12 16:01:53.312: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 12 16:01:53.312: INFO: Checking APIGroup: autoscaling
    Jan 12 16:01:53.313: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 12 16:01:53.313: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 12 16:01:53.313: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 12 16:01:53.313: INFO: Checking APIGroup: batch
    Jan 12 16:01:53.314: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 12 16:01:53.314: INFO: Versions found [{batch/v1 v1}]
    Jan 12 16:01:53.314: INFO: batch/v1 matches batch/v1
    Jan 12 16:01:53.314: INFO: Checking APIGroup: certificates.k8s.io
    Jan 12 16:01:53.315: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 12 16:01:53.315: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 12 16:01:53.315: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 12 16:01:53.315: INFO: Checking APIGroup: networking.k8s.io
    Jan 12 16:01:53.315: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 12 16:01:53.315: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 12 16:01:53.315: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 12 16:01:53.315: INFO: Checking APIGroup: policy
    Jan 12 16:01:53.316: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 12 16:01:53.316: INFO: Versions found [{policy/v1 v1}]
    Jan 12 16:01:53.316: INFO: policy/v1 matches policy/v1
    Jan 12 16:01:53.316: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 12 16:01:53.317: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 12 16:01:53.317: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 12 16:01:53.317: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 12 16:01:53.317: INFO: Checking APIGroup: storage.k8s.io
    Jan 12 16:01:53.318: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 12 16:01:53.318: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 12 16:01:53.318: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 12 16:01:53.318: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 12 16:01:53.318: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 12 16:01:53.318: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 12 16:01:53.318: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 12 16:01:53.318: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 12 16:01:53.319: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 12 16:01:53.319: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 12 16:01:53.319: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 12 16:01:53.319: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 12 16:01:53.320: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 12 16:01:53.320: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 12 16:01:53.320: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 12 16:01:53.320: INFO: Checking APIGroup: coordination.k8s.io
    Jan 12 16:01:53.321: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 12 16:01:53.321: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 12 16:01:53.321: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 12 16:01:53.321: INFO: Checking APIGroup: node.k8s.io
    Jan 12 16:01:53.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 12 16:01:53.321: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 12 16:01:53.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 12 16:01:53.321: INFO: Checking APIGroup: discovery.k8s.io
    Jan 12 16:01:53.322: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 12 16:01:53.322: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 12 16:01:53.322: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 12 16:01:53.322: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 12 16:01:53.323: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 12 16:01:53.323: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 12 16:01:53.323: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 12 16:01:53.323: INFO: Checking APIGroup: helm.k0sproject.io
    Jan 12 16:01:53.324: INFO: PreferredVersion.GroupVersion: helm.k0sproject.io/v1beta1
    Jan 12 16:01:53.324: INFO: Versions found [{helm.k0sproject.io/v1beta1 v1beta1}]
    Jan 12 16:01:53.324: INFO: helm.k0sproject.io/v1beta1 matches helm.k0sproject.io/v1beta1
    Jan 12 16:01:53.324: INFO: Checking APIGroup: autopilot.k0sproject.io
    Jan 12 16:01:53.325: INFO: PreferredVersion.GroupVersion: autopilot.k0sproject.io/v1beta2
    Jan 12 16:01:53.325: INFO: Versions found [{autopilot.k0sproject.io/v1beta2 v1beta2}]
    Jan 12 16:01:53.325: INFO: autopilot.k0sproject.io/v1beta2 matches autopilot.k0sproject.io/v1beta2
    Jan 12 16:01:53.325: INFO: Checking APIGroup: metrics.k8s.io
    Jan 12 16:01:53.326: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 12 16:01:53.326: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 12 16:01:53.326: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:01:53.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-7760" for this suite. 01/12/23 16:01:53.328
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:01:53.332
Jan 12 16:01:53.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:01:53.333
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:53.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:53.344
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-793004fe-68b2-4579-820a-b9d591ed3370 01/12/23 16:01:53.347
STEP: Creating secret with name secret-projected-all-test-volume-546fdf1f-66bf-4c41-8521-1ff7b60fedb4 01/12/23 16:01:53.35
STEP: Creating a pod to test Check all projections for projected volume plugin 01/12/23 16:01:53.352
Jan 12 16:01:53.359: INFO: Waiting up to 5m0s for pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213" in namespace "projected-7776" to be "Succeeded or Failed"
Jan 12 16:01:53.361: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Pending", Reason="", readiness=false. Elapsed: 1.726742ms
Jan 12 16:01:55.365: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005544669s
Jan 12 16:01:57.365: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006240115s
STEP: Saw pod success 01/12/23 16:01:57.365
Jan 12 16:01:57.366: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213" satisfied condition "Succeeded or Failed"
Jan 12 16:01:57.367: INFO: Trying to get logs from node worker-1 pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 container projected-all-volume-test: <nil>
STEP: delete the pod 01/12/23 16:01:57.379
Jan 12 16:01:57.387: INFO: Waiting for pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 to disappear
Jan 12 16:01:57.389: INFO: Pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 12 16:01:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7776" for this suite. 01/12/23 16:01:57.392
------------------------------
 [4.065 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:01:53.332
    Jan 12 16:01:53.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:01:53.333
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:53.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:53.344
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-793004fe-68b2-4579-820a-b9d591ed3370 01/12/23 16:01:53.347
    STEP: Creating secret with name secret-projected-all-test-volume-546fdf1f-66bf-4c41-8521-1ff7b60fedb4 01/12/23 16:01:53.35
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/12/23 16:01:53.352
    Jan 12 16:01:53.359: INFO: Waiting up to 5m0s for pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213" in namespace "projected-7776" to be "Succeeded or Failed"
    Jan 12 16:01:53.361: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Pending", Reason="", readiness=false. Elapsed: 1.726742ms
    Jan 12 16:01:55.365: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005544669s
    Jan 12 16:01:57.365: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006240115s
    STEP: Saw pod success 01/12/23 16:01:57.365
    Jan 12 16:01:57.366: INFO: Pod "projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213" satisfied condition "Succeeded or Failed"
    Jan 12 16:01:57.367: INFO: Trying to get logs from node worker-1 pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:01:57.379
    Jan 12 16:01:57.387: INFO: Waiting for pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 to disappear
    Jan 12 16:01:57.389: INFO: Pod projected-volume-da2f24f8-e126-4c7c-8103-0af1c14d8213 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:01:57.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7776" for this suite. 01/12/23 16:01:57.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:01:57.398
Jan 12 16:01:57.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-pred 01/12/23 16:01:57.398
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:57.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:57.408
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 16:01:57.410: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 16:01:57.414: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 16:01:57.416: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Jan 12 16:01:57.419: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:01:57.419: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:01:57.419: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:01:57.419: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:01:57.419: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container metrics-server ready: true, restart count 0
Jan 12 16:01:57.419: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container e2e ready: true, restart count 0
Jan 12 16:01:57.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:01:57.419: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:01:57.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:01:57.419: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 16:01:57.419: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Jan 12 16:01:57.422: INFO: pod-csi-inline-volumes from csiinlinevolumes-2166 started at 2023-01-12 16:01:52 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Jan 12 16:01:57.422: INFO: coredns-9864b985-rdwzq from kube-system started at 2023-01-12 15:52:44 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:01:57.422: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:01:57.422: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:01:57.422: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:01:57.422: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 16:01:57.422: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:01:57.422: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:01:57.422: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node worker-0 01/12/23 16:01:57.434
STEP: verifying the node has the label node worker-1 01/12/23 16:01:57.443
Jan 12 16:01:57.450: INFO: Pod pod-csi-inline-volumes requesting resource cpu=0m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod coredns-9864b985-9m6mg requesting resource cpu=100m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod coredns-9864b985-rdwzq requesting resource cpu=100m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod konnectivity-agent-n82db requesting resource cpu=0m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod konnectivity-agent-xds5c requesting resource cpu=0m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod kube-proxy-f2rbm requesting resource cpu=0m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod kube-proxy-stxrn requesting resource cpu=0m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod kube-router-ppxdq requesting resource cpu=250m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod kube-router-rtpgj requesting resource cpu=250m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod metrics-server-7446cc488c-tsnl2 requesting resource cpu=10m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod sonobuoy-e2e-job-829dc73999d04348 requesting resource cpu=0m on Node worker-0
Jan 12 16:01:57.450: INFO: Pod sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 requesting resource cpu=0m on Node worker-1
Jan 12 16:01:57.450: INFO: Pod sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh requesting resource cpu=0m on Node worker-0
STEP: Starting Pods to consume most of the cluster CPU. 01/12/23 16:01:57.45
Jan 12 16:01:57.450: INFO: Creating a pod which consumes cpu=2548m on Node worker-0
Jan 12 16:01:57.455: INFO: Creating a pod which consumes cpu=2555m on Node worker-1
Jan 12 16:01:57.462: INFO: Waiting up to 5m0s for pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52" in namespace "sched-pred-9891" to be "running"
Jan 12 16:01:57.466: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.594477ms
Jan 12 16:01:59.469: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52": Phase="Running", Reason="", readiness=true. Elapsed: 2.006670832s
Jan 12 16:01:59.469: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52" satisfied condition "running"
Jan 12 16:01:59.469: INFO: Waiting up to 5m0s for pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845" in namespace "sched-pred-9891" to be "running"
Jan 12 16:01:59.471: INFO: Pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845": Phase="Running", Reason="", readiness=true. Elapsed: 1.531543ms
Jan 12 16:01:59.471: INFO: Pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/12/23 16:01:59.471
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b2476e03e13], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9891/filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52 to worker-0] 01/12/23 16:01:59.473
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b249136fb1c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 16:01:59.473
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b249267418d], Reason = [Created], Message = [Created container filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b2496fff3a4], Reason = [Started], Message = [Started container filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24773f4228], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9891/filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845 to worker-1] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b2491ea9999], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24931a7c4d], Reason = [Created], Message = [Created container filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24977ddbee], Reason = [Started], Message = [Started container filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845] 01/12/23 16:01:59.474
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17399b24ef45b6ec], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/12/23 16:01:59.483
STEP: removing the label node off the node worker-0 01/12/23 16:02:00.482
STEP: verifying the node doesn't have the label node 01/12/23 16:02:00.491
STEP: removing the label node off the node worker-1 01/12/23 16:02:00.493
STEP: verifying the node doesn't have the label node 01/12/23 16:02:00.501
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:02:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9891" for this suite. 01/12/23 16:02:00.507
------------------------------
 [3.115 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:01:57.398
    Jan 12 16:01:57.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-pred 01/12/23 16:01:57.398
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:01:57.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:01:57.408
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 16:01:57.410: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 16:01:57.414: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 16:01:57.416: INFO: 
    Logging pods the apiserver thinks is on node worker-0 before test
    Jan 12 16:01:57.419: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:01:57.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 16:01:57.419: INFO: 
    Logging pods the apiserver thinks is on node worker-1 before test
    Jan 12 16:01:57.422: INFO: pod-csi-inline-volumes from csiinlinevolumes-2166 started at 2023-01-12 16:01:52 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Jan 12 16:01:57.422: INFO: coredns-9864b985-rdwzq from kube-system started at 2023-01-12 15:52:44 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:01:57.422: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:01:57.422: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node worker-0 01/12/23 16:01:57.434
    STEP: verifying the node has the label node worker-1 01/12/23 16:01:57.443
    Jan 12 16:01:57.450: INFO: Pod pod-csi-inline-volumes requesting resource cpu=0m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod coredns-9864b985-9m6mg requesting resource cpu=100m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod coredns-9864b985-rdwzq requesting resource cpu=100m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod konnectivity-agent-n82db requesting resource cpu=0m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod konnectivity-agent-xds5c requesting resource cpu=0m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod kube-proxy-f2rbm requesting resource cpu=0m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod kube-proxy-stxrn requesting resource cpu=0m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod kube-router-ppxdq requesting resource cpu=250m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod kube-router-rtpgj requesting resource cpu=250m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod metrics-server-7446cc488c-tsnl2 requesting resource cpu=10m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod sonobuoy-e2e-job-829dc73999d04348 requesting resource cpu=0m on Node worker-0
    Jan 12 16:01:57.450: INFO: Pod sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 requesting resource cpu=0m on Node worker-1
    Jan 12 16:01:57.450: INFO: Pod sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh requesting resource cpu=0m on Node worker-0
    STEP: Starting Pods to consume most of the cluster CPU. 01/12/23 16:01:57.45
    Jan 12 16:01:57.450: INFO: Creating a pod which consumes cpu=2548m on Node worker-0
    Jan 12 16:01:57.455: INFO: Creating a pod which consumes cpu=2555m on Node worker-1
    Jan 12 16:01:57.462: INFO: Waiting up to 5m0s for pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52" in namespace "sched-pred-9891" to be "running"
    Jan 12 16:01:57.466: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.594477ms
    Jan 12 16:01:59.469: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52": Phase="Running", Reason="", readiness=true. Elapsed: 2.006670832s
    Jan 12 16:01:59.469: INFO: Pod "filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52" satisfied condition "running"
    Jan 12 16:01:59.469: INFO: Waiting up to 5m0s for pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845" in namespace "sched-pred-9891" to be "running"
    Jan 12 16:01:59.471: INFO: Pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845": Phase="Running", Reason="", readiness=true. Elapsed: 1.531543ms
    Jan 12 16:01:59.471: INFO: Pod "filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/12/23 16:01:59.471
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b2476e03e13], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9891/filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52 to worker-0] 01/12/23 16:01:59.473
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b249136fb1c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 16:01:59.473
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b249267418d], Reason = [Created], Message = [Created container filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52.17399b2496fff3a4], Reason = [Started], Message = [Started container filler-pod-38682049-a041-4c7e-a0da-b5897f00cc52] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24773f4228], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9891/filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845 to worker-1] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b2491ea9999], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24931a7c4d], Reason = [Created], Message = [Created container filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845.17399b24977ddbee], Reason = [Started], Message = [Started container filler-pod-699422cf-2f76-4bff-91f2-059bbbe88845] 01/12/23 16:01:59.474
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17399b24ef45b6ec], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/12/23 16:01:59.483
    STEP: removing the label node off the node worker-0 01/12/23 16:02:00.482
    STEP: verifying the node doesn't have the label node 01/12/23 16:02:00.491
    STEP: removing the label node off the node worker-1 01/12/23 16:02:00.493
    STEP: verifying the node doesn't have the label node 01/12/23 16:02:00.501
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:02:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9891" for this suite. 01/12/23 16:02:00.507
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:02:00.513
Jan 12 16:02:00.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 16:02:00.514
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:02:00.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:02:00.526
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/12/23 16:02:00.53
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:00.536
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:00.536
STEP: creating a pod to probe DNS 01/12/23 16:02:00.536
STEP: submitting the pod to kubernetes 01/12/23 16:02:00.536
Jan 12 16:02:00.543: INFO: Waiting up to 15m0s for pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd" in namespace "dns-1922" to be "running"
Jan 12 16:02:00.544: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79366ms
Jan 12 16:02:02.547: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004537393s
Jan 12 16:02:02.547: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:02:02.547
STEP: looking for the results for each expected name from probers 01/12/23 16:02:02.549
Jan 12 16:02:02.557: INFO: DNS probes using dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd succeeded

STEP: deleting the pod 01/12/23 16:02:02.557
STEP: changing the externalName to bar.example.com 01/12/23 16:02:02.566
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:02.571
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:02.571
STEP: creating a second pod to probe DNS 01/12/23 16:02:02.571
STEP: submitting the pod to kubernetes 01/12/23 16:02:02.572
Jan 12 16:02:02.578: INFO: Waiting up to 15m0s for pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed" in namespace "dns-1922" to be "running"
Jan 12 16:02:02.580: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829402ms
Jan 12 16:02:04.583: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed": Phase="Running", Reason="", readiness=true. Elapsed: 2.005162563s
Jan 12 16:02:04.583: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:02:04.583
STEP: looking for the results for each expected name from probers 01/12/23 16:02:04.585
Jan 12 16:02:04.590: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:04.592: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:04.592: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:09.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:09.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:09.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:14.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:14.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:14.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:19.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:19.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:19.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:24.598: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:24.601: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:24.601: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:29.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:29.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 12 16:02:29.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

Jan 12 16:02:34.599: INFO: DNS probes using dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed succeeded

STEP: deleting the pod 01/12/23 16:02:34.599
STEP: changing the service to type=ClusterIP 01/12/23 16:02:34.608
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:34.623
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
 01/12/23 16:02:34.623
STEP: creating a third pod to probe DNS 01/12/23 16:02:34.623
STEP: submitting the pod to kubernetes 01/12/23 16:02:34.625
Jan 12 16:02:34.632: INFO: Waiting up to 15m0s for pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb" in namespace "dns-1922" to be "running"
Jan 12 16:02:34.634: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051523ms
Jan 12 16:02:36.636: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004046732s
Jan 12 16:02:36.636: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:02:36.636
STEP: looking for the results for each expected name from probers 01/12/23 16:02:36.638
Jan 12 16:02:36.645: INFO: DNS probes using dns-test-9ec339d0-ca70-4410-9085-599e760498cb succeeded

STEP: deleting the pod 01/12/23 16:02:36.645
STEP: deleting the test externalName service 01/12/23 16:02:36.654
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 16:02:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1922" for this suite. 01/12/23 16:02:36.672
------------------------------
 [SLOW TEST] [36.163 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:02:00.513
    Jan 12 16:02:00.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 16:02:00.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:02:00.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:02:00.526
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/12/23 16:02:00.53
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:00.536
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:00.536
    STEP: creating a pod to probe DNS 01/12/23 16:02:00.536
    STEP: submitting the pod to kubernetes 01/12/23 16:02:00.536
    Jan 12 16:02:00.543: INFO: Waiting up to 15m0s for pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd" in namespace "dns-1922" to be "running"
    Jan 12 16:02:00.544: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.79366ms
    Jan 12 16:02:02.547: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.004537393s
    Jan 12 16:02:02.547: INFO: Pod "dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:02:02.547
    STEP: looking for the results for each expected name from probers 01/12/23 16:02:02.549
    Jan 12 16:02:02.557: INFO: DNS probes using dns-test-8b4fa467-f4a4-4bca-9af7-4dfade80a5fd succeeded

    STEP: deleting the pod 01/12/23 16:02:02.557
    STEP: changing the externalName to bar.example.com 01/12/23 16:02:02.566
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:02.571
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:02.571
    STEP: creating a second pod to probe DNS 01/12/23 16:02:02.571
    STEP: submitting the pod to kubernetes 01/12/23 16:02:02.572
    Jan 12 16:02:02.578: INFO: Waiting up to 15m0s for pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed" in namespace "dns-1922" to be "running"
    Jan 12 16:02:02.580: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.829402ms
    Jan 12 16:02:04.583: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed": Phase="Running", Reason="", readiness=true. Elapsed: 2.005162563s
    Jan 12 16:02:04.583: INFO: Pod "dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:02:04.583
    STEP: looking for the results for each expected name from probers 01/12/23 16:02:04.585
    Jan 12 16:02:04.590: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:04.592: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:04.592: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:09.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:09.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:09.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:14.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:14.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:14.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:19.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:19.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:19.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:24.598: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:24.601: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:24.601: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:29.596: INFO: File wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:29.599: INFO: File jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local from pod  dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 12 16:02:29.599: INFO: Lookups using dns-1922/dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed failed for: [wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local]

    Jan 12 16:02:34.599: INFO: DNS probes using dns-test-64a9ecb3-0d28-4193-b5fe-c0ef27380eed succeeded

    STEP: deleting the pod 01/12/23 16:02:34.599
    STEP: changing the service to type=ClusterIP 01/12/23 16:02:34.608
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:34.623
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1922.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1922.svc.cluster.local; sleep 1; done
     01/12/23 16:02:34.623
    STEP: creating a third pod to probe DNS 01/12/23 16:02:34.623
    STEP: submitting the pod to kubernetes 01/12/23 16:02:34.625
    Jan 12 16:02:34.632: INFO: Waiting up to 15m0s for pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb" in namespace "dns-1922" to be "running"
    Jan 12 16:02:34.634: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051523ms
    Jan 12 16:02:36.636: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.004046732s
    Jan 12 16:02:36.636: INFO: Pod "dns-test-9ec339d0-ca70-4410-9085-599e760498cb" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:02:36.636
    STEP: looking for the results for each expected name from probers 01/12/23 16:02:36.638
    Jan 12 16:02:36.645: INFO: DNS probes using dns-test-9ec339d0-ca70-4410-9085-599e760498cb succeeded

    STEP: deleting the pod 01/12/23 16:02:36.645
    STEP: deleting the test externalName service 01/12/23 16:02:36.654
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:02:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1922" for this suite. 01/12/23 16:02:36.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:02:36.678
Jan 12 16:02:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 16:02:36.679
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:02:36.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:02:36.694
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 in namespace container-probe-5061 01/12/23 16:02:36.697
Jan 12 16:02:36.703: INFO: Waiting up to 5m0s for pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680" in namespace "container-probe-5061" to be "not pending"
Jan 12 16:02:36.704: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821272ms
Jan 12 16:02:38.707: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680": Phase="Running", Reason="", readiness=true. Elapsed: 2.004270919s
Jan 12 16:02:38.707: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680" satisfied condition "not pending"
Jan 12 16:02:38.707: INFO: Started pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 in namespace container-probe-5061
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:02:38.707
Jan 12 16:02:38.709: INFO: Initial restart count of pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 is 0
STEP: deleting the pod 01/12/23 16:06:39.094
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 16:06:39.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5061" for this suite. 01/12/23 16:06:39.108
------------------------------
 [SLOW TEST] [242.433 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:02:36.678
    Jan 12 16:02:36.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 16:02:36.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:02:36.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:02:36.694
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 in namespace container-probe-5061 01/12/23 16:02:36.697
    Jan 12 16:02:36.703: INFO: Waiting up to 5m0s for pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680" in namespace "container-probe-5061" to be "not pending"
    Jan 12 16:02:36.704: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821272ms
    Jan 12 16:02:38.707: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680": Phase="Running", Reason="", readiness=true. Elapsed: 2.004270919s
    Jan 12 16:02:38.707: INFO: Pod "test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680" satisfied condition "not pending"
    Jan 12 16:02:38.707: INFO: Started pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 in namespace container-probe-5061
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:02:38.707
    Jan 12 16:02:38.709: INFO: Initial restart count of pod test-webserver-84b878a2-bc48-454e-b35e-0a40ba92d680 is 0
    STEP: deleting the pod 01/12/23 16:06:39.094
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:06:39.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5061" for this suite. 01/12/23 16:06:39.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:06:39.112
Jan 12 16:06:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:06:39.112
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:39.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:39.123
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/12/23 16:06:39.131
Jan 12 16:06:39.131: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6" in namespace "kubelet-test-3492" to be "completed"
Jan 12 16:06:39.133: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.447095ms
Jan 12 16:06:41.136: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005146698s
Jan 12 16:06:43.137: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00632345s
Jan 12 16:06:43.137: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:06:43.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3492" for this suite. 01/12/23 16:06:43.15
------------------------------
 [4.042 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:06:39.112
    Jan 12 16:06:39.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:06:39.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:39.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:39.123
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/12/23 16:06:39.131
    Jan 12 16:06:39.131: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6" in namespace "kubelet-test-3492" to be "completed"
    Jan 12 16:06:39.133: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.447095ms
    Jan 12 16:06:41.136: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005146698s
    Jan 12 16:06:43.137: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00632345s
    Jan 12 16:06:43.137: INFO: Pod "agnhost-host-aliasesf55d20d2-e069-41e8-9de0-19ed792e3ac6" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:06:43.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3492" for this suite. 01/12/23 16:06:43.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:06:43.155
Jan 12 16:06:43.155: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:06:43.156
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:43.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:43.166
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:06:43.178
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:06:43.842
STEP: Deploying the webhook pod 01/12/23 16:06:43.848
STEP: Wait for the deployment to be ready 01/12/23 16:06:43.857
Jan 12 16:06:43.864: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:06:45.871
STEP: Verifying the service has paired with the endpoint 01/12/23 16:06:45.881
Jan 12 16:06:46.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/12/23 16:06:46.936
STEP: Creating a configMap that should be mutated 01/12/23 16:06:46.949
STEP: Deleting the collection of validation webhooks 01/12/23 16:06:46.979
STEP: Creating a configMap that should not be mutated 01/12/23 16:06:47.005
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:06:47.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3954" for this suite. 01/12/23 16:06:47.047
STEP: Destroying namespace "webhook-3954-markers" for this suite. 01/12/23 16:06:47.054
------------------------------
 [3.903 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:06:43.155
    Jan 12 16:06:43.155: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:06:43.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:43.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:43.166
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:06:43.178
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:06:43.842
    STEP: Deploying the webhook pod 01/12/23 16:06:43.848
    STEP: Wait for the deployment to be ready 01/12/23 16:06:43.857
    Jan 12 16:06:43.864: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:06:45.871
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:06:45.881
    Jan 12 16:06:46.881: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/12/23 16:06:46.936
    STEP: Creating a configMap that should be mutated 01/12/23 16:06:46.949
    STEP: Deleting the collection of validation webhooks 01/12/23 16:06:46.979
    STEP: Creating a configMap that should not be mutated 01/12/23 16:06:47.005
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:06:47.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3954" for this suite. 01/12/23 16:06:47.047
    STEP: Destroying namespace "webhook-3954-markers" for this suite. 01/12/23 16:06:47.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:06:47.061
Jan 12 16:06:47.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename subpath 01/12/23 16:06:47.062
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:47.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:47.075
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 16:06:47.077
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-2ndp 01/12/23 16:06:47.083
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:06:47.083
Jan 12 16:06:47.088: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2ndp" in namespace "subpath-2853" to be "Succeeded or Failed"
Jan 12 16:06:47.089: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.735547ms
Jan 12 16:06:49.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005007965s
Jan 12 16:06:51.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 4.004585023s
Jan 12 16:06:53.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 6.004498893s
Jan 12 16:06:55.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 8.004922166s
Jan 12 16:06:57.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 10.005126416s
Jan 12 16:06:59.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 12.004698914s
Jan 12 16:07:01.094: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 14.005948976s
Jan 12 16:07:03.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 16.004261504s
Jan 12 16:07:05.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00498895s
Jan 12 16:07:07.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 20.004899773s
Jan 12 16:07:09.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=false. Elapsed: 22.004635438s
Jan 12 16:07:11.094: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005837984s
STEP: Saw pod success 01/12/23 16:07:11.094
Jan 12 16:07:11.094: INFO: Pod "pod-subpath-test-configmap-2ndp" satisfied condition "Succeeded or Failed"
Jan 12 16:07:11.095: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-2ndp container test-container-subpath-configmap-2ndp: <nil>
STEP: delete the pod 01/12/23 16:07:11.101
Jan 12 16:07:11.110: INFO: Waiting for pod pod-subpath-test-configmap-2ndp to disappear
Jan 12 16:07:11.111: INFO: Pod pod-subpath-test-configmap-2ndp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-2ndp 01/12/23 16:07:11.111
Jan 12 16:07:11.112: INFO: Deleting pod "pod-subpath-test-configmap-2ndp" in namespace "subpath-2853"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 16:07:11.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2853" for this suite. 01/12/23 16:07:11.116
------------------------------
 [SLOW TEST] [24.058 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:06:47.061
    Jan 12 16:06:47.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename subpath 01/12/23 16:06:47.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:06:47.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:06:47.075
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 16:06:47.077
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-2ndp 01/12/23 16:06:47.083
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:06:47.083
    Jan 12 16:06:47.088: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-2ndp" in namespace "subpath-2853" to be "Succeeded or Failed"
    Jan 12 16:06:47.089: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.735547ms
    Jan 12 16:06:49.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005007965s
    Jan 12 16:06:51.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 4.004585023s
    Jan 12 16:06:53.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 6.004498893s
    Jan 12 16:06:55.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 8.004922166s
    Jan 12 16:06:57.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 10.005126416s
    Jan 12 16:06:59.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 12.004698914s
    Jan 12 16:07:01.094: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 14.005948976s
    Jan 12 16:07:03.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 16.004261504s
    Jan 12 16:07:05.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 18.00498895s
    Jan 12 16:07:07.093: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=true. Elapsed: 20.004899773s
    Jan 12 16:07:09.092: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Running", Reason="", readiness=false. Elapsed: 22.004635438s
    Jan 12 16:07:11.094: INFO: Pod "pod-subpath-test-configmap-2ndp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005837984s
    STEP: Saw pod success 01/12/23 16:07:11.094
    Jan 12 16:07:11.094: INFO: Pod "pod-subpath-test-configmap-2ndp" satisfied condition "Succeeded or Failed"
    Jan 12 16:07:11.095: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-configmap-2ndp container test-container-subpath-configmap-2ndp: <nil>
    STEP: delete the pod 01/12/23 16:07:11.101
    Jan 12 16:07:11.110: INFO: Waiting for pod pod-subpath-test-configmap-2ndp to disappear
    Jan 12 16:07:11.111: INFO: Pod pod-subpath-test-configmap-2ndp no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-2ndp 01/12/23 16:07:11.111
    Jan 12 16:07:11.112: INFO: Deleting pod "pod-subpath-test-configmap-2ndp" in namespace "subpath-2853"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:07:11.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2853" for this suite. 01/12/23 16:07:11.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:07:11.122
Jan 12 16:07:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:07:11.123
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:07:11.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:07:11.135
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:07:11.144
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:07:11.462
STEP: Deploying the webhook pod 01/12/23 16:07:11.465
STEP: Wait for the deployment to be ready 01/12/23 16:07:11.474
Jan 12 16:07:11.480: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:07:13.486
STEP: Verifying the service has paired with the endpoint 01/12/23 16:07:13.495
Jan 12 16:07:14.496: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 12 16:07:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/12/23 16:07:15.006
STEP: Creating a custom resource that should be denied by the webhook 01/12/23 16:07:15.022
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/12/23 16:07:17.054
STEP: Updating the custom resource with disallowed data should be denied 01/12/23 16:07:17.059
STEP: Deleting the custom resource should be denied 01/12/23 16:07:17.066
STEP: Remove the offending key and value from the custom resource data 01/12/23 16:07:17.071
STEP: Deleting the updated custom resource should be successful 01/12/23 16:07:17.078
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:07:17.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7498" for this suite. 01/12/23 16:07:17.624
STEP: Destroying namespace "webhook-7498-markers" for this suite. 01/12/23 16:07:17.63
------------------------------
 [SLOW TEST] [6.512 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:07:11.122
    Jan 12 16:07:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:07:11.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:07:11.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:07:11.135
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:07:11.144
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:07:11.462
    STEP: Deploying the webhook pod 01/12/23 16:07:11.465
    STEP: Wait for the deployment to be ready 01/12/23 16:07:11.474
    Jan 12 16:07:11.480: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:07:13.486
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:07:13.495
    Jan 12 16:07:14.496: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 12 16:07:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/12/23 16:07:15.006
    STEP: Creating a custom resource that should be denied by the webhook 01/12/23 16:07:15.022
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/12/23 16:07:17.054
    STEP: Updating the custom resource with disallowed data should be denied 01/12/23 16:07:17.059
    STEP: Deleting the custom resource should be denied 01/12/23 16:07:17.066
    STEP: Remove the offending key and value from the custom resource data 01/12/23 16:07:17.071
    STEP: Deleting the updated custom resource should be successful 01/12/23 16:07:17.078
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:07:17.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7498" for this suite. 01/12/23 16:07:17.624
    STEP: Destroying namespace "webhook-7498-markers" for this suite. 01/12/23 16:07:17.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:07:17.636
Jan 12 16:07:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename taint-single-pod 01/12/23 16:07:17.637
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:07:17.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:07:17.649
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 12 16:07:17.651: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 16:08:17.663: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 12 16:08:17.665: INFO: Starting informer...
STEP: Starting pod... 01/12/23 16:08:17.665
Jan 12 16:08:17.876: INFO: Pod is running on worker-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/12/23 16:08:17.876
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 16:08:17.885
STEP: Waiting short time to make sure Pod is queued for deletion 01/12/23 16:08:17.887
Jan 12 16:08:17.888: INFO: Pod wasn't evicted. Proceeding
Jan 12 16:08:17.888: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 16:08:17.897
STEP: Waiting some time to make sure that toleration time passed. 01/12/23 16:08:17.902
Jan 12 16:09:32.904: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:09:32.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3670" for this suite. 01/12/23 16:09:32.907
------------------------------
 [SLOW TEST] [135.275 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:07:17.636
    Jan 12 16:07:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename taint-single-pod 01/12/23 16:07:17.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:07:17.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:07:17.649
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 12 16:07:17.651: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 16:08:17.663: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 12 16:08:17.665: INFO: Starting informer...
    STEP: Starting pod... 01/12/23 16:08:17.665
    Jan 12 16:08:17.876: INFO: Pod is running on worker-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/12/23 16:08:17.876
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 16:08:17.885
    STEP: Waiting short time to make sure Pod is queued for deletion 01/12/23 16:08:17.887
    Jan 12 16:08:17.888: INFO: Pod wasn't evicted. Proceeding
    Jan 12 16:08:17.888: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/12/23 16:08:17.897
    STEP: Waiting some time to make sure that toleration time passed. 01/12/23 16:08:17.902
    Jan 12 16:09:32.904: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:09:32.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3670" for this suite. 01/12/23 16:09:32.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:09:32.912
Jan 12 16:09:32.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:09:32.913
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:32.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:32.922
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:09:32.924
Jan 12 16:09:32.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c" in namespace "projected-5151" to be "Succeeded or Failed"
Jan 12 16:09:32.931: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763699ms
Jan 12 16:09:34.934: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005225453s
Jan 12 16:09:36.933: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004077665s
STEP: Saw pod success 01/12/23 16:09:36.933
Jan 12 16:09:36.933: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c" satisfied condition "Succeeded or Failed"
Jan 12 16:09:36.935: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c container client-container: <nil>
STEP: delete the pod 01/12/23 16:09:36.946
Jan 12 16:09:36.952: INFO: Waiting for pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c to disappear
Jan 12 16:09:36.954: INFO: Pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:09:36.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5151" for this suite. 01/12/23 16:09:36.956
------------------------------
 [4.050 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:09:32.912
    Jan 12 16:09:32.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:09:32.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:32.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:32.922
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:09:32.924
    Jan 12 16:09:32.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c" in namespace "projected-5151" to be "Succeeded or Failed"
    Jan 12 16:09:32.931: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763699ms
    Jan 12 16:09:34.934: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005225453s
    Jan 12 16:09:36.933: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004077665s
    STEP: Saw pod success 01/12/23 16:09:36.933
    Jan 12 16:09:36.933: INFO: Pod "downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c" satisfied condition "Succeeded or Failed"
    Jan 12 16:09:36.935: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c container client-container: <nil>
    STEP: delete the pod 01/12/23 16:09:36.946
    Jan 12 16:09:36.952: INFO: Waiting for pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c to disappear
    Jan 12 16:09:36.954: INFO: Pod downwardapi-volume-1374ac21-2a46-4ee8-9214-32cc135ffc4c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:09:36.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5151" for this suite. 01/12/23 16:09:36.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:09:36.965
Jan 12 16:09:36.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 16:09:36.966
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:36.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:36.975
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97 01/12/23 16:09:36.977
Jan 12 16:09:36.984: INFO: Pod name my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Found 0 pods out of 1
Jan 12 16:09:41.989: INFO: Pod name my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Found 1 pods out of 1
Jan 12 16:09:41.989: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97" are running
Jan 12 16:09:41.989: INFO: Waiting up to 5m0s for pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" in namespace "replication-controller-3884" to be "running"
Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2": Phase="Running", Reason="", readiness=true. Elapsed: 1.769768ms
Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" satisfied condition "running"
Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:36 +0000 UTC Reason: Message:}])
Jan 12 16:09:41.990: INFO: Trying to dial the pod
Jan 12 16:09:47.000: INFO: Controller my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Got expected result from replica 1 [my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2]: "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:09:47.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3884" for this suite. 01/12/23 16:09:47.002
------------------------------
 [SLOW TEST] [10.041 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:09:36.965
    Jan 12 16:09:36.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 16:09:36.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:36.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:36.975
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97 01/12/23 16:09:36.977
    Jan 12 16:09:36.984: INFO: Pod name my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Found 0 pods out of 1
    Jan 12 16:09:41.989: INFO: Pod name my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Found 1 pods out of 1
    Jan 12 16:09:41.989: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97" are running
    Jan 12 16:09:41.989: INFO: Waiting up to 5m0s for pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" in namespace "replication-controller-3884" to be "running"
    Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2": Phase="Running", Reason="", readiness=true. Elapsed: 1.769768ms
    Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" satisfied condition "running"
    Jan 12 16:09:41.990: INFO: Pod "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:36 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-12 16:09:36 +0000 UTC Reason: Message:}])
    Jan 12 16:09:41.990: INFO: Trying to dial the pod
    Jan 12 16:09:47.000: INFO: Controller my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97: Got expected result from replica 1 [my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2]: "my-hostname-basic-67a75dfb-2e8c-426d-8bcc-42403dcd8b97-6hjj2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:09:47.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3884" for this suite. 01/12/23 16:09:47.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:09:47.007
Jan 12 16:09:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:09:47.008
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:47.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:47.018
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3128 01/12/23 16:09:47.021
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3128 01/12/23 16:09:47.024
Jan 12 16:09:47.032: INFO: Found 0 stateful pods, waiting for 1
Jan 12 16:09:57.036: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/12/23 16:09:57.039
STEP: updating a scale subresource 01/12/23 16:09:57.041
STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 16:09:57.046
STEP: Patch a scale subresource 01/12/23 16:09:57.049
STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 16:09:57.054
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:09:57.058: INFO: Deleting all statefulset in ns statefulset-3128
Jan 12 16:09:57.060: INFO: Scaling statefulset ss to 0
Jan 12 16:10:07.073: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:10:07.075: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:07.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3128" for this suite. 01/12/23 16:10:07.085
------------------------------
 [SLOW TEST] [20.082 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:09:47.007
    Jan 12 16:09:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:09:47.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:09:47.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:09:47.018
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3128 01/12/23 16:09:47.021
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3128 01/12/23 16:09:47.024
    Jan 12 16:09:47.032: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 16:09:57.036: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/12/23 16:09:57.039
    STEP: updating a scale subresource 01/12/23 16:09:57.041
    STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 16:09:57.046
    STEP: Patch a scale subresource 01/12/23 16:09:57.049
    STEP: verifying the statefulset Spec.Replicas was modified 01/12/23 16:09:57.054
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:09:57.058: INFO: Deleting all statefulset in ns statefulset-3128
    Jan 12 16:09:57.060: INFO: Scaling statefulset ss to 0
    Jan 12 16:10:07.073: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:10:07.075: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:07.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3128" for this suite. 01/12/23 16:10:07.085
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:07.09
Jan 12 16:10:07.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sysctl 01/12/23 16:10:07.091
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:07.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:07.103
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/12/23 16:10:07.105
STEP: Watching for error events or started pod 01/12/23 16:10:07.116
STEP: Waiting for pod completion 01/12/23 16:10:09.119
Jan 12 16:10:09.119: INFO: Waiting up to 3m0s for pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed" in namespace "sysctl-242" to be "completed"
Jan 12 16:10:09.120: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666601ms
Jan 12 16:10:11.124: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0055616s
Jan 12 16:10:11.124: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/12/23 16:10:11.126
STEP: Getting logs from the pod 01/12/23 16:10:11.126
STEP: Checking that the sysctl is actually updated 01/12/23 16:10:11.13
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:11.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-242" for this suite. 01/12/23 16:10:11.132
------------------------------
 [4.046 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:07.09
    Jan 12 16:10:07.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sysctl 01/12/23 16:10:07.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:07.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:07.103
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/12/23 16:10:07.105
    STEP: Watching for error events or started pod 01/12/23 16:10:07.116
    STEP: Waiting for pod completion 01/12/23 16:10:09.119
    Jan 12 16:10:09.119: INFO: Waiting up to 3m0s for pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed" in namespace "sysctl-242" to be "completed"
    Jan 12 16:10:09.120: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666601ms
    Jan 12 16:10:11.124: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0055616s
    Jan 12 16:10:11.124: INFO: Pod "sysctl-bf3bc020-fdcf-433f-8b58-a71c947620ed" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/12/23 16:10:11.126
    STEP: Getting logs from the pod 01/12/23 16:10:11.126
    STEP: Checking that the sysctl is actually updated 01/12/23 16:10:11.13
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:11.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-242" for this suite. 01/12/23 16:10:11.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:11.14
Jan 12 16:10:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename subpath 01/12/23 16:10:11.141
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:11.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:11.15
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 16:10:11.152
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rwjn 01/12/23 16:10:11.158
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:10:11.158
Jan 12 16:10:11.163: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rwjn" in namespace "subpath-5046" to be "Succeeded or Failed"
Jan 12 16:10:11.165: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.712825ms
Jan 12 16:10:13.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005060626s
Jan 12 16:10:15.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005323969s
Jan 12 16:10:17.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 6.004603182s
Jan 12 16:10:19.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 8.004354956s
Jan 12 16:10:21.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 10.005000853s
Jan 12 16:10:23.167: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 12.003657894s
Jan 12 16:10:25.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 14.005252887s
Jan 12 16:10:27.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 16.004275967s
Jan 12 16:10:29.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 18.005129655s
Jan 12 16:10:31.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 20.004800037s
Jan 12 16:10:33.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=false. Elapsed: 22.005874105s
Jan 12 16:10:35.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005154533s
STEP: Saw pod success 01/12/23 16:10:35.169
Jan 12 16:10:35.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn" satisfied condition "Succeeded or Failed"
Jan 12 16:10:35.170: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-downwardapi-rwjn container test-container-subpath-downwardapi-rwjn: <nil>
STEP: delete the pod 01/12/23 16:10:35.175
Jan 12 16:10:35.184: INFO: Waiting for pod pod-subpath-test-downwardapi-rwjn to disappear
Jan 12 16:10:35.187: INFO: Pod pod-subpath-test-downwardapi-rwjn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rwjn 01/12/23 16:10:35.187
Jan 12 16:10:35.188: INFO: Deleting pod "pod-subpath-test-downwardapi-rwjn" in namespace "subpath-5046"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5046" for this suite. 01/12/23 16:10:35.192
------------------------------
 [SLOW TEST] [24.056 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:11.14
    Jan 12 16:10:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename subpath 01/12/23 16:10:11.141
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:11.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:11.15
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 16:10:11.152
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rwjn 01/12/23 16:10:11.158
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:10:11.158
    Jan 12 16:10:11.163: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rwjn" in namespace "subpath-5046" to be "Succeeded or Failed"
    Jan 12 16:10:11.165: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Pending", Reason="", readiness=false. Elapsed: 1.712825ms
    Jan 12 16:10:13.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005060626s
    Jan 12 16:10:15.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 4.005323969s
    Jan 12 16:10:17.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 6.004603182s
    Jan 12 16:10:19.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 8.004354956s
    Jan 12 16:10:21.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 10.005000853s
    Jan 12 16:10:23.167: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 12.003657894s
    Jan 12 16:10:25.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 14.005252887s
    Jan 12 16:10:27.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 16.004275967s
    Jan 12 16:10:29.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 18.005129655s
    Jan 12 16:10:31.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=true. Elapsed: 20.004800037s
    Jan 12 16:10:33.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Running", Reason="", readiness=false. Elapsed: 22.005874105s
    Jan 12 16:10:35.168: INFO: Pod "pod-subpath-test-downwardapi-rwjn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005154533s
    STEP: Saw pod success 01/12/23 16:10:35.169
    Jan 12 16:10:35.169: INFO: Pod "pod-subpath-test-downwardapi-rwjn" satisfied condition "Succeeded or Failed"
    Jan 12 16:10:35.170: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-downwardapi-rwjn container test-container-subpath-downwardapi-rwjn: <nil>
    STEP: delete the pod 01/12/23 16:10:35.175
    Jan 12 16:10:35.184: INFO: Waiting for pod pod-subpath-test-downwardapi-rwjn to disappear
    Jan 12 16:10:35.187: INFO: Pod pod-subpath-test-downwardapi-rwjn no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rwjn 01/12/23 16:10:35.187
    Jan 12 16:10:35.188: INFO: Deleting pod "pod-subpath-test-downwardapi-rwjn" in namespace "subpath-5046"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:35.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5046" for this suite. 01/12/23 16:10:35.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:35.198
Jan 12 16:10:35.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:10:35.198
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:35.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:35.209
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/12/23 16:10:35.211
Jan 12 16:10:35.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 12 16:10:35.277: INFO: stderr: ""
Jan 12 16:10:35.277: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/12/23 16:10:35.277
Jan 12 16:10:35.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 12 16:10:35.277: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7107" to be "running and ready, or succeeded"
Jan 12 16:10:35.279: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028957ms
Jan 12 16:10:35.279: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-1' to be 'Running' but was 'Pending'
Jan 12 16:10:37.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005306803s
Jan 12 16:10:37.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 12 16:10:37.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/12/23 16:10:37.282
Jan 12 16:10:37.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator'
Jan 12 16:10:37.344: INFO: stderr: ""
Jan 12 16:10:37.344: INFO: stdout: "I0112 16:10:35.804671       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2jd2 306\nI0112 16:10:36.005058       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/27t 201\nI0112 16:10:36.205389       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/lnw4 215\nI0112 16:10:36.405731       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/7rf 508\nI0112 16:10:36.605084       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pkb 503\nI0112 16:10:36.805461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4j9 306\nI0112 16:10:37.004780       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/skgh 512\nI0112 16:10:37.205170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/772x 452\n"
STEP: limiting log lines 01/12/23 16:10:37.344
Jan 12 16:10:37.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --tail=1'
Jan 12 16:10:37.409: INFO: stderr: ""
Jan 12 16:10:37.409: INFO: stdout: "I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
Jan 12 16:10:37.409: INFO: got output "I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
STEP: limiting log bytes 01/12/23 16:10:37.409
Jan 12 16:10:37.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --limit-bytes=1'
Jan 12 16:10:37.471: INFO: stderr: ""
Jan 12 16:10:37.471: INFO: stdout: "I"
Jan 12 16:10:37.471: INFO: got output "I"
STEP: exposing timestamps 01/12/23 16:10:37.471
Jan 12 16:10:37.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 12 16:10:37.531: INFO: stderr: ""
Jan 12 16:10:37.531: INFO: stdout: "2023-01-12T16:10:37.405608891Z I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
Jan 12 16:10:37.531: INFO: got output "2023-01-12T16:10:37.405608891Z I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
STEP: restricting to a time range 01/12/23 16:10:37.531
Jan 12 16:10:40.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --since=1s'
Jan 12 16:10:40.095: INFO: stderr: ""
Jan 12 16:10:40.095: INFO: stdout: "I0112 16:10:39.205517       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8qb 218\nI0112 16:10:39.404767       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/865w 215\nI0112 16:10:39.605146       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/k72x 395\nI0112 16:10:39.805481       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/bmn 551\nI0112 16:10:40.004769       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/tcm2 504\n"
Jan 12 16:10:40.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --since=24h'
Jan 12 16:10:40.156: INFO: stderr: ""
Jan 12 16:10:40.156: INFO: stdout: "I0112 16:10:35.804671       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2jd2 306\nI0112 16:10:36.005058       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/27t 201\nI0112 16:10:36.205389       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/lnw4 215\nI0112 16:10:36.405731       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/7rf 508\nI0112 16:10:36.605084       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pkb 503\nI0112 16:10:36.805461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4j9 306\nI0112 16:10:37.004780       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/skgh 512\nI0112 16:10:37.205170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/772x 452\nI0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\nI0112 16:10:37.604791       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/lb8 379\nI0112 16:10:37.805136       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/6f5 290\nI0112 16:10:38.005515       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/zrl8 517\nI0112 16:10:38.204830       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/dsw 237\nI0112 16:10:38.405174       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/vz8w 246\nI0112 16:10:38.605503       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cxxs 323\nI0112 16:10:38.804770       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/bm8 452\nI0112 16:10:39.005105       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/9d6p 368\nI0112 16:10:39.205517       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8qb 218\nI0112 16:10:39.404767       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/865w 215\nI0112 16:10:39.605146       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/k72x 395\nI0112 16:10:39.805481       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/bmn 551\nI0112 16:10:40.004769       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/tcm2 504\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 12 16:10:40.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 delete pod logs-generator'
Jan 12 16:10:40.599: INFO: stderr: ""
Jan 12 16:10:40.599: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7107" for this suite. 01/12/23 16:10:40.602
------------------------------
 [SLOW TEST] [5.409 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:35.198
    Jan 12 16:10:35.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:10:35.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:35.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:35.209
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/12/23 16:10:35.211
    Jan 12 16:10:35.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 12 16:10:35.277: INFO: stderr: ""
    Jan 12 16:10:35.277: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/12/23 16:10:35.277
    Jan 12 16:10:35.277: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 12 16:10:35.277: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7107" to be "running and ready, or succeeded"
    Jan 12 16:10:35.279: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028957ms
    Jan 12 16:10:35.279: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'worker-1' to be 'Running' but was 'Pending'
    Jan 12 16:10:37.282: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.005306803s
    Jan 12 16:10:37.282: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 12 16:10:37.282: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/12/23 16:10:37.282
    Jan 12 16:10:37.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator'
    Jan 12 16:10:37.344: INFO: stderr: ""
    Jan 12 16:10:37.344: INFO: stdout: "I0112 16:10:35.804671       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2jd2 306\nI0112 16:10:36.005058       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/27t 201\nI0112 16:10:36.205389       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/lnw4 215\nI0112 16:10:36.405731       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/7rf 508\nI0112 16:10:36.605084       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pkb 503\nI0112 16:10:36.805461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4j9 306\nI0112 16:10:37.004780       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/skgh 512\nI0112 16:10:37.205170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/772x 452\n"
    STEP: limiting log lines 01/12/23 16:10:37.344
    Jan 12 16:10:37.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --tail=1'
    Jan 12 16:10:37.409: INFO: stderr: ""
    Jan 12 16:10:37.409: INFO: stdout: "I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
    Jan 12 16:10:37.409: INFO: got output "I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
    STEP: limiting log bytes 01/12/23 16:10:37.409
    Jan 12 16:10:37.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --limit-bytes=1'
    Jan 12 16:10:37.471: INFO: stderr: ""
    Jan 12 16:10:37.471: INFO: stdout: "I"
    Jan 12 16:10:37.471: INFO: got output "I"
    STEP: exposing timestamps 01/12/23 16:10:37.471
    Jan 12 16:10:37.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 12 16:10:37.531: INFO: stderr: ""
    Jan 12 16:10:37.531: INFO: stdout: "2023-01-12T16:10:37.405608891Z I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
    Jan 12 16:10:37.531: INFO: got output "2023-01-12T16:10:37.405608891Z I0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\n"
    STEP: restricting to a time range 01/12/23 16:10:37.531
    Jan 12 16:10:40.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --since=1s'
    Jan 12 16:10:40.095: INFO: stderr: ""
    Jan 12 16:10:40.095: INFO: stdout: "I0112 16:10:39.205517       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8qb 218\nI0112 16:10:39.404767       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/865w 215\nI0112 16:10:39.605146       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/k72x 395\nI0112 16:10:39.805481       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/bmn 551\nI0112 16:10:40.004769       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/tcm2 504\n"
    Jan 12 16:10:40.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 logs logs-generator logs-generator --since=24h'
    Jan 12 16:10:40.156: INFO: stderr: ""
    Jan 12 16:10:40.156: INFO: stdout: "I0112 16:10:35.804671       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/2jd2 306\nI0112 16:10:36.005058       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/27t 201\nI0112 16:10:36.205389       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/lnw4 215\nI0112 16:10:36.405731       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/7rf 508\nI0112 16:10:36.605084       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/pkb 503\nI0112 16:10:36.805461       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/t4j9 306\nI0112 16:10:37.004780       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/skgh 512\nI0112 16:10:37.205170       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/772x 452\nI0112 16:10:37.405506       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/6vz 425\nI0112 16:10:37.604791       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/lb8 379\nI0112 16:10:37.805136       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/6f5 290\nI0112 16:10:38.005515       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/zrl8 517\nI0112 16:10:38.204830       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/dsw 237\nI0112 16:10:38.405174       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/vz8w 246\nI0112 16:10:38.605503       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/cxxs 323\nI0112 16:10:38.804770       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/bm8 452\nI0112 16:10:39.005105       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/9d6p 368\nI0112 16:10:39.205517       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/8qb 218\nI0112 16:10:39.404767       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/865w 215\nI0112 16:10:39.605146       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/k72x 395\nI0112 16:10:39.805481       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/bmn 551\nI0112 16:10:40.004769       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/tcm2 504\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 12 16:10:40.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-7107 delete pod logs-generator'
    Jan 12 16:10:40.599: INFO: stderr: ""
    Jan 12 16:10:40.599: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7107" for this suite. 01/12/23 16:10:40.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:40.608
Jan 12 16:10:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:10:40.608
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:40.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:40.619
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-980de54f-6b97-4c1f-b5d4-473395ceff40 01/12/23 16:10:40.621
STEP: Creating a pod to test consume configMaps 01/12/23 16:10:40.626
Jan 12 16:10:40.631: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9" in namespace "projected-2724" to be "Succeeded or Failed"
Jan 12 16:10:40.632: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632007ms
Jan 12 16:10:42.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004658347s
Jan 12 16:10:44.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004283509s
Jan 12 16:10:46.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004474924s
STEP: Saw pod success 01/12/23 16:10:46.635
Jan 12 16:10:46.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9" satisfied condition "Succeeded or Failed"
Jan 12 16:10:46.637: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:10:46.641
Jan 12 16:10:46.649: INFO: Waiting for pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 to disappear
Jan 12 16:10:46.650: INFO: Pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2724" for this suite. 01/12/23 16:10:46.652
------------------------------
 [SLOW TEST] [6.048 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:40.608
    Jan 12 16:10:40.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:10:40.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:40.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:40.619
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-980de54f-6b97-4c1f-b5d4-473395ceff40 01/12/23 16:10:40.621
    STEP: Creating a pod to test consume configMaps 01/12/23 16:10:40.626
    Jan 12 16:10:40.631: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9" in namespace "projected-2724" to be "Succeeded or Failed"
    Jan 12 16:10:40.632: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632007ms
    Jan 12 16:10:42.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004658347s
    Jan 12 16:10:44.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004283509s
    Jan 12 16:10:46.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004474924s
    STEP: Saw pod success 01/12/23 16:10:46.635
    Jan 12 16:10:46.635: INFO: Pod "pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9" satisfied condition "Succeeded or Failed"
    Jan 12 16:10:46.637: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:10:46.641
    Jan 12 16:10:46.649: INFO: Waiting for pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 to disappear
    Jan 12 16:10:46.650: INFO: Pod pod-projected-configmaps-abe2dce2-65a9-45b3-a5f8-0d4004f006a9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:46.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2724" for this suite. 01/12/23 16:10:46.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:46.658
Jan 12 16:10:46.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:10:46.658
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:46.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:46.671
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/12/23 16:10:46.673
Jan 12 16:10:46.678: INFO: created test-pod-1
Jan 12 16:10:46.682: INFO: created test-pod-2
Jan 12 16:10:46.689: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/12/23 16:10:46.689
Jan 12 16:10:46.689: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6202' to be running and ready
Jan 12 16:10:46.700: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 16:10:46.700: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 16:10:46.700: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 12 16:10:46.700: INFO: 0 / 3 pods in namespace 'pods-6202' are running and ready (0 seconds elapsed)
Jan 12 16:10:46.700: INFO: expected 0 pod replicas in namespace 'pods-6202', 0 are Running and Ready.
Jan 12 16:10:46.700: INFO: POD         NODE      PHASE    GRACE  CONDITIONS
Jan 12 16:10:46.700: INFO: test-pod-1  worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
Jan 12 16:10:46.700: INFO: test-pod-2  worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
Jan 12 16:10:46.700: INFO: test-pod-3  worker-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
Jan 12 16:10:46.700: INFO: 
Jan 12 16:10:48.707: INFO: 3 / 3 pods in namespace 'pods-6202' are running and ready (2 seconds elapsed)
Jan 12 16:10:48.707: INFO: expected 0 pod replicas in namespace 'pods-6202', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/12/23 16:10:48.721
Jan 12 16:10:48.723: INFO: Pod quantity 3 is different from expected quantity 0
Jan 12 16:10:49.726: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6202" for this suite. 01/12/23 16:10:50.728
------------------------------
 [4.075 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:46.658
    Jan 12 16:10:46.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:10:46.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:46.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:46.671
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/12/23 16:10:46.673
    Jan 12 16:10:46.678: INFO: created test-pod-1
    Jan 12 16:10:46.682: INFO: created test-pod-2
    Jan 12 16:10:46.689: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/12/23 16:10:46.689
    Jan 12 16:10:46.689: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6202' to be running and ready
    Jan 12 16:10:46.700: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 16:10:46.700: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 16:10:46.700: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 12 16:10:46.700: INFO: 0 / 3 pods in namespace 'pods-6202' are running and ready (0 seconds elapsed)
    Jan 12 16:10:46.700: INFO: expected 0 pod replicas in namespace 'pods-6202', 0 are Running and Ready.
    Jan 12 16:10:46.700: INFO: POD         NODE      PHASE    GRACE  CONDITIONS
    Jan 12 16:10:46.700: INFO: test-pod-1  worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
    Jan 12 16:10:46.700: INFO: test-pod-2  worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
    Jan 12 16:10:46.700: INFO: test-pod-3  worker-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:10:46 +0000 UTC  }]
    Jan 12 16:10:46.700: INFO: 
    Jan 12 16:10:48.707: INFO: 3 / 3 pods in namespace 'pods-6202' are running and ready (2 seconds elapsed)
    Jan 12 16:10:48.707: INFO: expected 0 pod replicas in namespace 'pods-6202', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/12/23 16:10:48.721
    Jan 12 16:10:48.723: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 12 16:10:49.726: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6202" for this suite. 01/12/23 16:10:50.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:50.733
Jan 12 16:10:50.733: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:10:50.734
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:50.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:50.745
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 12 16:10:50.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:10:52.161
Jan 12 16:10:52.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 create -f -'
Jan 12 16:10:52.719: INFO: stderr: ""
Jan 12 16:10:52.719: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 12 16:10:52.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 delete e2e-test-crd-publish-openapi-7329-crds test-cr'
Jan 12 16:10:52.781: INFO: stderr: ""
Jan 12 16:10:52.781: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 12 16:10:52.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 apply -f -'
Jan 12 16:10:53.011: INFO: stderr: ""
Jan 12 16:10:53.011: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 12 16:10:53.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 delete e2e-test-crd-publish-openapi-7329-crds test-cr'
Jan 12 16:10:53.072: INFO: stderr: ""
Jan 12 16:10:53.072: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/12/23 16:10:53.072
Jan 12 16:10:53.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 explain e2e-test-crd-publish-openapi-7329-crds'
Jan 12 16:10:53.267: INFO: stderr: ""
Jan 12 16:10:53.267: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7329-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:10:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1945" for this suite. 01/12/23 16:10:54.641
------------------------------
 [3.914 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:50.733
    Jan 12 16:10:50.733: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:10:50.734
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:50.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:50.745
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 12 16:10:50.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:10:52.161
    Jan 12 16:10:52.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 create -f -'
    Jan 12 16:10:52.719: INFO: stderr: ""
    Jan 12 16:10:52.719: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 12 16:10:52.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 delete e2e-test-crd-publish-openapi-7329-crds test-cr'
    Jan 12 16:10:52.781: INFO: stderr: ""
    Jan 12 16:10:52.781: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 12 16:10:52.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 apply -f -'
    Jan 12 16:10:53.011: INFO: stderr: ""
    Jan 12 16:10:53.011: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 12 16:10:53.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 --namespace=crd-publish-openapi-1945 delete e2e-test-crd-publish-openapi-7329-crds test-cr'
    Jan 12 16:10:53.072: INFO: stderr: ""
    Jan 12 16:10:53.072: INFO: stdout: "e2e-test-crd-publish-openapi-7329-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/12/23 16:10:53.072
    Jan 12 16:10:53.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-1945 explain e2e-test-crd-publish-openapi-7329-crds'
    Jan 12 16:10:53.267: INFO: stderr: ""
    Jan 12 16:10:53.267: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7329-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:10:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1945" for this suite. 01/12/23 16:10:54.641
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:10:54.648
Jan 12 16:10:54.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 16:10:54.649
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:54.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:54.658
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/12/23 16:10:54.66
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:54.669
STEP: Creating a pod in the namespace 01/12/23 16:10:54.671
STEP: Waiting for the pod to have running status 01/12/23 16:10:54.675
Jan 12 16:10:54.676: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6931" to be "running"
Jan 12 16:10:54.678: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.840658ms
Jan 12 16:10:56.680: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00437651s
Jan 12 16:10:56.680: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/12/23 16:10:56.68
STEP: Waiting for the namespace to be removed. 01/12/23 16:10:56.684
STEP: Recreating the namespace 01/12/23 16:11:07.687
STEP: Verifying there are no pods in the namespace 01/12/23 16:11:07.703
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8763" for this suite. 01/12/23 16:11:07.707
STEP: Destroying namespace "nsdeletetest-6931" for this suite. 01/12/23 16:11:07.711
Jan 12 16:11:07.712: INFO: Namespace nsdeletetest-6931 was already deleted
STEP: Destroying namespace "nsdeletetest-6409" for this suite. 01/12/23 16:11:07.712
------------------------------
 [SLOW TEST] [13.068 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:10:54.648
    Jan 12 16:10:54.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 16:10:54.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:54.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:10:54.658
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/12/23 16:10:54.66
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:10:54.669
    STEP: Creating a pod in the namespace 01/12/23 16:10:54.671
    STEP: Waiting for the pod to have running status 01/12/23 16:10:54.675
    Jan 12 16:10:54.676: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6931" to be "running"
    Jan 12 16:10:54.678: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.840658ms
    Jan 12 16:10:56.680: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00437651s
    Jan 12 16:10:56.680: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/12/23 16:10:56.68
    STEP: Waiting for the namespace to be removed. 01/12/23 16:10:56.684
    STEP: Recreating the namespace 01/12/23 16:11:07.687
    STEP: Verifying there are no pods in the namespace 01/12/23 16:11:07.703
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8763" for this suite. 01/12/23 16:11:07.707
    STEP: Destroying namespace "nsdeletetest-6931" for this suite. 01/12/23 16:11:07.711
    Jan 12 16:11:07.712: INFO: Namespace nsdeletetest-6931 was already deleted
    STEP: Destroying namespace "nsdeletetest-6409" for this suite. 01/12/23 16:11:07.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:07.716
Jan 12 16:11:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:11:07.717
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:07.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:07.729
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/12/23 16:11:07.731
Jan 12 16:11:07.731: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8203 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/12/23 16:11:07.776
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:07.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8203" for this suite. 01/12/23 16:11:07.79
------------------------------
 [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:07.716
    Jan 12 16:11:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:11:07.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:07.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:07.729
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/12/23 16:11:07.731
    Jan 12 16:11:07.731: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8203 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/12/23 16:11:07.776
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:07.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8203" for this suite. 01/12/23 16:11:07.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:07.795
Jan 12 16:11:07.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 16:11:07.796
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:07.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:07.806
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/12/23 16:11:07.808
STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 16:11:07.812
STEP: delete the deployment 01/12/23 16:11:08.32
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/12/23 16:11:08.323
STEP: Gathering metrics 01/12/23 16:11:08.834
W0112 16:11:08.837058      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 16:11:08.837: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:08.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-237" for this suite. 01/12/23 16:11:08.839
------------------------------
 [1.047 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:07.795
    Jan 12 16:11:07.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 16:11:07.796
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:07.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:07.806
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/12/23 16:11:07.808
    STEP: Wait for the Deployment to create new ReplicaSet 01/12/23 16:11:07.812
    STEP: delete the deployment 01/12/23 16:11:08.32
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/12/23 16:11:08.323
    STEP: Gathering metrics 01/12/23 16:11:08.834
    W0112 16:11:08.837058      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 16:11:08.837: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:08.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-237" for this suite. 01/12/23 16:11:08.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:08.845
Jan 12 16:11:08.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context 01/12/23 16:11:08.846
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:08.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:08.857
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 16:11:08.859
Jan 12 16:11:08.866: INFO: Waiting up to 5m0s for pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb" in namespace "security-context-1267" to be "Succeeded or Failed"
Jan 12 16:11:08.867: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822647ms
Jan 12 16:11:10.870: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004405459s
Jan 12 16:11:12.872: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006198374s
STEP: Saw pod success 01/12/23 16:11:12.872
Jan 12 16:11:12.872: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb" satisfied condition "Succeeded or Failed"
Jan 12 16:11:12.874: INFO: Trying to get logs from node worker-1 pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb container test-container: <nil>
STEP: delete the pod 01/12/23 16:11:12.881
Jan 12 16:11:12.894: INFO: Waiting for pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb to disappear
Jan 12 16:11:12.896: INFO: Pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:12.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1267" for this suite. 01/12/23 16:11:12.9
------------------------------
 [4.063 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:08.845
    Jan 12 16:11:08.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context 01/12/23 16:11:08.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:08.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:08.857
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 16:11:08.859
    Jan 12 16:11:08.866: INFO: Waiting up to 5m0s for pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb" in namespace "security-context-1267" to be "Succeeded or Failed"
    Jan 12 16:11:08.867: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.822647ms
    Jan 12 16:11:10.870: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004405459s
    Jan 12 16:11:12.872: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006198374s
    STEP: Saw pod success 01/12/23 16:11:12.872
    Jan 12 16:11:12.872: INFO: Pod "security-context-4e533028-4321-484c-80fb-cfad1c007bdb" satisfied condition "Succeeded or Failed"
    Jan 12 16:11:12.874: INFO: Trying to get logs from node worker-1 pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb container test-container: <nil>
    STEP: delete the pod 01/12/23 16:11:12.881
    Jan 12 16:11:12.894: INFO: Waiting for pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb to disappear
    Jan 12 16:11:12.896: INFO: Pod security-context-4e533028-4321-484c-80fb-cfad1c007bdb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:12.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1267" for this suite. 01/12/23 16:11:12.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:12.909
Jan 12 16:11:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:11:12.91
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:12.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:12.925
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-ae7ebb19-8e22-4266-8924-89f7d2a7e0a9 01/12/23 16:11:12.927
STEP: Creating a pod to test consume configMaps 01/12/23 16:11:12.932
Jan 12 16:11:12.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a" in namespace "configmap-8830" to be "Succeeded or Failed"
Jan 12 16:11:12.939: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.797964ms
Jan 12 16:11:14.941: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0045978s
Jan 12 16:11:16.942: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004623103s
STEP: Saw pod success 01/12/23 16:11:16.942
Jan 12 16:11:16.942: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a" satisfied condition "Succeeded or Failed"
Jan 12 16:11:16.943: INFO: Trying to get logs from node worker-1 pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:11:16.948
Jan 12 16:11:16.955: INFO: Waiting for pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a to disappear
Jan 12 16:11:16.957: INFO: Pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:16.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8830" for this suite. 01/12/23 16:11:16.959
------------------------------
 [4.053 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:12.909
    Jan 12 16:11:12.909: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:11:12.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:12.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:12.925
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-ae7ebb19-8e22-4266-8924-89f7d2a7e0a9 01/12/23 16:11:12.927
    STEP: Creating a pod to test consume configMaps 01/12/23 16:11:12.932
    Jan 12 16:11:12.937: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a" in namespace "configmap-8830" to be "Succeeded or Failed"
    Jan 12 16:11:12.939: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.797964ms
    Jan 12 16:11:14.941: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0045978s
    Jan 12 16:11:16.942: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004623103s
    STEP: Saw pod success 01/12/23 16:11:16.942
    Jan 12 16:11:16.942: INFO: Pod "pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a" satisfied condition "Succeeded or Failed"
    Jan 12 16:11:16.943: INFO: Trying to get logs from node worker-1 pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:11:16.948
    Jan 12 16:11:16.955: INFO: Waiting for pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a to disappear
    Jan 12 16:11:16.957: INFO: Pod pod-configmaps-fa6d1995-3671-4c91-a797-3e4cc795de4a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:16.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8830" for this suite. 01/12/23 16:11:16.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:16.963
Jan 12 16:11:16.963: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:11:16.964
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:16.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:16.976
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5718 01/12/23 16:11:16.978
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/12/23 16:11:16.982
STEP: Creating pod with conflicting port in namespace statefulset-5718 01/12/23 16:11:16.985
STEP: Waiting until pod test-pod will start running in namespace statefulset-5718 01/12/23 16:11:16.992
Jan 12 16:11:16.992: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5718" to be "running"
Jan 12 16:11:16.995: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379329ms
Jan 12 16:11:18.998: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005268679s
Jan 12 16:11:18.998: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-5718 01/12/23 16:11:18.998
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5718 01/12/23 16:11:19.002
Jan 12 16:11:19.010: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Pending. Waiting for statefulset controller to delete.
Jan 12 16:11:19.027: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Failed. Waiting for statefulset controller to delete.
Jan 12 16:11:19.037: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Failed. Waiting for statefulset controller to delete.
Jan 12 16:11:19.039: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5718
STEP: Removing pod with conflicting port in namespace statefulset-5718 01/12/23 16:11:19.039
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5718 and will be in running state 01/12/23 16:11:19.05
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:11:21.055: INFO: Deleting all statefulset in ns statefulset-5718
Jan 12 16:11:21.057: INFO: Scaling statefulset ss to 0
Jan 12 16:11:31.070: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:11:31.072: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:31.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5718" for this suite. 01/12/23 16:11:31.086
------------------------------
 [SLOW TEST] [14.126 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:16.963
    Jan 12 16:11:16.963: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:11:16.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:16.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:16.976
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5718 01/12/23 16:11:16.978
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/12/23 16:11:16.982
    STEP: Creating pod with conflicting port in namespace statefulset-5718 01/12/23 16:11:16.985
    STEP: Waiting until pod test-pod will start running in namespace statefulset-5718 01/12/23 16:11:16.992
    Jan 12 16:11:16.992: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-5718" to be "running"
    Jan 12 16:11:16.995: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379329ms
    Jan 12 16:11:18.998: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005268679s
    Jan 12 16:11:18.998: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-5718 01/12/23 16:11:18.998
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5718 01/12/23 16:11:19.002
    Jan 12 16:11:19.010: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 12 16:11:19.027: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 12 16:11:19.037: INFO: Observed stateful pod in namespace: statefulset-5718, name: ss-0, uid: 123b2fde-baf7-48d4-8c57-2bf04bd2b5bd, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 12 16:11:19.039: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5718
    STEP: Removing pod with conflicting port in namespace statefulset-5718 01/12/23 16:11:19.039
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5718 and will be in running state 01/12/23 16:11:19.05
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:11:21.055: INFO: Deleting all statefulset in ns statefulset-5718
    Jan 12 16:11:21.057: INFO: Scaling statefulset ss to 0
    Jan 12 16:11:31.070: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:11:31.072: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:31.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5718" for this suite. 01/12/23 16:11:31.086
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:31.09
Jan 12 16:11:31.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:11:31.091
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:31.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:31.102
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4936 01/12/23 16:11:31.104
STEP: creating a selector 01/12/23 16:11:31.104
STEP: Creating the service pods in kubernetes 01/12/23 16:11:31.104
Jan 12 16:11:31.104: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 16:11:31.121: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4936" to be "running and ready"
Jan 12 16:11:31.124: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063473ms
Jan 12 16:11:31.124: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:11:33.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005374289s
Jan 12 16:11:33.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:11:35.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00644376s
Jan 12 16:11:35.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:11:37.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005993637s
Jan 12 16:11:37.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:11:39.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006331064s
Jan 12 16:11:39.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:11:41.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005508366s
Jan 12 16:11:41.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:11:43.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006599865s
Jan 12 16:11:43.128: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 16:11:43.128: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 16:11:43.129: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4936" to be "running and ready"
Jan 12 16:11:43.131: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.508019ms
Jan 12 16:11:43.131: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 16:11:43.131: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 16:11:43.132
Jan 12 16:11:43.137: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4936" to be "running"
Jan 12 16:11:43.139: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671904ms
Jan 12 16:11:45.141: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003944429s
Jan 12 16:11:45.141: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 16:11:45.143: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 16:11:45.143: INFO: Breadth first check of 10.244.0.137 on host 10.0.42.160...
Jan 12 16:11:45.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.218:9080/dial?request=hostname&protocol=udp&host=10.244.0.137&port=8081&tries=1'] Namespace:pod-network-test-4936 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:11:45.145: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:11:45.145: INFO: ExecWithOptions: Clientset creation
Jan 12 16:11:45.145: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4936/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.218%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 16:11:45.222: INFO: Waiting for responses: map[]
Jan 12 16:11:45.222: INFO: reached 10.244.0.137 after 0/1 tries
Jan 12 16:11:45.222: INFO: Breadth first check of 10.244.1.217 on host 10.0.40.50...
Jan 12 16:11:45.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.218:9080/dial?request=hostname&protocol=udp&host=10.244.1.217&port=8081&tries=1'] Namespace:pod-network-test-4936 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:11:45.224: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:11:45.224: INFO: ExecWithOptions: Clientset creation
Jan 12 16:11:45.225: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4936/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.218%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.217%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 16:11:45.264: INFO: Waiting for responses: map[]
Jan 12 16:11:45.264: INFO: reached 10.244.1.217 after 0/1 tries
Jan 12 16:11:45.264: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4936" for this suite. 01/12/23 16:11:45.267
------------------------------
 [SLOW TEST] [14.180 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:31.09
    Jan 12 16:11:31.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:11:31.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:31.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:31.102
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4936 01/12/23 16:11:31.104
    STEP: creating a selector 01/12/23 16:11:31.104
    STEP: Creating the service pods in kubernetes 01/12/23 16:11:31.104
    Jan 12 16:11:31.104: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 16:11:31.121: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4936" to be "running and ready"
    Jan 12 16:11:31.124: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.063473ms
    Jan 12 16:11:31.124: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:11:33.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005374289s
    Jan 12 16:11:33.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:11:35.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.00644376s
    Jan 12 16:11:35.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:11:37.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005993637s
    Jan 12 16:11:37.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:11:39.127: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006331064s
    Jan 12 16:11:39.127: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:11:41.126: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005508366s
    Jan 12 16:11:41.126: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:11:43.128: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006599865s
    Jan 12 16:11:43.128: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 16:11:43.128: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 16:11:43.129: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4936" to be "running and ready"
    Jan 12 16:11:43.131: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.508019ms
    Jan 12 16:11:43.131: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 16:11:43.131: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 16:11:43.132
    Jan 12 16:11:43.137: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4936" to be "running"
    Jan 12 16:11:43.139: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671904ms
    Jan 12 16:11:45.141: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003944429s
    Jan 12 16:11:45.141: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 16:11:45.143: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 16:11:45.143: INFO: Breadth first check of 10.244.0.137 on host 10.0.42.160...
    Jan 12 16:11:45.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.218:9080/dial?request=hostname&protocol=udp&host=10.244.0.137&port=8081&tries=1'] Namespace:pod-network-test-4936 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:11:45.145: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:11:45.145: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:11:45.145: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4936/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.218%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 16:11:45.222: INFO: Waiting for responses: map[]
    Jan 12 16:11:45.222: INFO: reached 10.244.0.137 after 0/1 tries
    Jan 12 16:11:45.222: INFO: Breadth first check of 10.244.1.217 on host 10.0.40.50...
    Jan 12 16:11:45.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.218:9080/dial?request=hostname&protocol=udp&host=10.244.1.217&port=8081&tries=1'] Namespace:pod-network-test-4936 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:11:45.224: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:11:45.224: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:11:45.225: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4936/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.218%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.217%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 16:11:45.264: INFO: Waiting for responses: map[]
    Jan 12 16:11:45.264: INFO: reached 10.244.1.217 after 0/1 tries
    Jan 12 16:11:45.264: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4936" for this suite. 01/12/23 16:11:45.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:45.272
Jan 12 16:11:45.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename watch 01/12/23 16:11:45.273
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.283
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/12/23 16:11:45.285
STEP: creating a new configmap 01/12/23 16:11:45.286
STEP: modifying the configmap once 01/12/23 16:11:45.291
STEP: closing the watch once it receives two notifications 01/12/23 16:11:45.296
Jan 12 16:11:45.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15280 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:11:45.296: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15281 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/12/23 16:11:45.296
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/12/23 16:11:45.301
STEP: deleting the configmap 01/12/23 16:11:45.302
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/12/23 16:11:45.305
Jan 12 16:11:45.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15282 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:11:45.306: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15283 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:45.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8414" for this suite. 01/12/23 16:11:45.308
------------------------------
 [0.040 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:45.272
    Jan 12 16:11:45.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename watch 01/12/23 16:11:45.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.283
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/12/23 16:11:45.285
    STEP: creating a new configmap 01/12/23 16:11:45.286
    STEP: modifying the configmap once 01/12/23 16:11:45.291
    STEP: closing the watch once it receives two notifications 01/12/23 16:11:45.296
    Jan 12 16:11:45.296: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15280 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:11:45.296: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15281 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/12/23 16:11:45.296
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/12/23 16:11:45.301
    STEP: deleting the configmap 01/12/23 16:11:45.302
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/12/23 16:11:45.305
    Jan 12 16:11:45.306: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15282 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:11:45.306: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8414  3165a5cd-b4eb-4be9-8474-0056ddf8120c 15283 0 2023-01-12 16:11:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-12 16:11:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:45.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8414" for this suite. 01/12/23 16:11:45.308
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:45.312
Jan 12 16:11:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:11:45.313
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.322
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/12/23 16:11:45.324
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/12/23 16:11:45.33
STEP: patching the secret 01/12/23 16:11:45.333
STEP: deleting the secret using a LabelSelector 01/12/23 16:11:45.338
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/12/23 16:11:45.342
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:11:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8973" for this suite. 01/12/23 16:11:45.345
------------------------------
 [0.037 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:45.312
    Jan 12 16:11:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:11:45.313
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.322
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/12/23 16:11:45.324
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/12/23 16:11:45.33
    STEP: patching the secret 01/12/23 16:11:45.333
    STEP: deleting the secret using a LabelSelector 01/12/23 16:11:45.338
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/12/23 16:11:45.342
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:11:45.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8973" for this suite. 01/12/23 16:11:45.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:11:45.35
Jan 12 16:11:45.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 16:11:45.351
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.362
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/12/23 16:11:45.364
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7669;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7669;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +notcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_tcp@PTR;sleep 1; done
 01/12/23 16:11:45.382
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7669;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7669;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +notcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_tcp@PTR;sleep 1; done
 01/12/23 16:11:45.383
STEP: creating a pod to probe DNS 01/12/23 16:11:45.383
STEP: submitting the pod to kubernetes 01/12/23 16:11:45.383
Jan 12 16:11:45.391: INFO: Waiting up to 15m0s for pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c" in namespace "dns-7669" to be "running"
Jan 12 16:11:45.394: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119336ms
Jan 12 16:11:47.397: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0060979s
Jan 12 16:11:47.398: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:11:47.398
STEP: looking for the results for each expected name from probers 01/12/23 16:11:47.399
Jan 12 16:11:47.404: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.407: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.409: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.412: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.414: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.417: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.419: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.422: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.434: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.437: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.440: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.442: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.444: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.447: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.449: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.452: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:47.461: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:11:52.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.471: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.476: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.480: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.483: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.498: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.501: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.503: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.506: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.508: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.511: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.513: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.516: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:52.525: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:11:57.466: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.469: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.482: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.496: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.501: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.506: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.511: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.513: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:11:57.524: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:12:02.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.470: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.480: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.482: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.485: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.502: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.507: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.512: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.515: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:02.525: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:12:07.466: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.469: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.481: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.496: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.498: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.501: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.512: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.518: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.520: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:07.530: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:12:12.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.470: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.477: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.506: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.509: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.512: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.517: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.523: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
Jan 12 16:12:12.538: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

Jan 12 16:12:17.520: INFO: DNS probes using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c succeeded

STEP: deleting the pod 01/12/23 16:12:17.52
STEP: deleting the test service 01/12/23 16:12:17.534
STEP: deleting the test headless service 01/12/23 16:12:17.55
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7669" for this suite. 01/12/23 16:12:17.56
------------------------------
 [SLOW TEST] [32.214 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:11:45.35
    Jan 12 16:11:45.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 16:11:45.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:11:45.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:11:45.362
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/12/23 16:11:45.364
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7669;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7669;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +notcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_tcp@PTR;sleep 1; done
     01/12/23 16:11:45.382
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7669;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7669;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7669.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7669.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7669.svc;check="$$(dig +notcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_udp@PTR;check="$$(dig +tcp +noall +answer +search 204.79.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.79.204_tcp@PTR;sleep 1; done
     01/12/23 16:11:45.383
    STEP: creating a pod to probe DNS 01/12/23 16:11:45.383
    STEP: submitting the pod to kubernetes 01/12/23 16:11:45.383
    Jan 12 16:11:45.391: INFO: Waiting up to 15m0s for pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c" in namespace "dns-7669" to be "running"
    Jan 12 16:11:45.394: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119336ms
    Jan 12 16:11:47.397: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c": Phase="Running", Reason="", readiness=true. Elapsed: 2.0060979s
    Jan 12 16:11:47.398: INFO: Pod "dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:11:47.398
    STEP: looking for the results for each expected name from probers 01/12/23 16:11:47.399
    Jan 12 16:11:47.404: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.407: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.409: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.412: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.414: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.417: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.419: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.422: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.434: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.437: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.440: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.442: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.444: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.447: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.449: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.452: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:47.461: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:11:52.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.471: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.476: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.480: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.483: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.498: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.501: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.503: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.506: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.508: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.511: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.513: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.516: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:52.525: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:11:57.466: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.469: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.482: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.496: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.501: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.506: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.511: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.513: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:11:57.524: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:12:02.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.470: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.480: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.482: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.485: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.497: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.499: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.502: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.504: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.507: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.509: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.512: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.515: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:02.525: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:12:07.466: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.469: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.476: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.481: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.496: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.498: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.501: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.512: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.518: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.520: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:07.530: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:12:12.468: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.470: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.477: INFO: Unable to read wheezy_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.492: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.506: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.509: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.512: INFO: Unable to read jessie_udp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669 from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.517: INFO: Unable to read jessie_udp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.523: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.526: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc from pod dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c: the server could not find the requested resource (get pods dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c)
    Jan 12 16:12:12.538: INFO: Lookups using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7669 wheezy_tcp@dns-test-service.dns-7669 wheezy_udp@dns-test-service.dns-7669.svc wheezy_tcp@dns-test-service.dns-7669.svc wheezy_udp@_http._tcp.dns-test-service.dns-7669.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7669.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7669 jessie_tcp@dns-test-service.dns-7669 jessie_udp@dns-test-service.dns-7669.svc jessie_tcp@dns-test-service.dns-7669.svc jessie_udp@_http._tcp.dns-test-service.dns-7669.svc jessie_tcp@_http._tcp.dns-test-service.dns-7669.svc]

    Jan 12 16:12:17.520: INFO: DNS probes using dns-7669/dns-test-12df2b04-62bd-4245-9d32-6a0140b1440c succeeded

    STEP: deleting the pod 01/12/23 16:12:17.52
    STEP: deleting the test service 01/12/23 16:12:17.534
    STEP: deleting the test headless service 01/12/23 16:12:17.55
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7669" for this suite. 01/12/23 16:12:17.56
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:17.568
Jan 12 16:12:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:12:17.569
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:17.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:17.581
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-4056 01/12/23 16:12:17.584
STEP: creating service affinity-clusterip-transition in namespace services-4056 01/12/23 16:12:17.584
STEP: creating replication controller affinity-clusterip-transition in namespace services-4056 01/12/23 16:12:17.594
I0112 16:12:17.601490      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4056, replica count: 3
I0112 16:12:20.653518      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 16:12:20.657: INFO: Creating new exec pod
Jan 12 16:12:20.662: INFO: Waiting up to 5m0s for pod "execpod-affinitysljr2" in namespace "services-4056" to be "running"
Jan 12 16:12:20.664: INFO: Pod "execpod-affinitysljr2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.76875ms
Jan 12 16:12:22.666: INFO: Pod "execpod-affinitysljr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004575436s
Jan 12 16:12:22.666: INFO: Pod "execpod-affinitysljr2" satisfied condition "running"
Jan 12 16:12:23.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 12 16:12:23.787: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 12 16:12:23.787: INFO: stdout: ""
Jan 12 16:12:23.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c nc -v -z -w 2 10.100.204.141 80'
Jan 12 16:12:23.905: INFO: stderr: "+ nc -v -z -w 2 10.100.204.141 80\nConnection to 10.100.204.141 80 port [tcp/http] succeeded!\n"
Jan 12 16:12:23.905: INFO: stdout: ""
Jan 12 16:12:23.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.204.141:80/ ; done'
Jan 12 16:12:24.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n"
Jan 12 16:12:24.088: INFO: stdout: "\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-r4m5m"
Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-r4m5m
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
Jan 12 16:12:24.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.204.141:80/ ; done'
Jan 12 16:12:24.274: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n"
Jan 12 16:12:24.274: INFO: stdout: "\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6"
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
Jan 12 16:12:24.274: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4056, will wait for the garbage collector to delete the pods 01/12/23 16:12:24.283
Jan 12 16:12:24.338: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.251585ms
Jan 12 16:12:24.440: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.226338ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:26.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4056" for this suite. 01/12/23 16:12:26.554
------------------------------
 [SLOW TEST] [8.991 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:17.568
    Jan 12 16:12:17.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:12:17.569
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:17.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:17.581
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-4056 01/12/23 16:12:17.584
    STEP: creating service affinity-clusterip-transition in namespace services-4056 01/12/23 16:12:17.584
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4056 01/12/23 16:12:17.594
    I0112 16:12:17.601490      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4056, replica count: 3
    I0112 16:12:20.653518      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 16:12:20.657: INFO: Creating new exec pod
    Jan 12 16:12:20.662: INFO: Waiting up to 5m0s for pod "execpod-affinitysljr2" in namespace "services-4056" to be "running"
    Jan 12 16:12:20.664: INFO: Pod "execpod-affinitysljr2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.76875ms
    Jan 12 16:12:22.666: INFO: Pod "execpod-affinitysljr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004575436s
    Jan 12 16:12:22.666: INFO: Pod "execpod-affinitysljr2" satisfied condition "running"
    Jan 12 16:12:23.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 12 16:12:23.787: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 12 16:12:23.787: INFO: stdout: ""
    Jan 12 16:12:23.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c nc -v -z -w 2 10.100.204.141 80'
    Jan 12 16:12:23.905: INFO: stderr: "+ nc -v -z -w 2 10.100.204.141 80\nConnection to 10.100.204.141 80 port [tcp/http] succeeded!\n"
    Jan 12 16:12:23.905: INFO: stdout: ""
    Jan 12 16:12:23.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.204.141:80/ ; done'
    Jan 12 16:12:24.088: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n"
    Jan 12 16:12:24.088: INFO: stdout: "\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-r4m5m\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-bf26l\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-r4m5m"
    Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.088: INFO: Received response from host: affinity-clusterip-transition-r4m5m
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-bf26l
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.089: INFO: Received response from host: affinity-clusterip-transition-r4m5m
    Jan 12 16:12:24.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4056 exec execpod-affinitysljr2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.204.141:80/ ; done'
    Jan 12 16:12:24.274: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.204.141:80/\n"
    Jan 12 16:12:24.274: INFO: stdout: "\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6\naffinity-clusterip-transition-d2lg6"
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Received response from host: affinity-clusterip-transition-d2lg6
    Jan 12 16:12:24.274: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4056, will wait for the garbage collector to delete the pods 01/12/23 16:12:24.283
    Jan 12 16:12:24.338: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.251585ms
    Jan 12 16:12:24.440: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.226338ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:26.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4056" for this suite. 01/12/23 16:12:26.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:26.561
Jan 12 16:12:26.561: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:12:26.562
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:26.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:26.573
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-968 01/12/23 16:12:26.575
STEP: creating service affinity-nodeport-transition in namespace services-968 01/12/23 16:12:26.575
STEP: creating replication controller affinity-nodeport-transition in namespace services-968 01/12/23 16:12:26.588
I0112 16:12:26.592472      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-968, replica count: 3
I0112 16:12:29.643967      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 16:12:29.649: INFO: Creating new exec pod
Jan 12 16:12:29.655: INFO: Waiting up to 5m0s for pod "execpod-affinity4nccs" in namespace "services-968" to be "running"
Jan 12 16:12:29.657: INFO: Pod "execpod-affinity4nccs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716187ms
Jan 12 16:12:31.661: INFO: Pod "execpod-affinity4nccs": Phase="Running", Reason="", readiness=true. Elapsed: 2.005365891s
Jan 12 16:12:31.661: INFO: Pod "execpod-affinity4nccs" satisfied condition "running"
Jan 12 16:12:32.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 12 16:12:32.786: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 12 16:12:32.786: INFO: stdout: ""
Jan 12 16:12:32.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.102.211.213 80'
Jan 12 16:12:32.902: INFO: stderr: "+ nc -v -z -w 2 10.102.211.213 80\nConnection to 10.102.211.213 80 port [tcp/http] succeeded!\n"
Jan 12 16:12:32.902: INFO: stdout: ""
Jan 12 16:12:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 31348'
Jan 12 16:12:33.027: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 31348\nConnection to 10.0.42.160 31348 port [tcp/*] succeeded!\n"
Jan 12 16:12:33.027: INFO: stdout: ""
Jan 12 16:12:33.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 31348'
Jan 12 16:12:33.146: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 31348\nConnection to 10.0.40.50 31348 port [tcp/*] succeeded!\n"
Jan 12 16:12:33.146: INFO: stdout: ""
Jan 12 16:12:33.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:31348/ ; done'
Jan 12 16:12:33.330: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n"
Jan 12 16:12:33.330: INFO: stdout: "\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-dwg5x"
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:31348/ ; done'
Jan 12 16:12:33.515: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n"
Jan 12 16:12:33.515: INFO: stdout: "\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x"
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
Jan 12 16:12:33.515: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-968, will wait for the garbage collector to delete the pods 01/12/23 16:12:33.524
Jan 12 16:12:33.579: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.541424ms
Jan 12 16:12:33.681: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.146177ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-968" for this suite. 01/12/23 16:12:35.601
------------------------------
 [SLOW TEST] [9.046 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:26.561
    Jan 12 16:12:26.561: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:12:26.562
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:26.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:26.573
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-968 01/12/23 16:12:26.575
    STEP: creating service affinity-nodeport-transition in namespace services-968 01/12/23 16:12:26.575
    STEP: creating replication controller affinity-nodeport-transition in namespace services-968 01/12/23 16:12:26.588
    I0112 16:12:26.592472      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-968, replica count: 3
    I0112 16:12:29.643967      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 16:12:29.649: INFO: Creating new exec pod
    Jan 12 16:12:29.655: INFO: Waiting up to 5m0s for pod "execpod-affinity4nccs" in namespace "services-968" to be "running"
    Jan 12 16:12:29.657: INFO: Pod "execpod-affinity4nccs": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716187ms
    Jan 12 16:12:31.661: INFO: Pod "execpod-affinity4nccs": Phase="Running", Reason="", readiness=true. Elapsed: 2.005365891s
    Jan 12 16:12:31.661: INFO: Pod "execpod-affinity4nccs" satisfied condition "running"
    Jan 12 16:12:32.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 12 16:12:32.786: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 12 16:12:32.786: INFO: stdout: ""
    Jan 12 16:12:32.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.102.211.213 80'
    Jan 12 16:12:32.902: INFO: stderr: "+ nc -v -z -w 2 10.102.211.213 80\nConnection to 10.102.211.213 80 port [tcp/http] succeeded!\n"
    Jan 12 16:12:32.902: INFO: stdout: ""
    Jan 12 16:12:32.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 31348'
    Jan 12 16:12:33.027: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 31348\nConnection to 10.0.42.160 31348 port [tcp/*] succeeded!\n"
    Jan 12 16:12:33.027: INFO: stdout: ""
    Jan 12 16:12:33.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 31348'
    Jan 12 16:12:33.146: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 31348\nConnection to 10.0.40.50 31348 port [tcp/*] succeeded!\n"
    Jan 12 16:12:33.146: INFO: stdout: ""
    Jan 12 16:12:33.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:31348/ ; done'
    Jan 12 16:12:33.330: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n"
    Jan 12 16:12:33.330: INFO: stdout: "\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-j6zxg\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-vbg92\naffinity-nodeport-transition-dwg5x"
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-j6zxg
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-vbg92
    Jan 12 16:12:33.330: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-968 exec execpod-affinity4nccs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.42.160:31348/ ; done'
    Jan 12 16:12:33.515: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.42.160:31348/\n"
    Jan 12 16:12:33.515: INFO: stdout: "\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x\naffinity-nodeport-transition-dwg5x"
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Received response from host: affinity-nodeport-transition-dwg5x
    Jan 12 16:12:33.515: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-968, will wait for the garbage collector to delete the pods 01/12/23 16:12:33.524
    Jan 12 16:12:33.579: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.541424ms
    Jan 12 16:12:33.681: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.146177ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:35.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-968" for this suite. 01/12/23 16:12:35.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:35.608
Jan 12 16:12:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 16:12:35.608
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:35.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:35.618
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 12 16:12:35.631: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/12/23 16:12:35.634
Jan 12 16:12:35.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:35.636: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/12/23 16:12:35.636
Jan 12 16:12:35.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:35.653: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:12:36.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:12:36.656: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/12/23 16:12:36.658
Jan 12 16:12:36.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:12:36.669: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 12 16:12:37.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:37.672: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/12/23 16:12:37.672
Jan 12 16:12:37.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:37.681: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:12:38.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:38.684: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:12:39.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:39.684: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:12:40.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:12:40.684: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:12:40.688
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8329, will wait for the garbage collector to delete the pods 01/12/23 16:12:40.688
Jan 12 16:12:40.744: INFO: Deleting DaemonSet.extensions daemon-set took: 3.602509ms
Jan 12 16:12:40.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.333821ms
Jan 12 16:12:43.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:12:43.648: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 16:12:43.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15805"},"items":null}

Jan 12 16:12:43.651: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15805"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8329" for this suite. 01/12/23 16:12:43.666
------------------------------
 [SLOW TEST] [8.062 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:35.608
    Jan 12 16:12:35.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 16:12:35.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:35.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:35.618
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 12 16:12:35.631: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/12/23 16:12:35.634
    Jan 12 16:12:35.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:35.636: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/12/23 16:12:35.636
    Jan 12 16:12:35.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:35.653: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:12:36.656: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:12:36.656: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/12/23 16:12:36.658
    Jan 12 16:12:36.669: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:12:36.669: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 12 16:12:37.672: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:37.672: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/12/23 16:12:37.672
    Jan 12 16:12:37.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:37.681: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:12:38.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:38.684: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:12:39.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:39.684: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:12:40.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:12:40.684: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:12:40.688
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8329, will wait for the garbage collector to delete the pods 01/12/23 16:12:40.688
    Jan 12 16:12:40.744: INFO: Deleting DaemonSet.extensions daemon-set took: 3.602509ms
    Jan 12 16:12:40.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.333821ms
    Jan 12 16:12:43.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:12:43.648: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 16:12:43.649: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15805"},"items":null}

    Jan 12 16:12:43.651: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15805"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8329" for this suite. 01/12/23 16:12:43.666
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:43.67
Jan 12 16:12:43.671: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:12:43.672
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:43.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:43.681
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/12/23 16:12:43.683
STEP: watching for the ServiceAccount to be added 01/12/23 16:12:43.69
STEP: patching the ServiceAccount 01/12/23 16:12:43.691
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/12/23 16:12:43.695
STEP: deleting the ServiceAccount 01/12/23 16:12:43.697
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:43.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7837" for this suite. 01/12/23 16:12:43.706
------------------------------
 [0.040 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:43.67
    Jan 12 16:12:43.671: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:12:43.672
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:43.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:43.681
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/12/23 16:12:43.683
    STEP: watching for the ServiceAccount to be added 01/12/23 16:12:43.69
    STEP: patching the ServiceAccount 01/12/23 16:12:43.691
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/12/23 16:12:43.695
    STEP: deleting the ServiceAccount 01/12/23 16:12:43.697
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:43.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7837" for this suite. 01/12/23 16:12:43.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:43.714
Jan 12 16:12:43.714: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:12:43.714
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:43.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:43.723
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-933/configmap-test-67fe626d-4997-4fc2-898d-1b3dbd5eedbb 01/12/23 16:12:43.726
STEP: Creating a pod to test consume configMaps 01/12/23 16:12:43.73
Jan 12 16:12:43.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4" in namespace "configmap-933" to be "Succeeded or Failed"
Jan 12 16:12:43.736: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743999ms
Jan 12 16:12:45.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004668061s
Jan 12 16:12:47.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004843556s
STEP: Saw pod success 01/12/23 16:12:47.739
Jan 12 16:12:47.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4" satisfied condition "Succeeded or Failed"
Jan 12 16:12:47.741: INFO: Trying to get logs from node worker-1 pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 container env-test: <nil>
STEP: delete the pod 01/12/23 16:12:47.753
Jan 12 16:12:47.762: INFO: Waiting for pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 to disappear
Jan 12 16:12:47.763: INFO: Pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:47.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-933" for this suite. 01/12/23 16:12:47.765
------------------------------
 [4.056 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:43.714
    Jan 12 16:12:43.714: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:12:43.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:43.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:43.723
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-933/configmap-test-67fe626d-4997-4fc2-898d-1b3dbd5eedbb 01/12/23 16:12:43.726
    STEP: Creating a pod to test consume configMaps 01/12/23 16:12:43.73
    Jan 12 16:12:43.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4" in namespace "configmap-933" to be "Succeeded or Failed"
    Jan 12 16:12:43.736: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.743999ms
    Jan 12 16:12:45.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004668061s
    Jan 12 16:12:47.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004843556s
    STEP: Saw pod success 01/12/23 16:12:47.739
    Jan 12 16:12:47.739: INFO: Pod "pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4" satisfied condition "Succeeded or Failed"
    Jan 12 16:12:47.741: INFO: Trying to get logs from node worker-1 pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 container env-test: <nil>
    STEP: delete the pod 01/12/23 16:12:47.753
    Jan 12 16:12:47.762: INFO: Waiting for pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 to disappear
    Jan 12 16:12:47.763: INFO: Pod pod-configmaps-0b0d9ec6-71a1-4029-a51f-b367a27479e4 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:47.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-933" for this suite. 01/12/23 16:12:47.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:47.77
Jan 12 16:12:47.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:12:47.771
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:47.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:47.783
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-d426c160-ad14-41e5-bd25-6fca226756b1 01/12/23 16:12:47.786
STEP: Creating a pod to test consume configMaps 01/12/23 16:12:47.789
Jan 12 16:12:47.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10" in namespace "projected-6697" to be "Succeeded or Failed"
Jan 12 16:12:47.795: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520105ms
Jan 12 16:12:49.798: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004666488s
Jan 12 16:12:51.799: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005767595s
STEP: Saw pod success 01/12/23 16:12:51.799
Jan 12 16:12:51.799: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10" satisfied condition "Succeeded or Failed"
Jan 12 16:12:51.801: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:12:51.805
Jan 12 16:12:51.813: INFO: Waiting for pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 to disappear
Jan 12 16:12:51.815: INFO: Pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6697" for this suite. 01/12/23 16:12:51.817
------------------------------
 [4.050 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:47.77
    Jan 12 16:12:47.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:12:47.771
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:47.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:47.783
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-d426c160-ad14-41e5-bd25-6fca226756b1 01/12/23 16:12:47.786
    STEP: Creating a pod to test consume configMaps 01/12/23 16:12:47.789
    Jan 12 16:12:47.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10" in namespace "projected-6697" to be "Succeeded or Failed"
    Jan 12 16:12:47.795: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.520105ms
    Jan 12 16:12:49.798: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004666488s
    Jan 12 16:12:51.799: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005767595s
    STEP: Saw pod success 01/12/23 16:12:51.799
    Jan 12 16:12:51.799: INFO: Pod "pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10" satisfied condition "Succeeded or Failed"
    Jan 12 16:12:51.801: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:12:51.805
    Jan 12 16:12:51.813: INFO: Waiting for pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 to disappear
    Jan 12 16:12:51.815: INFO: Pod pod-projected-configmaps-37038492-bf16-4615-bfa9-666a25337e10 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6697" for this suite. 01/12/23 16:12:51.817
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:51.822
Jan 12 16:12:51.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:12:51.823
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:51.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:51.832
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-c8b450db-7b0c-458b-b525-c4cd90de9662 01/12/23 16:12:51.837
STEP: Creating configMap with name cm-test-opt-upd-c4a9454e-91ee-4abc-9589-39a6329d4882 01/12/23 16:12:51.84
STEP: Creating the pod 01/12/23 16:12:51.842
Jan 12 16:12:51.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69" in namespace "configmap-8828" to be "running and ready"
Jan 12 16:12:51.851: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.176536ms
Jan 12 16:12:51.851: INFO: The phase of Pod pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:12:53.854: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69": Phase="Running", Reason="", readiness=true. Elapsed: 2.00606883s
Jan 12 16:12:53.854: INFO: The phase of Pod pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69 is Running (Ready = true)
Jan 12 16:12:53.854: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c8b450db-7b0c-458b-b525-c4cd90de9662 01/12/23 16:12:53.867
STEP: Updating configmap cm-test-opt-upd-c4a9454e-91ee-4abc-9589-39a6329d4882 01/12/23 16:12:53.871
STEP: Creating configMap with name cm-test-opt-create-828d530c-5968-478e-a113-d805cc0100bf 01/12/23 16:12:53.874
STEP: waiting to observe update in volume 01/12/23 16:12:53.877
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:12:55.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8828" for this suite. 01/12/23 16:12:55.896
------------------------------
 [4.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:51.822
    Jan 12 16:12:51.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:12:51.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:51.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:51.832
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-c8b450db-7b0c-458b-b525-c4cd90de9662 01/12/23 16:12:51.837
    STEP: Creating configMap with name cm-test-opt-upd-c4a9454e-91ee-4abc-9589-39a6329d4882 01/12/23 16:12:51.84
    STEP: Creating the pod 01/12/23 16:12:51.842
    Jan 12 16:12:51.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69" in namespace "configmap-8828" to be "running and ready"
    Jan 12 16:12:51.851: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69": Phase="Pending", Reason="", readiness=false. Elapsed: 3.176536ms
    Jan 12 16:12:51.851: INFO: The phase of Pod pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:12:53.854: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69": Phase="Running", Reason="", readiness=true. Elapsed: 2.00606883s
    Jan 12 16:12:53.854: INFO: The phase of Pod pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69 is Running (Ready = true)
    Jan 12 16:12:53.854: INFO: Pod "pod-configmaps-b3044361-09a9-4414-9446-2b6ba5a2da69" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c8b450db-7b0c-458b-b525-c4cd90de9662 01/12/23 16:12:53.867
    STEP: Updating configmap cm-test-opt-upd-c4a9454e-91ee-4abc-9589-39a6329d4882 01/12/23 16:12:53.871
    STEP: Creating configMap with name cm-test-opt-create-828d530c-5968-478e-a113-d805cc0100bf 01/12/23 16:12:53.874
    STEP: waiting to observe update in volume 01/12/23 16:12:53.877
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:12:55.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8828" for this suite. 01/12/23 16:12:55.896
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:12:55.901
Jan 12 16:12:55.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:12:55.902
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:55.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:55.912
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/12/23 16:12:55.914
Jan 12 16:12:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:12:57.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:13:03.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9068" for this suite. 01/12/23 16:13:03.336
------------------------------
 [SLOW TEST] [7.439 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:12:55.901
    Jan 12 16:12:55.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:12:55.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:12:55.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:12:55.912
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/12/23 16:12:55.914
    Jan 12 16:12:55.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:12:57.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:13:03.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9068" for this suite. 01/12/23 16:13:03.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:13:03.341
Jan 12 16:13:03.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:13:03.341
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:03.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:03.354
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qphtz"  01/12/23 16:13:03.356
Jan 12 16:13:03.359: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qphtz"  01/12/23 16:13:03.359
Jan 12 16:13:03.365: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 16:13:03.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-414" for this suite. 01/12/23 16:13:03.369
------------------------------
 [0.031 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:13:03.341
    Jan 12 16:13:03.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:13:03.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:03.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:03.354
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qphtz"  01/12/23 16:13:03.356
    Jan 12 16:13:03.359: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qphtz"  01/12/23 16:13:03.359
    Jan 12 16:13:03.365: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:13:03.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-414" for this suite. 01/12/23 16:13:03.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:13:03.373
Jan 12 16:13:03.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 16:13:03.374
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:03.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:03.382
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/12/23 16:13:03.394
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:13:03.397
Jan 12 16:13:03.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:13:03.401: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:13:04.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:13:04.406: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:13:05.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:13:05.405: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/12/23 16:13:05.407
Jan 12 16:13:05.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:13:05.422: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:13:06.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:13:06.426: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:13:07.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:13:07.426: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/12/23 16:13:07.426
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:13:07.43
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4001, will wait for the garbage collector to delete the pods 01/12/23 16:13:07.43
Jan 12 16:13:07.486: INFO: Deleting DaemonSet.extensions daemon-set took: 4.147454ms
Jan 12 16:13:07.587: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.564966ms
Jan 12 16:13:09.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:13:09.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 16:13:09.692: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16076"},"items":null}

Jan 12 16:13:09.694: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16076"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:13:09.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4001" for this suite. 01/12/23 16:13:09.702
------------------------------
 [SLOW TEST] [6.335 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:13:03.373
    Jan 12 16:13:03.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 16:13:03.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:03.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:03.382
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/12/23 16:13:03.394
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:13:03.397
    Jan 12 16:13:03.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:13:03.401: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:13:04.406: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:13:04.406: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:13:05.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:13:05.405: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/12/23 16:13:05.407
    Jan 12 16:13:05.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:13:05.422: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:13:06.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:13:06.426: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:13:07.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:13:07.426: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/12/23 16:13:07.426
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:13:07.43
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4001, will wait for the garbage collector to delete the pods 01/12/23 16:13:07.43
    Jan 12 16:13:07.486: INFO: Deleting DaemonSet.extensions daemon-set took: 4.147454ms
    Jan 12 16:13:07.587: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.564966ms
    Jan 12 16:13:09.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:13:09.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 16:13:09.692: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16076"},"items":null}

    Jan 12 16:13:09.694: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16076"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:13:09.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4001" for this suite. 01/12/23 16:13:09.702
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:13:09.708
Jan 12 16:13:09.708: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:13:09.709
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.718
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/12/23 16:13:09.72
STEP: getting /apis/node.k8s.io 01/12/23 16:13:09.722
STEP: getting /apis/node.k8s.io/v1 01/12/23 16:13:09.723
STEP: creating 01/12/23 16:13:09.724
STEP: watching 01/12/23 16:13:09.737
Jan 12 16:13:09.737: INFO: starting watch
STEP: getting 01/12/23 16:13:09.741
STEP: listing 01/12/23 16:13:09.742
STEP: patching 01/12/23 16:13:09.744
STEP: updating 01/12/23 16:13:09.747
Jan 12 16:13:09.751: INFO: waiting for watch events with expected annotations
STEP: deleting 01/12/23 16:13:09.751
STEP: deleting a collection 01/12/23 16:13:09.757
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 16:13:09.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2014" for this suite. 01/12/23 16:13:09.769
------------------------------
 [0.064 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:13:09.708
    Jan 12 16:13:09.708: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:13:09.709
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.718
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/12/23 16:13:09.72
    STEP: getting /apis/node.k8s.io 01/12/23 16:13:09.722
    STEP: getting /apis/node.k8s.io/v1 01/12/23 16:13:09.723
    STEP: creating 01/12/23 16:13:09.724
    STEP: watching 01/12/23 16:13:09.737
    Jan 12 16:13:09.737: INFO: starting watch
    STEP: getting 01/12/23 16:13:09.741
    STEP: listing 01/12/23 16:13:09.742
    STEP: patching 01/12/23 16:13:09.744
    STEP: updating 01/12/23 16:13:09.747
    Jan 12 16:13:09.751: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/12/23 16:13:09.751
    STEP: deleting a collection 01/12/23 16:13:09.757
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:13:09.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2014" for this suite. 01/12/23 16:13:09.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:13:09.773
Jan 12 16:13:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename events 01/12/23 16:13:09.774
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.782
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/12/23 16:13:09.784
Jan 12 16:13:09.790: INFO: created test-event-1
Jan 12 16:13:09.792: INFO: created test-event-2
Jan 12 16:13:09.795: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/12/23 16:13:09.795
STEP: delete collection of events 01/12/23 16:13:09.797
Jan 12 16:13:09.797: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/12/23 16:13:09.806
Jan 12 16:13:09.806: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 12 16:13:09.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8450" for this suite. 01/12/23 16:13:09.81
------------------------------
 [0.040 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:13:09.773
    Jan 12 16:13:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename events 01/12/23 16:13:09.774
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.782
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/12/23 16:13:09.784
    Jan 12 16:13:09.790: INFO: created test-event-1
    Jan 12 16:13:09.792: INFO: created test-event-2
    Jan 12 16:13:09.795: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/12/23 16:13:09.795
    STEP: delete collection of events 01/12/23 16:13:09.797
    Jan 12 16:13:09.797: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/12/23 16:13:09.806
    Jan 12 16:13:09.806: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:13:09.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8450" for this suite. 01/12/23 16:13:09.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:13:09.815
Jan 12 16:13:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:13:09.816
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.827
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-48 01/12/23 16:13:09.829
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/12/23 16:13:09.834
Jan 12 16:13:09.841: INFO: Found 0 stateful pods, waiting for 3
Jan 12 16:13:19.844: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:13:19.844: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:13:19.844: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 16:13:19.849
Jan 12 16:13:19.865: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/12/23 16:13:19.865
STEP: Not applying an update when the partition is greater than the number of replicas 01/12/23 16:13:29.873
STEP: Performing a canary update 01/12/23 16:13:29.873
Jan 12 16:13:29.892: INFO: Updating stateful set ss2
Jan 12 16:13:29.896: INFO: Waiting for Pod statefulset-48/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/12/23 16:13:39.901
Jan 12 16:13:39.927: INFO: Found 1 stateful pods, waiting for 3
Jan 12 16:13:49.931: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:13:49.931: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:13:49.931: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/12/23 16:13:49.935
Jan 12 16:13:49.951: INFO: Updating stateful set ss2
Jan 12 16:13:49.955: INFO: Waiting for Pod statefulset-48/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 12 16:13:59.976: INFO: Updating stateful set ss2
Jan 12 16:13:59.983: INFO: Waiting for StatefulSet statefulset-48/ss2 to complete update
Jan 12 16:13:59.983: INFO: Waiting for Pod statefulset-48/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:14:09.988: INFO: Deleting all statefulset in ns statefulset-48
Jan 12 16:14:09.989: INFO: Scaling statefulset ss2 to 0
Jan 12 16:14:20.002: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:14:20.004: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:14:20.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-48" for this suite. 01/12/23 16:14:20.018
------------------------------
 [SLOW TEST] [70.209 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:13:09.815
    Jan 12 16:13:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:13:09.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:13:09.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:13:09.827
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-48 01/12/23 16:13:09.829
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/12/23 16:13:09.834
    Jan 12 16:13:09.841: INFO: Found 0 stateful pods, waiting for 3
    Jan 12 16:13:19.844: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:13:19.844: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:13:19.844: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 16:13:19.849
    Jan 12 16:13:19.865: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/12/23 16:13:19.865
    STEP: Not applying an update when the partition is greater than the number of replicas 01/12/23 16:13:29.873
    STEP: Performing a canary update 01/12/23 16:13:29.873
    Jan 12 16:13:29.892: INFO: Updating stateful set ss2
    Jan 12 16:13:29.896: INFO: Waiting for Pod statefulset-48/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/12/23 16:13:39.901
    Jan 12 16:13:39.927: INFO: Found 1 stateful pods, waiting for 3
    Jan 12 16:13:49.931: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:13:49.931: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:13:49.931: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/12/23 16:13:49.935
    Jan 12 16:13:49.951: INFO: Updating stateful set ss2
    Jan 12 16:13:49.955: INFO: Waiting for Pod statefulset-48/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 12 16:13:59.976: INFO: Updating stateful set ss2
    Jan 12 16:13:59.983: INFO: Waiting for StatefulSet statefulset-48/ss2 to complete update
    Jan 12 16:13:59.983: INFO: Waiting for Pod statefulset-48/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:14:09.988: INFO: Deleting all statefulset in ns statefulset-48
    Jan 12 16:14:09.989: INFO: Scaling statefulset ss2 to 0
    Jan 12 16:14:20.002: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:14:20.004: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:14:20.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-48" for this suite. 01/12/23 16:14:20.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:14:20.025
Jan 12 16:14:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:14:20.025
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:14:20.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:14:20.035
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4189 01/12/23 16:14:20.037
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/12/23 16:14:20.042
STEP: Creating stateful set ss in namespace statefulset-4189 01/12/23 16:14:20.044
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4189 01/12/23 16:14:20.048
Jan 12 16:14:20.050: INFO: Found 0 stateful pods, waiting for 1
Jan 12 16:14:30.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/12/23 16:14:30.053
Jan 12 16:14:30.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:14:30.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:14:30.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:14:30.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:14:30.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 12 16:14:40.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:14:40.174: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:14:40.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998788s
Jan 12 16:14:41.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998292975s
Jan 12 16:14:42.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995656915s
Jan 12 16:14:43.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9923999s
Jan 12 16:14:44.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989637336s
Jan 12 16:14:45.198: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986880432s
Jan 12 16:14:46.201: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984155921s
Jan 12 16:14:47.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.98170595s
Jan 12 16:14:48.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978617808s
Jan 12 16:14:49.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.239347ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4189 01/12/23 16:14:50.21
Jan 12 16:14:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:14:50.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:14:50.326: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:14:50.326: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:14:50.328: INFO: Found 1 stateful pods, waiting for 3
Jan 12 16:15:00.332: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:15:00.332: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:15:00.332: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/12/23 16:15:00.332
STEP: Scale down will halt with unhealthy stateful pod 01/12/23 16:15:00.332
Jan 12 16:15:00.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:15:00.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:15:00.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:15:00.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:15:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:15:00.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:15:00.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:15:00.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:15:00.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:15:00.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:15:00.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:15:00.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:15:00.689: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:15:00.691: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 12 16:15:10.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:15:10.696: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:15:10.696: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:15:10.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996623s
Jan 12 16:15:11.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997743908s
Jan 12 16:15:12.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995025147s
Jan 12 16:15:13.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991229638s
Jan 12 16:15:14.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98779722s
Jan 12 16:15:15.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985148375s
Jan 12 16:15:16.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982198893s
Jan 12 16:15:17.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97554184s
Jan 12 16:15:18.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97298284s
Jan 12 16:15:19.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.141068ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4189 01/12/23 16:15:20.734
Jan 12 16:15:20.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:15:20.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:15:20.850: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:15:20.850: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:15:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:15:20.967: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:15:20.967: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:15:20.967: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:15:20.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:15:21.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:15:21.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:15:21.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:15:21.083: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/12/23 16:15:31.093
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:15:31.093: INFO: Deleting all statefulset in ns statefulset-4189
Jan 12 16:15:31.095: INFO: Scaling statefulset ss to 0
Jan 12 16:15:31.100: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:15:31.102: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:15:31.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4189" for this suite. 01/12/23 16:15:31.115
------------------------------
 [SLOW TEST] [71.094 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:14:20.025
    Jan 12 16:14:20.025: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:14:20.025
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:14:20.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:14:20.035
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4189 01/12/23 16:14:20.037
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/12/23 16:14:20.042
    STEP: Creating stateful set ss in namespace statefulset-4189 01/12/23 16:14:20.044
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4189 01/12/23 16:14:20.048
    Jan 12 16:14:20.050: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 16:14:30.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/12/23 16:14:30.053
    Jan 12 16:14:30.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:14:30.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:14:30.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:14:30.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:14:30.171: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 12 16:14:40.174: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:14:40.174: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:14:40.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998788s
    Jan 12 16:14:41.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998292975s
    Jan 12 16:14:42.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995656915s
    Jan 12 16:14:43.193: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9923999s
    Jan 12 16:14:44.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989637336s
    Jan 12 16:14:45.198: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986880432s
    Jan 12 16:14:46.201: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984155921s
    Jan 12 16:14:47.203: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.98170595s
    Jan 12 16:14:48.206: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978617808s
    Jan 12 16:14:49.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.239347ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4189 01/12/23 16:14:50.21
    Jan 12 16:14:50.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:14:50.326: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:14:50.326: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:14:50.326: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:14:50.328: INFO: Found 1 stateful pods, waiting for 3
    Jan 12 16:15:00.332: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:15:00.332: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:15:00.332: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/12/23 16:15:00.332
    STEP: Scale down will halt with unhealthy stateful pod 01/12/23 16:15:00.332
    Jan 12 16:15:00.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:15:00.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:15:00.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:15:00.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:15:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:15:00.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:15:00.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:15:00.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:15:00.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:15:00.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:15:00.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:15:00.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:15:00.689: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:15:00.691: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 12 16:15:10.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:15:10.696: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:15:10.696: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:15:10.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996623s
    Jan 12 16:15:11.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997743908s
    Jan 12 16:15:12.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995025147s
    Jan 12 16:15:13.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991229638s
    Jan 12 16:15:14.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98779722s
    Jan 12 16:15:15.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985148375s
    Jan 12 16:15:16.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982198893s
    Jan 12 16:15:17.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97554184s
    Jan 12 16:15:18.731: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97298284s
    Jan 12 16:15:19.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.141068ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4189 01/12/23 16:15:20.734
    Jan 12 16:15:20.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:15:20.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:15:20.850: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:15:20.850: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:15:20.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:15:20.967: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:15:20.967: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:15:20.967: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:15:20.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-4189 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:15:21.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:15:21.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:15:21.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:15:21.083: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/12/23 16:15:31.093
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:15:31.093: INFO: Deleting all statefulset in ns statefulset-4189
    Jan 12 16:15:31.095: INFO: Scaling statefulset ss to 0
    Jan 12 16:15:31.100: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:15:31.102: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:15:31.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4189" for this suite. 01/12/23 16:15:31.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:15:31.119
Jan 12 16:15:31.119: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption 01/12/23 16:15:31.12
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:31.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:31.132
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/12/23 16:15:31.138
STEP: Updating PodDisruptionBudget status 01/12/23 16:15:33.142
STEP: Waiting for all pods to be running 01/12/23 16:15:33.147
Jan 12 16:15:33.150: INFO: running pods: 0 < 1
STEP: locating a running pod 01/12/23 16:15:35.153
STEP: Waiting for the pdb to be processed 01/12/23 16:15:35.161
STEP: Patching PodDisruptionBudget status 01/12/23 16:15:35.165
STEP: Waiting for the pdb to be processed 01/12/23 16:15:35.173
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:15:35.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4708" for this suite. 01/12/23 16:15:35.177
------------------------------
 [4.061 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:15:31.119
    Jan 12 16:15:31.119: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption 01/12/23 16:15:31.12
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:31.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:31.132
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/12/23 16:15:31.138
    STEP: Updating PodDisruptionBudget status 01/12/23 16:15:33.142
    STEP: Waiting for all pods to be running 01/12/23 16:15:33.147
    Jan 12 16:15:33.150: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/12/23 16:15:35.153
    STEP: Waiting for the pdb to be processed 01/12/23 16:15:35.161
    STEP: Patching PodDisruptionBudget status 01/12/23 16:15:35.165
    STEP: Waiting for the pdb to be processed 01/12/23 16:15:35.173
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:15:35.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4708" for this suite. 01/12/23 16:15:35.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:15:35.18
Jan 12 16:15:35.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:15:35.181
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:35.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:35.191
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-7011 01/12/23 16:15:35.193
STEP: creating replication controller nodeport-test in namespace services-7011 01/12/23 16:15:35.207
I0112 16:15:35.211481      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7011, replica count: 2
I0112 16:15:38.262168      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 16:15:38.262: INFO: Creating new exec pod
Jan 12 16:15:38.266: INFO: Waiting up to 5m0s for pod "execpodx5mg5" in namespace "services-7011" to be "running"
Jan 12 16:15:38.268: INFO: Pod "execpodx5mg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055222ms
Jan 12 16:15:40.271: INFO: Pod "execpodx5mg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004832549s
Jan 12 16:15:40.271: INFO: Pod "execpodx5mg5" satisfied condition "running"
Jan 12 16:15:41.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 12 16:15:41.403: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 12 16:15:41.403: INFO: stdout: ""
Jan 12 16:15:41.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.104.170.189 80'
Jan 12 16:15:41.526: INFO: stderr: "+ nc -v -z -w 2 10.104.170.189 80\nConnection to 10.104.170.189 80 port [tcp/http] succeeded!\n"
Jan 12 16:15:41.526: INFO: stdout: ""
Jan 12 16:15:41.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 32371'
Jan 12 16:15:41.638: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 32371\nConnection to 10.0.42.160 32371 port [tcp/*] succeeded!\n"
Jan 12 16:15:41.638: INFO: stdout: ""
Jan 12 16:15:41.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 32371'
Jan 12 16:15:41.749: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 32371\nConnection to 10.0.40.50 32371 port [tcp/*] succeeded!\n"
Jan 12 16:15:41.749: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:15:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7011" for this suite. 01/12/23 16:15:41.752
------------------------------
 [SLOW TEST] [6.577 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:15:35.18
    Jan 12 16:15:35.180: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:15:35.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:35.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:35.191
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-7011 01/12/23 16:15:35.193
    STEP: creating replication controller nodeport-test in namespace services-7011 01/12/23 16:15:35.207
    I0112 16:15:35.211481      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-7011, replica count: 2
    I0112 16:15:38.262168      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 16:15:38.262: INFO: Creating new exec pod
    Jan 12 16:15:38.266: INFO: Waiting up to 5m0s for pod "execpodx5mg5" in namespace "services-7011" to be "running"
    Jan 12 16:15:38.268: INFO: Pod "execpodx5mg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055222ms
    Jan 12 16:15:40.271: INFO: Pod "execpodx5mg5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004832549s
    Jan 12 16:15:40.271: INFO: Pod "execpodx5mg5" satisfied condition "running"
    Jan 12 16:15:41.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 12 16:15:41.403: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 12 16:15:41.403: INFO: stdout: ""
    Jan 12 16:15:41.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.104.170.189 80'
    Jan 12 16:15:41.526: INFO: stderr: "+ nc -v -z -w 2 10.104.170.189 80\nConnection to 10.104.170.189 80 port [tcp/http] succeeded!\n"
    Jan 12 16:15:41.526: INFO: stdout: ""
    Jan 12 16:15:41.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.0.42.160 32371'
    Jan 12 16:15:41.638: INFO: stderr: "+ nc -v -z -w 2 10.0.42.160 32371\nConnection to 10.0.42.160 32371 port [tcp/*] succeeded!\n"
    Jan 12 16:15:41.638: INFO: stdout: ""
    Jan 12 16:15:41.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-7011 exec execpodx5mg5 -- /bin/sh -x -c nc -v -z -w 2 10.0.40.50 32371'
    Jan 12 16:15:41.749: INFO: stderr: "+ nc -v -z -w 2 10.0.40.50 32371\nConnection to 10.0.40.50 32371 port [tcp/*] succeeded!\n"
    Jan 12 16:15:41.749: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:15:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7011" for this suite. 01/12/23 16:15:41.752
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:15:41.758
Jan 12 16:15:41.758: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:15:41.758
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:41.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:41.768
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/12/23 16:15:41.77
Jan 12 16:15:41.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:15:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:15:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5609" for this suite. 01/12/23 16:15:48.717
------------------------------
 [SLOW TEST] [6.963 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:15:41.758
    Jan 12 16:15:41.758: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:15:41.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:41.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:41.768
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/12/23 16:15:41.77
    Jan 12 16:15:41.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:15:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:15:48.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5609" for this suite. 01/12/23 16:15:48.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:15:48.724
Jan 12 16:15:48.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 16:15:48.725
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:48.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:48.737
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/12/23 16:15:48.741
STEP: waiting for Deployment to be created 01/12/23 16:15:48.744
STEP: waiting for all Replicas to be Ready 01/12/23 16:15:48.745
Jan 12 16:15:48.746: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.747: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.754: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.754: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.762: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.762: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.787: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:48.787: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 12 16:15:49.363: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 12 16:15:49.363: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 12 16:15:49.951: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/12/23 16:15:49.951
W0112 16:15:49.960652      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 16:15:49.962: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/12/23 16:15:49.962
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.972: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.972: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.986: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.986: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:49.993: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:49.993: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:50.002: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:50.002: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:51.383: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:51.383: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:51.397: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
STEP: listing Deployments 01/12/23 16:15:51.397
Jan 12 16:15:51.400: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/12/23 16:15:51.4
Jan 12 16:15:51.409: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/12/23 16:15:51.409
Jan 12 16:15:51.414: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:51.419: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:51.438: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:51.448: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:51.457: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.384: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.395: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.404: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.413: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.418: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 12 16:15:52.995: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/12/23 16:15:53.008
STEP: fetching the DeploymentStatus 01/12/23 16:15:53.02
Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 3
STEP: deleting the Deployment 01/12/23 16:15:53.025
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.032: INFO: observed event type MODIFIED
Jan 12 16:15:53.033: INFO: observed event type MODIFIED
Jan 12 16:15:53.033: INFO: observed event type MODIFIED
Jan 12 16:15:53.033: INFO: observed event type MODIFIED
Jan 12 16:15:53.033: INFO: observed event type MODIFIED
Jan 12 16:15:53.033: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 16:15:53.035: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 12 16:15:53.037: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3591  3c5debca-1f7d-49a2-b2b1-0d05e62d909a 17254 2 2023-01-12 16:15:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196897 0xc006196898}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196920 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 12 16:15:53.044: INFO: pod: "test-deployment-7b7876f9d6-8zxb4":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-8zxb4 test-deployment-7b7876f9d6- deployment-3591  535a2aaa-1cea-41f5-81e5-ce8f4764f7c3 17253 0 2023-01-12 16:15:52 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3c5debca-1f7d-49a2-b2b1-0d05e62d909a 0xc006196d97 0xc006196d98}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c5debca-1f7d-49a2-b2b1-0d05e62d909a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhcnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhcnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.150,StartTime:2023-01-12 16:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bee43dba8294bdb85691c9231d3feabac299aff51e24dde1a6332547c54878c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 12 16:15:53.045: INFO: pod: "test-deployment-7b7876f9d6-98d9m":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-98d9m test-deployment-7b7876f9d6- deployment-3591  72770749-7069-4b61-a20b-b49dec29669b 17222 0 2023-01-12 16:15:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3c5debca-1f7d-49a2-b2b1-0d05e62d909a 0xc006196f87 0xc006196f88}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c5debca-1f7d-49a2-b2b1-0d05e62d909a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v2f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v2f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.243,StartTime:2023-01-12 16:15:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d35f9df4e343173efb469bf4480e6c61ec70d678f0f4686a90aedd69f619835b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 12 16:15:53.045: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3591  1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 17262 4 2023-01-12 16:15:49 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196987 0xc006196988}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196a10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 12 16:15:53.048: INFO: pod: "test-deployment-7df74c55ff-mrhrc":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-mrhrc test-deployment-7df74c55ff- deployment-3591  512496e9-e92c-48a0-865a-818cb2678db1 17257 0 2023-01-12 16:15:49 +0000 UTC 2023-01-12 16:15:53 +0000 UTC 0xc000f363e8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 0xc000f36417 0xc000f36418}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf48n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf48n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.242,StartTime:2023-01-12 16:15:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c53f8b712779383a67dc3835f5bfc72ae03a2ccf826cb76361f7aee2058aa637,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 12 16:15:53.048: INFO: pod: "test-deployment-7df74c55ff-rzq68":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-rzq68 test-deployment-7df74c55ff- deployment-3591  49efb145-2efb-4ce2-925d-1a39ed2fb292 17255 0 2023-01-12 16:15:51 +0000 UTC 2023-01-12 16:15:53 +0000 UTC 0xc000f365e0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 0xc000f36617 0xc000f36618}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49t5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49t5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.149,StartTime:2023-01-12 16:15:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://4949806a50585e6cb9e0c2ad86f08a3a4aca57d3e9f63891c22383f6d05f53cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 12 16:15:53.048: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3591  955ebd18-d01e-4892-a3f7-568d8dc25b6a 17186 3 2023-01-12 16:15:48 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196a77 0xc006196a78}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196b00 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 16:15:53.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3591" for this suite. 01/12/23 16:15:53.059
------------------------------
 [4.340 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:15:48.724
    Jan 12 16:15:48.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 16:15:48.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:48.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:48.737
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/12/23 16:15:48.741
    STEP: waiting for Deployment to be created 01/12/23 16:15:48.744
    STEP: waiting for all Replicas to be Ready 01/12/23 16:15:48.745
    Jan 12 16:15:48.746: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.747: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.754: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.754: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.762: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.762: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.787: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:48.787: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 12 16:15:49.363: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 12 16:15:49.363: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 12 16:15:49.951: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/12/23 16:15:49.951
    W0112 16:15:49.960652      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 16:15:49.962: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/12/23 16:15:49.962
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.963: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 0
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.964: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.972: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.972: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.986: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.986: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:49.993: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:49.993: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:50.002: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:50.002: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:51.383: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:51.383: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:51.397: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    STEP: listing Deployments 01/12/23 16:15:51.397
    Jan 12 16:15:51.400: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/12/23 16:15:51.4
    Jan 12 16:15:51.409: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/12/23 16:15:51.409
    Jan 12 16:15:51.414: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:51.419: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:51.438: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:51.448: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:51.457: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.384: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.395: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.404: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.413: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.418: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 12 16:15:52.995: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/12/23 16:15:53.008
    STEP: fetching the DeploymentStatus 01/12/23 16:15:53.02
    Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:53.024: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 1
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 2
    Jan 12 16:15:53.025: INFO: observed Deployment test-deployment in namespace deployment-3591 with ReadyReplicas 3
    STEP: deleting the Deployment 01/12/23 16:15:53.025
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.032: INFO: observed event type MODIFIED
    Jan 12 16:15:53.033: INFO: observed event type MODIFIED
    Jan 12 16:15:53.033: INFO: observed event type MODIFIED
    Jan 12 16:15:53.033: INFO: observed event type MODIFIED
    Jan 12 16:15:53.033: INFO: observed event type MODIFIED
    Jan 12 16:15:53.033: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 16:15:53.035: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 12 16:15:53.037: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-3591  3c5debca-1f7d-49a2-b2b1-0d05e62d909a 17254 2 2023-01-12 16:15:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196897 0xc006196898}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196920 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 12 16:15:53.044: INFO: pod: "test-deployment-7b7876f9d6-8zxb4":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-8zxb4 test-deployment-7b7876f9d6- deployment-3591  535a2aaa-1cea-41f5-81e5-ce8f4764f7c3 17253 0 2023-01-12 16:15:52 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3c5debca-1f7d-49a2-b2b1-0d05e62d909a 0xc006196d97 0xc006196d98}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c5debca-1f7d-49a2-b2b1-0d05e62d909a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhcnd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhcnd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.150,StartTime:2023-01-12 16:15:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bee43dba8294bdb85691c9231d3feabac299aff51e24dde1a6332547c54878c2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 12 16:15:53.045: INFO: pod: "test-deployment-7b7876f9d6-98d9m":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-98d9m test-deployment-7b7876f9d6- deployment-3591  72770749-7069-4b61-a20b-b49dec29669b 17222 0 2023-01-12 16:15:51 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3c5debca-1f7d-49a2-b2b1-0d05e62d909a 0xc006196f87 0xc006196f88}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c5debca-1f7d-49a2-b2b1-0d05e62d909a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v2f9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v2f9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.243,StartTime:2023-01-12 16:15:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d35f9df4e343173efb469bf4480e6c61ec70d678f0f4686a90aedd69f619835b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.243,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 12 16:15:53.045: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-3591  1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 17262 4 2023-01-12 16:15:49 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196987 0xc006196988}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196a10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 12 16:15:53.048: INFO: pod: "test-deployment-7df74c55ff-mrhrc":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-mrhrc test-deployment-7df74c55ff- deployment-3591  512496e9-e92c-48a0-865a-818cb2678db1 17257 0 2023-01-12 16:15:49 +0000 UTC 2023-01-12 16:15:53 +0000 UTC 0xc000f363e8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 0xc000f36417 0xc000f36418}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf48n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf48n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.242,StartTime:2023-01-12 16:15:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c53f8b712779383a67dc3835f5bfc72ae03a2ccf826cb76361f7aee2058aa637,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 12 16:15:53.048: INFO: pod: "test-deployment-7df74c55ff-rzq68":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-rzq68 test-deployment-7df74c55ff- deployment-3591  49efb145-2efb-4ce2-925d-1a39ed2fb292 17255 0 2023-01-12 16:15:51 +0000 UTC 2023-01-12 16:15:53 +0000 UTC 0xc000f365e0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9 0xc000f36617 0xc000f36618}] [] [{kube-controller-manager Update v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1eccb706-6ca6-4e4b-aa97-e476c5a0c9f9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:15:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49t5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49t5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:15:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.149,StartTime:2023-01-12 16:15:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:15:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://4949806a50585e6cb9e0c2ad86f08a3a4aca57d3e9f63891c22383f6d05f53cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 12 16:15:53.048: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-3591  955ebd18-d01e-4892-a3f7-568d8dc25b6a 17186 3 2023-01-12 16:15:48 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0762a092-c07c-428d-870c-fd9d50e06177 0xc006196a77 0xc006196a78}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0762a092-c07c-428d-870c-fd9d50e06177\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:15:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006196b00 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:15:53.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3591" for this suite. 01/12/23 16:15:53.059
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:15:53.065
Jan 12 16:15:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:15:53.066
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:53.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:53.083
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-6615 01/12/23 16:15:53.086
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[] 01/12/23 16:15:53.103
Jan 12 16:15:53.107: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 12 16:15:54.113: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6615 01/12/23 16:15:54.113
Jan 12 16:15:54.118: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6615" to be "running and ready"
Jan 12 16:15:54.120: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993074ms
Jan 12 16:15:54.120: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:15:56.123: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623388s
Jan 12 16:15:56.123: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 16:15:56.123: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod1:[80]] 01/12/23 16:15:56.125
Jan 12 16:15:56.130: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/12/23 16:15:56.13
Jan 12 16:15:56.130: INFO: Creating new exec pod
Jan 12 16:15:56.135: INFO: Waiting up to 5m0s for pod "execpodhpjsv" in namespace "services-6615" to be "running"
Jan 12 16:15:56.137: INFO: Pod "execpodhpjsv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831352ms
Jan 12 16:15:58.142: INFO: Pod "execpodhpjsv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006226873s
Jan 12 16:15:58.142: INFO: Pod "execpodhpjsv" satisfied condition "running"
Jan 12 16:15:59.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 16:15:59.263: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 16:15:59.263: INFO: stdout: ""
Jan 12 16:15:59.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
Jan 12 16:15:59.381: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
Jan 12 16:15:59.381: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-6615 01/12/23 16:15:59.381
Jan 12 16:15:59.385: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6615" to be "running and ready"
Jan 12 16:15:59.387: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717219ms
Jan 12 16:15:59.387: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:16:01.390: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004928935s
Jan 12 16:16:01.390: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 16:16:01.390: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod1:[80] pod2:[80]] 01/12/23 16:16:01.391
Jan 12 16:16:01.398: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/12/23 16:16:01.398
Jan 12 16:16:02.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 16:16:02.510: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 16:16:02.510: INFO: stdout: ""
Jan 12 16:16:02.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
Jan 12 16:16:02.639: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
Jan 12 16:16:02.639: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-6615 01/12/23 16:16:02.639
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod2:[80]] 01/12/23 16:16:02.65
Jan 12 16:16:02.660: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/12/23 16:16:02.66
Jan 12 16:16:03.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 12 16:16:03.771: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 12 16:16:03.771: INFO: stdout: ""
Jan 12 16:16:03.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
Jan 12 16:16:03.881: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
Jan 12 16:16:03.881: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-6615 01/12/23 16:16:03.881
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[] 01/12/23 16:16:03.894
Jan 12 16:16:03.901: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6615" for this suite. 01/12/23 16:16:03.92
------------------------------
 [SLOW TEST] [10.861 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:15:53.065
    Jan 12 16:15:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:15:53.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:15:53.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:15:53.083
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-6615 01/12/23 16:15:53.086
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[] 01/12/23 16:15:53.103
    Jan 12 16:15:53.107: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 12 16:15:54.113: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-6615 01/12/23 16:15:54.113
    Jan 12 16:15:54.118: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-6615" to be "running and ready"
    Jan 12 16:15:54.120: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.993074ms
    Jan 12 16:15:54.120: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:15:56.123: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623388s
    Jan 12 16:15:56.123: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 16:15:56.123: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod1:[80]] 01/12/23 16:15:56.125
    Jan 12 16:15:56.130: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/12/23 16:15:56.13
    Jan 12 16:15:56.130: INFO: Creating new exec pod
    Jan 12 16:15:56.135: INFO: Waiting up to 5m0s for pod "execpodhpjsv" in namespace "services-6615" to be "running"
    Jan 12 16:15:56.137: INFO: Pod "execpodhpjsv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831352ms
    Jan 12 16:15:58.142: INFO: Pod "execpodhpjsv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006226873s
    Jan 12 16:15:58.142: INFO: Pod "execpodhpjsv" satisfied condition "running"
    Jan 12 16:15:59.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 16:15:59.263: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 16:15:59.263: INFO: stdout: ""
    Jan 12 16:15:59.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
    Jan 12 16:15:59.381: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
    Jan 12 16:15:59.381: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-6615 01/12/23 16:15:59.381
    Jan 12 16:15:59.385: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-6615" to be "running and ready"
    Jan 12 16:15:59.387: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.717219ms
    Jan 12 16:15:59.387: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:16:01.390: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004928935s
    Jan 12 16:16:01.390: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 16:16:01.390: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod1:[80] pod2:[80]] 01/12/23 16:16:01.391
    Jan 12 16:16:01.398: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/12/23 16:16:01.398
    Jan 12 16:16:02.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 16:16:02.510: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 16:16:02.510: INFO: stdout: ""
    Jan 12 16:16:02.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
    Jan 12 16:16:02.639: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
    Jan 12 16:16:02.639: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-6615 01/12/23 16:16:02.639
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[pod2:[80]] 01/12/23 16:16:02.65
    Jan 12 16:16:02.660: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/12/23 16:16:02.66
    Jan 12 16:16:03.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 12 16:16:03.771: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 12 16:16:03.771: INFO: stdout: ""
    Jan 12 16:16:03.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-6615 exec execpodhpjsv -- /bin/sh -x -c nc -v -z -w 2 10.103.47.14 80'
    Jan 12 16:16:03.881: INFO: stderr: "+ nc -v -z -w 2 10.103.47.14 80\nConnection to 10.103.47.14 80 port [tcp/http] succeeded!\n"
    Jan 12 16:16:03.881: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-6615 01/12/23 16:16:03.881
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6615 to expose endpoints map[] 01/12/23 16:16:03.894
    Jan 12 16:16:03.901: INFO: successfully validated that service endpoint-test2 in namespace services-6615 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6615" for this suite. 01/12/23 16:16:03.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:03.929
Jan 12 16:16:03.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:16:03.93
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:03.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:03.944
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-3c5f8986-df02-4fb2-b64a-fa8fd5097675 01/12/23 16:16:03.946
STEP: Creating a pod to test consume secrets 01/12/23 16:16:03.951
Jan 12 16:16:03.956: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba" in namespace "projected-1343" to be "Succeeded or Failed"
Jan 12 16:16:03.958: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740271ms
Jan 12 16:16:05.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004956527s
Jan 12 16:16:07.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004811178s
STEP: Saw pod success 01/12/23 16:16:07.961
Jan 12 16:16:07.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba" satisfied condition "Succeeded or Failed"
Jan 12 16:16:07.963: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:16:07.973
Jan 12 16:16:07.980: INFO: Waiting for pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba to disappear
Jan 12 16:16:07.982: INFO: Pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1343" for this suite. 01/12/23 16:16:07.984
------------------------------
 [4.058 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:03.929
    Jan 12 16:16:03.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:16:03.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:03.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:03.944
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-3c5f8986-df02-4fb2-b64a-fa8fd5097675 01/12/23 16:16:03.946
    STEP: Creating a pod to test consume secrets 01/12/23 16:16:03.951
    Jan 12 16:16:03.956: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba" in namespace "projected-1343" to be "Succeeded or Failed"
    Jan 12 16:16:03.958: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740271ms
    Jan 12 16:16:05.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004956527s
    Jan 12 16:16:07.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004811178s
    STEP: Saw pod success 01/12/23 16:16:07.961
    Jan 12 16:16:07.961: INFO: Pod "pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba" satisfied condition "Succeeded or Failed"
    Jan 12 16:16:07.963: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:16:07.973
    Jan 12 16:16:07.980: INFO: Waiting for pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba to disappear
    Jan 12 16:16:07.982: INFO: Pod pod-projected-secrets-fcf1cf5a-718c-46fa-bd98-4e098fa400ba no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1343" for this suite. 01/12/23 16:16:07.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:07.99
Jan 12 16:16:07.990: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:16:07.991
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:07.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:08.001
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-a12ec460-6f58-4bd5-93e4-d8cb25204729 01/12/23 16:16:08.003
STEP: Creating a pod to test consume secrets 01/12/23 16:16:08.006
Jan 12 16:16:08.012: INFO: Waiting up to 5m0s for pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf" in namespace "secrets-7560" to be "Succeeded or Failed"
Jan 12 16:16:08.014: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.719762ms
Jan 12 16:16:10.017: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004934488s
Jan 12 16:16:12.016: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004119534s
STEP: Saw pod success 01/12/23 16:16:12.016
Jan 12 16:16:12.016: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf" satisfied condition "Succeeded or Failed"
Jan 12 16:16:12.018: INFO: Trying to get logs from node worker-1 pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:16:12.022
Jan 12 16:16:12.033: INFO: Waiting for pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf to disappear
Jan 12 16:16:12.035: INFO: Pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:12.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7560" for this suite. 01/12/23 16:16:12.037
------------------------------
 [4.051 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:07.99
    Jan 12 16:16:07.990: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:16:07.991
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:07.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:08.001
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-a12ec460-6f58-4bd5-93e4-d8cb25204729 01/12/23 16:16:08.003
    STEP: Creating a pod to test consume secrets 01/12/23 16:16:08.006
    Jan 12 16:16:08.012: INFO: Waiting up to 5m0s for pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf" in namespace "secrets-7560" to be "Succeeded or Failed"
    Jan 12 16:16:08.014: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.719762ms
    Jan 12 16:16:10.017: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004934488s
    Jan 12 16:16:12.016: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004119534s
    STEP: Saw pod success 01/12/23 16:16:12.016
    Jan 12 16:16:12.016: INFO: Pod "pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf" satisfied condition "Succeeded or Failed"
    Jan 12 16:16:12.018: INFO: Trying to get logs from node worker-1 pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:16:12.022
    Jan 12 16:16:12.033: INFO: Waiting for pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf to disappear
    Jan 12 16:16:12.035: INFO: Pod pod-secrets-051cbe08-c2a2-4a97-be54-45629ad9c7bf no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:12.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7560" for this suite. 01/12/23 16:16:12.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:12.042
Jan 12 16:16:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:16:12.043
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:12.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:12.052
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:16:12.063
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:12.379
STEP: Deploying the webhook pod 01/12/23 16:16:12.386
STEP: Wait for the deployment to be ready 01/12/23 16:16:12.395
Jan 12 16:16:12.401: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:16:14.407
STEP: Verifying the service has paired with the endpoint 01/12/23 16:16:14.416
Jan 12 16:16:15.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 12 16:16:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8696-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:16:15.926
STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 16:16:15.943
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:18.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9395" for this suite. 01/12/23 16:16:18.522
STEP: Destroying namespace "webhook-9395-markers" for this suite. 01/12/23 16:16:18.527
------------------------------
 [SLOW TEST] [6.490 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:12.042
    Jan 12 16:16:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:16:12.043
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:12.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:12.052
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:16:12.063
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:12.379
    STEP: Deploying the webhook pod 01/12/23 16:16:12.386
    STEP: Wait for the deployment to be ready 01/12/23 16:16:12.395
    Jan 12 16:16:12.401: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:16:14.407
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:16:14.416
    Jan 12 16:16:15.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 12 16:16:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8696-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:16:15.926
    STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 16:16:15.943
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:18.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9395" for this suite. 01/12/23 16:16:18.522
    STEP: Destroying namespace "webhook-9395-markers" for this suite. 01/12/23 16:16:18.527
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:18.537
Jan 12 16:16:18.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:16:18.538
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:18.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:18.549
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 16:16:18.551
Jan 12 16:16:18.559: INFO: Waiting up to 5m0s for pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4" in namespace "emptydir-3236" to be "Succeeded or Failed"
Jan 12 16:16:18.560: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.679309ms
Jan 12 16:16:20.564: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493814s
Jan 12 16:16:22.563: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004798562s
STEP: Saw pod success 01/12/23 16:16:22.563
Jan 12 16:16:22.564: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4" satisfied condition "Succeeded or Failed"
Jan 12 16:16:22.565: INFO: Trying to get logs from node worker-1 pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 container test-container: <nil>
STEP: delete the pod 01/12/23 16:16:22.57
Jan 12 16:16:22.578: INFO: Waiting for pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 to disappear
Jan 12 16:16:22.580: INFO: Pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:22.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3236" for this suite. 01/12/23 16:16:22.582
------------------------------
 [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:18.537
    Jan 12 16:16:18.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:16:18.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:18.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:18.549
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 16:16:18.551
    Jan 12 16:16:18.559: INFO: Waiting up to 5m0s for pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4" in namespace "emptydir-3236" to be "Succeeded or Failed"
    Jan 12 16:16:18.560: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.679309ms
    Jan 12 16:16:20.564: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00493814s
    Jan 12 16:16:22.563: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004798562s
    STEP: Saw pod success 01/12/23 16:16:22.563
    Jan 12 16:16:22.564: INFO: Pod "pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4" satisfied condition "Succeeded or Failed"
    Jan 12 16:16:22.565: INFO: Trying to get logs from node worker-1 pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:16:22.57
    Jan 12 16:16:22.578: INFO: Waiting for pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 to disappear
    Jan 12 16:16:22.580: INFO: Pod pod-c1f44db3-e3d3-4e77-9d1c-8cc18ccea0d4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:22.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3236" for this suite. 01/12/23 16:16:22.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:22.588
Jan 12 16:16:22.588: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:16:22.589
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:22.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:22.599
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/12/23 16:16:22.603
STEP: watching for the Service to be added 01/12/23 16:16:22.614
Jan 12 16:16:22.616: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 12 16:16:22.616: INFO: Service test-service-phcpb created
STEP: Getting /status 01/12/23 16:16:22.616
Jan 12 16:16:22.618: INFO: Service test-service-phcpb has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/12/23 16:16:22.618
STEP: watching for the Service to be patched 01/12/23 16:16:22.623
Jan 12 16:16:22.624: INFO: observed Service test-service-phcpb in namespace services-8568 with annotations: map[] & LoadBalancer: {[]}
Jan 12 16:16:22.625: INFO: Found Service test-service-phcpb in namespace services-8568 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 12 16:16:22.625: INFO: Service test-service-phcpb has service status patched
STEP: updating the ServiceStatus 01/12/23 16:16:22.625
Jan 12 16:16:22.630: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/12/23 16:16:22.63
Jan 12 16:16:22.631: INFO: Observed Service test-service-phcpb in namespace services-8568 with annotations: map[] & Conditions: {[]}
Jan 12 16:16:22.631: INFO: Observed event: &Service{ObjectMeta:{test-service-phcpb  services-8568  96b2c627-76c9-4dce-a84d-ac94576051ce 17598 0 2023-01-12 16:16:22 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-12 16:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-12 16:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.111.13.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.111.13.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 12 16:16:22.631: INFO: Found Service test-service-phcpb in namespace services-8568 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 16:16:22.631: INFO: Service test-service-phcpb has service status updated
STEP: patching the service 01/12/23 16:16:22.631
STEP: watching for the Service to be patched 01/12/23 16:16:22.641
Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
Jan 12 16:16:22.643: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service:patched test-service-static:true]
Jan 12 16:16:22.643: INFO: Service test-service-phcpb patched
STEP: deleting the service 01/12/23 16:16:22.643
STEP: watching for the Service to be deleted 01/12/23 16:16:22.653
Jan 12 16:16:22.654: INFO: Observed event: ADDED
Jan 12 16:16:22.654: INFO: Observed event: MODIFIED
Jan 12 16:16:22.654: INFO: Observed event: MODIFIED
Jan 12 16:16:22.655: INFO: Observed event: MODIFIED
Jan 12 16:16:22.655: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 12 16:16:22.655: INFO: Service test-service-phcpb deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:22.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8568" for this suite. 01/12/23 16:16:22.657
------------------------------
 [0.073 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:22.588
    Jan 12 16:16:22.588: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:16:22.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:22.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:22.599
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/12/23 16:16:22.603
    STEP: watching for the Service to be added 01/12/23 16:16:22.614
    Jan 12 16:16:22.616: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 12 16:16:22.616: INFO: Service test-service-phcpb created
    STEP: Getting /status 01/12/23 16:16:22.616
    Jan 12 16:16:22.618: INFO: Service test-service-phcpb has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/12/23 16:16:22.618
    STEP: watching for the Service to be patched 01/12/23 16:16:22.623
    Jan 12 16:16:22.624: INFO: observed Service test-service-phcpb in namespace services-8568 with annotations: map[] & LoadBalancer: {[]}
    Jan 12 16:16:22.625: INFO: Found Service test-service-phcpb in namespace services-8568 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 12 16:16:22.625: INFO: Service test-service-phcpb has service status patched
    STEP: updating the ServiceStatus 01/12/23 16:16:22.625
    Jan 12 16:16:22.630: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/12/23 16:16:22.63
    Jan 12 16:16:22.631: INFO: Observed Service test-service-phcpb in namespace services-8568 with annotations: map[] & Conditions: {[]}
    Jan 12 16:16:22.631: INFO: Observed event: &Service{ObjectMeta:{test-service-phcpb  services-8568  96b2c627-76c9-4dce-a84d-ac94576051ce 17598 0 2023-01-12 16:16:22 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-12 16:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-12 16:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.111.13.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.111.13.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 12 16:16:22.631: INFO: Found Service test-service-phcpb in namespace services-8568 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 16:16:22.631: INFO: Service test-service-phcpb has service status updated
    STEP: patching the service 01/12/23 16:16:22.631
    STEP: watching for the Service to be patched 01/12/23 16:16:22.641
    Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
    Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
    Jan 12 16:16:22.643: INFO: observed Service test-service-phcpb in namespace services-8568 with labels: map[test-service-static:true]
    Jan 12 16:16:22.643: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service:patched test-service-static:true]
    Jan 12 16:16:22.643: INFO: Service test-service-phcpb patched
    STEP: deleting the service 01/12/23 16:16:22.643
    STEP: watching for the Service to be deleted 01/12/23 16:16:22.653
    Jan 12 16:16:22.654: INFO: Observed event: ADDED
    Jan 12 16:16:22.654: INFO: Observed event: MODIFIED
    Jan 12 16:16:22.654: INFO: Observed event: MODIFIED
    Jan 12 16:16:22.655: INFO: Observed event: MODIFIED
    Jan 12 16:16:22.655: INFO: Found Service test-service-phcpb in namespace services-8568 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 12 16:16:22.655: INFO: Service test-service-phcpb deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:22.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8568" for this suite. 01/12/23 16:16:22.657
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:22.661
Jan 12 16:16:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:16:22.662
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:22.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:22.672
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:16:22.674
Jan 12 16:16:22.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728" in namespace "projected-605" to be "Succeeded or Failed"
Jan 12 16:16:22.684: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003641ms
Jan 12 16:16:24.687: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005272573s
Jan 12 16:16:26.687: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006210427s
STEP: Saw pod success 01/12/23 16:16:26.687
Jan 12 16:16:26.688: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728" satisfied condition "Succeeded or Failed"
Jan 12 16:16:26.689: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 container client-container: <nil>
STEP: delete the pod 01/12/23 16:16:26.694
Jan 12 16:16:26.702: INFO: Waiting for pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 to disappear
Jan 12 16:16:26.703: INFO: Pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:26.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-605" for this suite. 01/12/23 16:16:26.705
------------------------------
 [4.047 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:22.661
    Jan 12 16:16:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:16:22.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:22.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:22.672
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:16:22.674
    Jan 12 16:16:22.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728" in namespace "projected-605" to be "Succeeded or Failed"
    Jan 12 16:16:22.684: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Pending", Reason="", readiness=false. Elapsed: 3.003641ms
    Jan 12 16:16:24.687: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005272573s
    Jan 12 16:16:26.687: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006210427s
    STEP: Saw pod success 01/12/23 16:16:26.687
    Jan 12 16:16:26.688: INFO: Pod "downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728" satisfied condition "Succeeded or Failed"
    Jan 12 16:16:26.689: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:16:26.694
    Jan 12 16:16:26.702: INFO: Waiting for pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 to disappear
    Jan 12 16:16:26.703: INFO: Pod downwardapi-volume-f925e6c1-d7de-4dc4-903e-75e55a489728 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:26.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-605" for this suite. 01/12/23 16:16:26.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:26.71
Jan 12 16:16:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption 01/12/23 16:16:26.711
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:26.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:26.723
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/12/23 16:16:26.728
STEP: Waiting for all pods to be running 01/12/23 16:16:28.751
Jan 12 16:16:28.758: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:30.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8824" for this suite. 01/12/23 16:16:30.765
------------------------------
 [4.059 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:26.71
    Jan 12 16:16:26.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption 01/12/23 16:16:26.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:26.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:26.723
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/12/23 16:16:26.728
    STEP: Waiting for all pods to be running 01/12/23 16:16:28.751
    Jan 12 16:16:28.758: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:30.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8824" for this suite. 01/12/23 16:16:30.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:30.77
Jan 12 16:16:30.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:16:30.771
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:30.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:30.785
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-fe4fe9ab-7cb8-4bd6-8bc8-fc15cf2945d6 01/12/23 16:16:30.787
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:30.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2134" for this suite. 01/12/23 16:16:30.791
------------------------------
 [0.024 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:30.77
    Jan 12 16:16:30.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:16:30.771
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:30.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:30.785
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-fe4fe9ab-7cb8-4bd6-8bc8-fc15cf2945d6 01/12/23 16:16:30.787
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:30.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2134" for this suite. 01/12/23 16:16:30.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:30.797
Jan 12 16:16:30.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 16:16:30.798
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:30.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:30.812
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/12/23 16:16:30.814
STEP: delete the rc 01/12/23 16:16:35.823
STEP: wait for all pods to be garbage collected 01/12/23 16:16:35.827
STEP: Gathering metrics 01/12/23 16:16:40.831
W0112 16:16:40.834877      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 16:16:40.834: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9386" for this suite. 01/12/23 16:16:40.837
------------------------------
 [SLOW TEST] [10.043 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:30.797
    Jan 12 16:16:30.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 16:16:30.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:30.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:30.812
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/12/23 16:16:30.814
    STEP: delete the rc 01/12/23 16:16:35.823
    STEP: wait for all pods to be garbage collected 01/12/23 16:16:35.827
    STEP: Gathering metrics 01/12/23 16:16:40.831
    W0112 16:16:40.834877      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 16:16:40.834: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9386" for this suite. 01/12/23 16:16:40.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:40.841
Jan 12 16:16:40.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:16:40.841
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:40.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:40.854
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:16:40.863
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:41.49
STEP: Deploying the webhook pod 01/12/23 16:16:41.494
STEP: Wait for the deployment to be ready 01/12/23 16:16:41.502
Jan 12 16:16:41.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:16:43.515
STEP: Verifying the service has paired with the endpoint 01/12/23 16:16:43.525
Jan 12 16:16:44.526: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 12 16:16:44.528: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3778-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:16:45.036
STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 16:16:45.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:47.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6475" for this suite. 01/12/23 16:16:47.651
STEP: Destroying namespace "webhook-6475-markers" for this suite. 01/12/23 16:16:47.656
------------------------------
 [SLOW TEST] [6.821 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:40.841
    Jan 12 16:16:40.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:16:40.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:40.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:40.854
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:16:40.863
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:41.49
    STEP: Deploying the webhook pod 01/12/23 16:16:41.494
    STEP: Wait for the deployment to be ready 01/12/23 16:16:41.502
    Jan 12 16:16:41.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:16:43.515
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:16:43.525
    Jan 12 16:16:44.526: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 12 16:16:44.528: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3778-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:16:45.036
    STEP: Creating a custom resource that should be mutated by the webhook 01/12/23 16:16:45.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:47.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6475" for this suite. 01/12/23 16:16:47.651
    STEP: Destroying namespace "webhook-6475-markers" for this suite. 01/12/23 16:16:47.656
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:47.662
Jan 12 16:16:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 16:16:47.663
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:47.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:47.675
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 12 16:16:47.678: INFO: Creating deployment "webserver-deployment"
Jan 12 16:16:47.681: INFO: Waiting for observed generation 1
Jan 12 16:16:49.686: INFO: Waiting for all required pods to come up
Jan 12 16:16:49.689: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/12/23 16:16:49.689
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wxkv7" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-d8h6r" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8hj9p" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-c9npx" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-57zbh" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lbvgq" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m9c7k" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sglpv" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hmjbh" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6dwm2" in namespace "deployment-8768" to be "running"
Jan 12 16:16:49.691: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860385ms
Jan 12 16:16:49.692: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.548176ms
Jan 12 16:16:49.693: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154728ms
Jan 12 16:16:49.693: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402205ms
Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080229ms
Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985766ms
Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.733727ms
Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92931ms
Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875017ms
Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022969ms
Jan 12 16:16:51.694: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.004839586s
Jan 12 16:16:51.694: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r" satisfied condition "running"
Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005624848s
Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7" satisfied condition "running"
Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005810963s
Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p" satisfied condition "running"
Jan 12 16:16:51.696: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.006220239s
Jan 12 16:16:51.696: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k" satisfied condition "running"
Jan 12 16:16:51.697: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.007215389s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00761819s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007166484s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007675015s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx": Phase="Running", Reason="", readiness=true. Elapsed: 2.007826122s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007779285s
Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2" satisfied condition "running"
Jan 12 16:16:51.698: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 12 16:16:51.702: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 12 16:16:51.708: INFO: Updating deployment webserver-deployment
Jan 12 16:16:51.709: INFO: Waiting for observed generation 2
Jan 12 16:16:53.713: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 12 16:16:53.714: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 12 16:16:53.716: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 12 16:16:53.720: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 12 16:16:53.721: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 12 16:16:53.722: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 12 16:16:53.725: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 12 16:16:53.725: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 12 16:16:53.730: INFO: Updating deployment webserver-deployment
Jan 12 16:16:53.730: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 12 16:16:53.733: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 12 16:16:53.736: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 16:16:55.744: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8768  d2b05400-7c41-4564-8879-08e736fdf772 18196 3 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006197e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 16:16:53 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-12 16:16:53 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 12 16:16:55.746: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8768  efb56efe-d5a8-413a-842d-e7abe43ff0ef 18191 3 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d2b05400-7c41-4564-8879-08e736fdf772 0xc005e7a307 0xc005e7a308}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2b05400-7c41-4564-8879-08e736fdf772\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:16:55.746: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 12 16:16:55.746: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8768  098e172b-976f-47c8-bcc0-1f8023709917 18192 3 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d2b05400-7c41-4564-8879-08e736fdf772 0xc005e7a217 0xc005e7a218}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2b05400-7c41-4564-8879-08e736fdf772\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:16:55.750: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-57zbh webserver-deployment-7f5969cbc7- deployment-8768  0200b38f-c767-4c5b-9915-7a9e8a4a07e1 18017 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7a8b7 0xc005e7a8b8}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwrpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwrpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.158,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b66e02bdfbeb0abf0c9c990c2d11f73a1a826ed3dd538f3f128f4b0a2dff98cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.750: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6dwm2 webserver-deployment-7f5969cbc7- deployment-8768  712e07a0-bc13-4f0b-abf7-f70dc9accae6 18007 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7aaa7 0xc005e7aaa8}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmqc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmqc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.8,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e8775bc0b36566857d1d1939410e9276c538c94044123a68793853dbbe5a146,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8hj9p webserver-deployment-7f5969cbc7- deployment-8768  f906e037-778b-40f9-a1df-1461c5cd315e 17992 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ac90 0xc005e7ac91}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrx7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrx7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.156,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://71e1a5ba8d6919d9ec764e7db16acac5359d3567c830f2a6e09f196611602ecc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-9rqpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9rqpn webserver-deployment-7f5969cbc7- deployment-8768  a457e846-5114-4430-9a4e-7a5cfd186c40 18155 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ae77 0xc005e7ae78}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fckh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fckh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-9v72c" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9v72c webserver-deployment-7f5969cbc7- deployment-8768  c15091fc-5fd3-4ccf-8b9c-07a538ddd80f 18128 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7afe0 0xc005e7afe1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csz7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csz7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-bdvv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bdvv7 webserver-deployment-7f5969cbc7- deployment-8768  57c32e62-99e7-412a-bd82-891ce46232f0 18182 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b160 0xc005e7b161}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnjrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnjrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-c5m6l" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c5m6l webserver-deployment-7f5969cbc7- deployment-8768  c175aed1-19f5-452b-928f-bc874934280c 18120 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b2c0 0xc005e7b2c1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmffb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmffb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c9npx webserver-deployment-7f5969cbc7- deployment-8768  7640b3d3-60b7-4392-957c-14aab5cc3a44 17978 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b420 0xc005e7b421}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wvm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wvm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.7,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1195278a9d18cc03fc5261bf474fa4cfd370a8fc6ea7f2aa82648ecc7b2b285b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8h6r webserver-deployment-7f5969cbc7- deployment-8768  29194d6a-736a-4e32-aff6-3f75a7c5beb7 17999 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b610 0xc005e7b611}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s6cw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s6cw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.5,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://190f730e3e064f2e6f21a7b8503aee32bbc229cebda41d4bbd2131b2c143b0f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m9c7k webserver-deployment-7f5969cbc7- deployment-8768  11e699cd-7ea6-4db4-9786-fefb200103d0 18010 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b830 0xc005e7b831}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mc9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mc9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.157,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c3739e96f3cf50308bea29bdf5a5179c8ca91757330b6d2e66697dce7d3f6322,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-mbnzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbnzb webserver-deployment-7f5969cbc7- deployment-8768  238b32dd-50e3-48f8-9eff-53c191ab2b6d 18156 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ba27 0xc005e7ba28}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpccz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpccz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-nhzb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nhzb6 webserver-deployment-7f5969cbc7- deployment-8768  979eebd9-e696-4076-86d1-ac2523a56082 18125 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7bba0 0xc005e7bba1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txml9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txml9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-q7zmf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q7zmf webserver-deployment-7f5969cbc7- deployment-8768  4fac2767-3a56-4fd6-880b-cb4185256c33 18174 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7bd00 0xc005e7bd01}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9qb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9qb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-qdmls" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qdmls webserver-deployment-7f5969cbc7- deployment-8768  ae4d09ca-158b-41b9-b433-cdc9b439f723 18199 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7be60 0xc005e7be61}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hn7dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hn7dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-qnch9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qnch9 webserver-deployment-7f5969cbc7- deployment-8768  2c1824ff-2961-4621-a060-2318979d25d3 18152 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc00007e177 0xc00007e178}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhwkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhwkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-rzhdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rzhdg webserver-deployment-7f5969cbc7- deployment-8768  a0741032-9820-4121-992e-b48a12ea979d 18175 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc00007f710 0xc00007f711}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkq8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkq8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sglpv webserver-deployment-7f5969cbc7- deployment-8768  f8f6e51b-25d8-4dfc-9523-d02ef5e73ffa 17994 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a402a0 0xc006a402a1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j62bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j62bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.154,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://203d391560616ab59816be9f123c4affb8a661b9d885d4b6e1476d00195768a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wxkv7 webserver-deployment-7f5969cbc7- deployment-8768  4e1f300c-0c9d-4c27-8d41-c7c6553e331f 18002 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a40997 0xc006a40998}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4847l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4847l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.155,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://635769bce0acdf5a78c8e4a91469639b669d6cc2f16b2ff6eb9897e046912cf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-7f5969cbc7-zght7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zght7 webserver-deployment-7f5969cbc7- deployment-8768  727a03ff-0cc5-40ac-ae9b-09c7745bfeeb 18180 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a41027 0xc006a41028}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flwvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flwvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-7f5969cbc7-zz7s5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zz7s5 webserver-deployment-7f5969cbc7- deployment-8768  5b7d0a0f-4b2b-4453-ab37-281682904b2e 18177 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a41710 0xc006a41711}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wt8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wt8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-d9f79cb5-55snr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-55snr webserver-deployment-d9f79cb5- deployment-8768  41028cdc-b8f7-4c8f-928e-2c38339b25b3 18183 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc006a4185f 0xc006a41890}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh79t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh79t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-d9f79cb5-b6q67" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6q67 webserver-deployment-d9f79cb5- deployment-8768  cd15fe4c-650b-43d5-b267-82ee8414b648 18105 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc006a41bff 0xc006a41cb0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n64hl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n64hl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-cwvkz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cwvkz webserver-deployment-d9f79cb5- deployment-8768  166c0387-aa1b-456e-ab82-30aef5dc9615 18071 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a0af 0xc000a2a0c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnktf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnktf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-fc5sg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fc5sg webserver-deployment-d9f79cb5- deployment-8768  f0521e4e-d5de-4e3b-8071-74dbfb0d21fc 18154 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a2af 0xc000a2a2c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4nhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4nhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-fzchr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fzchr webserver-deployment-d9f79cb5- deployment-8768  e4f6a95c-7d25-4305-a5d3-66cc947cfaa7 18142 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a41f 0xc000a2ace0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgt7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgt7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.760: INFO: Pod "webserver-deployment-d9f79cb5-kdsq9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kdsq9 webserver-deployment-d9f79cb5- deployment-8768  e059cbf4-b77c-4958-832b-3ba3fa16806f 18158 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2b96f 0xc000a2bb00}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zj85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zj85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-kfj6d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kfj6d webserver-deployment-d9f79cb5- deployment-8768  0bde6148-2bf1-4bcf-afef-eaaff3fac3f8 18103 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e15f 0xc003c1e180}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwpms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwpms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.159,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-n677j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-n677j webserver-deployment-d9f79cb5- deployment-8768  f9362e17-d869-489e-b8f9-33a03cfbdcd9 18176 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e37f 0xc003c1e390}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xhqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xhqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-pnzpk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pnzpk webserver-deployment-d9f79cb5- deployment-8768  89a04daa-ea7d-404c-a216-1b7b1ca95b7e 18159 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e59f 0xc003c1e5b0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnw2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnw2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-ptt49" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ptt49 webserver-deployment-d9f79cb5- deployment-8768  3ce9bc4d-66d5-4319-8256-7e410aa92541 18153 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ea3f 0xc003c1ea80}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9mbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9mbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-rrdtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rrdtk webserver-deployment-d9f79cb5- deployment-8768  081bf617-6920-4e95-9fa9-dc8cfcb8fee1 18157 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ebef 0xc003c1ec00}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d646,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d646,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-sp8d7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sp8d7 webserver-deployment-d9f79cb5- deployment-8768  a68b38f6-252a-429d-a90d-396df9271830 18073 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ed6f 0xc003c1ed80}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzpl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzpl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-w9vdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w9vdq webserver-deployment-d9f79cb5- deployment-8768  bced884e-266b-45dc-a74f-2fe0534da390 18197 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1f2af 0xc003c1f2c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c92q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c92q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 16:16:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8768" for this suite. 01/12/23 16:16:55.765
------------------------------
 [SLOW TEST] [8.107 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:47.662
    Jan 12 16:16:47.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 16:16:47.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:47.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:47.675
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 12 16:16:47.678: INFO: Creating deployment "webserver-deployment"
    Jan 12 16:16:47.681: INFO: Waiting for observed generation 1
    Jan 12 16:16:49.686: INFO: Waiting for all required pods to come up
    Jan 12 16:16:49.689: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/12/23 16:16:49.689
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-wxkv7" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-d8h6r" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8hj9p" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-c9npx" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-57zbh" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lbvgq" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m9c7k" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-sglpv" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-hmjbh" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.689: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6dwm2" in namespace "deployment-8768" to be "running"
    Jan 12 16:16:49.691: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r": Phase="Pending", Reason="", readiness=false. Elapsed: 1.860385ms
    Jan 12 16:16:49.692: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.548176ms
    Jan 12 16:16:49.693: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154728ms
    Jan 12 16:16:49.693: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402205ms
    Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080229ms
    Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.985766ms
    Jan 12 16:16:49.694: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.733727ms
    Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.92931ms
    Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.875017ms
    Jan 12 16:16:49.695: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022969ms
    Jan 12 16:16:51.694: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.004839586s
    Jan 12 16:16:51.694: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r" satisfied condition "running"
    Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7": Phase="Running", Reason="", readiness=true. Elapsed: 2.005624848s
    Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7" satisfied condition "running"
    Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005810963s
    Jan 12 16:16:51.695: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p" satisfied condition "running"
    Jan 12 16:16:51.696: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.006220239s
    Jan 12 16:16:51.696: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k" satisfied condition "running"
    Jan 12 16:16:51.697: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv": Phase="Running", Reason="", readiness=true. Elapsed: 2.007215389s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00761819s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-lbvgq" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007166484s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-hmjbh" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007675015s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx": Phase="Running", Reason="", readiness=true. Elapsed: 2.007826122s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007779285s
    Jan 12 16:16:51.698: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2" satisfied condition "running"
    Jan 12 16:16:51.698: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 12 16:16:51.702: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 12 16:16:51.708: INFO: Updating deployment webserver-deployment
    Jan 12 16:16:51.709: INFO: Waiting for observed generation 2
    Jan 12 16:16:53.713: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 12 16:16:53.714: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 12 16:16:53.716: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 16:16:53.720: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 12 16:16:53.721: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 12 16:16:53.722: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 16:16:53.725: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 12 16:16:53.725: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 12 16:16:53.730: INFO: Updating deployment webserver-deployment
    Jan 12 16:16:53.730: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 12 16:16:53.733: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 12 16:16:53.736: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 16:16:55.744: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8768  d2b05400-7c41-4564-8879-08e736fdf772 18196 3 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006197e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-12 16:16:53 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-12 16:16:53 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 12 16:16:55.746: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8768  efb56efe-d5a8-413a-842d-e7abe43ff0ef 18191 3 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d2b05400-7c41-4564-8879-08e736fdf772 0xc005e7a307 0xc005e7a308}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2b05400-7c41-4564-8879-08e736fdf772\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a3a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:16:55.746: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 12 16:16:55.746: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8768  098e172b-976f-47c8-bcc0-1f8023709917 18192 3 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d2b05400-7c41-4564-8879-08e736fdf772 0xc005e7a217 0xc005e7a218}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2b05400-7c41-4564-8879-08e736fdf772\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:16:55.750: INFO: Pod "webserver-deployment-7f5969cbc7-57zbh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-57zbh webserver-deployment-7f5969cbc7- deployment-8768  0200b38f-c767-4c5b-9915-7a9e8a4a07e1 18017 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7a8b7 0xc005e7a8b8}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwrpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwrpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.158,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b66e02bdfbeb0abf0c9c990c2d11f73a1a826ed3dd538f3f128f4b0a2dff98cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.750: INFO: Pod "webserver-deployment-7f5969cbc7-6dwm2" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6dwm2 webserver-deployment-7f5969cbc7- deployment-8768  712e07a0-bc13-4f0b-abf7-f70dc9accae6 18007 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7aaa7 0xc005e7aaa8}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmqc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmqc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.8,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e8775bc0b36566857d1d1939410e9276c538c94044123a68793853dbbe5a146,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-8hj9p" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8hj9p webserver-deployment-7f5969cbc7- deployment-8768  f906e037-778b-40f9-a1df-1461c5cd315e 17992 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ac90 0xc005e7ac91}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wrx7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wrx7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.156,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://71e1a5ba8d6919d9ec764e7db16acac5359d3567c830f2a6e09f196611602ecc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.156,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-9rqpn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9rqpn webserver-deployment-7f5969cbc7- deployment-8768  a457e846-5114-4430-9a4e-7a5cfd186c40 18155 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ae77 0xc005e7ae78}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fckh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fckh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.751: INFO: Pod "webserver-deployment-7f5969cbc7-9v72c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9v72c webserver-deployment-7f5969cbc7- deployment-8768  c15091fc-5fd3-4ccf-8b9c-07a538ddd80f 18128 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7afe0 0xc005e7afe1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csz7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csz7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-bdvv7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bdvv7 webserver-deployment-7f5969cbc7- deployment-8768  57c32e62-99e7-412a-bd82-891ce46232f0 18182 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b160 0xc005e7b161}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnjrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnjrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-c5m6l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c5m6l webserver-deployment-7f5969cbc7- deployment-8768  c175aed1-19f5-452b-928f-bc874934280c 18120 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b2c0 0xc005e7b2c1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmffb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmffb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-c9npx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c9npx webserver-deployment-7f5969cbc7- deployment-8768  7640b3d3-60b7-4392-957c-14aab5cc3a44 17978 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b420 0xc005e7b421}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wvm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wvm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.7,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1195278a9d18cc03fc5261bf474fa4cfd370a8fc6ea7f2aa82648ecc7b2b285b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.752: INFO: Pod "webserver-deployment-7f5969cbc7-d8h6r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d8h6r webserver-deployment-7f5969cbc7- deployment-8768  29194d6a-736a-4e32-aff6-3f75a7c5beb7 17999 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b610 0xc005e7b611}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s6cw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s6cw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.5,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://190f730e3e064f2e6f21a7b8503aee32bbc229cebda41d4bbd2131b2c143b0f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-m9c7k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-m9c7k webserver-deployment-7f5969cbc7- deployment-8768  11e699cd-7ea6-4db4-9786-fefb200103d0 18010 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7b830 0xc005e7b831}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mc9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mc9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.157,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c3739e96f3cf50308bea29bdf5a5179c8ca91757330b6d2e66697dce7d3f6322,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-mbnzb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbnzb webserver-deployment-7f5969cbc7- deployment-8768  238b32dd-50e3-48f8-9eff-53c191ab2b6d 18156 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7ba27 0xc005e7ba28}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpccz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpccz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.753: INFO: Pod "webserver-deployment-7f5969cbc7-nhzb6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nhzb6 webserver-deployment-7f5969cbc7- deployment-8768  979eebd9-e696-4076-86d1-ac2523a56082 18125 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7bba0 0xc005e7bba1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txml9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txml9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-q7zmf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q7zmf webserver-deployment-7f5969cbc7- deployment-8768  4fac2767-3a56-4fd6-880b-cb4185256c33 18174 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7bd00 0xc005e7bd01}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t9qb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t9qb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-qdmls" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qdmls webserver-deployment-7f5969cbc7- deployment-8768  ae4d09ca-158b-41b9-b433-cdc9b439f723 18199 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc005e7be60 0xc005e7be61}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hn7dj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hn7dj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.754: INFO: Pod "webserver-deployment-7f5969cbc7-qnch9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qnch9 webserver-deployment-7f5969cbc7- deployment-8768  2c1824ff-2961-4621-a060-2318979d25d3 18152 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc00007e177 0xc00007e178}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhwkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhwkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-rzhdg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rzhdg webserver-deployment-7f5969cbc7- deployment-8768  a0741032-9820-4121-992e-b48a12ea979d 18175 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc00007f710 0xc00007f711}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rkq8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rkq8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-sglpv" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sglpv webserver-deployment-7f5969cbc7- deployment-8768  f8f6e51b-25d8-4dfc-9523-d02ef5e73ffa 17994 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a402a0 0xc006a402a1}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j62bn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j62bn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.154,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://203d391560616ab59816be9f123c4affb8a661b9d885d4b6e1476d00195768a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.755: INFO: Pod "webserver-deployment-7f5969cbc7-wxkv7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wxkv7 webserver-deployment-7f5969cbc7- deployment-8768  4e1f300c-0c9d-4c27-8d41-c7c6553e331f 18002 0 2023-01-12 16:16:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a40997 0xc006a40998}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4847l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4847l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.155,StartTime:2023-01-12 16:16:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:16:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://635769bce0acdf5a78c8e4a91469639b669d6cc2f16b2ff6eb9897e046912cf7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-7f5969cbc7-zght7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zght7 webserver-deployment-7f5969cbc7- deployment-8768  727a03ff-0cc5-40ac-ae9b-09c7745bfeeb 18180 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a41027 0xc006a41028}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flwvm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flwvm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-7f5969cbc7-zz7s5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zz7s5 webserver-deployment-7f5969cbc7- deployment-8768  5b7d0a0f-4b2b-4453-ab37-281682904b2e 18177 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 098e172b-976f-47c8-bcc0-1f8023709917 0xc006a41710 0xc006a41711}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098e172b-976f-47c8-bcc0-1f8023709917\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wt8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wt8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-d9f79cb5-55snr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-55snr webserver-deployment-d9f79cb5- deployment-8768  41028cdc-b8f7-4c8f-928e-2c38339b25b3 18183 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc006a4185f 0xc006a41890}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh79t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh79t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.756: INFO: Pod "webserver-deployment-d9f79cb5-b6q67" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-b6q67 webserver-deployment-d9f79cb5- deployment-8768  cd15fe4c-650b-43d5-b267-82ee8414b648 18105 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc006a41bff 0xc006a41cb0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n64hl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n64hl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-cwvkz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cwvkz webserver-deployment-d9f79cb5- deployment-8768  166c0387-aa1b-456e-ab82-30aef5dc9615 18071 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a0af 0xc000a2a0c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnktf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnktf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-fc5sg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fc5sg webserver-deployment-d9f79cb5- deployment-8768  f0521e4e-d5de-4e3b-8071-74dbfb0d21fc 18154 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a2af 0xc000a2a2c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4nhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4nhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.757: INFO: Pod "webserver-deployment-d9f79cb5-fzchr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fzchr webserver-deployment-d9f79cb5- deployment-8768  e4f6a95c-7d25-4305-a5d3-66cc947cfaa7 18142 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2a41f 0xc000a2ace0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgt7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgt7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.760: INFO: Pod "webserver-deployment-d9f79cb5-kdsq9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kdsq9 webserver-deployment-d9f79cb5- deployment-8768  e059cbf4-b77c-4958-832b-3ba3fa16806f 18158 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc000a2b96f 0xc000a2bb00}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zj85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zj85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-kfj6d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kfj6d webserver-deployment-d9f79cb5- deployment-8768  0bde6148-2bf1-4bcf-afef-eaaff3fac3f8 18103 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e15f 0xc003c1e180}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwpms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwpms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.159,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-n677j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-n677j webserver-deployment-d9f79cb5- deployment-8768  f9362e17-d869-489e-b8f9-33a03cfbdcd9 18176 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e37f 0xc003c1e390}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xhqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xhqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:,StartTime:2023-01-12 16:16:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-pnzpk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pnzpk webserver-deployment-d9f79cb5- deployment-8768  89a04daa-ea7d-404c-a216-1b7b1ca95b7e 18159 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1e59f 0xc003c1e5b0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnw2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnw2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.761: INFO: Pod "webserver-deployment-d9f79cb5-ptt49" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ptt49 webserver-deployment-d9f79cb5- deployment-8768  3ce9bc4d-66d5-4319-8256-7e410aa92541 18153 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ea3f 0xc003c1ea80}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9mbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9mbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-rrdtk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rrdtk webserver-deployment-d9f79cb5- deployment-8768  081bf617-6920-4e95-9fa9-dc8cfcb8fee1 18157 0 2023-01-12 16:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ebef 0xc003c1ec00}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d646,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d646,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-sp8d7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-sp8d7 webserver-deployment-d9f79cb5- deployment-8768  a68b38f6-252a-429d-a90d-396df9271830 18073 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1ed6f 0xc003c1ed80}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzpl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzpl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:16:55.762: INFO: Pod "webserver-deployment-d9f79cb5-w9vdq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w9vdq webserver-deployment-d9f79cb5- deployment-8768  bced884e-266b-45dc-a74f-2fe0534da390 18197 0 2023-01-12 16:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 efb56efe-d5a8-413a-842d-e7abe43ff0ef 0xc003c1f2af 0xc003c1f2c0}] [] [{kube-controller-manager Update v1 2023-01-12 16:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"efb56efe-d5a8-413a-842d-e7abe43ff0ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2c92q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2c92q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:,StartTime:2023-01-12 16:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:16:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8768" for this suite. 01/12/23 16:16:55.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:16:55.771
Jan 12 16:16:55.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:16:55.771
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:55.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:55.785
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:16:55.795
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:56.501
STEP: Deploying the webhook pod 01/12/23 16:16:56.505
STEP: Wait for the deployment to be ready 01/12/23 16:16:56.515
Jan 12 16:16:56.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 16:16:58.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:17:00.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 16:17:02.532
STEP: Verifying the service has paired with the endpoint 01/12/23 16:17:02.546
Jan 12 16:17:03.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 16:17:03.549
STEP: create a pod that should be denied by the webhook 01/12/23 16:17:03.565
STEP: create a pod that causes the webhook to hang 01/12/23 16:17:03.575
STEP: create a configmap that should be denied by the webhook 01/12/23 16:17:13.581
STEP: create a configmap that should be admitted by the webhook 01/12/23 16:17:13.593
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 16:17:13.603
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 16:17:13.608
STEP: create a namespace that bypass the webhook 01/12/23 16:17:13.611
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/12/23 16:17:13.616
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2557" for this suite. 01/12/23 16:17:13.669
STEP: Destroying namespace "webhook-2557-markers" for this suite. 01/12/23 16:17:13.678
------------------------------
 [SLOW TEST] [17.911 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:16:55.771
    Jan 12 16:16:55.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:16:55.771
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:16:55.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:16:55.785
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:16:55.795
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:16:56.501
    STEP: Deploying the webhook pod 01/12/23 16:16:56.505
    STEP: Wait for the deployment to be ready 01/12/23 16:16:56.515
    Jan 12 16:16:56.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 16:16:58.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:17:00.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 16, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 16:17:02.532
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:17:02.546
    Jan 12 16:17:03.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/12/23 16:17:03.549
    STEP: create a pod that should be denied by the webhook 01/12/23 16:17:03.565
    STEP: create a pod that causes the webhook to hang 01/12/23 16:17:03.575
    STEP: create a configmap that should be denied by the webhook 01/12/23 16:17:13.581
    STEP: create a configmap that should be admitted by the webhook 01/12/23 16:17:13.593
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 16:17:13.603
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/12/23 16:17:13.608
    STEP: create a namespace that bypass the webhook 01/12/23 16:17:13.611
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/12/23 16:17:13.616
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2557" for this suite. 01/12/23 16:17:13.669
    STEP: Destroying namespace "webhook-2557-markers" for this suite. 01/12/23 16:17:13.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:13.683
Jan 12 16:17:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:17:13.684
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:13.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:13.696
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-1162/secret-test-a3e7102c-a1d8-40bb-ad9d-bb1dc8cc82f8 01/12/23 16:17:13.699
STEP: Creating a pod to test consume secrets 01/12/23 16:17:13.702
Jan 12 16:17:13.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69" in namespace "secrets-1162" to be "Succeeded or Failed"
Jan 12 16:17:13.713: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815347ms
Jan 12 16:17:15.716: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006039804s
Jan 12 16:17:17.715: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005456908s
STEP: Saw pod success 01/12/23 16:17:17.715
Jan 12 16:17:17.715: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69" satisfied condition "Succeeded or Failed"
Jan 12 16:17:17.717: INFO: Trying to get logs from node worker-1 pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 container env-test: <nil>
STEP: delete the pod 01/12/23 16:17:17.722
Jan 12 16:17:17.730: INFO: Waiting for pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 to disappear
Jan 12 16:17:17.732: INFO: Pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:17.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1162" for this suite. 01/12/23 16:17:17.735
------------------------------
 [4.057 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:13.683
    Jan 12 16:17:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:17:13.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:13.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:13.696
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-1162/secret-test-a3e7102c-a1d8-40bb-ad9d-bb1dc8cc82f8 01/12/23 16:17:13.699
    STEP: Creating a pod to test consume secrets 01/12/23 16:17:13.702
    Jan 12 16:17:13.710: INFO: Waiting up to 5m0s for pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69" in namespace "secrets-1162" to be "Succeeded or Failed"
    Jan 12 16:17:13.713: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815347ms
    Jan 12 16:17:15.716: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006039804s
    Jan 12 16:17:17.715: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005456908s
    STEP: Saw pod success 01/12/23 16:17:17.715
    Jan 12 16:17:17.715: INFO: Pod "pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69" satisfied condition "Succeeded or Failed"
    Jan 12 16:17:17.717: INFO: Trying to get logs from node worker-1 pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 container env-test: <nil>
    STEP: delete the pod 01/12/23 16:17:17.722
    Jan 12 16:17:17.730: INFO: Waiting for pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 to disappear
    Jan 12 16:17:17.732: INFO: Pod pod-configmaps-e1099848-eb49-4eeb-8281-d4073d3fff69 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:17.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1162" for this suite. 01/12/23 16:17:17.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:17.74
Jan 12 16:17:17.741: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 16:17:17.741
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:17.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:17.751
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 in namespace container-probe-4226 01/12/23 16:17:17.753
Jan 12 16:17:17.761: INFO: Waiting up to 5m0s for pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55" in namespace "container-probe-4226" to be "not pending"
Jan 12 16:17:17.763: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047425ms
Jan 12 16:17:19.766: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55": Phase="Running", Reason="", readiness=true. Elapsed: 2.005011922s
Jan 12 16:17:19.766: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55" satisfied condition "not pending"
Jan 12 16:17:19.766: INFO: Started pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 in namespace container-probe-4226
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:17:19.766
Jan 12 16:17:19.768: INFO: Initial restart count of pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 is 0
Jan 12 16:17:39.801: INFO: Restart count of pod container-probe-4226/liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 is now 1 (20.032464911s elapsed)
STEP: deleting the pod 01/12/23 16:17:39.801
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:39.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4226" for this suite. 01/12/23 16:17:39.808
------------------------------
 [SLOW TEST] [22.073 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:17.74
    Jan 12 16:17:17.741: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 16:17:17.741
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:17.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:17.751
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 in namespace container-probe-4226 01/12/23 16:17:17.753
    Jan 12 16:17:17.761: INFO: Waiting up to 5m0s for pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55" in namespace "container-probe-4226" to be "not pending"
    Jan 12 16:17:17.763: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047425ms
    Jan 12 16:17:19.766: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55": Phase="Running", Reason="", readiness=true. Elapsed: 2.005011922s
    Jan 12 16:17:19.766: INFO: Pod "liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55" satisfied condition "not pending"
    Jan 12 16:17:19.766: INFO: Started pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 in namespace container-probe-4226
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:17:19.766
    Jan 12 16:17:19.768: INFO: Initial restart count of pod liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 is 0
    Jan 12 16:17:39.801: INFO: Restart count of pod container-probe-4226/liveness-7c504c8c-c92a-4cf8-8bf9-ef6abee1dc55 is now 1 (20.032464911s elapsed)
    STEP: deleting the pod 01/12/23 16:17:39.801
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:39.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4226" for this suite. 01/12/23 16:17:39.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:39.815
Jan 12 16:17:39.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:17:39.816
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:39.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:39.826
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/12/23 16:17:39.828
Jan 12 16:17:39.835: INFO: Waiting up to 5m0s for pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316" in namespace "emptydir-647" to be "Succeeded or Failed"
Jan 12 16:17:39.837: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652068ms
Jan 12 16:17:41.841: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005592775s
Jan 12 16:17:43.840: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004657007s
STEP: Saw pod success 01/12/23 16:17:43.84
Jan 12 16:17:43.840: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316" satisfied condition "Succeeded or Failed"
Jan 12 16:17:43.842: INFO: Trying to get logs from node worker-1 pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 container test-container: <nil>
STEP: delete the pod 01/12/23 16:17:43.846
Jan 12 16:17:43.855: INFO: Waiting for pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 to disappear
Jan 12 16:17:43.856: INFO: Pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:43.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-647" for this suite. 01/12/23 16:17:43.859
------------------------------
 [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:39.815
    Jan 12 16:17:39.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:17:39.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:39.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:39.826
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/12/23 16:17:39.828
    Jan 12 16:17:39.835: INFO: Waiting up to 5m0s for pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316" in namespace "emptydir-647" to be "Succeeded or Failed"
    Jan 12 16:17:39.837: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652068ms
    Jan 12 16:17:41.841: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005592775s
    Jan 12 16:17:43.840: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004657007s
    STEP: Saw pod success 01/12/23 16:17:43.84
    Jan 12 16:17:43.840: INFO: Pod "pod-c4d650ba-80bc-4d74-9b10-f39184c3d316" satisfied condition "Succeeded or Failed"
    Jan 12 16:17:43.842: INFO: Trying to get logs from node worker-1 pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:17:43.846
    Jan 12 16:17:43.855: INFO: Waiting for pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 to disappear
    Jan 12 16:17:43.856: INFO: Pod pod-c4d650ba-80bc-4d74-9b10-f39184c3d316 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:43.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-647" for this suite. 01/12/23 16:17:43.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:43.866
Jan 12 16:17:43.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename podtemplate 01/12/23 16:17:43.867
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.878
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/12/23 16:17:43.88
Jan 12 16:17:43.884: INFO: created test-podtemplate-1
Jan 12 16:17:43.887: INFO: created test-podtemplate-2
Jan 12 16:17:43.892: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/12/23 16:17:43.892
STEP: delete collection of pod templates 01/12/23 16:17:43.894
Jan 12 16:17:43.894: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/12/23 16:17:43.902
Jan 12 16:17:43.902: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:43.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8735" for this suite. 01/12/23 16:17:43.906
------------------------------
 [0.044 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:43.866
    Jan 12 16:17:43.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename podtemplate 01/12/23 16:17:43.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.878
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/12/23 16:17:43.88
    Jan 12 16:17:43.884: INFO: created test-podtemplate-1
    Jan 12 16:17:43.887: INFO: created test-podtemplate-2
    Jan 12 16:17:43.892: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/12/23 16:17:43.892
    STEP: delete collection of pod templates 01/12/23 16:17:43.894
    Jan 12 16:17:43.894: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/12/23 16:17:43.902
    Jan 12 16:17:43.902: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:43.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8735" for this suite. 01/12/23 16:17:43.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:43.911
Jan 12 16:17:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:17:43.912
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.92
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/12/23 16:17:43.922
Jan 12 16:17:43.923: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6222 proxy --unix-socket=/tmp/kubectl-proxy-unix1315060222/test'
STEP: retrieving proxy /api/ output 01/12/23 16:17:43.975
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:43.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6222" for this suite. 01/12/23 16:17:43.983
------------------------------
 [0.075 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:43.911
    Jan 12 16:17:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:17:43.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.92
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/12/23 16:17:43.922
    Jan 12 16:17:43.923: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6222 proxy --unix-socket=/tmp/kubectl-proxy-unix1315060222/test'
    STEP: retrieving proxy /api/ output 01/12/23 16:17:43.975
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:43.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6222" for this suite. 01/12/23 16:17:43.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:43.987
Jan 12 16:17:43.987: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 16:17:43.988
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.997
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/12/23 16:17:43.999
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/12/23 16:17:44.002
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/12/23 16:17:44.003
STEP: creating a pod to probe DNS 01/12/23 16:17:44.003
STEP: submitting the pod to kubernetes 01/12/23 16:17:44.003
Jan 12 16:17:44.011: INFO: Waiting up to 15m0s for pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b" in namespace "dns-9974" to be "running"
Jan 12 16:17:44.013: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668549ms
Jan 12 16:17:46.016: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004923645s
Jan 12 16:17:46.016: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:17:46.016
STEP: looking for the results for each expected name from probers 01/12/23 16:17:46.018
Jan 12 16:17:46.030: INFO: DNS probes using dns-9974/dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b succeeded

STEP: deleting the pod 01/12/23 16:17:46.03
STEP: deleting the test headless service 01/12/23 16:17:46.039
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:46.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9974" for this suite. 01/12/23 16:17:46.059
------------------------------
 [2.078 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:43.987
    Jan 12 16:17:43.987: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 16:17:43.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:43.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:43.997
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/12/23 16:17:43.999
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/12/23 16:17:44.002
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9974.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/12/23 16:17:44.003
    STEP: creating a pod to probe DNS 01/12/23 16:17:44.003
    STEP: submitting the pod to kubernetes 01/12/23 16:17:44.003
    Jan 12 16:17:44.011: INFO: Waiting up to 15m0s for pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b" in namespace "dns-9974" to be "running"
    Jan 12 16:17:44.013: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668549ms
    Jan 12 16:17:46.016: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b": Phase="Running", Reason="", readiness=true. Elapsed: 2.004923645s
    Jan 12 16:17:46.016: INFO: Pod "dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:17:46.016
    STEP: looking for the results for each expected name from probers 01/12/23 16:17:46.018
    Jan 12 16:17:46.030: INFO: DNS probes using dns-9974/dns-test-db154d71-b7fc-467e-bcbb-f22d3f30513b succeeded

    STEP: deleting the pod 01/12/23 16:17:46.03
    STEP: deleting the test headless service 01/12/23 16:17:46.039
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:46.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9974" for this suite. 01/12/23 16:17:46.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:46.067
Jan 12 16:17:46.067: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:17:46.068
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:46.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:46.078
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/12/23 16:17:46.08
Jan 12 16:17:46.086: INFO: Waiting up to 5m0s for pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f" in namespace "downward-api-2435" to be "Succeeded or Failed"
Jan 12 16:17:46.088: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61242ms
Jan 12 16:17:48.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004880163s
Jan 12 16:17:50.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004746345s
STEP: Saw pod success 01/12/23 16:17:50.091
Jan 12 16:17:50.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f" satisfied condition "Succeeded or Failed"
Jan 12 16:17:50.094: INFO: Trying to get logs from node worker-1 pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f container dapi-container: <nil>
STEP: delete the pod 01/12/23 16:17:50.1
Jan 12 16:17:50.109: INFO: Waiting for pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f to disappear
Jan 12 16:17:50.110: INFO: Pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:50.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2435" for this suite. 01/12/23 16:17:50.113
------------------------------
 [4.049 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:46.067
    Jan 12 16:17:46.067: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:17:46.068
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:46.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:46.078
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/12/23 16:17:46.08
    Jan 12 16:17:46.086: INFO: Waiting up to 5m0s for pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f" in namespace "downward-api-2435" to be "Succeeded or Failed"
    Jan 12 16:17:46.088: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61242ms
    Jan 12 16:17:48.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004880163s
    Jan 12 16:17:50.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004746345s
    STEP: Saw pod success 01/12/23 16:17:50.091
    Jan 12 16:17:50.091: INFO: Pod "downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f" satisfied condition "Succeeded or Failed"
    Jan 12 16:17:50.094: INFO: Trying to get logs from node worker-1 pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f container dapi-container: <nil>
    STEP: delete the pod 01/12/23 16:17:50.1
    Jan 12 16:17:50.109: INFO: Waiting for pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f to disappear
    Jan 12 16:17:50.110: INFO: Pod downward-api-fa191ec5-5137-448d-91b2-cebb0c5dfa6f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:50.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2435" for this suite. 01/12/23 16:17:50.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:50.117
Jan 12 16:17:50.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:17:50.118
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:50.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:50.13
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/12/23 16:17:50.132
Jan 12 16:17:50.139: INFO: Waiting up to 5m0s for pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d" in namespace "downward-api-8416" to be "Succeeded or Failed"
Jan 12 16:17:50.141: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893539ms
Jan 12 16:17:52.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00499488s
Jan 12 16:17:54.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005122019s
STEP: Saw pod success 01/12/23 16:17:54.144
Jan 12 16:17:54.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d" satisfied condition "Succeeded or Failed"
Jan 12 16:17:54.146: INFO: Trying to get logs from node worker-1 pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d container dapi-container: <nil>
STEP: delete the pod 01/12/23 16:17:54.151
Jan 12 16:17:54.159: INFO: Waiting for pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d to disappear
Jan 12 16:17:54.160: INFO: Pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8416" for this suite. 01/12/23 16:17:54.162
------------------------------
 [4.050 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:50.117
    Jan 12 16:17:50.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:17:50.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:50.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:50.13
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/12/23 16:17:50.132
    Jan 12 16:17:50.139: INFO: Waiting up to 5m0s for pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d" in namespace "downward-api-8416" to be "Succeeded or Failed"
    Jan 12 16:17:50.141: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.893539ms
    Jan 12 16:17:52.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00499488s
    Jan 12 16:17:54.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005122019s
    STEP: Saw pod success 01/12/23 16:17:54.144
    Jan 12 16:17:54.144: INFO: Pod "downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d" satisfied condition "Succeeded or Failed"
    Jan 12 16:17:54.146: INFO: Trying to get logs from node worker-1 pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d container dapi-container: <nil>
    STEP: delete the pod 01/12/23 16:17:54.151
    Jan 12 16:17:54.159: INFO: Waiting for pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d to disappear
    Jan 12 16:17:54.160: INFO: Pod downward-api-1bda89d9-9329-4f1f-9f50-5acd362ab23d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8416" for this suite. 01/12/23 16:17:54.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:54.168
Jan 12 16:17:54.168: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:17:54.169
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:54.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:54.178
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 16:17:54.18
Jan 12 16:17:54.186: INFO: Waiting up to 5m0s for pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531" in namespace "emptydir-5485" to be "Succeeded or Failed"
Jan 12 16:17:54.187: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731382ms
Jan 12 16:17:56.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004650307s
Jan 12 16:17:58.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00452128s
STEP: Saw pod success 01/12/23 16:17:58.19
Jan 12 16:17:58.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531" satisfied condition "Succeeded or Failed"
Jan 12 16:17:58.192: INFO: Trying to get logs from node worker-1 pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 container test-container: <nil>
STEP: delete the pod 01/12/23 16:17:58.197
Jan 12 16:17:58.204: INFO: Waiting for pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 to disappear
Jan 12 16:17:58.206: INFO: Pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:17:58.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5485" for this suite. 01/12/23 16:17:58.208
------------------------------
 [4.043 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:54.168
    Jan 12 16:17:54.168: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:17:54.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:54.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:54.178
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/12/23 16:17:54.18
    Jan 12 16:17:54.186: INFO: Waiting up to 5m0s for pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531" in namespace "emptydir-5485" to be "Succeeded or Failed"
    Jan 12 16:17:54.187: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Pending", Reason="", readiness=false. Elapsed: 1.731382ms
    Jan 12 16:17:56.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004650307s
    Jan 12 16:17:58.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00452128s
    STEP: Saw pod success 01/12/23 16:17:58.19
    Jan 12 16:17:58.190: INFO: Pod "pod-1d7b0a1c-3180-4634-83e2-c971495f6531" satisfied condition "Succeeded or Failed"
    Jan 12 16:17:58.192: INFO: Trying to get logs from node worker-1 pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:17:58.197
    Jan 12 16:17:58.204: INFO: Waiting for pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 to disappear
    Jan 12 16:17:58.206: INFO: Pod pod-1d7b0a1c-3180-4634-83e2-c971495f6531 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:17:58.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5485" for this suite. 01/12/23 16:17:58.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:17:58.212
Jan 12 16:17:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:17:58.213
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:58.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:58.226
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/12/23 16:18:15.23
STEP: Creating a ResourceQuota 01/12/23 16:18:20.233
STEP: Ensuring resource quota status is calculated 01/12/23 16:18:20.238
STEP: Creating a ConfigMap 01/12/23 16:18:22.241
STEP: Ensuring resource quota status captures configMap creation 01/12/23 16:18:22.25
STEP: Deleting a ConfigMap 01/12/23 16:18:24.253
STEP: Ensuring resource quota status released usage 01/12/23 16:18:24.256
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:18:26.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2307" for this suite. 01/12/23 16:18:26.261
------------------------------
 [SLOW TEST] [28.053 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:17:58.212
    Jan 12 16:17:58.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:17:58.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:17:58.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:17:58.226
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/12/23 16:18:15.23
    STEP: Creating a ResourceQuota 01/12/23 16:18:20.233
    STEP: Ensuring resource quota status is calculated 01/12/23 16:18:20.238
    STEP: Creating a ConfigMap 01/12/23 16:18:22.241
    STEP: Ensuring resource quota status captures configMap creation 01/12/23 16:18:22.25
    STEP: Deleting a ConfigMap 01/12/23 16:18:24.253
    STEP: Ensuring resource quota status released usage 01/12/23 16:18:24.256
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:18:26.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2307" for this suite. 01/12/23 16:18:26.261
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:18:26.265
Jan 12 16:18:26.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svc-latency 01/12/23 16:18:26.266
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:18:26.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:18:26.277
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 12 16:18:26.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3246 01/12/23 16:18:26.28
I0112 16:18:26.284855      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3246, replica count: 1
I0112 16:18:27.335411      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 16:18:27.447: INFO: Created: latency-svc-nxgwl
Jan 12 16:18:27.451: INFO: Got endpoints: latency-svc-nxgwl [15.595226ms]
Jan 12 16:18:27.462: INFO: Created: latency-svc-f6cjr
Jan 12 16:18:27.470: INFO: Got endpoints: latency-svc-f6cjr [18.566074ms]
Jan 12 16:18:27.471: INFO: Created: latency-svc-r8p5l
Jan 12 16:18:27.474: INFO: Got endpoints: latency-svc-r8p5l [21.686332ms]
Jan 12 16:18:27.482: INFO: Created: latency-svc-xdkxt
Jan 12 16:18:27.485: INFO: Got endpoints: latency-svc-xdkxt [33.187376ms]
Jan 12 16:18:27.495: INFO: Created: latency-svc-knpld
Jan 12 16:18:27.502: INFO: Got endpoints: latency-svc-knpld [50.214011ms]
Jan 12 16:18:27.504: INFO: Created: latency-svc-6k2rz
Jan 12 16:18:27.512: INFO: Got endpoints: latency-svc-6k2rz [60.054419ms]
Jan 12 16:18:27.514: INFO: Created: latency-svc-sljxh
Jan 12 16:18:27.521: INFO: Got endpoints: latency-svc-sljxh [68.472066ms]
Jan 12 16:18:27.522: INFO: Created: latency-svc-8kq7v
Jan 12 16:18:27.540: INFO: Got endpoints: latency-svc-8kq7v [87.173654ms]
Jan 12 16:18:27.541: INFO: Created: latency-svc-52qhf
Jan 12 16:18:27.550: INFO: Got endpoints: latency-svc-52qhf [96.759333ms]
Jan 12 16:18:27.552: INFO: Created: latency-svc-srhdf
Jan 12 16:18:27.563: INFO: Got endpoints: latency-svc-srhdf [110.656339ms]
Jan 12 16:18:27.565: INFO: Created: latency-svc-sqt5f
Jan 12 16:18:27.570: INFO: Got endpoints: latency-svc-sqt5f [117.028694ms]
Jan 12 16:18:27.572: INFO: Created: latency-svc-llx8d
Jan 12 16:18:27.584: INFO: Got endpoints: latency-svc-llx8d [130.787083ms]
Jan 12 16:18:27.586: INFO: Created: latency-svc-tz4g2
Jan 12 16:18:27.589: INFO: Got endpoints: latency-svc-tz4g2 [136.126747ms]
Jan 12 16:18:27.597: INFO: Created: latency-svc-98ft5
Jan 12 16:18:27.600: INFO: Got endpoints: latency-svc-98ft5 [146.467481ms]
Jan 12 16:18:27.606: INFO: Created: latency-svc-2z45q
Jan 12 16:18:27.609: INFO: Got endpoints: latency-svc-2z45q [155.623364ms]
Jan 12 16:18:27.615: INFO: Created: latency-svc-glggg
Jan 12 16:18:27.628: INFO: Got endpoints: latency-svc-glggg [174.631538ms]
Jan 12 16:18:27.629: INFO: Created: latency-svc-4blmn
Jan 12 16:18:27.635: INFO: Got endpoints: latency-svc-4blmn [164.766888ms]
Jan 12 16:18:27.639: INFO: Created: latency-svc-4vqxb
Jan 12 16:18:27.653: INFO: Got endpoints: latency-svc-4vqxb [179.406488ms]
Jan 12 16:18:27.656: INFO: Created: latency-svc-s4v7g
Jan 12 16:18:27.663: INFO: Got endpoints: latency-svc-s4v7g [178.135975ms]
Jan 12 16:18:27.666: INFO: Created: latency-svc-zsmtp
Jan 12 16:18:27.676: INFO: Got endpoints: latency-svc-zsmtp [173.86753ms]
Jan 12 16:18:27.677: INFO: Created: latency-svc-dkfzl
Jan 12 16:18:27.685: INFO: Got endpoints: latency-svc-dkfzl [172.371107ms]
Jan 12 16:18:27.690: INFO: Created: latency-svc-chjgm
Jan 12 16:18:27.694: INFO: Got endpoints: latency-svc-chjgm [172.684422ms]
Jan 12 16:18:27.697: INFO: Created: latency-svc-tbtnk
Jan 12 16:18:27.704: INFO: Got endpoints: latency-svc-tbtnk [164.510233ms]
Jan 12 16:18:27.705: INFO: Created: latency-svc-v5scv
Jan 12 16:18:27.716: INFO: Got endpoints: latency-svc-v5scv [166.023101ms]
Jan 12 16:18:27.719: INFO: Created: latency-svc-p4pjc
Jan 12 16:18:27.721: INFO: Got endpoints: latency-svc-p4pjc [158.338328ms]
Jan 12 16:18:27.728: INFO: Created: latency-svc-w55rz
Jan 12 16:18:27.733: INFO: Got endpoints: latency-svc-w55rz [162.730942ms]
Jan 12 16:18:27.741: INFO: Created: latency-svc-jbvfv
Jan 12 16:18:27.743: INFO: Got endpoints: latency-svc-jbvfv [159.108805ms]
Jan 12 16:18:27.750: INFO: Created: latency-svc-24x94
Jan 12 16:18:27.766: INFO: Got endpoints: latency-svc-24x94 [176.390994ms]
Jan 12 16:18:27.771: INFO: Created: latency-svc-5spp8
Jan 12 16:18:27.774: INFO: Got endpoints: latency-svc-5spp8 [174.58696ms]
Jan 12 16:18:27.777: INFO: Created: latency-svc-c4xh9
Jan 12 16:18:27.784: INFO: Got endpoints: latency-svc-c4xh9 [175.519511ms]
Jan 12 16:18:27.785: INFO: Created: latency-svc-7sc4r
Jan 12 16:18:27.791: INFO: Got endpoints: latency-svc-7sc4r [163.447618ms]
Jan 12 16:18:27.794: INFO: Created: latency-svc-8fbwj
Jan 12 16:18:27.796: INFO: Got endpoints: latency-svc-8fbwj [160.73499ms]
Jan 12 16:18:27.820: INFO: Created: latency-svc-28q2t
Jan 12 16:18:27.826: INFO: Got endpoints: latency-svc-28q2t [172.517996ms]
Jan 12 16:18:27.830: INFO: Created: latency-svc-8dt7x
Jan 12 16:18:27.841: INFO: Got endpoints: latency-svc-8dt7x [177.204522ms]
Jan 12 16:18:27.843: INFO: Created: latency-svc-fps77
Jan 12 16:18:27.856: INFO: Got endpoints: latency-svc-fps77 [180.185849ms]
Jan 12 16:18:27.860: INFO: Created: latency-svc-m85gx
Jan 12 16:18:27.878: INFO: Got endpoints: latency-svc-m85gx [193.264397ms]
Jan 12 16:18:27.884: INFO: Created: latency-svc-fc8mt
Jan 12 16:18:27.890: INFO: Got endpoints: latency-svc-fc8mt [195.813961ms]
Jan 12 16:18:27.892: INFO: Created: latency-svc-4qx29
Jan 12 16:18:27.898: INFO: Got endpoints: latency-svc-4qx29 [193.050025ms]
Jan 12 16:18:27.903: INFO: Created: latency-svc-trhd9
Jan 12 16:18:27.911: INFO: Got endpoints: latency-svc-trhd9 [195.626443ms]
Jan 12 16:18:27.913: INFO: Created: latency-svc-kf6p2
Jan 12 16:18:27.923: INFO: Created: latency-svc-vhgrf
Jan 12 16:18:27.931: INFO: Created: latency-svc-9ql9h
Jan 12 16:18:27.943: INFO: Created: latency-svc-xnl4h
Jan 12 16:18:27.949: INFO: Got endpoints: latency-svc-kf6p2 [227.856243ms]
Jan 12 16:18:27.952: INFO: Created: latency-svc-dw82l
Jan 12 16:18:27.962: INFO: Created: latency-svc-qs9hf
Jan 12 16:18:27.970: INFO: Created: latency-svc-5rmnf
Jan 12 16:18:27.976: INFO: Created: latency-svc-xw2ll
Jan 12 16:18:27.995: INFO: Created: latency-svc-65qrv
Jan 12 16:18:28.004: INFO: Got endpoints: latency-svc-vhgrf [271.229746ms]
Jan 12 16:18:28.005: INFO: Created: latency-svc-csrrp
Jan 12 16:18:28.015: INFO: Created: latency-svc-7m94c
Jan 12 16:18:28.024: INFO: Created: latency-svc-xgsbn
Jan 12 16:18:28.029: INFO: Created: latency-svc-xsbzc
Jan 12 16:18:28.041: INFO: Created: latency-svc-zcmcp
Jan 12 16:18:28.054: INFO: Created: latency-svc-tfpfp
Jan 12 16:18:28.055: INFO: Got endpoints: latency-svc-9ql9h [311.975596ms]
Jan 12 16:18:28.060: INFO: Created: latency-svc-s5v7x
Jan 12 16:18:28.070: INFO: Created: latency-svc-vrq8f
Jan 12 16:18:28.078: INFO: Created: latency-svc-vpdcz
Jan 12 16:18:28.105: INFO: Got endpoints: latency-svc-xnl4h [338.918105ms]
Jan 12 16:18:28.115: INFO: Created: latency-svc-g5ggs
Jan 12 16:18:28.153: INFO: Got endpoints: latency-svc-dw82l [378.063759ms]
Jan 12 16:18:28.164: INFO: Created: latency-svc-ctkfl
Jan 12 16:18:28.202: INFO: Got endpoints: latency-svc-qs9hf [417.165169ms]
Jan 12 16:18:28.215: INFO: Created: latency-svc-cwtvb
Jan 12 16:18:28.251: INFO: Got endpoints: latency-svc-5rmnf [459.14226ms]
Jan 12 16:18:28.264: INFO: Created: latency-svc-lf8rd
Jan 12 16:18:28.300: INFO: Got endpoints: latency-svc-xw2ll [504.140538ms]
Jan 12 16:18:28.310: INFO: Created: latency-svc-dvk7k
Jan 12 16:18:28.352: INFO: Got endpoints: latency-svc-65qrv [526.068505ms]
Jan 12 16:18:28.363: INFO: Created: latency-svc-2t2fp
Jan 12 16:18:28.400: INFO: Got endpoints: latency-svc-csrrp [559.594899ms]
Jan 12 16:18:28.411: INFO: Created: latency-svc-zzmwk
Jan 12 16:18:28.450: INFO: Got endpoints: latency-svc-7m94c [594.189718ms]
Jan 12 16:18:28.475: INFO: Created: latency-svc-2shcg
Jan 12 16:18:28.501: INFO: Got endpoints: latency-svc-xgsbn [622.768451ms]
Jan 12 16:18:28.511: INFO: Created: latency-svc-6m2p9
Jan 12 16:18:28.553: INFO: Got endpoints: latency-svc-xsbzc [662.968277ms]
Jan 12 16:18:28.562: INFO: Created: latency-svc-tckzm
Jan 12 16:18:28.603: INFO: Got endpoints: latency-svc-zcmcp [705.012596ms]
Jan 12 16:18:28.614: INFO: Created: latency-svc-g2v96
Jan 12 16:18:28.650: INFO: Got endpoints: latency-svc-tfpfp [738.757792ms]
Jan 12 16:18:28.663: INFO: Created: latency-svc-zdxnk
Jan 12 16:18:28.700: INFO: Got endpoints: latency-svc-s5v7x [750.892749ms]
Jan 12 16:18:28.710: INFO: Created: latency-svc-mf67b
Jan 12 16:18:28.754: INFO: Got endpoints: latency-svc-vrq8f [749.777819ms]
Jan 12 16:18:28.764: INFO: Created: latency-svc-b5958
Jan 12 16:18:28.803: INFO: Got endpoints: latency-svc-vpdcz [747.463824ms]
Jan 12 16:18:28.816: INFO: Created: latency-svc-thn4q
Jan 12 16:18:28.850: INFO: Got endpoints: latency-svc-g5ggs [745.25834ms]
Jan 12 16:18:28.865: INFO: Created: latency-svc-ltgp6
Jan 12 16:18:28.900: INFO: Got endpoints: latency-svc-ctkfl [747.312595ms]
Jan 12 16:18:28.911: INFO: Created: latency-svc-s8mwv
Jan 12 16:18:28.951: INFO: Got endpoints: latency-svc-cwtvb [749.217965ms]
Jan 12 16:18:28.974: INFO: Created: latency-svc-fwn6t
Jan 12 16:18:29.003: INFO: Got endpoints: latency-svc-lf8rd [752.774553ms]
Jan 12 16:18:29.014: INFO: Created: latency-svc-vh42l
Jan 12 16:18:29.051: INFO: Got endpoints: latency-svc-dvk7k [751.204638ms]
Jan 12 16:18:29.066: INFO: Created: latency-svc-n9fpl
Jan 12 16:18:29.100: INFO: Got endpoints: latency-svc-2t2fp [747.785957ms]
Jan 12 16:18:29.111: INFO: Created: latency-svc-hpnlf
Jan 12 16:18:29.152: INFO: Got endpoints: latency-svc-zzmwk [751.115313ms]
Jan 12 16:18:29.162: INFO: Created: latency-svc-dfkhq
Jan 12 16:18:29.201: INFO: Got endpoints: latency-svc-2shcg [750.887858ms]
Jan 12 16:18:29.219: INFO: Created: latency-svc-wbs4r
Jan 12 16:18:29.253: INFO: Got endpoints: latency-svc-6m2p9 [752.034357ms]
Jan 12 16:18:29.268: INFO: Created: latency-svc-xfbpc
Jan 12 16:18:29.300: INFO: Got endpoints: latency-svc-tckzm [747.546855ms]
Jan 12 16:18:29.310: INFO: Created: latency-svc-5gzhg
Jan 12 16:18:29.352: INFO: Got endpoints: latency-svc-g2v96 [749.491362ms]
Jan 12 16:18:29.362: INFO: Created: latency-svc-htfbz
Jan 12 16:18:29.406: INFO: Got endpoints: latency-svc-zdxnk [755.591704ms]
Jan 12 16:18:29.417: INFO: Created: latency-svc-5lfjk
Jan 12 16:18:29.452: INFO: Got endpoints: latency-svc-mf67b [751.255788ms]
Jan 12 16:18:29.463: INFO: Created: latency-svc-pfcx9
Jan 12 16:18:29.501: INFO: Got endpoints: latency-svc-b5958 [747.402253ms]
Jan 12 16:18:29.518: INFO: Created: latency-svc-pt8j6
Jan 12 16:18:29.552: INFO: Got endpoints: latency-svc-thn4q [749.774066ms]
Jan 12 16:18:29.562: INFO: Created: latency-svc-8k8mg
Jan 12 16:18:29.600: INFO: Got endpoints: latency-svc-ltgp6 [750.13267ms]
Jan 12 16:18:29.610: INFO: Created: latency-svc-mcvtq
Jan 12 16:18:29.652: INFO: Got endpoints: latency-svc-s8mwv [752.230363ms]
Jan 12 16:18:29.665: INFO: Created: latency-svc-fztqb
Jan 12 16:18:29.702: INFO: Got endpoints: latency-svc-fwn6t [750.89151ms]
Jan 12 16:18:29.714: INFO: Created: latency-svc-czwk6
Jan 12 16:18:29.751: INFO: Got endpoints: latency-svc-vh42l [747.070816ms]
Jan 12 16:18:29.762: INFO: Created: latency-svc-svwj4
Jan 12 16:18:29.802: INFO: Got endpoints: latency-svc-n9fpl [750.237019ms]
Jan 12 16:18:29.813: INFO: Created: latency-svc-p4xwn
Jan 12 16:18:29.851: INFO: Got endpoints: latency-svc-hpnlf [751.532787ms]
Jan 12 16:18:29.865: INFO: Created: latency-svc-sh8bx
Jan 12 16:18:29.902: INFO: Got endpoints: latency-svc-dfkhq [750.505384ms]
Jan 12 16:18:29.913: INFO: Created: latency-svc-lbw6d
Jan 12 16:18:29.951: INFO: Got endpoints: latency-svc-wbs4r [749.398042ms]
Jan 12 16:18:29.961: INFO: Created: latency-svc-m74xz
Jan 12 16:18:30.003: INFO: Got endpoints: latency-svc-xfbpc [750.368358ms]
Jan 12 16:18:30.015: INFO: Created: latency-svc-cdd4z
Jan 12 16:18:30.053: INFO: Got endpoints: latency-svc-5gzhg [752.153892ms]
Jan 12 16:18:30.065: INFO: Created: latency-svc-5ldcm
Jan 12 16:18:30.101: INFO: Got endpoints: latency-svc-htfbz [748.496279ms]
Jan 12 16:18:30.110: INFO: Created: latency-svc-vclhz
Jan 12 16:18:30.151: INFO: Got endpoints: latency-svc-5lfjk [744.889336ms]
Jan 12 16:18:30.169: INFO: Created: latency-svc-kbch8
Jan 12 16:18:30.200: INFO: Got endpoints: latency-svc-pfcx9 [748.018057ms]
Jan 12 16:18:30.214: INFO: Created: latency-svc-nrswj
Jan 12 16:18:30.250: INFO: Got endpoints: latency-svc-pt8j6 [749.036163ms]
Jan 12 16:18:30.270: INFO: Created: latency-svc-lwknp
Jan 12 16:18:30.303: INFO: Got endpoints: latency-svc-8k8mg [750.361982ms]
Jan 12 16:18:30.313: INFO: Created: latency-svc-pqzr5
Jan 12 16:18:30.351: INFO: Got endpoints: latency-svc-mcvtq [750.405326ms]
Jan 12 16:18:30.361: INFO: Created: latency-svc-nrb24
Jan 12 16:18:30.400: INFO: Got endpoints: latency-svc-fztqb [748.322044ms]
Jan 12 16:18:30.411: INFO: Created: latency-svc-t96gm
Jan 12 16:18:30.454: INFO: Got endpoints: latency-svc-czwk6 [751.610361ms]
Jan 12 16:18:30.466: INFO: Created: latency-svc-crvtj
Jan 12 16:18:30.500: INFO: Got endpoints: latency-svc-svwj4 [749.803293ms]
Jan 12 16:18:30.510: INFO: Created: latency-svc-v9shf
Jan 12 16:18:30.553: INFO: Got endpoints: latency-svc-p4xwn [751.183881ms]
Jan 12 16:18:30.562: INFO: Created: latency-svc-sgfxg
Jan 12 16:18:30.600: INFO: Got endpoints: latency-svc-sh8bx [748.641656ms]
Jan 12 16:18:30.610: INFO: Created: latency-svc-2zxlz
Jan 12 16:18:30.655: INFO: Got endpoints: latency-svc-lbw6d [752.828382ms]
Jan 12 16:18:30.666: INFO: Created: latency-svc-9ph88
Jan 12 16:18:30.705: INFO: Got endpoints: latency-svc-m74xz [753.945358ms]
Jan 12 16:18:30.715: INFO: Created: latency-svc-9w79f
Jan 12 16:18:30.752: INFO: Got endpoints: latency-svc-cdd4z [748.898972ms]
Jan 12 16:18:30.763: INFO: Created: latency-svc-jgwnz
Jan 12 16:18:30.801: INFO: Got endpoints: latency-svc-5ldcm [748.122089ms]
Jan 12 16:18:30.825: INFO: Created: latency-svc-pnn5c
Jan 12 16:18:30.852: INFO: Got endpoints: latency-svc-vclhz [750.725302ms]
Jan 12 16:18:30.862: INFO: Created: latency-svc-7wzcx
Jan 12 16:18:30.900: INFO: Got endpoints: latency-svc-kbch8 [749.599331ms]
Jan 12 16:18:30.911: INFO: Created: latency-svc-l8l86
Jan 12 16:18:30.952: INFO: Got endpoints: latency-svc-nrswj [752.205094ms]
Jan 12 16:18:30.962: INFO: Created: latency-svc-lqx77
Jan 12 16:18:31.001: INFO: Got endpoints: latency-svc-lwknp [750.134063ms]
Jan 12 16:18:31.014: INFO: Created: latency-svc-fcf9t
Jan 12 16:18:31.053: INFO: Got endpoints: latency-svc-pqzr5 [750.424014ms]
Jan 12 16:18:31.068: INFO: Created: latency-svc-s2nn7
Jan 12 16:18:31.100: INFO: Got endpoints: latency-svc-nrb24 [749.695168ms]
Jan 12 16:18:31.113: INFO: Created: latency-svc-tlfvl
Jan 12 16:18:31.152: INFO: Got endpoints: latency-svc-t96gm [751.109421ms]
Jan 12 16:18:31.162: INFO: Created: latency-svc-dlbr7
Jan 12 16:18:31.200: INFO: Got endpoints: latency-svc-crvtj [746.27539ms]
Jan 12 16:18:31.215: INFO: Created: latency-svc-7lptw
Jan 12 16:18:31.250: INFO: Got endpoints: latency-svc-v9shf [749.716756ms]
Jan 12 16:18:31.261: INFO: Created: latency-svc-557sw
Jan 12 16:18:31.304: INFO: Got endpoints: latency-svc-sgfxg [750.97466ms]
Jan 12 16:18:31.318: INFO: Created: latency-svc-mzdhk
Jan 12 16:18:31.351: INFO: Got endpoints: latency-svc-2zxlz [751.313054ms]
Jan 12 16:18:31.365: INFO: Created: latency-svc-6dx65
Jan 12 16:18:31.402: INFO: Got endpoints: latency-svc-9ph88 [747.383826ms]
Jan 12 16:18:31.417: INFO: Created: latency-svc-qppbq
Jan 12 16:18:31.461: INFO: Got endpoints: latency-svc-9w79f [756.230896ms]
Jan 12 16:18:31.486: INFO: Created: latency-svc-bt2vd
Jan 12 16:18:31.505: INFO: Got endpoints: latency-svc-jgwnz [752.526636ms]
Jan 12 16:18:31.517: INFO: Created: latency-svc-q4qwx
Jan 12 16:18:31.550: INFO: Got endpoints: latency-svc-pnn5c [749.487723ms]
Jan 12 16:18:31.560: INFO: Created: latency-svc-flmg6
Jan 12 16:18:31.602: INFO: Got endpoints: latency-svc-7wzcx [750.235018ms]
Jan 12 16:18:31.616: INFO: Created: latency-svc-9xrpc
Jan 12 16:18:31.650: INFO: Got endpoints: latency-svc-l8l86 [749.666794ms]
Jan 12 16:18:31.660: INFO: Created: latency-svc-nn5cr
Jan 12 16:18:31.703: INFO: Got endpoints: latency-svc-lqx77 [750.583841ms]
Jan 12 16:18:31.713: INFO: Created: latency-svc-w7qpx
Jan 12 16:18:31.750: INFO: Got endpoints: latency-svc-fcf9t [749.502352ms]
Jan 12 16:18:31.762: INFO: Created: latency-svc-8wp26
Jan 12 16:18:31.801: INFO: Got endpoints: latency-svc-s2nn7 [747.750482ms]
Jan 12 16:18:31.814: INFO: Created: latency-svc-djh9c
Jan 12 16:18:31.853: INFO: Got endpoints: latency-svc-tlfvl [752.412321ms]
Jan 12 16:18:31.865: INFO: Created: latency-svc-hrjqd
Jan 12 16:18:31.900: INFO: Got endpoints: latency-svc-dlbr7 [748.069369ms]
Jan 12 16:18:31.910: INFO: Created: latency-svc-s4b7k
Jan 12 16:18:31.953: INFO: Got endpoints: latency-svc-7lptw [752.500037ms]
Jan 12 16:18:31.965: INFO: Created: latency-svc-gfjqp
Jan 12 16:18:32.001: INFO: Got endpoints: latency-svc-557sw [750.324308ms]
Jan 12 16:18:32.018: INFO: Created: latency-svc-79ktb
Jan 12 16:18:32.050: INFO: Got endpoints: latency-svc-mzdhk [746.320175ms]
Jan 12 16:18:32.067: INFO: Created: latency-svc-m6dlj
Jan 12 16:18:32.108: INFO: Got endpoints: latency-svc-6dx65 [756.635585ms]
Jan 12 16:18:32.118: INFO: Created: latency-svc-bjlq7
Jan 12 16:18:32.153: INFO: Got endpoints: latency-svc-qppbq [750.242682ms]
Jan 12 16:18:32.163: INFO: Created: latency-svc-j22n5
Jan 12 16:18:32.201: INFO: Got endpoints: latency-svc-bt2vd [739.614387ms]
Jan 12 16:18:32.216: INFO: Created: latency-svc-59jxg
Jan 12 16:18:32.250: INFO: Got endpoints: latency-svc-q4qwx [745.244338ms]
Jan 12 16:18:32.261: INFO: Created: latency-svc-rbm5x
Jan 12 16:18:32.300: INFO: Got endpoints: latency-svc-flmg6 [750.006613ms]
Jan 12 16:18:32.312: INFO: Created: latency-svc-dq244
Jan 12 16:18:32.351: INFO: Got endpoints: latency-svc-9xrpc [749.362617ms]
Jan 12 16:18:32.361: INFO: Created: latency-svc-mlfp2
Jan 12 16:18:32.400: INFO: Got endpoints: latency-svc-nn5cr [750.247056ms]
Jan 12 16:18:32.418: INFO: Created: latency-svc-gtwxd
Jan 12 16:18:32.451: INFO: Got endpoints: latency-svc-w7qpx [748.402194ms]
Jan 12 16:18:32.460: INFO: Created: latency-svc-5f5ww
Jan 12 16:18:32.503: INFO: Got endpoints: latency-svc-8wp26 [752.537268ms]
Jan 12 16:18:32.513: INFO: Created: latency-svc-w6bvv
Jan 12 16:18:32.550: INFO: Got endpoints: latency-svc-djh9c [748.846671ms]
Jan 12 16:18:32.562: INFO: Created: latency-svc-mgnpn
Jan 12 16:18:32.601: INFO: Got endpoints: latency-svc-hrjqd [747.870933ms]
Jan 12 16:18:32.613: INFO: Created: latency-svc-fgjhv
Jan 12 16:18:32.650: INFO: Got endpoints: latency-svc-s4b7k [750.013245ms]
Jan 12 16:18:32.662: INFO: Created: latency-svc-5v26g
Jan 12 16:18:32.701: INFO: Got endpoints: latency-svc-gfjqp [748.265244ms]
Jan 12 16:18:32.714: INFO: Created: latency-svc-f6c8s
Jan 12 16:18:32.757: INFO: Got endpoints: latency-svc-79ktb [756.240484ms]
Jan 12 16:18:32.768: INFO: Created: latency-svc-lvkb2
Jan 12 16:18:32.800: INFO: Got endpoints: latency-svc-m6dlj [749.774763ms]
Jan 12 16:18:32.814: INFO: Created: latency-svc-qr6p9
Jan 12 16:18:32.852: INFO: Got endpoints: latency-svc-bjlq7 [744.177948ms]
Jan 12 16:18:32.869: INFO: Created: latency-svc-wncl5
Jan 12 16:18:32.901: INFO: Got endpoints: latency-svc-j22n5 [747.830533ms]
Jan 12 16:18:32.910: INFO: Created: latency-svc-bjx96
Jan 12 16:18:32.951: INFO: Got endpoints: latency-svc-59jxg [749.970055ms]
Jan 12 16:18:32.960: INFO: Created: latency-svc-ll964
Jan 12 16:18:33.000: INFO: Got endpoints: latency-svc-rbm5x [750.016689ms]
Jan 12 16:18:33.013: INFO: Created: latency-svc-svlg6
Jan 12 16:18:33.050: INFO: Got endpoints: latency-svc-dq244 [749.233172ms]
Jan 12 16:18:33.060: INFO: Created: latency-svc-q7vp5
Jan 12 16:18:33.100: INFO: Got endpoints: latency-svc-mlfp2 [748.491951ms]
Jan 12 16:18:33.111: INFO: Created: latency-svc-pds9t
Jan 12 16:18:33.151: INFO: Got endpoints: latency-svc-gtwxd [751.002715ms]
Jan 12 16:18:33.161: INFO: Created: latency-svc-gmjgv
Jan 12 16:18:33.201: INFO: Got endpoints: latency-svc-5f5ww [749.360828ms]
Jan 12 16:18:33.214: INFO: Created: latency-svc-v5grq
Jan 12 16:18:33.253: INFO: Got endpoints: latency-svc-w6bvv [750.185923ms]
Jan 12 16:18:33.263: INFO: Created: latency-svc-cz9bn
Jan 12 16:18:33.301: INFO: Got endpoints: latency-svc-mgnpn [750.649927ms]
Jan 12 16:18:33.312: INFO: Created: latency-svc-z7b76
Jan 12 16:18:33.357: INFO: Got endpoints: latency-svc-fgjhv [756.598588ms]
Jan 12 16:18:33.368: INFO: Created: latency-svc-bbtxx
Jan 12 16:18:33.404: INFO: Got endpoints: latency-svc-5v26g [754.521823ms]
Jan 12 16:18:33.417: INFO: Created: latency-svc-hqm62
Jan 12 16:18:33.452: INFO: Got endpoints: latency-svc-f6c8s [751.435644ms]
Jan 12 16:18:33.462: INFO: Created: latency-svc-259nr
Jan 12 16:18:33.503: INFO: Got endpoints: latency-svc-lvkb2 [745.716962ms]
Jan 12 16:18:33.522: INFO: Created: latency-svc-2cssw
Jan 12 16:18:33.550: INFO: Got endpoints: latency-svc-qr6p9 [750.109527ms]
Jan 12 16:18:33.569: INFO: Created: latency-svc-nd56b
Jan 12 16:18:33.600: INFO: Got endpoints: latency-svc-wncl5 [747.366373ms]
Jan 12 16:18:33.612: INFO: Created: latency-svc-mrpz6
Jan 12 16:18:33.649: INFO: Got endpoints: latency-svc-bjx96 [748.593252ms]
Jan 12 16:18:33.659: INFO: Created: latency-svc-ls8nr
Jan 12 16:18:33.700: INFO: Got endpoints: latency-svc-ll964 [749.547442ms]
Jan 12 16:18:33.712: INFO: Created: latency-svc-gwwnz
Jan 12 16:18:33.750: INFO: Got endpoints: latency-svc-svlg6 [750.163052ms]
Jan 12 16:18:33.764: INFO: Created: latency-svc-tvd4j
Jan 12 16:18:33.802: INFO: Got endpoints: latency-svc-q7vp5 [751.875484ms]
Jan 12 16:18:33.812: INFO: Created: latency-svc-rll95
Jan 12 16:18:33.850: INFO: Got endpoints: latency-svc-pds9t [750.463385ms]
Jan 12 16:18:33.860: INFO: Created: latency-svc-cxbsl
Jan 12 16:18:33.903: INFO: Got endpoints: latency-svc-gmjgv [751.242764ms]
Jan 12 16:18:33.912: INFO: Created: latency-svc-ldkl6
Jan 12 16:18:33.950: INFO: Got endpoints: latency-svc-v5grq [749.210085ms]
Jan 12 16:18:33.964: INFO: Created: latency-svc-lj7b4
Jan 12 16:18:34.002: INFO: Got endpoints: latency-svc-cz9bn [749.122314ms]
Jan 12 16:18:34.012: INFO: Created: latency-svc-mx575
Jan 12 16:18:34.051: INFO: Got endpoints: latency-svc-z7b76 [750.264237ms]
Jan 12 16:18:34.061: INFO: Created: latency-svc-h5j5d
Jan 12 16:18:34.100: INFO: Got endpoints: latency-svc-bbtxx [742.551316ms]
Jan 12 16:18:34.111: INFO: Created: latency-svc-295ck
Jan 12 16:18:34.150: INFO: Got endpoints: latency-svc-hqm62 [745.36419ms]
Jan 12 16:18:34.170: INFO: Created: latency-svc-2nbfh
Jan 12 16:18:34.202: INFO: Got endpoints: latency-svc-259nr [749.021801ms]
Jan 12 16:18:34.213: INFO: Created: latency-svc-xkswf
Jan 12 16:18:34.250: INFO: Got endpoints: latency-svc-2cssw [747.327322ms]
Jan 12 16:18:34.269: INFO: Created: latency-svc-z8thn
Jan 12 16:18:34.303: INFO: Got endpoints: latency-svc-nd56b [753.277524ms]
Jan 12 16:18:34.318: INFO: Created: latency-svc-kv6fb
Jan 12 16:18:34.350: INFO: Got endpoints: latency-svc-mrpz6 [749.753883ms]
Jan 12 16:18:34.363: INFO: Created: latency-svc-jhfzz
Jan 12 16:18:34.402: INFO: Got endpoints: latency-svc-ls8nr [752.447454ms]
Jan 12 16:18:34.413: INFO: Created: latency-svc-9kzjp
Jan 12 16:18:34.456: INFO: Got endpoints: latency-svc-gwwnz [755.298795ms]
Jan 12 16:18:34.466: INFO: Created: latency-svc-fmbgn
Jan 12 16:18:34.500: INFO: Got endpoints: latency-svc-tvd4j [749.209096ms]
Jan 12 16:18:34.521: INFO: Created: latency-svc-hv2xl
Jan 12 16:18:34.556: INFO: Got endpoints: latency-svc-rll95 [754.23758ms]
Jan 12 16:18:34.570: INFO: Created: latency-svc-cjkpv
Jan 12 16:18:34.600: INFO: Got endpoints: latency-svc-cxbsl [749.982712ms]
Jan 12 16:18:34.611: INFO: Created: latency-svc-qvwdx
Jan 12 16:18:34.652: INFO: Got endpoints: latency-svc-ldkl6 [749.42801ms]
Jan 12 16:18:34.663: INFO: Created: latency-svc-zxchc
Jan 12 16:18:34.701: INFO: Got endpoints: latency-svc-lj7b4 [750.54772ms]
Jan 12 16:18:34.711: INFO: Created: latency-svc-g8rwt
Jan 12 16:18:34.753: INFO: Got endpoints: latency-svc-mx575 [750.352736ms]
Jan 12 16:18:34.767: INFO: Created: latency-svc-2f5sd
Jan 12 16:18:34.809: INFO: Got endpoints: latency-svc-h5j5d [758.271423ms]
Jan 12 16:18:34.822: INFO: Created: latency-svc-dz44x
Jan 12 16:18:34.851: INFO: Got endpoints: latency-svc-295ck [751.299212ms]
Jan 12 16:18:34.863: INFO: Created: latency-svc-p99mn
Jan 12 16:18:34.904: INFO: Got endpoints: latency-svc-2nbfh [754.50557ms]
Jan 12 16:18:34.920: INFO: Created: latency-svc-wb2hz
Jan 12 16:18:34.951: INFO: Got endpoints: latency-svc-xkswf [748.916435ms]
Jan 12 16:18:34.966: INFO: Created: latency-svc-brtj5
Jan 12 16:18:35.005: INFO: Got endpoints: latency-svc-z8thn [754.368836ms]
Jan 12 16:18:35.026: INFO: Created: latency-svc-9xqcx
Jan 12 16:18:35.051: INFO: Got endpoints: latency-svc-kv6fb [747.555494ms]
Jan 12 16:18:35.062: INFO: Created: latency-svc-5nvbw
Jan 12 16:18:35.103: INFO: Got endpoints: latency-svc-jhfzz [753.828993ms]
Jan 12 16:18:35.115: INFO: Created: latency-svc-9xd7f
Jan 12 16:18:35.151: INFO: Got endpoints: latency-svc-9kzjp [748.573724ms]
Jan 12 16:18:35.163: INFO: Created: latency-svc-jqw9w
Jan 12 16:18:35.201: INFO: Got endpoints: latency-svc-fmbgn [744.963206ms]
Jan 12 16:18:35.211: INFO: Created: latency-svc-458wt
Jan 12 16:18:35.250: INFO: Got endpoints: latency-svc-hv2xl [750.373307ms]
Jan 12 16:18:35.260: INFO: Created: latency-svc-499pb
Jan 12 16:18:35.300: INFO: Got endpoints: latency-svc-cjkpv [743.731501ms]
Jan 12 16:18:35.351: INFO: Got endpoints: latency-svc-qvwdx [750.516212ms]
Jan 12 16:18:35.401: INFO: Got endpoints: latency-svc-zxchc [748.285464ms]
Jan 12 16:18:35.451: INFO: Got endpoints: latency-svc-g8rwt [750.621497ms]
Jan 12 16:18:35.503: INFO: Got endpoints: latency-svc-2f5sd [750.623833ms]
Jan 12 16:18:35.550: INFO: Got endpoints: latency-svc-dz44x [741.086857ms]
Jan 12 16:18:35.604: INFO: Got endpoints: latency-svc-p99mn [753.172707ms]
Jan 12 16:18:35.649: INFO: Got endpoints: latency-svc-wb2hz [745.064332ms]
Jan 12 16:18:35.703: INFO: Got endpoints: latency-svc-brtj5 [751.917867ms]
Jan 12 16:18:35.750: INFO: Got endpoints: latency-svc-9xqcx [744.81812ms]
Jan 12 16:18:35.802: INFO: Got endpoints: latency-svc-5nvbw [751.208386ms]
Jan 12 16:18:35.851: INFO: Got endpoints: latency-svc-9xd7f [747.360153ms]
Jan 12 16:18:35.902: INFO: Got endpoints: latency-svc-jqw9w [751.635393ms]
Jan 12 16:18:35.949: INFO: Got endpoints: latency-svc-458wt [748.557323ms]
Jan 12 16:18:36.002: INFO: Got endpoints: latency-svc-499pb [751.918454ms]
Jan 12 16:18:36.002: INFO: Latencies: [18.566074ms 21.686332ms 33.187376ms 50.214011ms 60.054419ms 68.472066ms 87.173654ms 96.759333ms 110.656339ms 117.028694ms 130.787083ms 136.126747ms 146.467481ms 155.623364ms 158.338328ms 159.108805ms 160.73499ms 162.730942ms 163.447618ms 164.510233ms 164.766888ms 166.023101ms 172.371107ms 172.517996ms 172.684422ms 173.86753ms 174.58696ms 174.631538ms 175.519511ms 176.390994ms 177.204522ms 178.135975ms 179.406488ms 180.185849ms 193.050025ms 193.264397ms 195.626443ms 195.813961ms 227.856243ms 271.229746ms 311.975596ms 338.918105ms 378.063759ms 417.165169ms 459.14226ms 504.140538ms 526.068505ms 559.594899ms 594.189718ms 622.768451ms 662.968277ms 705.012596ms 738.757792ms 739.614387ms 741.086857ms 742.551316ms 743.731501ms 744.177948ms 744.81812ms 744.889336ms 744.963206ms 745.064332ms 745.244338ms 745.25834ms 745.36419ms 745.716962ms 746.27539ms 746.320175ms 747.070816ms 747.312595ms 747.327322ms 747.360153ms 747.366373ms 747.383826ms 747.402253ms 747.463824ms 747.546855ms 747.555494ms 747.750482ms 747.785957ms 747.830533ms 747.870933ms 748.018057ms 748.069369ms 748.122089ms 748.265244ms 748.285464ms 748.322044ms 748.402194ms 748.491951ms 748.496279ms 748.557323ms 748.573724ms 748.593252ms 748.641656ms 748.846671ms 748.898972ms 748.916435ms 749.021801ms 749.036163ms 749.122314ms 749.209096ms 749.210085ms 749.217965ms 749.233172ms 749.360828ms 749.362617ms 749.398042ms 749.42801ms 749.487723ms 749.491362ms 749.502352ms 749.547442ms 749.599331ms 749.666794ms 749.695168ms 749.716756ms 749.753883ms 749.774066ms 749.774763ms 749.777819ms 749.803293ms 749.970055ms 749.982712ms 750.006613ms 750.013245ms 750.016689ms 750.109527ms 750.13267ms 750.134063ms 750.163052ms 750.185923ms 750.235018ms 750.237019ms 750.242682ms 750.247056ms 750.264237ms 750.324308ms 750.352736ms 750.361982ms 750.368358ms 750.373307ms 750.405326ms 750.424014ms 750.463385ms 750.505384ms 750.516212ms 750.54772ms 750.583841ms 750.621497ms 750.623833ms 750.649927ms 750.725302ms 750.887858ms 750.89151ms 750.892749ms 750.97466ms 751.002715ms 751.109421ms 751.115313ms 751.183881ms 751.204638ms 751.208386ms 751.242764ms 751.255788ms 751.299212ms 751.313054ms 751.435644ms 751.532787ms 751.610361ms 751.635393ms 751.875484ms 751.917867ms 751.918454ms 752.034357ms 752.153892ms 752.205094ms 752.230363ms 752.412321ms 752.447454ms 752.500037ms 752.526636ms 752.537268ms 752.774553ms 752.828382ms 753.172707ms 753.277524ms 753.828993ms 753.945358ms 754.23758ms 754.368836ms 754.50557ms 754.521823ms 755.298795ms 755.591704ms 756.230896ms 756.240484ms 756.598588ms 756.635585ms 758.271423ms]
Jan 12 16:18:36.002: INFO: 50 %ile: 749.122314ms
Jan 12 16:18:36.002: INFO: 90 %ile: 752.500037ms
Jan 12 16:18:36.002: INFO: 99 %ile: 756.635585ms
Jan 12 16:18:36.002: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 12 16:18:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-3246" for this suite. 01/12/23 16:18:36.005
------------------------------
 [SLOW TEST] [9.743 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:18:26.265
    Jan 12 16:18:26.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svc-latency 01/12/23 16:18:26.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:18:26.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:18:26.277
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 12 16:18:26.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3246 01/12/23 16:18:26.28
    I0112 16:18:26.284855      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3246, replica count: 1
    I0112 16:18:27.335411      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 16:18:27.447: INFO: Created: latency-svc-nxgwl
    Jan 12 16:18:27.451: INFO: Got endpoints: latency-svc-nxgwl [15.595226ms]
    Jan 12 16:18:27.462: INFO: Created: latency-svc-f6cjr
    Jan 12 16:18:27.470: INFO: Got endpoints: latency-svc-f6cjr [18.566074ms]
    Jan 12 16:18:27.471: INFO: Created: latency-svc-r8p5l
    Jan 12 16:18:27.474: INFO: Got endpoints: latency-svc-r8p5l [21.686332ms]
    Jan 12 16:18:27.482: INFO: Created: latency-svc-xdkxt
    Jan 12 16:18:27.485: INFO: Got endpoints: latency-svc-xdkxt [33.187376ms]
    Jan 12 16:18:27.495: INFO: Created: latency-svc-knpld
    Jan 12 16:18:27.502: INFO: Got endpoints: latency-svc-knpld [50.214011ms]
    Jan 12 16:18:27.504: INFO: Created: latency-svc-6k2rz
    Jan 12 16:18:27.512: INFO: Got endpoints: latency-svc-6k2rz [60.054419ms]
    Jan 12 16:18:27.514: INFO: Created: latency-svc-sljxh
    Jan 12 16:18:27.521: INFO: Got endpoints: latency-svc-sljxh [68.472066ms]
    Jan 12 16:18:27.522: INFO: Created: latency-svc-8kq7v
    Jan 12 16:18:27.540: INFO: Got endpoints: latency-svc-8kq7v [87.173654ms]
    Jan 12 16:18:27.541: INFO: Created: latency-svc-52qhf
    Jan 12 16:18:27.550: INFO: Got endpoints: latency-svc-52qhf [96.759333ms]
    Jan 12 16:18:27.552: INFO: Created: latency-svc-srhdf
    Jan 12 16:18:27.563: INFO: Got endpoints: latency-svc-srhdf [110.656339ms]
    Jan 12 16:18:27.565: INFO: Created: latency-svc-sqt5f
    Jan 12 16:18:27.570: INFO: Got endpoints: latency-svc-sqt5f [117.028694ms]
    Jan 12 16:18:27.572: INFO: Created: latency-svc-llx8d
    Jan 12 16:18:27.584: INFO: Got endpoints: latency-svc-llx8d [130.787083ms]
    Jan 12 16:18:27.586: INFO: Created: latency-svc-tz4g2
    Jan 12 16:18:27.589: INFO: Got endpoints: latency-svc-tz4g2 [136.126747ms]
    Jan 12 16:18:27.597: INFO: Created: latency-svc-98ft5
    Jan 12 16:18:27.600: INFO: Got endpoints: latency-svc-98ft5 [146.467481ms]
    Jan 12 16:18:27.606: INFO: Created: latency-svc-2z45q
    Jan 12 16:18:27.609: INFO: Got endpoints: latency-svc-2z45q [155.623364ms]
    Jan 12 16:18:27.615: INFO: Created: latency-svc-glggg
    Jan 12 16:18:27.628: INFO: Got endpoints: latency-svc-glggg [174.631538ms]
    Jan 12 16:18:27.629: INFO: Created: latency-svc-4blmn
    Jan 12 16:18:27.635: INFO: Got endpoints: latency-svc-4blmn [164.766888ms]
    Jan 12 16:18:27.639: INFO: Created: latency-svc-4vqxb
    Jan 12 16:18:27.653: INFO: Got endpoints: latency-svc-4vqxb [179.406488ms]
    Jan 12 16:18:27.656: INFO: Created: latency-svc-s4v7g
    Jan 12 16:18:27.663: INFO: Got endpoints: latency-svc-s4v7g [178.135975ms]
    Jan 12 16:18:27.666: INFO: Created: latency-svc-zsmtp
    Jan 12 16:18:27.676: INFO: Got endpoints: latency-svc-zsmtp [173.86753ms]
    Jan 12 16:18:27.677: INFO: Created: latency-svc-dkfzl
    Jan 12 16:18:27.685: INFO: Got endpoints: latency-svc-dkfzl [172.371107ms]
    Jan 12 16:18:27.690: INFO: Created: latency-svc-chjgm
    Jan 12 16:18:27.694: INFO: Got endpoints: latency-svc-chjgm [172.684422ms]
    Jan 12 16:18:27.697: INFO: Created: latency-svc-tbtnk
    Jan 12 16:18:27.704: INFO: Got endpoints: latency-svc-tbtnk [164.510233ms]
    Jan 12 16:18:27.705: INFO: Created: latency-svc-v5scv
    Jan 12 16:18:27.716: INFO: Got endpoints: latency-svc-v5scv [166.023101ms]
    Jan 12 16:18:27.719: INFO: Created: latency-svc-p4pjc
    Jan 12 16:18:27.721: INFO: Got endpoints: latency-svc-p4pjc [158.338328ms]
    Jan 12 16:18:27.728: INFO: Created: latency-svc-w55rz
    Jan 12 16:18:27.733: INFO: Got endpoints: latency-svc-w55rz [162.730942ms]
    Jan 12 16:18:27.741: INFO: Created: latency-svc-jbvfv
    Jan 12 16:18:27.743: INFO: Got endpoints: latency-svc-jbvfv [159.108805ms]
    Jan 12 16:18:27.750: INFO: Created: latency-svc-24x94
    Jan 12 16:18:27.766: INFO: Got endpoints: latency-svc-24x94 [176.390994ms]
    Jan 12 16:18:27.771: INFO: Created: latency-svc-5spp8
    Jan 12 16:18:27.774: INFO: Got endpoints: latency-svc-5spp8 [174.58696ms]
    Jan 12 16:18:27.777: INFO: Created: latency-svc-c4xh9
    Jan 12 16:18:27.784: INFO: Got endpoints: latency-svc-c4xh9 [175.519511ms]
    Jan 12 16:18:27.785: INFO: Created: latency-svc-7sc4r
    Jan 12 16:18:27.791: INFO: Got endpoints: latency-svc-7sc4r [163.447618ms]
    Jan 12 16:18:27.794: INFO: Created: latency-svc-8fbwj
    Jan 12 16:18:27.796: INFO: Got endpoints: latency-svc-8fbwj [160.73499ms]
    Jan 12 16:18:27.820: INFO: Created: latency-svc-28q2t
    Jan 12 16:18:27.826: INFO: Got endpoints: latency-svc-28q2t [172.517996ms]
    Jan 12 16:18:27.830: INFO: Created: latency-svc-8dt7x
    Jan 12 16:18:27.841: INFO: Got endpoints: latency-svc-8dt7x [177.204522ms]
    Jan 12 16:18:27.843: INFO: Created: latency-svc-fps77
    Jan 12 16:18:27.856: INFO: Got endpoints: latency-svc-fps77 [180.185849ms]
    Jan 12 16:18:27.860: INFO: Created: latency-svc-m85gx
    Jan 12 16:18:27.878: INFO: Got endpoints: latency-svc-m85gx [193.264397ms]
    Jan 12 16:18:27.884: INFO: Created: latency-svc-fc8mt
    Jan 12 16:18:27.890: INFO: Got endpoints: latency-svc-fc8mt [195.813961ms]
    Jan 12 16:18:27.892: INFO: Created: latency-svc-4qx29
    Jan 12 16:18:27.898: INFO: Got endpoints: latency-svc-4qx29 [193.050025ms]
    Jan 12 16:18:27.903: INFO: Created: latency-svc-trhd9
    Jan 12 16:18:27.911: INFO: Got endpoints: latency-svc-trhd9 [195.626443ms]
    Jan 12 16:18:27.913: INFO: Created: latency-svc-kf6p2
    Jan 12 16:18:27.923: INFO: Created: latency-svc-vhgrf
    Jan 12 16:18:27.931: INFO: Created: latency-svc-9ql9h
    Jan 12 16:18:27.943: INFO: Created: latency-svc-xnl4h
    Jan 12 16:18:27.949: INFO: Got endpoints: latency-svc-kf6p2 [227.856243ms]
    Jan 12 16:18:27.952: INFO: Created: latency-svc-dw82l
    Jan 12 16:18:27.962: INFO: Created: latency-svc-qs9hf
    Jan 12 16:18:27.970: INFO: Created: latency-svc-5rmnf
    Jan 12 16:18:27.976: INFO: Created: latency-svc-xw2ll
    Jan 12 16:18:27.995: INFO: Created: latency-svc-65qrv
    Jan 12 16:18:28.004: INFO: Got endpoints: latency-svc-vhgrf [271.229746ms]
    Jan 12 16:18:28.005: INFO: Created: latency-svc-csrrp
    Jan 12 16:18:28.015: INFO: Created: latency-svc-7m94c
    Jan 12 16:18:28.024: INFO: Created: latency-svc-xgsbn
    Jan 12 16:18:28.029: INFO: Created: latency-svc-xsbzc
    Jan 12 16:18:28.041: INFO: Created: latency-svc-zcmcp
    Jan 12 16:18:28.054: INFO: Created: latency-svc-tfpfp
    Jan 12 16:18:28.055: INFO: Got endpoints: latency-svc-9ql9h [311.975596ms]
    Jan 12 16:18:28.060: INFO: Created: latency-svc-s5v7x
    Jan 12 16:18:28.070: INFO: Created: latency-svc-vrq8f
    Jan 12 16:18:28.078: INFO: Created: latency-svc-vpdcz
    Jan 12 16:18:28.105: INFO: Got endpoints: latency-svc-xnl4h [338.918105ms]
    Jan 12 16:18:28.115: INFO: Created: latency-svc-g5ggs
    Jan 12 16:18:28.153: INFO: Got endpoints: latency-svc-dw82l [378.063759ms]
    Jan 12 16:18:28.164: INFO: Created: latency-svc-ctkfl
    Jan 12 16:18:28.202: INFO: Got endpoints: latency-svc-qs9hf [417.165169ms]
    Jan 12 16:18:28.215: INFO: Created: latency-svc-cwtvb
    Jan 12 16:18:28.251: INFO: Got endpoints: latency-svc-5rmnf [459.14226ms]
    Jan 12 16:18:28.264: INFO: Created: latency-svc-lf8rd
    Jan 12 16:18:28.300: INFO: Got endpoints: latency-svc-xw2ll [504.140538ms]
    Jan 12 16:18:28.310: INFO: Created: latency-svc-dvk7k
    Jan 12 16:18:28.352: INFO: Got endpoints: latency-svc-65qrv [526.068505ms]
    Jan 12 16:18:28.363: INFO: Created: latency-svc-2t2fp
    Jan 12 16:18:28.400: INFO: Got endpoints: latency-svc-csrrp [559.594899ms]
    Jan 12 16:18:28.411: INFO: Created: latency-svc-zzmwk
    Jan 12 16:18:28.450: INFO: Got endpoints: latency-svc-7m94c [594.189718ms]
    Jan 12 16:18:28.475: INFO: Created: latency-svc-2shcg
    Jan 12 16:18:28.501: INFO: Got endpoints: latency-svc-xgsbn [622.768451ms]
    Jan 12 16:18:28.511: INFO: Created: latency-svc-6m2p9
    Jan 12 16:18:28.553: INFO: Got endpoints: latency-svc-xsbzc [662.968277ms]
    Jan 12 16:18:28.562: INFO: Created: latency-svc-tckzm
    Jan 12 16:18:28.603: INFO: Got endpoints: latency-svc-zcmcp [705.012596ms]
    Jan 12 16:18:28.614: INFO: Created: latency-svc-g2v96
    Jan 12 16:18:28.650: INFO: Got endpoints: latency-svc-tfpfp [738.757792ms]
    Jan 12 16:18:28.663: INFO: Created: latency-svc-zdxnk
    Jan 12 16:18:28.700: INFO: Got endpoints: latency-svc-s5v7x [750.892749ms]
    Jan 12 16:18:28.710: INFO: Created: latency-svc-mf67b
    Jan 12 16:18:28.754: INFO: Got endpoints: latency-svc-vrq8f [749.777819ms]
    Jan 12 16:18:28.764: INFO: Created: latency-svc-b5958
    Jan 12 16:18:28.803: INFO: Got endpoints: latency-svc-vpdcz [747.463824ms]
    Jan 12 16:18:28.816: INFO: Created: latency-svc-thn4q
    Jan 12 16:18:28.850: INFO: Got endpoints: latency-svc-g5ggs [745.25834ms]
    Jan 12 16:18:28.865: INFO: Created: latency-svc-ltgp6
    Jan 12 16:18:28.900: INFO: Got endpoints: latency-svc-ctkfl [747.312595ms]
    Jan 12 16:18:28.911: INFO: Created: latency-svc-s8mwv
    Jan 12 16:18:28.951: INFO: Got endpoints: latency-svc-cwtvb [749.217965ms]
    Jan 12 16:18:28.974: INFO: Created: latency-svc-fwn6t
    Jan 12 16:18:29.003: INFO: Got endpoints: latency-svc-lf8rd [752.774553ms]
    Jan 12 16:18:29.014: INFO: Created: latency-svc-vh42l
    Jan 12 16:18:29.051: INFO: Got endpoints: latency-svc-dvk7k [751.204638ms]
    Jan 12 16:18:29.066: INFO: Created: latency-svc-n9fpl
    Jan 12 16:18:29.100: INFO: Got endpoints: latency-svc-2t2fp [747.785957ms]
    Jan 12 16:18:29.111: INFO: Created: latency-svc-hpnlf
    Jan 12 16:18:29.152: INFO: Got endpoints: latency-svc-zzmwk [751.115313ms]
    Jan 12 16:18:29.162: INFO: Created: latency-svc-dfkhq
    Jan 12 16:18:29.201: INFO: Got endpoints: latency-svc-2shcg [750.887858ms]
    Jan 12 16:18:29.219: INFO: Created: latency-svc-wbs4r
    Jan 12 16:18:29.253: INFO: Got endpoints: latency-svc-6m2p9 [752.034357ms]
    Jan 12 16:18:29.268: INFO: Created: latency-svc-xfbpc
    Jan 12 16:18:29.300: INFO: Got endpoints: latency-svc-tckzm [747.546855ms]
    Jan 12 16:18:29.310: INFO: Created: latency-svc-5gzhg
    Jan 12 16:18:29.352: INFO: Got endpoints: latency-svc-g2v96 [749.491362ms]
    Jan 12 16:18:29.362: INFO: Created: latency-svc-htfbz
    Jan 12 16:18:29.406: INFO: Got endpoints: latency-svc-zdxnk [755.591704ms]
    Jan 12 16:18:29.417: INFO: Created: latency-svc-5lfjk
    Jan 12 16:18:29.452: INFO: Got endpoints: latency-svc-mf67b [751.255788ms]
    Jan 12 16:18:29.463: INFO: Created: latency-svc-pfcx9
    Jan 12 16:18:29.501: INFO: Got endpoints: latency-svc-b5958 [747.402253ms]
    Jan 12 16:18:29.518: INFO: Created: latency-svc-pt8j6
    Jan 12 16:18:29.552: INFO: Got endpoints: latency-svc-thn4q [749.774066ms]
    Jan 12 16:18:29.562: INFO: Created: latency-svc-8k8mg
    Jan 12 16:18:29.600: INFO: Got endpoints: latency-svc-ltgp6 [750.13267ms]
    Jan 12 16:18:29.610: INFO: Created: latency-svc-mcvtq
    Jan 12 16:18:29.652: INFO: Got endpoints: latency-svc-s8mwv [752.230363ms]
    Jan 12 16:18:29.665: INFO: Created: latency-svc-fztqb
    Jan 12 16:18:29.702: INFO: Got endpoints: latency-svc-fwn6t [750.89151ms]
    Jan 12 16:18:29.714: INFO: Created: latency-svc-czwk6
    Jan 12 16:18:29.751: INFO: Got endpoints: latency-svc-vh42l [747.070816ms]
    Jan 12 16:18:29.762: INFO: Created: latency-svc-svwj4
    Jan 12 16:18:29.802: INFO: Got endpoints: latency-svc-n9fpl [750.237019ms]
    Jan 12 16:18:29.813: INFO: Created: latency-svc-p4xwn
    Jan 12 16:18:29.851: INFO: Got endpoints: latency-svc-hpnlf [751.532787ms]
    Jan 12 16:18:29.865: INFO: Created: latency-svc-sh8bx
    Jan 12 16:18:29.902: INFO: Got endpoints: latency-svc-dfkhq [750.505384ms]
    Jan 12 16:18:29.913: INFO: Created: latency-svc-lbw6d
    Jan 12 16:18:29.951: INFO: Got endpoints: latency-svc-wbs4r [749.398042ms]
    Jan 12 16:18:29.961: INFO: Created: latency-svc-m74xz
    Jan 12 16:18:30.003: INFO: Got endpoints: latency-svc-xfbpc [750.368358ms]
    Jan 12 16:18:30.015: INFO: Created: latency-svc-cdd4z
    Jan 12 16:18:30.053: INFO: Got endpoints: latency-svc-5gzhg [752.153892ms]
    Jan 12 16:18:30.065: INFO: Created: latency-svc-5ldcm
    Jan 12 16:18:30.101: INFO: Got endpoints: latency-svc-htfbz [748.496279ms]
    Jan 12 16:18:30.110: INFO: Created: latency-svc-vclhz
    Jan 12 16:18:30.151: INFO: Got endpoints: latency-svc-5lfjk [744.889336ms]
    Jan 12 16:18:30.169: INFO: Created: latency-svc-kbch8
    Jan 12 16:18:30.200: INFO: Got endpoints: latency-svc-pfcx9 [748.018057ms]
    Jan 12 16:18:30.214: INFO: Created: latency-svc-nrswj
    Jan 12 16:18:30.250: INFO: Got endpoints: latency-svc-pt8j6 [749.036163ms]
    Jan 12 16:18:30.270: INFO: Created: latency-svc-lwknp
    Jan 12 16:18:30.303: INFO: Got endpoints: latency-svc-8k8mg [750.361982ms]
    Jan 12 16:18:30.313: INFO: Created: latency-svc-pqzr5
    Jan 12 16:18:30.351: INFO: Got endpoints: latency-svc-mcvtq [750.405326ms]
    Jan 12 16:18:30.361: INFO: Created: latency-svc-nrb24
    Jan 12 16:18:30.400: INFO: Got endpoints: latency-svc-fztqb [748.322044ms]
    Jan 12 16:18:30.411: INFO: Created: latency-svc-t96gm
    Jan 12 16:18:30.454: INFO: Got endpoints: latency-svc-czwk6 [751.610361ms]
    Jan 12 16:18:30.466: INFO: Created: latency-svc-crvtj
    Jan 12 16:18:30.500: INFO: Got endpoints: latency-svc-svwj4 [749.803293ms]
    Jan 12 16:18:30.510: INFO: Created: latency-svc-v9shf
    Jan 12 16:18:30.553: INFO: Got endpoints: latency-svc-p4xwn [751.183881ms]
    Jan 12 16:18:30.562: INFO: Created: latency-svc-sgfxg
    Jan 12 16:18:30.600: INFO: Got endpoints: latency-svc-sh8bx [748.641656ms]
    Jan 12 16:18:30.610: INFO: Created: latency-svc-2zxlz
    Jan 12 16:18:30.655: INFO: Got endpoints: latency-svc-lbw6d [752.828382ms]
    Jan 12 16:18:30.666: INFO: Created: latency-svc-9ph88
    Jan 12 16:18:30.705: INFO: Got endpoints: latency-svc-m74xz [753.945358ms]
    Jan 12 16:18:30.715: INFO: Created: latency-svc-9w79f
    Jan 12 16:18:30.752: INFO: Got endpoints: latency-svc-cdd4z [748.898972ms]
    Jan 12 16:18:30.763: INFO: Created: latency-svc-jgwnz
    Jan 12 16:18:30.801: INFO: Got endpoints: latency-svc-5ldcm [748.122089ms]
    Jan 12 16:18:30.825: INFO: Created: latency-svc-pnn5c
    Jan 12 16:18:30.852: INFO: Got endpoints: latency-svc-vclhz [750.725302ms]
    Jan 12 16:18:30.862: INFO: Created: latency-svc-7wzcx
    Jan 12 16:18:30.900: INFO: Got endpoints: latency-svc-kbch8 [749.599331ms]
    Jan 12 16:18:30.911: INFO: Created: latency-svc-l8l86
    Jan 12 16:18:30.952: INFO: Got endpoints: latency-svc-nrswj [752.205094ms]
    Jan 12 16:18:30.962: INFO: Created: latency-svc-lqx77
    Jan 12 16:18:31.001: INFO: Got endpoints: latency-svc-lwknp [750.134063ms]
    Jan 12 16:18:31.014: INFO: Created: latency-svc-fcf9t
    Jan 12 16:18:31.053: INFO: Got endpoints: latency-svc-pqzr5 [750.424014ms]
    Jan 12 16:18:31.068: INFO: Created: latency-svc-s2nn7
    Jan 12 16:18:31.100: INFO: Got endpoints: latency-svc-nrb24 [749.695168ms]
    Jan 12 16:18:31.113: INFO: Created: latency-svc-tlfvl
    Jan 12 16:18:31.152: INFO: Got endpoints: latency-svc-t96gm [751.109421ms]
    Jan 12 16:18:31.162: INFO: Created: latency-svc-dlbr7
    Jan 12 16:18:31.200: INFO: Got endpoints: latency-svc-crvtj [746.27539ms]
    Jan 12 16:18:31.215: INFO: Created: latency-svc-7lptw
    Jan 12 16:18:31.250: INFO: Got endpoints: latency-svc-v9shf [749.716756ms]
    Jan 12 16:18:31.261: INFO: Created: latency-svc-557sw
    Jan 12 16:18:31.304: INFO: Got endpoints: latency-svc-sgfxg [750.97466ms]
    Jan 12 16:18:31.318: INFO: Created: latency-svc-mzdhk
    Jan 12 16:18:31.351: INFO: Got endpoints: latency-svc-2zxlz [751.313054ms]
    Jan 12 16:18:31.365: INFO: Created: latency-svc-6dx65
    Jan 12 16:18:31.402: INFO: Got endpoints: latency-svc-9ph88 [747.383826ms]
    Jan 12 16:18:31.417: INFO: Created: latency-svc-qppbq
    Jan 12 16:18:31.461: INFO: Got endpoints: latency-svc-9w79f [756.230896ms]
    Jan 12 16:18:31.486: INFO: Created: latency-svc-bt2vd
    Jan 12 16:18:31.505: INFO: Got endpoints: latency-svc-jgwnz [752.526636ms]
    Jan 12 16:18:31.517: INFO: Created: latency-svc-q4qwx
    Jan 12 16:18:31.550: INFO: Got endpoints: latency-svc-pnn5c [749.487723ms]
    Jan 12 16:18:31.560: INFO: Created: latency-svc-flmg6
    Jan 12 16:18:31.602: INFO: Got endpoints: latency-svc-7wzcx [750.235018ms]
    Jan 12 16:18:31.616: INFO: Created: latency-svc-9xrpc
    Jan 12 16:18:31.650: INFO: Got endpoints: latency-svc-l8l86 [749.666794ms]
    Jan 12 16:18:31.660: INFO: Created: latency-svc-nn5cr
    Jan 12 16:18:31.703: INFO: Got endpoints: latency-svc-lqx77 [750.583841ms]
    Jan 12 16:18:31.713: INFO: Created: latency-svc-w7qpx
    Jan 12 16:18:31.750: INFO: Got endpoints: latency-svc-fcf9t [749.502352ms]
    Jan 12 16:18:31.762: INFO: Created: latency-svc-8wp26
    Jan 12 16:18:31.801: INFO: Got endpoints: latency-svc-s2nn7 [747.750482ms]
    Jan 12 16:18:31.814: INFO: Created: latency-svc-djh9c
    Jan 12 16:18:31.853: INFO: Got endpoints: latency-svc-tlfvl [752.412321ms]
    Jan 12 16:18:31.865: INFO: Created: latency-svc-hrjqd
    Jan 12 16:18:31.900: INFO: Got endpoints: latency-svc-dlbr7 [748.069369ms]
    Jan 12 16:18:31.910: INFO: Created: latency-svc-s4b7k
    Jan 12 16:18:31.953: INFO: Got endpoints: latency-svc-7lptw [752.500037ms]
    Jan 12 16:18:31.965: INFO: Created: latency-svc-gfjqp
    Jan 12 16:18:32.001: INFO: Got endpoints: latency-svc-557sw [750.324308ms]
    Jan 12 16:18:32.018: INFO: Created: latency-svc-79ktb
    Jan 12 16:18:32.050: INFO: Got endpoints: latency-svc-mzdhk [746.320175ms]
    Jan 12 16:18:32.067: INFO: Created: latency-svc-m6dlj
    Jan 12 16:18:32.108: INFO: Got endpoints: latency-svc-6dx65 [756.635585ms]
    Jan 12 16:18:32.118: INFO: Created: latency-svc-bjlq7
    Jan 12 16:18:32.153: INFO: Got endpoints: latency-svc-qppbq [750.242682ms]
    Jan 12 16:18:32.163: INFO: Created: latency-svc-j22n5
    Jan 12 16:18:32.201: INFO: Got endpoints: latency-svc-bt2vd [739.614387ms]
    Jan 12 16:18:32.216: INFO: Created: latency-svc-59jxg
    Jan 12 16:18:32.250: INFO: Got endpoints: latency-svc-q4qwx [745.244338ms]
    Jan 12 16:18:32.261: INFO: Created: latency-svc-rbm5x
    Jan 12 16:18:32.300: INFO: Got endpoints: latency-svc-flmg6 [750.006613ms]
    Jan 12 16:18:32.312: INFO: Created: latency-svc-dq244
    Jan 12 16:18:32.351: INFO: Got endpoints: latency-svc-9xrpc [749.362617ms]
    Jan 12 16:18:32.361: INFO: Created: latency-svc-mlfp2
    Jan 12 16:18:32.400: INFO: Got endpoints: latency-svc-nn5cr [750.247056ms]
    Jan 12 16:18:32.418: INFO: Created: latency-svc-gtwxd
    Jan 12 16:18:32.451: INFO: Got endpoints: latency-svc-w7qpx [748.402194ms]
    Jan 12 16:18:32.460: INFO: Created: latency-svc-5f5ww
    Jan 12 16:18:32.503: INFO: Got endpoints: latency-svc-8wp26 [752.537268ms]
    Jan 12 16:18:32.513: INFO: Created: latency-svc-w6bvv
    Jan 12 16:18:32.550: INFO: Got endpoints: latency-svc-djh9c [748.846671ms]
    Jan 12 16:18:32.562: INFO: Created: latency-svc-mgnpn
    Jan 12 16:18:32.601: INFO: Got endpoints: latency-svc-hrjqd [747.870933ms]
    Jan 12 16:18:32.613: INFO: Created: latency-svc-fgjhv
    Jan 12 16:18:32.650: INFO: Got endpoints: latency-svc-s4b7k [750.013245ms]
    Jan 12 16:18:32.662: INFO: Created: latency-svc-5v26g
    Jan 12 16:18:32.701: INFO: Got endpoints: latency-svc-gfjqp [748.265244ms]
    Jan 12 16:18:32.714: INFO: Created: latency-svc-f6c8s
    Jan 12 16:18:32.757: INFO: Got endpoints: latency-svc-79ktb [756.240484ms]
    Jan 12 16:18:32.768: INFO: Created: latency-svc-lvkb2
    Jan 12 16:18:32.800: INFO: Got endpoints: latency-svc-m6dlj [749.774763ms]
    Jan 12 16:18:32.814: INFO: Created: latency-svc-qr6p9
    Jan 12 16:18:32.852: INFO: Got endpoints: latency-svc-bjlq7 [744.177948ms]
    Jan 12 16:18:32.869: INFO: Created: latency-svc-wncl5
    Jan 12 16:18:32.901: INFO: Got endpoints: latency-svc-j22n5 [747.830533ms]
    Jan 12 16:18:32.910: INFO: Created: latency-svc-bjx96
    Jan 12 16:18:32.951: INFO: Got endpoints: latency-svc-59jxg [749.970055ms]
    Jan 12 16:18:32.960: INFO: Created: latency-svc-ll964
    Jan 12 16:18:33.000: INFO: Got endpoints: latency-svc-rbm5x [750.016689ms]
    Jan 12 16:18:33.013: INFO: Created: latency-svc-svlg6
    Jan 12 16:18:33.050: INFO: Got endpoints: latency-svc-dq244 [749.233172ms]
    Jan 12 16:18:33.060: INFO: Created: latency-svc-q7vp5
    Jan 12 16:18:33.100: INFO: Got endpoints: latency-svc-mlfp2 [748.491951ms]
    Jan 12 16:18:33.111: INFO: Created: latency-svc-pds9t
    Jan 12 16:18:33.151: INFO: Got endpoints: latency-svc-gtwxd [751.002715ms]
    Jan 12 16:18:33.161: INFO: Created: latency-svc-gmjgv
    Jan 12 16:18:33.201: INFO: Got endpoints: latency-svc-5f5ww [749.360828ms]
    Jan 12 16:18:33.214: INFO: Created: latency-svc-v5grq
    Jan 12 16:18:33.253: INFO: Got endpoints: latency-svc-w6bvv [750.185923ms]
    Jan 12 16:18:33.263: INFO: Created: latency-svc-cz9bn
    Jan 12 16:18:33.301: INFO: Got endpoints: latency-svc-mgnpn [750.649927ms]
    Jan 12 16:18:33.312: INFO: Created: latency-svc-z7b76
    Jan 12 16:18:33.357: INFO: Got endpoints: latency-svc-fgjhv [756.598588ms]
    Jan 12 16:18:33.368: INFO: Created: latency-svc-bbtxx
    Jan 12 16:18:33.404: INFO: Got endpoints: latency-svc-5v26g [754.521823ms]
    Jan 12 16:18:33.417: INFO: Created: latency-svc-hqm62
    Jan 12 16:18:33.452: INFO: Got endpoints: latency-svc-f6c8s [751.435644ms]
    Jan 12 16:18:33.462: INFO: Created: latency-svc-259nr
    Jan 12 16:18:33.503: INFO: Got endpoints: latency-svc-lvkb2 [745.716962ms]
    Jan 12 16:18:33.522: INFO: Created: latency-svc-2cssw
    Jan 12 16:18:33.550: INFO: Got endpoints: latency-svc-qr6p9 [750.109527ms]
    Jan 12 16:18:33.569: INFO: Created: latency-svc-nd56b
    Jan 12 16:18:33.600: INFO: Got endpoints: latency-svc-wncl5 [747.366373ms]
    Jan 12 16:18:33.612: INFO: Created: latency-svc-mrpz6
    Jan 12 16:18:33.649: INFO: Got endpoints: latency-svc-bjx96 [748.593252ms]
    Jan 12 16:18:33.659: INFO: Created: latency-svc-ls8nr
    Jan 12 16:18:33.700: INFO: Got endpoints: latency-svc-ll964 [749.547442ms]
    Jan 12 16:18:33.712: INFO: Created: latency-svc-gwwnz
    Jan 12 16:18:33.750: INFO: Got endpoints: latency-svc-svlg6 [750.163052ms]
    Jan 12 16:18:33.764: INFO: Created: latency-svc-tvd4j
    Jan 12 16:18:33.802: INFO: Got endpoints: latency-svc-q7vp5 [751.875484ms]
    Jan 12 16:18:33.812: INFO: Created: latency-svc-rll95
    Jan 12 16:18:33.850: INFO: Got endpoints: latency-svc-pds9t [750.463385ms]
    Jan 12 16:18:33.860: INFO: Created: latency-svc-cxbsl
    Jan 12 16:18:33.903: INFO: Got endpoints: latency-svc-gmjgv [751.242764ms]
    Jan 12 16:18:33.912: INFO: Created: latency-svc-ldkl6
    Jan 12 16:18:33.950: INFO: Got endpoints: latency-svc-v5grq [749.210085ms]
    Jan 12 16:18:33.964: INFO: Created: latency-svc-lj7b4
    Jan 12 16:18:34.002: INFO: Got endpoints: latency-svc-cz9bn [749.122314ms]
    Jan 12 16:18:34.012: INFO: Created: latency-svc-mx575
    Jan 12 16:18:34.051: INFO: Got endpoints: latency-svc-z7b76 [750.264237ms]
    Jan 12 16:18:34.061: INFO: Created: latency-svc-h5j5d
    Jan 12 16:18:34.100: INFO: Got endpoints: latency-svc-bbtxx [742.551316ms]
    Jan 12 16:18:34.111: INFO: Created: latency-svc-295ck
    Jan 12 16:18:34.150: INFO: Got endpoints: latency-svc-hqm62 [745.36419ms]
    Jan 12 16:18:34.170: INFO: Created: latency-svc-2nbfh
    Jan 12 16:18:34.202: INFO: Got endpoints: latency-svc-259nr [749.021801ms]
    Jan 12 16:18:34.213: INFO: Created: latency-svc-xkswf
    Jan 12 16:18:34.250: INFO: Got endpoints: latency-svc-2cssw [747.327322ms]
    Jan 12 16:18:34.269: INFO: Created: latency-svc-z8thn
    Jan 12 16:18:34.303: INFO: Got endpoints: latency-svc-nd56b [753.277524ms]
    Jan 12 16:18:34.318: INFO: Created: latency-svc-kv6fb
    Jan 12 16:18:34.350: INFO: Got endpoints: latency-svc-mrpz6 [749.753883ms]
    Jan 12 16:18:34.363: INFO: Created: latency-svc-jhfzz
    Jan 12 16:18:34.402: INFO: Got endpoints: latency-svc-ls8nr [752.447454ms]
    Jan 12 16:18:34.413: INFO: Created: latency-svc-9kzjp
    Jan 12 16:18:34.456: INFO: Got endpoints: latency-svc-gwwnz [755.298795ms]
    Jan 12 16:18:34.466: INFO: Created: latency-svc-fmbgn
    Jan 12 16:18:34.500: INFO: Got endpoints: latency-svc-tvd4j [749.209096ms]
    Jan 12 16:18:34.521: INFO: Created: latency-svc-hv2xl
    Jan 12 16:18:34.556: INFO: Got endpoints: latency-svc-rll95 [754.23758ms]
    Jan 12 16:18:34.570: INFO: Created: latency-svc-cjkpv
    Jan 12 16:18:34.600: INFO: Got endpoints: latency-svc-cxbsl [749.982712ms]
    Jan 12 16:18:34.611: INFO: Created: latency-svc-qvwdx
    Jan 12 16:18:34.652: INFO: Got endpoints: latency-svc-ldkl6 [749.42801ms]
    Jan 12 16:18:34.663: INFO: Created: latency-svc-zxchc
    Jan 12 16:18:34.701: INFO: Got endpoints: latency-svc-lj7b4 [750.54772ms]
    Jan 12 16:18:34.711: INFO: Created: latency-svc-g8rwt
    Jan 12 16:18:34.753: INFO: Got endpoints: latency-svc-mx575 [750.352736ms]
    Jan 12 16:18:34.767: INFO: Created: latency-svc-2f5sd
    Jan 12 16:18:34.809: INFO: Got endpoints: latency-svc-h5j5d [758.271423ms]
    Jan 12 16:18:34.822: INFO: Created: latency-svc-dz44x
    Jan 12 16:18:34.851: INFO: Got endpoints: latency-svc-295ck [751.299212ms]
    Jan 12 16:18:34.863: INFO: Created: latency-svc-p99mn
    Jan 12 16:18:34.904: INFO: Got endpoints: latency-svc-2nbfh [754.50557ms]
    Jan 12 16:18:34.920: INFO: Created: latency-svc-wb2hz
    Jan 12 16:18:34.951: INFO: Got endpoints: latency-svc-xkswf [748.916435ms]
    Jan 12 16:18:34.966: INFO: Created: latency-svc-brtj5
    Jan 12 16:18:35.005: INFO: Got endpoints: latency-svc-z8thn [754.368836ms]
    Jan 12 16:18:35.026: INFO: Created: latency-svc-9xqcx
    Jan 12 16:18:35.051: INFO: Got endpoints: latency-svc-kv6fb [747.555494ms]
    Jan 12 16:18:35.062: INFO: Created: latency-svc-5nvbw
    Jan 12 16:18:35.103: INFO: Got endpoints: latency-svc-jhfzz [753.828993ms]
    Jan 12 16:18:35.115: INFO: Created: latency-svc-9xd7f
    Jan 12 16:18:35.151: INFO: Got endpoints: latency-svc-9kzjp [748.573724ms]
    Jan 12 16:18:35.163: INFO: Created: latency-svc-jqw9w
    Jan 12 16:18:35.201: INFO: Got endpoints: latency-svc-fmbgn [744.963206ms]
    Jan 12 16:18:35.211: INFO: Created: latency-svc-458wt
    Jan 12 16:18:35.250: INFO: Got endpoints: latency-svc-hv2xl [750.373307ms]
    Jan 12 16:18:35.260: INFO: Created: latency-svc-499pb
    Jan 12 16:18:35.300: INFO: Got endpoints: latency-svc-cjkpv [743.731501ms]
    Jan 12 16:18:35.351: INFO: Got endpoints: latency-svc-qvwdx [750.516212ms]
    Jan 12 16:18:35.401: INFO: Got endpoints: latency-svc-zxchc [748.285464ms]
    Jan 12 16:18:35.451: INFO: Got endpoints: latency-svc-g8rwt [750.621497ms]
    Jan 12 16:18:35.503: INFO: Got endpoints: latency-svc-2f5sd [750.623833ms]
    Jan 12 16:18:35.550: INFO: Got endpoints: latency-svc-dz44x [741.086857ms]
    Jan 12 16:18:35.604: INFO: Got endpoints: latency-svc-p99mn [753.172707ms]
    Jan 12 16:18:35.649: INFO: Got endpoints: latency-svc-wb2hz [745.064332ms]
    Jan 12 16:18:35.703: INFO: Got endpoints: latency-svc-brtj5 [751.917867ms]
    Jan 12 16:18:35.750: INFO: Got endpoints: latency-svc-9xqcx [744.81812ms]
    Jan 12 16:18:35.802: INFO: Got endpoints: latency-svc-5nvbw [751.208386ms]
    Jan 12 16:18:35.851: INFO: Got endpoints: latency-svc-9xd7f [747.360153ms]
    Jan 12 16:18:35.902: INFO: Got endpoints: latency-svc-jqw9w [751.635393ms]
    Jan 12 16:18:35.949: INFO: Got endpoints: latency-svc-458wt [748.557323ms]
    Jan 12 16:18:36.002: INFO: Got endpoints: latency-svc-499pb [751.918454ms]
    Jan 12 16:18:36.002: INFO: Latencies: [18.566074ms 21.686332ms 33.187376ms 50.214011ms 60.054419ms 68.472066ms 87.173654ms 96.759333ms 110.656339ms 117.028694ms 130.787083ms 136.126747ms 146.467481ms 155.623364ms 158.338328ms 159.108805ms 160.73499ms 162.730942ms 163.447618ms 164.510233ms 164.766888ms 166.023101ms 172.371107ms 172.517996ms 172.684422ms 173.86753ms 174.58696ms 174.631538ms 175.519511ms 176.390994ms 177.204522ms 178.135975ms 179.406488ms 180.185849ms 193.050025ms 193.264397ms 195.626443ms 195.813961ms 227.856243ms 271.229746ms 311.975596ms 338.918105ms 378.063759ms 417.165169ms 459.14226ms 504.140538ms 526.068505ms 559.594899ms 594.189718ms 622.768451ms 662.968277ms 705.012596ms 738.757792ms 739.614387ms 741.086857ms 742.551316ms 743.731501ms 744.177948ms 744.81812ms 744.889336ms 744.963206ms 745.064332ms 745.244338ms 745.25834ms 745.36419ms 745.716962ms 746.27539ms 746.320175ms 747.070816ms 747.312595ms 747.327322ms 747.360153ms 747.366373ms 747.383826ms 747.402253ms 747.463824ms 747.546855ms 747.555494ms 747.750482ms 747.785957ms 747.830533ms 747.870933ms 748.018057ms 748.069369ms 748.122089ms 748.265244ms 748.285464ms 748.322044ms 748.402194ms 748.491951ms 748.496279ms 748.557323ms 748.573724ms 748.593252ms 748.641656ms 748.846671ms 748.898972ms 748.916435ms 749.021801ms 749.036163ms 749.122314ms 749.209096ms 749.210085ms 749.217965ms 749.233172ms 749.360828ms 749.362617ms 749.398042ms 749.42801ms 749.487723ms 749.491362ms 749.502352ms 749.547442ms 749.599331ms 749.666794ms 749.695168ms 749.716756ms 749.753883ms 749.774066ms 749.774763ms 749.777819ms 749.803293ms 749.970055ms 749.982712ms 750.006613ms 750.013245ms 750.016689ms 750.109527ms 750.13267ms 750.134063ms 750.163052ms 750.185923ms 750.235018ms 750.237019ms 750.242682ms 750.247056ms 750.264237ms 750.324308ms 750.352736ms 750.361982ms 750.368358ms 750.373307ms 750.405326ms 750.424014ms 750.463385ms 750.505384ms 750.516212ms 750.54772ms 750.583841ms 750.621497ms 750.623833ms 750.649927ms 750.725302ms 750.887858ms 750.89151ms 750.892749ms 750.97466ms 751.002715ms 751.109421ms 751.115313ms 751.183881ms 751.204638ms 751.208386ms 751.242764ms 751.255788ms 751.299212ms 751.313054ms 751.435644ms 751.532787ms 751.610361ms 751.635393ms 751.875484ms 751.917867ms 751.918454ms 752.034357ms 752.153892ms 752.205094ms 752.230363ms 752.412321ms 752.447454ms 752.500037ms 752.526636ms 752.537268ms 752.774553ms 752.828382ms 753.172707ms 753.277524ms 753.828993ms 753.945358ms 754.23758ms 754.368836ms 754.50557ms 754.521823ms 755.298795ms 755.591704ms 756.230896ms 756.240484ms 756.598588ms 756.635585ms 758.271423ms]
    Jan 12 16:18:36.002: INFO: 50 %ile: 749.122314ms
    Jan 12 16:18:36.002: INFO: 90 %ile: 752.500037ms
    Jan 12 16:18:36.002: INFO: 99 %ile: 756.635585ms
    Jan 12 16:18:36.002: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:18:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-3246" for this suite. 01/12/23 16:18:36.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:18:36.01
Jan 12 16:18:36.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:18:36.01
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:18:36.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:18:36.02
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3138 01/12/23 16:18:36.023
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3138 01/12/23 16:18:36.026
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3138 01/12/23 16:18:36.03
Jan 12 16:18:36.032: INFO: Found 0 stateful pods, waiting for 1
Jan 12 16:18:46.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/12/23 16:18:46.034
Jan 12 16:18:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:18:46.158: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:18:46.158: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:18:46.158: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:18:46.160: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 12 16:18:56.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:18:56.165: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:18:56.176: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 12 16:18:56.176: INFO: ss-0  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
Jan 12 16:18:56.176: INFO: 
Jan 12 16:18:56.176: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 12 16:18:57.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998199132s
Jan 12 16:18:58.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995556505s
Jan 12 16:18:59.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991887742s
Jan 12 16:19:00.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989114379s
Jan 12 16:19:01.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986485683s
Jan 12 16:19:02.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983856829s
Jan 12 16:19:03.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981506318s
Jan 12 16:19:04.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977933883s
Jan 12 16:19:05.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 975.086672ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3138 01/12/23 16:19:06.202
Jan 12 16:19:06.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:19:06.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:19:06.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:19:06.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:19:06.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:19:06.453: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 12 16:19:06.453: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:19:06.453: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:19:06.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:19:06.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 12 16:19:06.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:19:06.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 12 16:19:06.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:19:06.576: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:19:06.576: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/12/23 16:19:06.576
Jan 12 16:19:06.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:19:06.694: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:19:06.694: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:19:06.694: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:19:06.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:19:06.825: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:19:06.825: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:19:06.825: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:19:06.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:19:06.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:19:06.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:19:06.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:19:06.941: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:19:06.943: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 12 16:19:16.948: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:19:16.948: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:19:16.948: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 12 16:19:16.955: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 12 16:19:16.955: INFO: ss-0  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
Jan 12 16:19:16.956: INFO: ss-1  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
Jan 12 16:19:16.956: INFO: ss-2  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
Jan 12 16:19:16.956: INFO: 
Jan 12 16:19:16.956: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 12 16:19:17.958: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Jan 12 16:19:17.958: INFO: ss-0  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
Jan 12 16:19:17.958: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
Jan 12 16:19:17.958: INFO: 
Jan 12 16:19:17.958: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 12 16:19:18.961: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.994498443s
Jan 12 16:19:19.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992426135s
Jan 12 16:19:20.966: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990172848s
Jan 12 16:19:21.968: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.987645029s
Jan 12 16:19:22.970: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985271695s
Jan 12 16:19:23.973: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.983147094s
Jan 12 16:19:24.975: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980839829s
Jan 12 16:19:25.977: INFO: Verifying statefulset ss doesn't scale past 0 for another 978.690705ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3138 01/12/23 16:19:26.977
Jan 12 16:19:26.980: INFO: Scaling statefulset ss to 0
Jan 12 16:19:26.986: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:19:26.987: INFO: Deleting all statefulset in ns statefulset-3138
Jan 12 16:19:26.989: INFO: Scaling statefulset ss to 0
Jan 12 16:19:26.994: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:19:26.995: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:27.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3138" for this suite. 01/12/23 16:19:27.01
------------------------------
 [SLOW TEST] [51.004 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:18:36.01
    Jan 12 16:18:36.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:18:36.01
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:18:36.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:18:36.02
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3138 01/12/23 16:18:36.023
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3138 01/12/23 16:18:36.026
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3138 01/12/23 16:18:36.03
    Jan 12 16:18:36.032: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 16:18:46.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/12/23 16:18:46.034
    Jan 12 16:18:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:18:46.158: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:18:46.158: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:18:46.158: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:18:46.160: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 12 16:18:56.165: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:18:56.165: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:18:56.176: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
    Jan 12 16:18:56.176: INFO: ss-0  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:46 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
    Jan 12 16:18:56.176: INFO: 
    Jan 12 16:18:56.176: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 12 16:18:57.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998199132s
    Jan 12 16:18:58.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995556505s
    Jan 12 16:18:59.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991887742s
    Jan 12 16:19:00.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989114379s
    Jan 12 16:19:01.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986485683s
    Jan 12 16:19:02.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983856829s
    Jan 12 16:19:03.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981506318s
    Jan 12 16:19:04.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977933883s
    Jan 12 16:19:05.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 975.086672ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3138 01/12/23 16:19:06.202
    Jan 12 16:19:06.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:19:06.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:19:06.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:19:06.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:19:06.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:19:06.453: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 12 16:19:06.453: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:19:06.453: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:19:06.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:19:06.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 12 16:19:06.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:19:06.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 12 16:19:06.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:19:06.576: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:19:06.576: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/12/23 16:19:06.576
    Jan 12 16:19:06.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:19:06.694: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:19:06.694: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:19:06.694: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:19:06.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:19:06.825: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:19:06.825: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:19:06.825: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:19:06.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3138 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:19:06.941: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:19:06.941: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:19:06.941: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:19:06.941: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:19:06.943: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 12 16:19:16.948: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:19:16.948: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:19:16.948: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 12 16:19:16.955: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
    Jan 12 16:19:16.955: INFO: ss-0  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
    Jan 12 16:19:16.956: INFO: ss-1  worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
    Jan 12 16:19:16.956: INFO: ss-2  worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
    Jan 12 16:19:16.956: INFO: 
    Jan 12 16:19:16.956: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 12 16:19:17.958: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
    Jan 12 16:19:17.958: INFO: ss-0  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:36 +0000 UTC  }]
    Jan 12 16:19:17.958: INFO: ss-2  worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:19:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:18:56 +0000 UTC  }]
    Jan 12 16:19:17.958: INFO: 
    Jan 12 16:19:17.958: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 12 16:19:18.961: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.994498443s
    Jan 12 16:19:19.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992426135s
    Jan 12 16:19:20.966: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990172848s
    Jan 12 16:19:21.968: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.987645029s
    Jan 12 16:19:22.970: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.985271695s
    Jan 12 16:19:23.973: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.983147094s
    Jan 12 16:19:24.975: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.980839829s
    Jan 12 16:19:25.977: INFO: Verifying statefulset ss doesn't scale past 0 for another 978.690705ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3138 01/12/23 16:19:26.977
    Jan 12 16:19:26.980: INFO: Scaling statefulset ss to 0
    Jan 12 16:19:26.986: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:19:26.987: INFO: Deleting all statefulset in ns statefulset-3138
    Jan 12 16:19:26.989: INFO: Scaling statefulset ss to 0
    Jan 12 16:19:26.994: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:19:26.995: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:27.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3138" for this suite. 01/12/23 16:19:27.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:27.014
Jan 12 16:19:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:19:27.015
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:27.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:27.027
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:27.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8635" for this suite. 01/12/23 16:19:27.046
------------------------------
 [0.035 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:27.014
    Jan 12 16:19:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:19:27.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:27.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:27.027
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:27.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8635" for this suite. 01/12/23 16:19:27.046
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:27.05
Jan 12 16:19:27.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context-test 01/12/23 16:19:27.051
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:27.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:27.062
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 12 16:19:27.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294" in namespace "security-context-test-455" to be "Succeeded or Failed"
Jan 12 16:19:27.070: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844344ms
Jan 12 16:19:29.073: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004324687s
Jan 12 16:19:31.074: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005738489s
Jan 12 16:19:31.074: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-455" for this suite. 01/12/23 16:19:31.078
------------------------------
 [4.032 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:27.05
    Jan 12 16:19:27.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context-test 01/12/23 16:19:27.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:27.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:27.062
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 12 16:19:27.068: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294" in namespace "security-context-test-455" to be "Succeeded or Failed"
    Jan 12 16:19:27.070: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844344ms
    Jan 12 16:19:29.073: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004324687s
    Jan 12 16:19:31.074: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005738489s
    Jan 12 16:19:31.074: INFO: Pod "busybox-user-65534-0797b3e6-6b66-498c-b0b1-f772e532e294" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-455" for this suite. 01/12/23 16:19:31.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:31.085
Jan 12 16:19:31.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename endpointslice 01/12/23 16:19:31.086
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.096
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 12 16:19:31.103: INFO: Endpoints addresses: [10.0.37.42] , ports: [6443]
Jan 12 16:19:31.103: INFO: EndpointSlices addresses: [10.0.37.42] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2129" for this suite. 01/12/23 16:19:31.105
------------------------------
 [0.025 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:31.085
    Jan 12 16:19:31.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename endpointslice 01/12/23 16:19:31.086
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.096
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 12 16:19:31.103: INFO: Endpoints addresses: [10.0.37.42] , ports: [6443]
    Jan 12 16:19:31.103: INFO: EndpointSlices addresses: [10.0.37.42] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2129" for this suite. 01/12/23 16:19:31.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:31.11
Jan 12 16:19:31.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename tables 01/12/23 16:19:31.111
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.121
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-8997" for this suite. 01/12/23 16:19:31.127
------------------------------
 [0.022 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:31.11
    Jan 12 16:19:31.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename tables 01/12/23 16:19:31.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.121
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-8997" for this suite. 01/12/23 16:19:31.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:31.134
Jan 12 16:19:31.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:19:31.135
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.143
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:19:31.145
Jan 12 16:19:31.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c" in namespace "downward-api-6128" to be "Succeeded or Failed"
Jan 12 16:19:31.153: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596045ms
Jan 12 16:19:33.156: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004653058s
Jan 12 16:19:35.157: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004901806s
STEP: Saw pod success 01/12/23 16:19:35.157
Jan 12 16:19:35.157: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c" satisfied condition "Succeeded or Failed"
Jan 12 16:19:35.158: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c container client-container: <nil>
STEP: delete the pod 01/12/23 16:19:35.168
Jan 12 16:19:35.179: INFO: Waiting for pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c to disappear
Jan 12 16:19:35.180: INFO: Pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:35.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6128" for this suite. 01/12/23 16:19:35.183
------------------------------
 [4.053 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:31.134
    Jan 12 16:19:31.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:19:31.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:31.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:31.143
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:19:31.145
    Jan 12 16:19:31.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c" in namespace "downward-api-6128" to be "Succeeded or Failed"
    Jan 12 16:19:31.153: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596045ms
    Jan 12 16:19:33.156: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004653058s
    Jan 12 16:19:35.157: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004901806s
    STEP: Saw pod success 01/12/23 16:19:35.157
    Jan 12 16:19:35.157: INFO: Pod "downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c" satisfied condition "Succeeded or Failed"
    Jan 12 16:19:35.158: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c container client-container: <nil>
    STEP: delete the pod 01/12/23 16:19:35.168
    Jan 12 16:19:35.179: INFO: Waiting for pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c to disappear
    Jan 12 16:19:35.180: INFO: Pod downwardapi-volume-f250a3ac-121c-4048-911c-5868a4904e3c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:35.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6128" for this suite. 01/12/23 16:19:35.183
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:35.188
Jan 12 16:19:35.188: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:19:35.189
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:35.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:35.2
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 16:19:35.202
Jan 12 16:19:35.206: INFO: Waiting up to 5m0s for pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec" in namespace "emptydir-5862" to be "Succeeded or Failed"
Jan 12 16:19:35.208: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.954117ms
Jan 12 16:19:37.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004794756s
Jan 12 16:19:39.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004882708s
STEP: Saw pod success 01/12/23 16:19:39.211
Jan 12 16:19:39.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec" satisfied condition "Succeeded or Failed"
Jan 12 16:19:39.213: INFO: Trying to get logs from node worker-1 pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec container test-container: <nil>
STEP: delete the pod 01/12/23 16:19:39.217
Jan 12 16:19:39.225: INFO: Waiting for pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec to disappear
Jan 12 16:19:39.227: INFO: Pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5862" for this suite. 01/12/23 16:19:39.229
------------------------------
 [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:35.188
    Jan 12 16:19:35.188: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:19:35.189
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:35.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:35.2
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/12/23 16:19:35.202
    Jan 12 16:19:35.206: INFO: Waiting up to 5m0s for pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec" in namespace "emptydir-5862" to be "Succeeded or Failed"
    Jan 12 16:19:35.208: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.954117ms
    Jan 12 16:19:37.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004794756s
    Jan 12 16:19:39.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004882708s
    STEP: Saw pod success 01/12/23 16:19:39.211
    Jan 12 16:19:39.211: INFO: Pod "pod-15f70486-032a-49de-98ae-1a38d5dbfeec" satisfied condition "Succeeded or Failed"
    Jan 12 16:19:39.213: INFO: Trying to get logs from node worker-1 pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec container test-container: <nil>
    STEP: delete the pod 01/12/23 16:19:39.217
    Jan 12 16:19:39.225: INFO: Waiting for pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec to disappear
    Jan 12 16:19:39.227: INFO: Pod pod-15f70486-032a-49de-98ae-1a38d5dbfeec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:39.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5862" for this suite. 01/12/23 16:19:39.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:39.233
Jan 12 16:19:39.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:19:39.234
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:39.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:39.243
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:19:39.245
Jan 12 16:19:39.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 12 16:19:39.310: INFO: stderr: ""
Jan 12 16:19:39.310: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/12/23 16:19:39.31
STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 16:19:44.361
Jan 12 16:19:44.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 get pod e2e-test-httpd-pod -o json'
Jan 12 16:19:44.422: INFO: stderr: ""
Jan 12 16:19:44.422: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-12T16:19:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4329\",\n        \"resourceVersion\": \"21072\",\n        \"uid\": \"af6677f9-23cb-40de-b245-d55f9d79df51\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h59g8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h59g8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://8f2411fdd7806762dd7c8b92ba97611a98b1b19d50d43089708249d5b4d31a8e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-12T16:19:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.40.50\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.34\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.34\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-12T16:19:39Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/12/23 16:19:44.423
Jan 12 16:19:44.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 replace -f -'
Jan 12 16:19:45.248: INFO: stderr: ""
Jan 12 16:19:45.248: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/12/23 16:19:45.248
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 12 16:19:45.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 delete pods e2e-test-httpd-pod'
Jan 12 16:19:47.095: INFO: stderr: ""
Jan 12 16:19:47.095: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:47.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4329" for this suite. 01/12/23 16:19:47.097
------------------------------
 [SLOW TEST] [7.868 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:39.233
    Jan 12 16:19:39.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:19:39.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:39.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:39.243
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:19:39.245
    Jan 12 16:19:39.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 12 16:19:39.310: INFO: stderr: ""
    Jan 12 16:19:39.310: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/12/23 16:19:39.31
    STEP: verifying the pod e2e-test-httpd-pod was created 01/12/23 16:19:44.361
    Jan 12 16:19:44.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 get pod e2e-test-httpd-pod -o json'
    Jan 12 16:19:44.422: INFO: stderr: ""
    Jan 12 16:19:44.422: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-12T16:19:39Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4329\",\n        \"resourceVersion\": \"21072\",\n        \"uid\": \"af6677f9-23cb-40de-b245-d55f9d79df51\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h59g8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h59g8\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-12T16:19:39Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://8f2411fdd7806762dd7c8b92ba97611a98b1b19d50d43089708249d5b4d31a8e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-12T16:19:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.40.50\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.34\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.34\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-12T16:19:39Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/12/23 16:19:44.423
    Jan 12 16:19:44.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 replace -f -'
    Jan 12 16:19:45.248: INFO: stderr: ""
    Jan 12 16:19:45.248: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/12/23 16:19:45.248
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 12 16:19:45.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4329 delete pods e2e-test-httpd-pod'
    Jan 12 16:19:47.095: INFO: stderr: ""
    Jan 12 16:19:47.095: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:47.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4329" for this suite. 01/12/23 16:19:47.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:47.102
Jan 12 16:19:47.102: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename endpointslice 01/12/23 16:19:47.102
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:47.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:47.116
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6306" for this suite. 01/12/23 16:19:49.157
------------------------------
 [2.059 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:47.102
    Jan 12 16:19:47.102: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename endpointslice 01/12/23 16:19:47.102
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:47.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:47.116
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6306" for this suite. 01/12/23 16:19:49.157
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:49.161
Jan 12 16:19:49.161: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename events 01/12/23 16:19:49.162
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:49.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:49.173
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/12/23 16:19:49.175
STEP: get a list of Events with a label in the current namespace 01/12/23 16:19:49.185
STEP: delete a list of events 01/12/23 16:19:49.187
Jan 12 16:19:49.187: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/12/23 16:19:49.198
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 12 16:19:49.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6954" for this suite. 01/12/23 16:19:49.202
------------------------------
 [0.045 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:49.161
    Jan 12 16:19:49.161: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename events 01/12/23 16:19:49.162
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:49.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:49.173
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/12/23 16:19:49.175
    STEP: get a list of Events with a label in the current namespace 01/12/23 16:19:49.185
    STEP: delete a list of events 01/12/23 16:19:49.187
    Jan 12 16:19:49.187: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/12/23 16:19:49.198
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:19:49.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6954" for this suite. 01/12/23 16:19:49.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:19:49.206
Jan 12 16:19:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename hostport 01/12/23 16:19:49.208
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:49.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:49.22
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/12/23 16:19:49.224
Jan 12 16:19:49.228: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3341" to be "running and ready"
Jan 12 16:19:49.230: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605382ms
Jan 12 16:19:49.230: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:19:51.233: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004100517s
Jan 12 16:19:51.233: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 16:19:51.233: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.40.50 on the node which pod1 resides and expect scheduled 01/12/23 16:19:51.233
Jan 12 16:19:51.236: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3341" to be "running and ready"
Jan 12 16:19:51.238: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689553ms
Jan 12 16:19:51.238: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:19:53.240: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004048155s
Jan 12 16:19:53.240: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 16:19:53.240: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.40.50 but use UDP protocol on the node which pod2 resides 01/12/23 16:19:53.24
Jan 12 16:19:53.246: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3341" to be "running and ready"
Jan 12 16:19:53.249: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.902578ms
Jan 12 16:19:53.249: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:19:55.252: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00549031s
Jan 12 16:19:55.252: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 12 16:19:55.252: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 12 16:19:55.257: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3341" to be "running and ready"
Jan 12 16:19:55.259: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.938644ms
Jan 12 16:19:55.259: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:19:57.262: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004275282s
Jan 12 16:19:57.262: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 12 16:19:57.262: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/12/23 16:19:57.263
Jan 12 16:19:57.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.40.50 http://127.0.0.1:54323/hostname] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:19:57.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:19:57.264: INFO: ExecWithOptions: Clientset creation
Jan 12 16:19:57.264: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.40.50+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.40.50, port: 54323 01/12/23 16:19:57.332
Jan 12 16:19:57.332: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.40.50:54323/hostname] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:19:57.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:19:57.332: INFO: ExecWithOptions: Clientset creation
Jan 12 16:19:57.333: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.40.50%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.40.50, port: 54323 UDP 01/12/23 16:19:57.386
Jan 12 16:19:57.386: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.40.50 54323] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:19:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:19:57.387: INFO: ExecWithOptions: Clientset creation
Jan 12 16:19:57.387: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.40.50+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 12 16:20:02.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-3341" for this suite. 01/12/23 16:20:02.444
------------------------------
 [SLOW TEST] [13.243 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:19:49.206
    Jan 12 16:19:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename hostport 01/12/23 16:19:49.208
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:19:49.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:19:49.22
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/12/23 16:19:49.224
    Jan 12 16:19:49.228: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3341" to be "running and ready"
    Jan 12 16:19:49.230: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.605382ms
    Jan 12 16:19:49.230: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:19:51.233: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004100517s
    Jan 12 16:19:51.233: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 16:19:51.233: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.40.50 on the node which pod1 resides and expect scheduled 01/12/23 16:19:51.233
    Jan 12 16:19:51.236: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3341" to be "running and ready"
    Jan 12 16:19:51.238: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.689553ms
    Jan 12 16:19:51.238: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:19:53.240: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004048155s
    Jan 12 16:19:53.240: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 16:19:53.240: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.40.50 but use UDP protocol on the node which pod2 resides 01/12/23 16:19:53.24
    Jan 12 16:19:53.246: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3341" to be "running and ready"
    Jan 12 16:19:53.249: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.902578ms
    Jan 12 16:19:53.249: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:19:55.252: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00549031s
    Jan 12 16:19:55.252: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 12 16:19:55.252: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 12 16:19:55.257: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3341" to be "running and ready"
    Jan 12 16:19:55.259: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.938644ms
    Jan 12 16:19:55.259: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:19:57.262: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004275282s
    Jan 12 16:19:57.262: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 12 16:19:57.262: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/12/23 16:19:57.263
    Jan 12 16:19:57.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.40.50 http://127.0.0.1:54323/hostname] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:19:57.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:19:57.264: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:19:57.264: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.40.50+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.40.50, port: 54323 01/12/23 16:19:57.332
    Jan 12 16:19:57.332: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.40.50:54323/hostname] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:19:57.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:19:57.332: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:19:57.333: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.40.50%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.40.50, port: 54323 UDP 01/12/23 16:19:57.386
    Jan 12 16:19:57.386: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.40.50 54323] Namespace:hostport-3341 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:19:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:19:57.387: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:19:57.387: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3341/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.40.50+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:20:02.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-3341" for this suite. 01/12/23 16:20:02.444
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:20:02.45
Jan 12 16:20:02.450: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:20:02.451
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:02.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:02.463
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/12/23 16:20:02.465
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/12/23 16:20:02.466
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 16:20:02.466
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/12/23 16:20:02.466
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/12/23 16:20:02.467
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 16:20:02.467
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 16:20:02.468
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:20:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7092" for this suite. 01/12/23 16:20:02.47
------------------------------
 [0.027 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:20:02.45
    Jan 12 16:20:02.450: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:20:02.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:02.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:02.463
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/12/23 16:20:02.465
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/12/23 16:20:02.466
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 16:20:02.466
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/12/23 16:20:02.466
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/12/23 16:20:02.467
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 16:20:02.467
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/12/23 16:20:02.468
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:20:02.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7092" for this suite. 01/12/23 16:20:02.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:20:02.478
Jan 12 16:20:02.478: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename aggregator 01/12/23 16:20:02.479
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:02.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:02.488
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 12 16:20:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/12/23 16:20:02.491
Jan 12 16:20:02.834: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 12 16:20:04.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:06.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:08.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:10.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:12.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:14.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:16.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:18.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:20.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:22.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:24.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:20:26.990: INFO: Waited 118.383192ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/12/23 16:20:27.032
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/12/23 16:20:27.034
STEP: List APIServices 01/12/23 16:20:27.041
Jan 12 16:20:27.045: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 12 16:20:27.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2204" for this suite. 01/12/23 16:20:27.277
------------------------------
 [SLOW TEST] [24.850 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:20:02.478
    Jan 12 16:20:02.478: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename aggregator 01/12/23 16:20:02.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:02.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:02.488
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 12 16:20:02.491: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/12/23 16:20:02.491
    Jan 12 16:20:02.834: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 12 16:20:04.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:06.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:08.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:10.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:12.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:14.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:16.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:18.866: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:20.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:22.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:24.865: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 20, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:20:26.990: INFO: Waited 118.383192ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/12/23 16:20:27.032
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/12/23 16:20:27.034
    STEP: List APIServices 01/12/23 16:20:27.041
    Jan 12 16:20:27.045: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:20:27.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2204" for this suite. 01/12/23 16:20:27.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:20:27.33
Jan 12 16:20:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 16:20:27.331
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:27.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:27.342
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/12/23 16:20:27.344
STEP: When the matched label of one of its pods change 01/12/23 16:20:27.347
Jan 12 16:20:27.349: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 12 16:20:32.352: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/12/23 16:20:32.36
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:20:33.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3685" for this suite. 01/12/23 16:20:33.367
------------------------------
 [SLOW TEST] [6.041 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:20:27.33
    Jan 12 16:20:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 16:20:27.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:27.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:27.342
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/12/23 16:20:27.344
    STEP: When the matched label of one of its pods change 01/12/23 16:20:27.347
    Jan 12 16:20:27.349: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 12 16:20:32.352: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/12/23 16:20:32.36
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:20:33.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3685" for this suite. 01/12/23 16:20:33.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:20:33.371
Jan 12 16:20:33.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename init-container 01/12/23 16:20:33.372
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:33.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:33.382
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/12/23 16:20:33.384
Jan 12 16:20:33.384: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:20:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2704" for this suite. 01/12/23 16:20:38.705
------------------------------
 [SLOW TEST] [5.337 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:20:33.371
    Jan 12 16:20:33.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename init-container 01/12/23 16:20:33.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:33.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:33.382
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/12/23 16:20:33.384
    Jan 12 16:20:33.384: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:20:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2704" for this suite. 01/12/23 16:20:38.705
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:20:38.709
Jan 12 16:20:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-pred 01/12/23 16:20:38.71
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:38.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:38.722
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 16:20:38.724: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 16:20:38.728: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 16:20:38.729: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Jan 12 16:20:38.733: INFO: pod-init-1f085e6c-d132-4c3b-9e66-5da604e54ffb from init-container-2704 started at 2023-01-12 16:20:33 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container run1 ready: false, restart count 0
Jan 12 16:20:38.733: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:20:38.733: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:20:38.733: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:20:38.733: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:20:38.733: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container metrics-server ready: true, restart count 0
Jan 12 16:20:38.733: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container e2e ready: true, restart count 0
Jan 12 16:20:38.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:20:38.733: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:20:38.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:20:38.733: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 16:20:38.733: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Jan 12 16:20:38.736: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.736: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:20:38.736: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.737: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:20:38.737: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.737: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:20:38.737: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.737: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:20:38.737: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
Jan 12 16:20:38.737: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 16:20:38.737: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:20:38.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:20:38.737: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 16:20:38.737
Jan 12 16:20:38.743: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2258" to be "running"
Jan 12 16:20:38.744: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.609417ms
Jan 12 16:20:40.747: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004873517s
Jan 12 16:20:40.748: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 16:20:40.749
STEP: Trying to apply a random label on the found node. 01/12/23 16:20:40.76
STEP: verifying the node has the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f 95 01/12/23 16:20:40.767
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/12/23 16:20:40.769
Jan 12 16:20:40.773: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2258" to be "not pending"
Jan 12 16:20:40.775: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942862ms
Jan 12 16:20:42.778: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005143148s
Jan 12 16:20:42.778: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.40.50 on the node which pod4 resides and expect not scheduled 01/12/23 16:20:42.778
Jan 12 16:20:42.782: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2258" to be "not pending"
Jan 12 16:20:42.784: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136022ms
Jan 12 16:20:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005120391s
Jan 12 16:20:46.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006543792s
Jan 12 16:20:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005198427s
Jan 12 16:20:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006338892s
Jan 12 16:20:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005908227s
Jan 12 16:20:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005331241s
Jan 12 16:20:56.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005405343s
Jan 12 16:20:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005117093s
Jan 12 16:21:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006542888s
Jan 12 16:21:02.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005454167s
Jan 12 16:21:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005104632s
Jan 12 16:21:06.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00544842s
Jan 12 16:21:08.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004594037s
Jan 12 16:21:10.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004569045s
Jan 12 16:21:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005528865s
Jan 12 16:21:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005500913s
Jan 12 16:21:16.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004493246s
Jan 12 16:21:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00552295s
Jan 12 16:21:20.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00541555s
Jan 12 16:21:22.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005557384s
Jan 12 16:21:24.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004215133s
Jan 12 16:21:26.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005591447s
Jan 12 16:21:28.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004556037s
Jan 12 16:21:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005667181s
Jan 12 16:21:32.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00554421s
Jan 12 16:21:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004747422s
Jan 12 16:21:36.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004947261s
Jan 12 16:21:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005232599s
Jan 12 16:21:40.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0064793s
Jan 12 16:21:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005822244s
Jan 12 16:21:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005261328s
Jan 12 16:21:46.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004391802s
Jan 12 16:21:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005476814s
Jan 12 16:21:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005736955s
Jan 12 16:21:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005721689s
Jan 12 16:21:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00558001s
Jan 12 16:21:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006004054s
Jan 12 16:21:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004920743s
Jan 12 16:22:00.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005131873s
Jan 12 16:22:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006329598s
Jan 12 16:22:04.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004490848s
Jan 12 16:22:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005834378s
Jan 12 16:22:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005466646s
Jan 12 16:22:10.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005976331s
Jan 12 16:22:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005591493s
Jan 12 16:22:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005592349s
Jan 12 16:22:16.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005781381s
Jan 12 16:22:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004853619s
Jan 12 16:22:20.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006243329s
Jan 12 16:22:22.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006578159s
Jan 12 16:22:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004984086s
Jan 12 16:22:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006290476s
Jan 12 16:22:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005165413s
Jan 12 16:22:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005466055s
Jan 12 16:22:32.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006243656s
Jan 12 16:22:34.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004364799s
Jan 12 16:22:36.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005624705s
Jan 12 16:22:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004891822s
Jan 12 16:22:40.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006114034s
Jan 12 16:22:42.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006868698s
Jan 12 16:22:44.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005772659s
Jan 12 16:22:46.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005888415s
Jan 12 16:22:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005026016s
Jan 12 16:22:50.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005693159s
Jan 12 16:22:52.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004389596s
Jan 12 16:22:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005076627s
Jan 12 16:22:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006496715s
Jan 12 16:22:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004874243s
Jan 12 16:23:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006222712s
Jan 12 16:23:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006505121s
Jan 12 16:23:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005371372s
Jan 12 16:23:06.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005072659s
Jan 12 16:23:08.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004390209s
Jan 12 16:23:10.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005677024s
Jan 12 16:23:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005375961s
Jan 12 16:23:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005563489s
Jan 12 16:23:16.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004468472s
Jan 12 16:23:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005087068s
Jan 12 16:23:20.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004931152s
Jan 12 16:23:22.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005701433s
Jan 12 16:23:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005210319s
Jan 12 16:23:26.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006735599s
Jan 12 16:23:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00531568s
Jan 12 16:23:30.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005782402s
Jan 12 16:23:32.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005804073s
Jan 12 16:23:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005081929s
Jan 12 16:23:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006311513s
Jan 12 16:23:38.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004471624s
Jan 12 16:23:40.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005546943s
Jan 12 16:23:42.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005612193s
Jan 12 16:23:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005164337s
Jan 12 16:23:46.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006843775s
Jan 12 16:23:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005050977s
Jan 12 16:23:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006562636s
Jan 12 16:23:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005788189s
Jan 12 16:23:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005376229s
Jan 12 16:23:56.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00525183s
Jan 12 16:23:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005392877s
Jan 12 16:24:00.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005626625s
Jan 12 16:24:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005894713s
Jan 12 16:24:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005403609s
Jan 12 16:24:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005852566s
Jan 12 16:24:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004701517s
Jan 12 16:24:10.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006006727s
Jan 12 16:24:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005546293s
Jan 12 16:24:14.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.00580391s
Jan 12 16:24:16.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005560864s
Jan 12 16:24:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005456201s
Jan 12 16:24:20.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.0058363s
Jan 12 16:24:22.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.006802429s
Jan 12 16:24:24.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004667226s
Jan 12 16:24:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006232676s
Jan 12 16:24:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004910262s
Jan 12 16:24:30.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006395543s
Jan 12 16:24:32.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005484981s
Jan 12 16:24:34.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004648908s
Jan 12 16:24:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.006172017s
Jan 12 16:24:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004709338s
Jan 12 16:24:40.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004426731s
Jan 12 16:24:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005706744s
Jan 12 16:24:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005389571s
Jan 12 16:24:46.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004249026s
Jan 12 16:24:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004840219s
Jan 12 16:24:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006096892s
Jan 12 16:24:52.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00679818s
Jan 12 16:24:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005185824s
Jan 12 16:24:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006414088s
Jan 12 16:24:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004887902s
Jan 12 16:25:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00612881s
Jan 12 16:25:02.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00446376s
Jan 12 16:25:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004911526s
Jan 12 16:25:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.006384472s
Jan 12 16:25:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.005303961s
Jan 12 16:25:10.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004465406s
Jan 12 16:25:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005369995s
Jan 12 16:25:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005284693s
Jan 12 16:25:16.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005536774s
Jan 12 16:25:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004804371s
Jan 12 16:25:20.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004422883s
Jan 12 16:25:22.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005519788s
Jan 12 16:25:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004866516s
Jan 12 16:25:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006130026s
Jan 12 16:25:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004841862s
Jan 12 16:25:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005205464s
Jan 12 16:25:32.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00465774s
Jan 12 16:25:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005189144s
Jan 12 16:25:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005909929s
Jan 12 16:25:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005437863s
Jan 12 16:25:40.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005323647s
Jan 12 16:25:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005756605s
Jan 12 16:25:42.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007372962s
STEP: removing the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f off the node worker-1 01/12/23 16:25:42.789
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f 01/12/23 16:25:42.799
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:25:42.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2258" for this suite. 01/12/23 16:25:42.803
------------------------------
 [SLOW TEST] [304.098 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:20:38.709
    Jan 12 16:20:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-pred 01/12/23 16:20:38.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:20:38.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:20:38.722
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 16:20:38.724: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 16:20:38.728: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 16:20:38.729: INFO: 
    Logging pods the apiserver thinks is on node worker-0 before test
    Jan 12 16:20:38.733: INFO: pod-init-1f085e6c-d132-4c3b-9e66-5da604e54ffb from init-container-2704 started at 2023-01-12 16:20:33 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container run1 ready: false, restart count 0
    Jan 12 16:20:38.733: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:20:38.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 16:20:38.733: INFO: 
    Logging pods the apiserver thinks is on node worker-1 before test
    Jan 12 16:20:38.736: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.736: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:20:38.736: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.737: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:20:38.737: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.737: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:20:38.737: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.737: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:20:38.737: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
    Jan 12 16:20:38.737: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 16:20:38.737: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:20:38.737: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:20:38.737: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 16:20:38.737
    Jan 12 16:20:38.743: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2258" to be "running"
    Jan 12 16:20:38.744: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.609417ms
    Jan 12 16:20:40.747: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004873517s
    Jan 12 16:20:40.748: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 16:20:40.749
    STEP: Trying to apply a random label on the found node. 01/12/23 16:20:40.76
    STEP: verifying the node has the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f 95 01/12/23 16:20:40.767
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/12/23 16:20:40.769
    Jan 12 16:20:40.773: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2258" to be "not pending"
    Jan 12 16:20:40.775: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.942862ms
    Jan 12 16:20:42.778: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005143148s
    Jan 12 16:20:42.778: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.40.50 on the node which pod4 resides and expect not scheduled 01/12/23 16:20:42.778
    Jan 12 16:20:42.782: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2258" to be "not pending"
    Jan 12 16:20:42.784: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136022ms
    Jan 12 16:20:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005120391s
    Jan 12 16:20:46.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006543792s
    Jan 12 16:20:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005198427s
    Jan 12 16:20:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006338892s
    Jan 12 16:20:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005908227s
    Jan 12 16:20:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005331241s
    Jan 12 16:20:56.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005405343s
    Jan 12 16:20:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005117093s
    Jan 12 16:21:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006542888s
    Jan 12 16:21:02.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.005454167s
    Jan 12 16:21:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005104632s
    Jan 12 16:21:06.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.00544842s
    Jan 12 16:21:08.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004594037s
    Jan 12 16:21:10.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004569045s
    Jan 12 16:21:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005528865s
    Jan 12 16:21:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005500913s
    Jan 12 16:21:16.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004493246s
    Jan 12 16:21:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00552295s
    Jan 12 16:21:20.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00541555s
    Jan 12 16:21:22.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005557384s
    Jan 12 16:21:24.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004215133s
    Jan 12 16:21:26.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005591447s
    Jan 12 16:21:28.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004556037s
    Jan 12 16:21:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005667181s
    Jan 12 16:21:32.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00554421s
    Jan 12 16:21:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004747422s
    Jan 12 16:21:36.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004947261s
    Jan 12 16:21:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.005232599s
    Jan 12 16:21:40.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0064793s
    Jan 12 16:21:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005822244s
    Jan 12 16:21:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005261328s
    Jan 12 16:21:46.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.004391802s
    Jan 12 16:21:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005476814s
    Jan 12 16:21:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005736955s
    Jan 12 16:21:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005721689s
    Jan 12 16:21:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00558001s
    Jan 12 16:21:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.006004054s
    Jan 12 16:21:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004920743s
    Jan 12 16:22:00.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005131873s
    Jan 12 16:22:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006329598s
    Jan 12 16:22:04.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004490848s
    Jan 12 16:22:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005834378s
    Jan 12 16:22:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.005466646s
    Jan 12 16:22:10.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005976331s
    Jan 12 16:22:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005591493s
    Jan 12 16:22:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005592349s
    Jan 12 16:22:16.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005781381s
    Jan 12 16:22:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004853619s
    Jan 12 16:22:20.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006243329s
    Jan 12 16:22:22.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006578159s
    Jan 12 16:22:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004984086s
    Jan 12 16:22:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006290476s
    Jan 12 16:22:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005165413s
    Jan 12 16:22:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005466055s
    Jan 12 16:22:32.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006243656s
    Jan 12 16:22:34.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004364799s
    Jan 12 16:22:36.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005624705s
    Jan 12 16:22:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004891822s
    Jan 12 16:22:40.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006114034s
    Jan 12 16:22:42.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006868698s
    Jan 12 16:22:44.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005772659s
    Jan 12 16:22:46.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.005888415s
    Jan 12 16:22:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005026016s
    Jan 12 16:22:50.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005693159s
    Jan 12 16:22:52.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004389596s
    Jan 12 16:22:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.005076627s
    Jan 12 16:22:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006496715s
    Jan 12 16:22:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.004874243s
    Jan 12 16:23:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.006222712s
    Jan 12 16:23:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006505121s
    Jan 12 16:23:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005371372s
    Jan 12 16:23:06.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005072659s
    Jan 12 16:23:08.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004390209s
    Jan 12 16:23:10.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005677024s
    Jan 12 16:23:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005375961s
    Jan 12 16:23:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005563489s
    Jan 12 16:23:16.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004468472s
    Jan 12 16:23:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.005087068s
    Jan 12 16:23:20.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004931152s
    Jan 12 16:23:22.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005701433s
    Jan 12 16:23:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005210319s
    Jan 12 16:23:26.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006735599s
    Jan 12 16:23:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.00531568s
    Jan 12 16:23:30.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005782402s
    Jan 12 16:23:32.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005804073s
    Jan 12 16:23:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.005081929s
    Jan 12 16:23:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006311513s
    Jan 12 16:23:38.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004471624s
    Jan 12 16:23:40.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.005546943s
    Jan 12 16:23:42.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.005612193s
    Jan 12 16:23:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.005164337s
    Jan 12 16:23:46.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006843775s
    Jan 12 16:23:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005050977s
    Jan 12 16:23:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006562636s
    Jan 12 16:23:52.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005788189s
    Jan 12 16:23:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005376229s
    Jan 12 16:23:56.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.00525183s
    Jan 12 16:23:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005392877s
    Jan 12 16:24:00.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005626625s
    Jan 12 16:24:02.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005894713s
    Jan 12 16:24:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005403609s
    Jan 12 16:24:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005852566s
    Jan 12 16:24:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004701517s
    Jan 12 16:24:10.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006006727s
    Jan 12 16:24:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005546293s
    Jan 12 16:24:14.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.00580391s
    Jan 12 16:24:16.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005560864s
    Jan 12 16:24:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005456201s
    Jan 12 16:24:20.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.0058363s
    Jan 12 16:24:22.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.006802429s
    Jan 12 16:24:24.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004667226s
    Jan 12 16:24:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006232676s
    Jan 12 16:24:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.004910262s
    Jan 12 16:24:30.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006395543s
    Jan 12 16:24:32.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005484981s
    Jan 12 16:24:34.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004648908s
    Jan 12 16:24:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.006172017s
    Jan 12 16:24:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004709338s
    Jan 12 16:24:40.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004426731s
    Jan 12 16:24:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.005706744s
    Jan 12 16:24:44.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005389571s
    Jan 12 16:24:46.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.004249026s
    Jan 12 16:24:48.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004840219s
    Jan 12 16:24:50.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006096892s
    Jan 12 16:24:52.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.00679818s
    Jan 12 16:24:54.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.005185824s
    Jan 12 16:24:56.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006414088s
    Jan 12 16:24:58.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.004887902s
    Jan 12 16:25:00.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00612881s
    Jan 12 16:25:02.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00446376s
    Jan 12 16:25:04.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004911526s
    Jan 12 16:25:06.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.006384472s
    Jan 12 16:25:08.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.005303961s
    Jan 12 16:25:10.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004465406s
    Jan 12 16:25:12.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005369995s
    Jan 12 16:25:14.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005284693s
    Jan 12 16:25:16.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005536774s
    Jan 12 16:25:18.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004804371s
    Jan 12 16:25:20.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.004422883s
    Jan 12 16:25:22.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005519788s
    Jan 12 16:25:24.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004866516s
    Jan 12 16:25:26.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006130026s
    Jan 12 16:25:28.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004841862s
    Jan 12 16:25:30.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005205464s
    Jan 12 16:25:32.786: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00465774s
    Jan 12 16:25:34.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005189144s
    Jan 12 16:25:36.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005909929s
    Jan 12 16:25:38.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.005437863s
    Jan 12 16:25:40.787: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.005323647s
    Jan 12 16:25:42.788: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005756605s
    Jan 12 16:25:42.789: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007372962s
    STEP: removing the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f off the node worker-1 01/12/23 16:25:42.789
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-bbfc8f27-c5c4-45fa-8e69-35669e8dac2f 01/12/23 16:25:42.799
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:25:42.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2258" for this suite. 01/12/23 16:25:42.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:25:42.809
Jan 12 16:25:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:25:42.81
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:42.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:42.821
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:25:42.824
Jan 12 16:25:42.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5" in namespace "downward-api-1175" to be "Succeeded or Failed"
Jan 12 16:25:42.832: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576545ms
Jan 12 16:25:44.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00477522s
Jan 12 16:25:46.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005231355s
STEP: Saw pod success 01/12/23 16:25:46.836
Jan 12 16:25:46.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5" satisfied condition "Succeeded or Failed"
Jan 12 16:25:46.838: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 container client-container: <nil>
STEP: delete the pod 01/12/23 16:25:46.849
Jan 12 16:25:46.856: INFO: Waiting for pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 to disappear
Jan 12 16:25:46.858: INFO: Pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:25:46.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1175" for this suite. 01/12/23 16:25:46.86
------------------------------
 [4.055 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:25:42.809
    Jan 12 16:25:42.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:25:42.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:42.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:42.821
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:25:42.824
    Jan 12 16:25:42.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5" in namespace "downward-api-1175" to be "Succeeded or Failed"
    Jan 12 16:25:42.832: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576545ms
    Jan 12 16:25:44.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00477522s
    Jan 12 16:25:46.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005231355s
    STEP: Saw pod success 01/12/23 16:25:46.836
    Jan 12 16:25:46.836: INFO: Pod "downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5" satisfied condition "Succeeded or Failed"
    Jan 12 16:25:46.838: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:25:46.849
    Jan 12 16:25:46.856: INFO: Waiting for pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 to disappear
    Jan 12 16:25:46.858: INFO: Pod downwardapi-volume-e24fd1d5-6743-4a08-9d23-b32c46972ee5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:25:46.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1175" for this suite. 01/12/23 16:25:46.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:25:46.865
Jan 12 16:25:46.865: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 16:25:46.866
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:46.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:46.878
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 12 16:25:46.890: INFO: Create a RollingUpdate DaemonSet
Jan 12 16:25:46.894: INFO: Check that daemon pods launch on every node of the cluster
Jan 12 16:25:46.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:25:46.900: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:25:47.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:25:47.905: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:25:48.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:25:48.904: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 12 16:25:48.904: INFO: Update the DaemonSet to trigger a rollout
Jan 12 16:25:48.909: INFO: Updating DaemonSet daemon-set
Jan 12 16:25:50.918: INFO: Roll back the DaemonSet before rollout is complete
Jan 12 16:25:50.925: INFO: Updating DaemonSet daemon-set
Jan 12 16:25:50.925: INFO: Make sure DaemonSet rollback is complete
Jan 12 16:25:50.927: INFO: Wrong image for pod: daemon-set-vdhnq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 12 16:25:50.927: INFO: Pod daemon-set-vdhnq is not available
Jan 12 16:25:52.932: INFO: Pod daemon-set-bz47z is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:25:52.938
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3237, will wait for the garbage collector to delete the pods 01/12/23 16:25:52.938
Jan 12 16:25:52.994: INFO: Deleting DaemonSet.extensions daemon-set took: 4.506868ms
Jan 12 16:25:53.095: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.399716ms
Jan 12 16:25:54.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:25:54.897: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 16:25:54.899: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22298"},"items":null}

Jan 12 16:25:54.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22298"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:25:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3237" for this suite. 01/12/23 16:25:54.907
------------------------------
 [SLOW TEST] [8.046 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:25:46.865
    Jan 12 16:25:46.865: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 16:25:46.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:46.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:46.878
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 12 16:25:46.890: INFO: Create a RollingUpdate DaemonSet
    Jan 12 16:25:46.894: INFO: Check that daemon pods launch on every node of the cluster
    Jan 12 16:25:46.900: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:25:46.900: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:25:47.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:25:47.905: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:25:48.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:25:48.904: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 12 16:25:48.904: INFO: Update the DaemonSet to trigger a rollout
    Jan 12 16:25:48.909: INFO: Updating DaemonSet daemon-set
    Jan 12 16:25:50.918: INFO: Roll back the DaemonSet before rollout is complete
    Jan 12 16:25:50.925: INFO: Updating DaemonSet daemon-set
    Jan 12 16:25:50.925: INFO: Make sure DaemonSet rollback is complete
    Jan 12 16:25:50.927: INFO: Wrong image for pod: daemon-set-vdhnq. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 12 16:25:50.927: INFO: Pod daemon-set-vdhnq is not available
    Jan 12 16:25:52.932: INFO: Pod daemon-set-bz47z is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:25:52.938
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3237, will wait for the garbage collector to delete the pods 01/12/23 16:25:52.938
    Jan 12 16:25:52.994: INFO: Deleting DaemonSet.extensions daemon-set took: 4.506868ms
    Jan 12 16:25:53.095: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.399716ms
    Jan 12 16:25:54.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:25:54.897: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 16:25:54.899: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22298"},"items":null}

    Jan 12 16:25:54.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22298"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:25:54.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3237" for this suite. 01/12/23 16:25:54.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:25:54.912
Jan 12 16:25:54.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:25:54.913
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:54.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:54.923
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/12/23 16:25:54.925
STEP: Creating a ResourceQuota 01/12/23 16:25:59.927
STEP: Ensuring resource quota status is calculated 01/12/23 16:25:59.932
STEP: Creating a ReplicationController 01/12/23 16:26:01.935
STEP: Ensuring resource quota status captures replication controller creation 01/12/23 16:26:01.945
STEP: Deleting a ReplicationController 01/12/23 16:26:03.948
STEP: Ensuring resource quota status released usage 01/12/23 16:26:03.951
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:26:05.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-844" for this suite. 01/12/23 16:26:05.956
------------------------------
 [SLOW TEST] [11.047 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:25:54.912
    Jan 12 16:25:54.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:25:54.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:25:54.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:25:54.923
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/12/23 16:25:54.925
    STEP: Creating a ResourceQuota 01/12/23 16:25:59.927
    STEP: Ensuring resource quota status is calculated 01/12/23 16:25:59.932
    STEP: Creating a ReplicationController 01/12/23 16:26:01.935
    STEP: Ensuring resource quota status captures replication controller creation 01/12/23 16:26:01.945
    STEP: Deleting a ReplicationController 01/12/23 16:26:03.948
    STEP: Ensuring resource quota status released usage 01/12/23 16:26:03.951
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:26:05.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-844" for this suite. 01/12/23 16:26:05.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:26:05.962
Jan 12 16:26:05.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:26:05.963
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:26:05.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:26:05.973
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-c7403896-34b9-46a0-b6b7-4b7fe8f66f4a 01/12/23 16:26:05.975
STEP: Creating a pod to test consume configMaps 01/12/23 16:26:05.978
Jan 12 16:26:05.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7" in namespace "configmap-2436" to be "Succeeded or Failed"
Jan 12 16:26:05.984: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.680095ms
Jan 12 16:26:07.988: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005542639s
Jan 12 16:26:09.987: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004523502s
STEP: Saw pod success 01/12/23 16:26:09.987
Jan 12 16:26:09.987: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7" satisfied condition "Succeeded or Failed"
Jan 12 16:26:09.988: INFO: Trying to get logs from node worker-1 pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:26:10
Jan 12 16:26:10.008: INFO: Waiting for pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 to disappear
Jan 12 16:26:10.009: INFO: Pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:26:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2436" for this suite. 01/12/23 16:26:10.012
------------------------------
 [4.054 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:26:05.962
    Jan 12 16:26:05.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:26:05.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:26:05.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:26:05.973
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-c7403896-34b9-46a0-b6b7-4b7fe8f66f4a 01/12/23 16:26:05.975
    STEP: Creating a pod to test consume configMaps 01/12/23 16:26:05.978
    Jan 12 16:26:05.982: INFO: Waiting up to 5m0s for pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7" in namespace "configmap-2436" to be "Succeeded or Failed"
    Jan 12 16:26:05.984: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.680095ms
    Jan 12 16:26:07.988: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005542639s
    Jan 12 16:26:09.987: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004523502s
    STEP: Saw pod success 01/12/23 16:26:09.987
    Jan 12 16:26:09.987: INFO: Pod "pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7" satisfied condition "Succeeded or Failed"
    Jan 12 16:26:09.988: INFO: Trying to get logs from node worker-1 pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:26:10
    Jan 12 16:26:10.008: INFO: Waiting for pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 to disappear
    Jan 12 16:26:10.009: INFO: Pod pod-configmaps-ad75451c-1ae3-43ac-bb8a-9d6757d985e7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:26:10.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2436" for this suite. 01/12/23 16:26:10.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:26:10.016
Jan 12 16:26:10.016: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption 01/12/23 16:26:10.017
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:26:10.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:26:10.029
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 16:26:10.041: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 16:27:10.055: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:27:10.057
Jan 12 16:27:10.057: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 16:27:10.058
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:10.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:10.068
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan 12 16:27:10.081: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 12 16:27:10.083: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 12 16:27:10.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:27:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3335" for this suite. 01/12/23 16:27:10.127
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-156" for this suite. 01/12/23 16:27:10.131
------------------------------
 [SLOW TEST] [60.118 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:26:10.016
    Jan 12 16:26:10.016: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 16:26:10.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:26:10.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:26:10.029
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 16:26:10.041: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 16:27:10.055: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:27:10.057
    Jan 12 16:27:10.057: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption-path 01/12/23 16:27:10.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:10.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:10.068
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan 12 16:27:10.081: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 12 16:27:10.083: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:27:10.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:27:10.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3335" for this suite. 01/12/23 16:27:10.127
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-156" for this suite. 01/12/23 16:27:10.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:27:10.137
Jan 12 16:27:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:27:10.137
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:10.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:10.148
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5944 01/12/23 16:27:10.15
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[] 01/12/23 16:27:10.16
Jan 12 16:27:10.163: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 12 16:27:11.168: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5944 01/12/23 16:27:11.168
Jan 12 16:27:11.173: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5944" to be "running and ready"
Jan 12 16:27:11.174: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702534ms
Jan 12 16:27:11.174: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:27:13.177: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00494326s
Jan 12 16:27:13.178: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 12 16:27:13.178: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod1:[100]] 01/12/23 16:27:13.179
Jan 12 16:27:13.184: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5944 01/12/23 16:27:13.184
Jan 12 16:27:13.190: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5944" to be "running and ready"
Jan 12 16:27:13.191: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656819ms
Jan 12 16:27:13.191: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:27:15.195: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005492891s
Jan 12 16:27:15.195: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 12 16:27:15.195: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod1:[100] pod2:[101]] 01/12/23 16:27:15.197
Jan 12 16:27:15.205: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/12/23 16:27:15.205
Jan 12 16:27:15.206: INFO: Creating new exec pod
Jan 12 16:27:15.211: INFO: Waiting up to 5m0s for pod "execpod4mp2g" in namespace "services-5944" to be "running"
Jan 12 16:27:15.213: INFO: Pod "execpod4mp2g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075967ms
Jan 12 16:27:17.217: INFO: Pod "execpod4mp2g": Phase="Running", Reason="", readiness=true. Elapsed: 2.005532852s
Jan 12 16:27:17.217: INFO: Pod "execpod4mp2g" satisfied condition "running"
Jan 12 16:27:18.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 12 16:27:18.338: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 12 16:27:18.338: INFO: stdout: ""
Jan 12 16:27:18.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 10.106.177.17 80'
Jan 12 16:27:18.462: INFO: stderr: "+ nc -v -z -w 2 10.106.177.17 80\nConnection to 10.106.177.17 80 port [tcp/http] succeeded!\n"
Jan 12 16:27:18.462: INFO: stdout: ""
Jan 12 16:27:18.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 12 16:27:18.574: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 12 16:27:18.574: INFO: stdout: ""
Jan 12 16:27:18.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 10.106.177.17 81'
Jan 12 16:27:18.681: INFO: stderr: "+ nc -v -z -w 2 10.106.177.17 81\nConnection to 10.106.177.17 81 port [tcp/*] succeeded!\n"
Jan 12 16:27:18.681: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5944 01/12/23 16:27:18.681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod2:[101]] 01/12/23 16:27:18.687
Jan 12 16:27:18.697: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5944 01/12/23 16:27:18.697
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[] 01/12/23 16:27:18.707
Jan 12 16:27:18.715: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:27:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5944" for this suite. 01/12/23 16:27:18.732
------------------------------
 [SLOW TEST] [8.599 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:27:10.137
    Jan 12 16:27:10.137: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:27:10.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:10.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:10.148
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5944 01/12/23 16:27:10.15
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[] 01/12/23 16:27:10.16
    Jan 12 16:27:10.163: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 12 16:27:11.168: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5944 01/12/23 16:27:11.168
    Jan 12 16:27:11.173: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5944" to be "running and ready"
    Jan 12 16:27:11.174: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702534ms
    Jan 12 16:27:11.174: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:27:13.177: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00494326s
    Jan 12 16:27:13.178: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 12 16:27:13.178: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod1:[100]] 01/12/23 16:27:13.179
    Jan 12 16:27:13.184: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5944 01/12/23 16:27:13.184
    Jan 12 16:27:13.190: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5944" to be "running and ready"
    Jan 12 16:27:13.191: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.656819ms
    Jan 12 16:27:13.191: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:27:15.195: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005492891s
    Jan 12 16:27:15.195: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 12 16:27:15.195: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod1:[100] pod2:[101]] 01/12/23 16:27:15.197
    Jan 12 16:27:15.205: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/12/23 16:27:15.205
    Jan 12 16:27:15.206: INFO: Creating new exec pod
    Jan 12 16:27:15.211: INFO: Waiting up to 5m0s for pod "execpod4mp2g" in namespace "services-5944" to be "running"
    Jan 12 16:27:15.213: INFO: Pod "execpod4mp2g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075967ms
    Jan 12 16:27:17.217: INFO: Pod "execpod4mp2g": Phase="Running", Reason="", readiness=true. Elapsed: 2.005532852s
    Jan 12 16:27:17.217: INFO: Pod "execpod4mp2g" satisfied condition "running"
    Jan 12 16:27:18.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 12 16:27:18.338: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 12 16:27:18.338: INFO: stdout: ""
    Jan 12 16:27:18.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 10.106.177.17 80'
    Jan 12 16:27:18.462: INFO: stderr: "+ nc -v -z -w 2 10.106.177.17 80\nConnection to 10.106.177.17 80 port [tcp/http] succeeded!\n"
    Jan 12 16:27:18.462: INFO: stdout: ""
    Jan 12 16:27:18.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 12 16:27:18.574: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 12 16:27:18.574: INFO: stdout: ""
    Jan 12 16:27:18.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-5944 exec execpod4mp2g -- /bin/sh -x -c nc -v -z -w 2 10.106.177.17 81'
    Jan 12 16:27:18.681: INFO: stderr: "+ nc -v -z -w 2 10.106.177.17 81\nConnection to 10.106.177.17 81 port [tcp/*] succeeded!\n"
    Jan 12 16:27:18.681: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5944 01/12/23 16:27:18.681
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[pod2:[101]] 01/12/23 16:27:18.687
    Jan 12 16:27:18.697: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5944 01/12/23 16:27:18.697
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5944 to expose endpoints map[] 01/12/23 16:27:18.707
    Jan 12 16:27:18.715: INFO: successfully validated that service multi-endpoint-test in namespace services-5944 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:27:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5944" for this suite. 01/12/23 16:27:18.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:27:18.738
Jan 12 16:27:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:27:18.74
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:18.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:18.75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 12 16:27:18.762: INFO: created pod pod-service-account-defaultsa
Jan 12 16:27:18.762: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 12 16:27:18.768: INFO: created pod pod-service-account-mountsa
Jan 12 16:27:18.768: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 12 16:27:18.775: INFO: created pod pod-service-account-nomountsa
Jan 12 16:27:18.775: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 12 16:27:18.779: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 12 16:27:18.779: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 12 16:27:18.787: INFO: created pod pod-service-account-mountsa-mountspec
Jan 12 16:27:18.787: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 12 16:27:18.793: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 12 16:27:18.793: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 12 16:27:18.799: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 12 16:27:18.799: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 12 16:27:18.805: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 12 16:27:18.805: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 12 16:27:18.812: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 12 16:27:18.812: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 16:27:18.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4174" for this suite. 01/12/23 16:27:18.817
------------------------------
 [0.085 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:27:18.738
    Jan 12 16:27:18.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:27:18.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:18.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:18.75
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 12 16:27:18.762: INFO: created pod pod-service-account-defaultsa
    Jan 12 16:27:18.762: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 12 16:27:18.768: INFO: created pod pod-service-account-mountsa
    Jan 12 16:27:18.768: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 12 16:27:18.775: INFO: created pod pod-service-account-nomountsa
    Jan 12 16:27:18.775: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 12 16:27:18.779: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 12 16:27:18.779: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 12 16:27:18.787: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 12 16:27:18.787: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 12 16:27:18.793: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 12 16:27:18.793: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 12 16:27:18.799: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 12 16:27:18.799: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 12 16:27:18.805: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 12 16:27:18.805: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 12 16:27:18.812: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 12 16:27:18.812: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:27:18.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4174" for this suite. 01/12/23 16:27:18.817
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:27:18.823
Jan 12 16:27:18.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-watch 01/12/23 16:27:18.825
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:18.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:18.839
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 12 16:27:18.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Creating first CR  01/12/23 16:27:21.382
Jan 12 16:27:21.386: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:21Z]] name:name1 resourceVersion:22684 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/12/23 16:27:31.388
Jan 12 16:27:31.392: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:31Z]] name:name2 resourceVersion:22761 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/12/23 16:27:41.393
Jan 12 16:27:41.398: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:41Z]] name:name1 resourceVersion:22782 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/12/23 16:27:51.401
Jan 12 16:27:51.406: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:51Z]] name:name2 resourceVersion:22804 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/12/23 16:28:01.408
Jan 12 16:28:01.413: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:41Z]] name:name1 resourceVersion:22825 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/12/23 16:28:11.415
Jan 12 16:28:11.420: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:51Z]] name:name2 resourceVersion:22846 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:28:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-5436" for this suite. 01/12/23 16:28:21.931
------------------------------
 [SLOW TEST] [63.113 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:27:18.823
    Jan 12 16:27:18.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-watch 01/12/23 16:27:18.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:27:18.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:27:18.839
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 12 16:27:18.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Creating first CR  01/12/23 16:27:21.382
    Jan 12 16:27:21.386: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:21Z]] name:name1 resourceVersion:22684 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/12/23 16:27:31.388
    Jan 12 16:27:31.392: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:31Z]] name:name2 resourceVersion:22761 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/12/23 16:27:41.393
    Jan 12 16:27:41.398: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:41Z]] name:name1 resourceVersion:22782 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/12/23 16:27:51.401
    Jan 12 16:27:51.406: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:51Z]] name:name2 resourceVersion:22804 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/12/23 16:28:01.408
    Jan 12 16:28:01.413: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:41Z]] name:name1 resourceVersion:22825 uid:7b2e3bcf-054d-4286-a1d3-c90b7542e4ca] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/12/23 16:28:11.415
    Jan 12 16:28:11.420: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-12T16:27:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-12T16:27:51Z]] name:name2 resourceVersion:22846 uid:60dde51e-5de7-4ab8-954e-9c07c814b8e9] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:28:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-5436" for this suite. 01/12/23 16:28:21.931
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:28:21.938
Jan 12 16:28:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename endpointslice 01/12/23 16:28:21.938
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:28:21.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:28:21.949
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/12/23 16:28:27.02
STEP: referencing matching pods with named port 01/12/23 16:28:32.025
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/12/23 16:28:37.029
STEP: recreating EndpointSlices after they've been deleted 01/12/23 16:28:42.036
Jan 12 16:28:42.047: INFO: EndpointSlice for Service endpointslice-9642/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 12 16:28:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9642" for this suite. 01/12/23 16:28:52.057
------------------------------
 [SLOW TEST] [30.123 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:28:21.938
    Jan 12 16:28:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename endpointslice 01/12/23 16:28:21.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:28:21.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:28:21.949
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/12/23 16:28:27.02
    STEP: referencing matching pods with named port 01/12/23 16:28:32.025
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/12/23 16:28:37.029
    STEP: recreating EndpointSlices after they've been deleted 01/12/23 16:28:42.036
    Jan 12 16:28:42.047: INFO: EndpointSlice for Service endpointslice-9642/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:28:52.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9642" for this suite. 01/12/23 16:28:52.057
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:28:52.062
Jan 12 16:28:52.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:28:52.063
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:28:52.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:28:52.075
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-4806 01/12/23 16:28:52.077
STEP: creating service affinity-clusterip in namespace services-4806 01/12/23 16:28:52.077
STEP: creating replication controller affinity-clusterip in namespace services-4806 01/12/23 16:28:52.088
I0112 16:28:52.091603      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4806, replica count: 3
I0112 16:28:55.143677      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 12 16:28:55.147: INFO: Creating new exec pod
Jan 12 16:28:55.152: INFO: Waiting up to 5m0s for pod "execpod-affinitycvt52" in namespace "services-4806" to be "running"
Jan 12 16:28:55.154: INFO: Pod "execpod-affinitycvt52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610249ms
Jan 12 16:28:57.157: INFO: Pod "execpod-affinitycvt52": Phase="Running", Reason="", readiness=true. Elapsed: 2.00466191s
Jan 12 16:28:57.157: INFO: Pod "execpod-affinitycvt52" satisfied condition "running"
Jan 12 16:28:58.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 12 16:28:58.280: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 12 16:28:58.280: INFO: stdout: ""
Jan 12 16:28:58.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c nc -v -z -w 2 10.102.194.116 80'
Jan 12 16:28:58.412: INFO: stderr: "+ nc -v -z -w 2 10.102.194.116 80\nConnection to 10.102.194.116 80 port [tcp/http] succeeded!\n"
Jan 12 16:28:58.412: INFO: stdout: ""
Jan 12 16:28:58.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.194.116:80/ ; done'
Jan 12 16:28:58.586: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n"
Jan 12 16:28:58.586: INFO: stdout: "\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj"
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
Jan 12 16:28:58.586: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4806, will wait for the garbage collector to delete the pods 01/12/23 16:28:58.592
Jan 12 16:28:58.648: INFO: Deleting ReplicationController affinity-clusterip took: 3.560061ms
Jan 12 16:28:58.749: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.064632ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:00.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4806" for this suite. 01/12/23 16:29:00.764
------------------------------
 [SLOW TEST] [8.705 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:28:52.062
    Jan 12 16:28:52.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:28:52.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:28:52.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:28:52.075
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-4806 01/12/23 16:28:52.077
    STEP: creating service affinity-clusterip in namespace services-4806 01/12/23 16:28:52.077
    STEP: creating replication controller affinity-clusterip in namespace services-4806 01/12/23 16:28:52.088
    I0112 16:28:52.091603      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4806, replica count: 3
    I0112 16:28:55.143677      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 12 16:28:55.147: INFO: Creating new exec pod
    Jan 12 16:28:55.152: INFO: Waiting up to 5m0s for pod "execpod-affinitycvt52" in namespace "services-4806" to be "running"
    Jan 12 16:28:55.154: INFO: Pod "execpod-affinitycvt52": Phase="Pending", Reason="", readiness=false. Elapsed: 1.610249ms
    Jan 12 16:28:57.157: INFO: Pod "execpod-affinitycvt52": Phase="Running", Reason="", readiness=true. Elapsed: 2.00466191s
    Jan 12 16:28:57.157: INFO: Pod "execpod-affinitycvt52" satisfied condition "running"
    Jan 12 16:28:58.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 12 16:28:58.280: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 12 16:28:58.280: INFO: stdout: ""
    Jan 12 16:28:58.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c nc -v -z -w 2 10.102.194.116 80'
    Jan 12 16:28:58.412: INFO: stderr: "+ nc -v -z -w 2 10.102.194.116 80\nConnection to 10.102.194.116 80 port [tcp/http] succeeded!\n"
    Jan 12 16:28:58.412: INFO: stdout: ""
    Jan 12 16:28:58.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-4806 exec execpod-affinitycvt52 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.194.116:80/ ; done'
    Jan 12 16:28:58.586: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.194.116:80/\n"
    Jan 12 16:28:58.586: INFO: stdout: "\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj\naffinity-clusterip-q7mrj"
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Received response from host: affinity-clusterip-q7mrj
    Jan 12 16:28:58.586: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4806, will wait for the garbage collector to delete the pods 01/12/23 16:28:58.592
    Jan 12 16:28:58.648: INFO: Deleting ReplicationController affinity-clusterip took: 3.560061ms
    Jan 12 16:28:58.749: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.064632ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:00.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4806" for this suite. 01/12/23 16:29:00.764
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:00.767
Jan 12 16:29:00.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename proxy 01/12/23 16:29:00.768
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:00.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:00.78
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 12 16:29:00.783: INFO: Creating pod...
Jan 12 16:29:00.787: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7349" to be "running"
Jan 12 16:29:00.789: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022773ms
Jan 12 16:29:02.792: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005271015s
Jan 12 16:29:02.792: INFO: Pod "agnhost" satisfied condition "running"
Jan 12 16:29:02.792: INFO: Creating service...
Jan 12 16:29:02.805: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=DELETE
Jan 12 16:29:02.810: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 16:29:02.810: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=OPTIONS
Jan 12 16:29:02.813: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 16:29:02.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=PATCH
Jan 12 16:29:02.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 16:29:02.815: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=POST
Jan 12 16:29:02.818: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 16:29:02.818: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=PUT
Jan 12 16:29:02.820: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 16:29:02.820: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 12 16:29:02.825: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 12 16:29:02.825: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 12 16:29:02.828: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 12 16:29:02.828: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 12 16:29:02.831: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 12 16:29:02.831: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=POST
Jan 12 16:29:02.834: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 12 16:29:02.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=PUT
Jan 12 16:29:02.837: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 12 16:29:02.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=GET
Jan 12 16:29:02.839: INFO: http.Client request:GET StatusCode:301
Jan 12 16:29:02.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=GET
Jan 12 16:29:02.841: INFO: http.Client request:GET StatusCode:301
Jan 12 16:29:02.841: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=HEAD
Jan 12 16:29:02.842: INFO: http.Client request:HEAD StatusCode:301
Jan 12 16:29:02.842: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 12 16:29:02.844: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7349" for this suite. 01/12/23 16:29:02.846
------------------------------
 [2.082 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:00.767
    Jan 12 16:29:00.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename proxy 01/12/23 16:29:00.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:00.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:00.78
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 12 16:29:00.783: INFO: Creating pod...
    Jan 12 16:29:00.787: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7349" to be "running"
    Jan 12 16:29:00.789: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022773ms
    Jan 12 16:29:02.792: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005271015s
    Jan 12 16:29:02.792: INFO: Pod "agnhost" satisfied condition "running"
    Jan 12 16:29:02.792: INFO: Creating service...
    Jan 12 16:29:02.805: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=DELETE
    Jan 12 16:29:02.810: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 16:29:02.810: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=OPTIONS
    Jan 12 16:29:02.813: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 16:29:02.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=PATCH
    Jan 12 16:29:02.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 16:29:02.815: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=POST
    Jan 12 16:29:02.818: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 16:29:02.818: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=PUT
    Jan 12 16:29:02.820: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 16:29:02.820: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 12 16:29:02.825: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 12 16:29:02.825: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 12 16:29:02.828: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 12 16:29:02.828: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 12 16:29:02.831: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 12 16:29:02.831: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=POST
    Jan 12 16:29:02.834: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 12 16:29:02.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 12 16:29:02.837: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 12 16:29:02.837: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=GET
    Jan 12 16:29:02.839: INFO: http.Client request:GET StatusCode:301
    Jan 12 16:29:02.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=GET
    Jan 12 16:29:02.841: INFO: http.Client request:GET StatusCode:301
    Jan 12 16:29:02.841: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/pods/agnhost/proxy?method=HEAD
    Jan 12 16:29:02.842: INFO: http.Client request:HEAD StatusCode:301
    Jan 12 16:29:02.842: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7349/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 12 16:29:02.844: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:02.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7349" for this suite. 01/12/23 16:29:02.846
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:02.85
Jan 12 16:29:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:29:02.851
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:02.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:02.86
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-82d2fc8a-b814-4a9d-9ee4-70df3e02d8cd 01/12/23 16:29:02.863
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:02.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3557" for this suite. 01/12/23 16:29:02.866
------------------------------
 [0.020 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:02.85
    Jan 12 16:29:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:29:02.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:02.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:02.86
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-82d2fc8a-b814-4a9d-9ee4-70df3e02d8cd 01/12/23 16:29:02.863
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:02.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3557" for this suite. 01/12/23 16:29:02.866
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:02.87
Jan 12 16:29:02.871: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context-test 01/12/23 16:29:02.871
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:02.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:02.883
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 12 16:29:02.890: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f" in namespace "security-context-test-6476" to be "Succeeded or Failed"
Jan 12 16:29:02.892: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027653ms
Jan 12 16:29:04.895: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005180902s
Jan 12 16:29:06.895: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005933004s
Jan 12 16:29:06.896: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6476" for this suite. 01/12/23 16:29:06.898
------------------------------
 [4.030 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:02.87
    Jan 12 16:29:02.871: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context-test 01/12/23 16:29:02.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:02.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:02.883
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 12 16:29:02.890: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f" in namespace "security-context-test-6476" to be "Succeeded or Failed"
    Jan 12 16:29:02.892: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027653ms
    Jan 12 16:29:04.895: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005180902s
    Jan 12 16:29:06.895: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005933004s
    Jan 12 16:29:06.896: INFO: Pod "busybox-readonly-false-52ee0a51-820a-47e8-9d1f-a55d5596b65f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:06.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6476" for this suite. 01/12/23 16:29:06.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:06.902
Jan 12 16:29:06.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:29:06.903
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:06.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:06.915
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/12/23 16:29:06.917
Jan 12 16:29:06.924: INFO: Waiting up to 5m0s for pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b" in namespace "downward-api-5622" to be "Succeeded or Failed"
Jan 12 16:29:06.925: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748561ms
Jan 12 16:29:08.929: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004788569s
Jan 12 16:29:10.928: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004120382s
STEP: Saw pod success 01/12/23 16:29:10.928
Jan 12 16:29:10.928: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b" satisfied condition "Succeeded or Failed"
Jan 12 16:29:10.930: INFO: Trying to get logs from node worker-1 pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b container dapi-container: <nil>
STEP: delete the pod 01/12/23 16:29:10.941
Jan 12 16:29:10.948: INFO: Waiting for pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b to disappear
Jan 12 16:29:10.950: INFO: Pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5622" for this suite. 01/12/23 16:29:10.952
------------------------------
 [4.055 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:06.902
    Jan 12 16:29:06.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:29:06.903
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:06.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:06.915
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/12/23 16:29:06.917
    Jan 12 16:29:06.924: INFO: Waiting up to 5m0s for pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b" in namespace "downward-api-5622" to be "Succeeded or Failed"
    Jan 12 16:29:06.925: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748561ms
    Jan 12 16:29:08.929: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004788569s
    Jan 12 16:29:10.928: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004120382s
    STEP: Saw pod success 01/12/23 16:29:10.928
    Jan 12 16:29:10.928: INFO: Pod "downward-api-1d84dda5-998c-467e-bc15-99982244a70b" satisfied condition "Succeeded or Failed"
    Jan 12 16:29:10.930: INFO: Trying to get logs from node worker-1 pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b container dapi-container: <nil>
    STEP: delete the pod 01/12/23 16:29:10.941
    Jan 12 16:29:10.948: INFO: Waiting for pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b to disappear
    Jan 12 16:29:10.950: INFO: Pod downward-api-1d84dda5-998c-467e-bc15-99982244a70b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5622" for this suite. 01/12/23 16:29:10.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:10.964
Jan 12 16:29:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 16:29:10.965
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:10.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:10.975
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/12/23 16:29:10.977
STEP: Ensuring job reaches completions 01/12/23 16:29:10.982
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:22.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2846" for this suite. 01/12/23 16:29:22.988
------------------------------
 [SLOW TEST] [12.027 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:10.964
    Jan 12 16:29:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 16:29:10.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:10.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:10.975
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/12/23 16:29:10.977
    STEP: Ensuring job reaches completions 01/12/23 16:29:10.982
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:22.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2846" for this suite. 01/12/23 16:29:22.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:22.993
Jan 12 16:29:22.993: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:29:22.994
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:23.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:23.005
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:23.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2168" for this suite. 01/12/23 16:29:23.031
------------------------------
 [0.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:22.993
    Jan 12 16:29:22.993: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:29:22.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:23.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:23.005
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:23.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2168" for this suite. 01/12/23 16:29:23.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:23.036
Jan 12 16:29:23.036: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:29:23.037
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:23.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:23.045
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/12/23 16:29:23.047
Jan 12 16:29:23.047: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 12 16:29:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:23.859: INFO: stderr: ""
Jan 12 16:29:23.859: INFO: stdout: "service/agnhost-replica created\n"
Jan 12 16:29:23.859: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 12 16:29:23.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:24.090: INFO: stderr: ""
Jan 12 16:29:24.090: INFO: stdout: "service/agnhost-primary created\n"
Jan 12 16:29:24.090: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 12 16:29:24.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:24.311: INFO: stderr: ""
Jan 12 16:29:24.311: INFO: stdout: "service/frontend created\n"
Jan 12 16:29:24.311: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 12 16:29:24.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:24.533: INFO: stderr: ""
Jan 12 16:29:24.533: INFO: stdout: "deployment.apps/frontend created\n"
Jan 12 16:29:24.533: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 12 16:29:24.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:24.726: INFO: stderr: ""
Jan 12 16:29:24.726: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 12 16:29:24.726: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 12 16:29:24.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
Jan 12 16:29:24.934: INFO: stderr: ""
Jan 12 16:29:24.934: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/12/23 16:29:24.934
Jan 12 16:29:24.934: INFO: Waiting for all frontend pods to be Running.
Jan 12 16:29:29.985: INFO: Waiting for frontend to serve content.
Jan 12 16:29:29.994: INFO: Trying to add a new entry to the guestbook.
Jan 12 16:29:30.001: INFO: Verifying that added entry can be retrieved.
Jan 12 16:29:30.007: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/12/23 16:29:35.015
Jan 12 16:29:35.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.084: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.084: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 16:29:35.084
Jan 12 16:29:35.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.160: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.160: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 16:29:35.16
Jan 12 16:29:35.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.229: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.229: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 16:29:35.229
Jan 12 16:29:35.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.287: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.287: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 16:29:35.287
Jan 12 16:29:35.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.356: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/12/23 16:29:35.356
Jan 12 16:29:35.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
Jan 12 16:29:35.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:29:35.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4624" for this suite. 01/12/23 16:29:35.43
------------------------------
 [SLOW TEST] [12.403 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:23.036
    Jan 12 16:29:23.036: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:29:23.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:23.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:23.045
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/12/23 16:29:23.047
    Jan 12 16:29:23.047: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 12 16:29:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:23.859: INFO: stderr: ""
    Jan 12 16:29:23.859: INFO: stdout: "service/agnhost-replica created\n"
    Jan 12 16:29:23.859: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 12 16:29:23.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:24.090: INFO: stderr: ""
    Jan 12 16:29:24.090: INFO: stdout: "service/agnhost-primary created\n"
    Jan 12 16:29:24.090: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 12 16:29:24.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:24.311: INFO: stderr: ""
    Jan 12 16:29:24.311: INFO: stdout: "service/frontend created\n"
    Jan 12 16:29:24.311: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 12 16:29:24.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:24.533: INFO: stderr: ""
    Jan 12 16:29:24.533: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 12 16:29:24.533: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 12 16:29:24.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:24.726: INFO: stderr: ""
    Jan 12 16:29:24.726: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 12 16:29:24.726: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 12 16:29:24.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 create -f -'
    Jan 12 16:29:24.934: INFO: stderr: ""
    Jan 12 16:29:24.934: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/12/23 16:29:24.934
    Jan 12 16:29:24.934: INFO: Waiting for all frontend pods to be Running.
    Jan 12 16:29:29.985: INFO: Waiting for frontend to serve content.
    Jan 12 16:29:29.994: INFO: Trying to add a new entry to the guestbook.
    Jan 12 16:29:30.001: INFO: Verifying that added entry can be retrieved.
    Jan 12 16:29:30.007: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/12/23 16:29:35.015
    Jan 12 16:29:35.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.084: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.084: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 16:29:35.084
    Jan 12 16:29:35.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.160: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.160: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 16:29:35.16
    Jan 12 16:29:35.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.229: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.229: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 16:29:35.229
    Jan 12 16:29:35.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.287: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.287: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 16:29:35.287
    Jan 12 16:29:35.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.356: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.356: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/12/23 16:29:35.356
    Jan 12 16:29:35.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-4624 delete --grace-period=0 --force -f -'
    Jan 12 16:29:35.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:29:35.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4624" for this suite. 01/12/23 16:29:35.43
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:35.44
Jan 12 16:29:35.440: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:29:35.441
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:35.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:35.452
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/12/23 16:29:35.458
STEP: watching for Pod to be ready 01/12/23 16:29:35.463
Jan 12 16:29:35.465: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 12 16:29:35.467: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
Jan 12 16:29:35.477: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
Jan 12 16:29:36.743: INFO: Found Pod pod-test in namespace pods-6779 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/12/23 16:29:36.745
STEP: getting the Pod and ensuring that it's patched 01/12/23 16:29:36.753
STEP: replacing the Pod's status Ready condition to False 01/12/23 16:29:36.755
STEP: check the Pod again to ensure its Ready conditions are False 01/12/23 16:29:36.762
STEP: deleting the Pod via a Collection with a LabelSelector 01/12/23 16:29:36.763
STEP: watching for the Pod to be deleted 01/12/23 16:29:36.769
Jan 12 16:29:36.771: INFO: observed event type MODIFIED
Jan 12 16:29:38.751: INFO: observed event type MODIFIED
Jan 12 16:29:39.751: INFO: observed event type MODIFIED
Jan 12 16:29:39.756: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6779" for this suite. 01/12/23 16:29:39.763
------------------------------
 [4.327 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:35.44
    Jan 12 16:29:35.440: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:29:35.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:35.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:35.452
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/12/23 16:29:35.458
    STEP: watching for Pod to be ready 01/12/23 16:29:35.463
    Jan 12 16:29:35.465: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 12 16:29:35.467: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
    Jan 12 16:29:35.477: INFO: observed Pod pod-test in namespace pods-6779 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
    Jan 12 16:29:36.743: INFO: Found Pod pod-test in namespace pods-6779 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-12 16:29:35 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/12/23 16:29:36.745
    STEP: getting the Pod and ensuring that it's patched 01/12/23 16:29:36.753
    STEP: replacing the Pod's status Ready condition to False 01/12/23 16:29:36.755
    STEP: check the Pod again to ensure its Ready conditions are False 01/12/23 16:29:36.762
    STEP: deleting the Pod via a Collection with a LabelSelector 01/12/23 16:29:36.763
    STEP: watching for the Pod to be deleted 01/12/23 16:29:36.769
    Jan 12 16:29:36.771: INFO: observed event type MODIFIED
    Jan 12 16:29:38.751: INFO: observed event type MODIFIED
    Jan 12 16:29:39.751: INFO: observed event type MODIFIED
    Jan 12 16:29:39.756: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:39.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6779" for this suite. 01/12/23 16:29:39.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:39.768
Jan 12 16:29:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:29:39.769
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:39.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:39.781
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/12/23 16:29:39.783
STEP: Getting a ResourceQuota 01/12/23 16:29:39.785
STEP: Listing all ResourceQuotas with LabelSelector 01/12/23 16:29:39.787
STEP: Patching the ResourceQuota 01/12/23 16:29:39.791
STEP: Deleting a Collection of ResourceQuotas 01/12/23 16:29:39.796
STEP: Verifying the deleted ResourceQuota 01/12/23 16:29:39.801
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:39.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4938" for this suite. 01/12/23 16:29:39.805
------------------------------
 [0.040 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:39.768
    Jan 12 16:29:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:29:39.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:39.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:39.781
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/12/23 16:29:39.783
    STEP: Getting a ResourceQuota 01/12/23 16:29:39.785
    STEP: Listing all ResourceQuotas with LabelSelector 01/12/23 16:29:39.787
    STEP: Patching the ResourceQuota 01/12/23 16:29:39.791
    STEP: Deleting a Collection of ResourceQuotas 01/12/23 16:29:39.796
    STEP: Verifying the deleted ResourceQuota 01/12/23 16:29:39.801
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:39.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4938" for this suite. 01/12/23 16:29:39.805
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:39.808
Jan 12 16:29:39.808: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:29:39.809
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:39.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:39.82
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 16:29:39.822
Jan 12 16:29:39.829: INFO: Waiting up to 5m0s for pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788" in namespace "emptydir-2980" to be "Succeeded or Failed"
Jan 12 16:29:39.830: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64556ms
Jan 12 16:29:41.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440667s
Jan 12 16:29:43.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004473864s
STEP: Saw pod success 01/12/23 16:29:43.833
Jan 12 16:29:43.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788" satisfied condition "Succeeded or Failed"
Jan 12 16:29:43.835: INFO: Trying to get logs from node worker-1 pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 container test-container: <nil>
STEP: delete the pod 01/12/23 16:29:43.84
Jan 12 16:29:43.849: INFO: Waiting for pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 to disappear
Jan 12 16:29:43.850: INFO: Pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:29:43.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2980" for this suite. 01/12/23 16:29:43.853
------------------------------
 [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:39.808
    Jan 12 16:29:39.808: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:29:39.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:39.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:39.82
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 16:29:39.822
    Jan 12 16:29:39.829: INFO: Waiting up to 5m0s for pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788" in namespace "emptydir-2980" to be "Succeeded or Failed"
    Jan 12 16:29:39.830: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Pending", Reason="", readiness=false. Elapsed: 1.64556ms
    Jan 12 16:29:41.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440667s
    Jan 12 16:29:43.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004473864s
    STEP: Saw pod success 01/12/23 16:29:43.833
    Jan 12 16:29:43.833: INFO: Pod "pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788" satisfied condition "Succeeded or Failed"
    Jan 12 16:29:43.835: INFO: Trying to get logs from node worker-1 pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:29:43.84
    Jan 12 16:29:43.849: INFO: Waiting for pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 to disappear
    Jan 12 16:29:43.850: INFO: Pod pod-edb008c0-c4c8-45b5-b261-9ee9b7e82788 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:29:43.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2980" for this suite. 01/12/23 16:29:43.853
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:29:43.857
Jan 12 16:29:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:29:43.858
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:43.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:43.869
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3298 01/12/23 16:29:43.872
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/12/23 16:29:43.875
Jan 12 16:29:43.884: INFO: Found 0 stateful pods, waiting for 3
Jan 12 16:29:53.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:29:53.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:29:53.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 12 16:29:53.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:29:54.010: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:29:54.010: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:29:54.010: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 16:30:04.019
Jan 12 16:30:04.035: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/12/23 16:30:04.035
STEP: Updating Pods in reverse ordinal order 01/12/23 16:30:14.044
Jan 12 16:30:14.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:30:14.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:30:14.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:30:14.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/12/23 16:30:24.176
Jan 12 16:30:24.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 12 16:30:24.294: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 12 16:30:24.294: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 12 16:30:24.294: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 12 16:30:34.321: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/12/23 16:30:44.329
Jan 12 16:30:44.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 12 16:30:44.455: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 12 16:30:44.455: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 12 16:30:44.455: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:30:54.469: INFO: Deleting all statefulset in ns statefulset-3298
Jan 12 16:30:54.470: INFO: Scaling statefulset ss2 to 0
Jan 12 16:31:04.483: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:31:04.484: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:04.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3298" for this suite. 01/12/23 16:31:04.496
------------------------------
 [SLOW TEST] [80.644 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:29:43.857
    Jan 12 16:29:43.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:29:43.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:29:43.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:29:43.869
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3298 01/12/23 16:29:43.872
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/12/23 16:29:43.875
    Jan 12 16:29:43.884: INFO: Found 0 stateful pods, waiting for 3
    Jan 12 16:29:53.887: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:29:53.887: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:29:53.887: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 12 16:29:53.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:29:54.010: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:29:54.010: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:29:54.010: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/12/23 16:30:04.019
    Jan 12 16:30:04.035: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/12/23 16:30:04.035
    STEP: Updating Pods in reverse ordinal order 01/12/23 16:30:14.044
    Jan 12 16:30:14.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:30:14.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:30:14.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:30:14.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/12/23 16:30:24.176
    Jan 12 16:30:24.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 12 16:30:24.294: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 12 16:30:24.294: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 12 16:30:24.294: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 12 16:30:34.321: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/12/23 16:30:44.329
    Jan 12 16:30:44.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=statefulset-3298 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 12 16:30:44.455: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 12 16:30:44.455: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 12 16:30:44.455: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:30:54.469: INFO: Deleting all statefulset in ns statefulset-3298
    Jan 12 16:30:54.470: INFO: Scaling statefulset ss2 to 0
    Jan 12 16:31:04.483: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:31:04.484: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:04.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3298" for this suite. 01/12/23 16:31:04.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:04.504
Jan 12 16:31:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:31:04.505
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:04.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:04.518
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 12 16:31:04.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5430 version'
Jan 12 16:31:04.574: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 12 16:31:04.574: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0+k0s\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2023-01-12T15:00:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:04.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5430" for this suite. 01/12/23 16:31:04.577
------------------------------
 [0.077 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:04.504
    Jan 12 16:31:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:31:04.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:04.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:04.518
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 12 16:31:04.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5430 version'
    Jan 12 16:31:04.574: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 12 16:31:04.574: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0+k0s\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2023-01-12T15:00:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:04.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5430" for this suite. 01/12/23 16:31:04.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:04.582
Jan 12 16:31:04.582: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:31:04.582
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:04.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:04.591
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 12 16:31:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:31:06.003
Jan 12 16:31:06.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 create -f -'
Jan 12 16:31:06.580: INFO: stderr: ""
Jan 12 16:31:06.580: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 12 16:31:06.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 delete e2e-test-crd-publish-openapi-8294-crds test-cr'
Jan 12 16:31:06.643: INFO: stderr: ""
Jan 12 16:31:06.643: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 12 16:31:06.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 apply -f -'
Jan 12 16:31:06.826: INFO: stderr: ""
Jan 12 16:31:06.826: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 12 16:31:06.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 delete e2e-test-crd-publish-openapi-8294-crds test-cr'
Jan 12 16:31:06.886: INFO: stderr: ""
Jan 12 16:31:06.886: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/12/23 16:31:06.886
Jan 12 16:31:06.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 explain e2e-test-crd-publish-openapi-8294-crds'
Jan 12 16:31:07.057: INFO: stderr: ""
Jan 12 16:31:07.057: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8294-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:08.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9402" for this suite. 01/12/23 16:31:08.426
------------------------------
 [3.849 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:04.582
    Jan 12 16:31:04.582: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:31:04.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:04.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:04.591
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 12 16:31:04.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:31:06.003
    Jan 12 16:31:06.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 create -f -'
    Jan 12 16:31:06.580: INFO: stderr: ""
    Jan 12 16:31:06.580: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 12 16:31:06.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 delete e2e-test-crd-publish-openapi-8294-crds test-cr'
    Jan 12 16:31:06.643: INFO: stderr: ""
    Jan 12 16:31:06.643: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 12 16:31:06.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 apply -f -'
    Jan 12 16:31:06.826: INFO: stderr: ""
    Jan 12 16:31:06.826: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 12 16:31:06.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 --namespace=crd-publish-openapi-9402 delete e2e-test-crd-publish-openapi-8294-crds test-cr'
    Jan 12 16:31:06.886: INFO: stderr: ""
    Jan 12 16:31:06.886: INFO: stdout: "e2e-test-crd-publish-openapi-8294-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/12/23 16:31:06.886
    Jan 12 16:31:06.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-9402 explain e2e-test-crd-publish-openapi-8294-crds'
    Jan 12 16:31:07.057: INFO: stderr: ""
    Jan 12 16:31:07.057: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8294-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:08.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9402" for this suite. 01/12/23 16:31:08.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:08.434
Jan 12 16:31:08.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 16:31:08.434
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:08.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:08.446
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/12/23 16:31:08.448
Jan 12 16:31:08.452: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1080" to be "running and ready"
Jan 12 16:31:08.454: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800158ms
Jan 12 16:31:08.454: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:31:10.457: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004764952s
Jan 12 16:31:10.457: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 12 16:31:10.457: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/12/23 16:31:10.459
STEP: Then the orphan pod is adopted 01/12/23 16:31:10.462
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:11.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1080" for this suite. 01/12/23 16:31:11.469
------------------------------
 [3.040 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:08.434
    Jan 12 16:31:08.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 16:31:08.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:08.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:08.446
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/12/23 16:31:08.448
    Jan 12 16:31:08.452: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1080" to be "running and ready"
    Jan 12 16:31:08.454: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800158ms
    Jan 12 16:31:08.454: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:31:10.457: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004764952s
    Jan 12 16:31:10.457: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 12 16:31:10.457: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/12/23 16:31:10.459
    STEP: Then the orphan pod is adopted 01/12/23 16:31:10.462
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:11.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1080" for this suite. 01/12/23 16:31:11.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:11.475
Jan 12 16:31:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename podtemplate 01/12/23 16:31:11.476
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:11.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:11.488
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/12/23 16:31:11.491
STEP: Replace a pod template 01/12/23 16:31:11.497
Jan 12 16:31:11.503: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:11.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5605" for this suite. 01/12/23 16:31:11.505
------------------------------
 [0.034 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:11.475
    Jan 12 16:31:11.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename podtemplate 01/12/23 16:31:11.476
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:11.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:11.488
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/12/23 16:31:11.491
    STEP: Replace a pod template 01/12/23 16:31:11.497
    Jan 12 16:31:11.503: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:11.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5605" for this suite. 01/12/23 16:31:11.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:11.514
Jan 12 16:31:11.515: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:31:11.515
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:11.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:11.527
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-5265dd73-d5ac-4fce-91df-7f477145d93c 01/12/23 16:31:11.53
STEP: Creating a pod to test consume secrets 01/12/23 16:31:11.534
Jan 12 16:31:11.541: INFO: Waiting up to 5m0s for pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4" in namespace "secrets-876" to be "Succeeded or Failed"
Jan 12 16:31:11.544: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490223ms
Jan 12 16:31:13.548: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0070582s
Jan 12 16:31:15.546: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005635163s
STEP: Saw pod success 01/12/23 16:31:15.546
Jan 12 16:31:15.546: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4" satisfied condition "Succeeded or Failed"
Jan 12 16:31:15.548: INFO: Trying to get logs from node worker-1 pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:31:15.56
Jan 12 16:31:15.567: INFO: Waiting for pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 to disappear
Jan 12 16:31:15.568: INFO: Pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-876" for this suite. 01/12/23 16:31:15.571
------------------------------
 [4.060 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:11.514
    Jan 12 16:31:11.515: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:31:11.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:11.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:11.527
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-5265dd73-d5ac-4fce-91df-7f477145d93c 01/12/23 16:31:11.53
    STEP: Creating a pod to test consume secrets 01/12/23 16:31:11.534
    Jan 12 16:31:11.541: INFO: Waiting up to 5m0s for pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4" in namespace "secrets-876" to be "Succeeded or Failed"
    Jan 12 16:31:11.544: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490223ms
    Jan 12 16:31:13.548: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0070582s
    Jan 12 16:31:15.546: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005635163s
    STEP: Saw pod success 01/12/23 16:31:15.546
    Jan 12 16:31:15.546: INFO: Pod "pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4" satisfied condition "Succeeded or Failed"
    Jan 12 16:31:15.548: INFO: Trying to get logs from node worker-1 pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:31:15.56
    Jan 12 16:31:15.567: INFO: Waiting for pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 to disappear
    Jan 12 16:31:15.568: INFO: Pod pod-secrets-8f305525-9a59-4f9c-bdc0-eeca2e1bd6e4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-876" for this suite. 01/12/23 16:31:15.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:15.574
Jan 12 16:31:15.575: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:31:15.575
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:15.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:15.587
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-8908 01/12/23 16:31:15.589
STEP: creating a selector 01/12/23 16:31:15.589
STEP: Creating the service pods in kubernetes 01/12/23 16:31:15.589
Jan 12 16:31:15.589: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 16:31:15.604: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8908" to be "running and ready"
Jan 12 16:31:15.608: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156224ms
Jan 12 16:31:15.608: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:31:17.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005567982s
Jan 12 16:31:17.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:19.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006178136s
Jan 12 16:31:19.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:21.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005350175s
Jan 12 16:31:21.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:23.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006598138s
Jan 12 16:31:23.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:25.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006266304s
Jan 12 16:31:25.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:27.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006606059s
Jan 12 16:31:27.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:29.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006258572s
Jan 12 16:31:29.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:31.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00707323s
Jan 12 16:31:31.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:33.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0073434s
Jan 12 16:31:33.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:35.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007257094s
Jan 12 16:31:35.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:31:37.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007826611s
Jan 12 16:31:37.612: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 16:31:37.612: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 16:31:37.614: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8908" to be "running and ready"
Jan 12 16:31:37.616: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.647686ms
Jan 12 16:31:37.616: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 16:31:37.616: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 16:31:37.617
Jan 12 16:31:37.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8908" to be "running"
Jan 12 16:31:37.628: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.644551ms
Jan 12 16:31:39.631: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005579764s
Jan 12 16:31:39.632: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 16:31:39.633: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8908" to be "running"
Jan 12 16:31:39.635: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.584703ms
Jan 12 16:31:39.635: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 12 16:31:39.636: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 16:31:39.636: INFO: Going to poll 10.244.0.185 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 12 16:31:39.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.185 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8908 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:31:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:31:39.639: INFO: ExecWithOptions: Clientset creation
Jan 12 16:31:39.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8908/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.185+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 16:31:40.700: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 12 16:31:40.700: INFO: Going to poll 10.244.1.75 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 12 16:31:40.702: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8908 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:31:40.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:31:40.702: INFO: ExecWithOptions: Clientset creation
Jan 12 16:31:40.702: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8908/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.75+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 16:31:41.755: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8908" for this suite. 01/12/23 16:31:41.758
------------------------------
 [SLOW TEST] [26.187 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:15.574
    Jan 12 16:31:15.575: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:31:15.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:15.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:15.587
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-8908 01/12/23 16:31:15.589
    STEP: creating a selector 01/12/23 16:31:15.589
    STEP: Creating the service pods in kubernetes 01/12/23 16:31:15.589
    Jan 12 16:31:15.589: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 16:31:15.604: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8908" to be "running and ready"
    Jan 12 16:31:15.608: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.156224ms
    Jan 12 16:31:15.608: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:31:17.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005567982s
    Jan 12 16:31:17.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:19.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006178136s
    Jan 12 16:31:19.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:21.610: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005350175s
    Jan 12 16:31:21.610: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:23.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.006598138s
    Jan 12 16:31:23.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:25.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.006266304s
    Jan 12 16:31:25.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:27.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006606059s
    Jan 12 16:31:27.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:29.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006258572s
    Jan 12 16:31:29.611: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:31.611: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00707323s
    Jan 12 16:31:31.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:33.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0073434s
    Jan 12 16:31:33.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:35.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007257094s
    Jan 12 16:31:35.612: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:31:37.612: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007826611s
    Jan 12 16:31:37.612: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 16:31:37.612: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 16:31:37.614: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8908" to be "running and ready"
    Jan 12 16:31:37.616: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.647686ms
    Jan 12 16:31:37.616: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 16:31:37.616: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 16:31:37.617
    Jan 12 16:31:37.626: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8908" to be "running"
    Jan 12 16:31:37.628: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.644551ms
    Jan 12 16:31:39.631: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005579764s
    Jan 12 16:31:39.632: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 16:31:39.633: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8908" to be "running"
    Jan 12 16:31:39.635: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.584703ms
    Jan 12 16:31:39.635: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 12 16:31:39.636: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 16:31:39.636: INFO: Going to poll 10.244.0.185 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 16:31:39.638: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.185 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8908 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:31:39.638: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:31:39.639: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:31:39.639: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8908/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.185+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 16:31:40.700: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 12 16:31:40.700: INFO: Going to poll 10.244.1.75 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 16:31:40.702: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8908 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:31:40.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:31:40.702: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:31:40.702: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8908/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.75+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 16:31:41.755: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8908" for this suite. 01/12/23 16:31:41.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:41.763
Jan 12 16:31:41.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:31:41.764
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:41.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:41.775
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/12/23 16:31:41.777
STEP: Creating a ResourceQuota 01/12/23 16:31:46.78
STEP: Ensuring resource quota status is calculated 01/12/23 16:31:46.784
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:48.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2459" for this suite. 01/12/23 16:31:48.789
------------------------------
 [SLOW TEST] [7.030 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:41.763
    Jan 12 16:31:41.763: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:31:41.764
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:41.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:41.775
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/12/23 16:31:41.777
    STEP: Creating a ResourceQuota 01/12/23 16:31:46.78
    STEP: Ensuring resource quota status is calculated 01/12/23 16:31:46.784
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:48.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2459" for this suite. 01/12/23 16:31:48.789
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:48.793
Jan 12 16:31:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:31:48.794
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:48.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:48.815
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/12/23 16:31:48.817
Jan 12 16:31:48.838: INFO: Waiting up to 5m0s for pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f" in namespace "downward-api-676" to be "running and ready"
Jan 12 16:31:48.839: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.736725ms
Jan 12 16:31:48.839: INFO: The phase of Pod annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:31:50.842: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004506663s
Jan 12 16:31:50.842: INFO: The phase of Pod annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f is Running (Ready = true)
Jan 12 16:31:50.842: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f" satisfied condition "running and ready"
Jan 12 16:31:51.358: INFO: Successfully updated pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:31:55.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-676" for this suite. 01/12/23 16:31:55.377
------------------------------
 [SLOW TEST] [6.587 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:48.793
    Jan 12 16:31:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:31:48.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:48.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:48.815
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/12/23 16:31:48.817
    Jan 12 16:31:48.838: INFO: Waiting up to 5m0s for pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f" in namespace "downward-api-676" to be "running and ready"
    Jan 12 16:31:48.839: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.736725ms
    Jan 12 16:31:48.839: INFO: The phase of Pod annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:31:50.842: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004506663s
    Jan 12 16:31:50.842: INFO: The phase of Pod annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f is Running (Ready = true)
    Jan 12 16:31:50.842: INFO: Pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f" satisfied condition "running and ready"
    Jan 12 16:31:51.358: INFO: Successfully updated pod "annotationupdatedebac01f-3fd8-4418-a105-53f2094dd13f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:31:55.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-676" for this suite. 01/12/23 16:31:55.377
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:31:55.381
Jan 12 16:31:55.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption 01/12/23 16:31:55.382
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:55.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:55.394
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/12/23 16:31:55.397
STEP: Waiting for the pdb to be processed 01/12/23 16:31:55.4
STEP: updating the pdb 01/12/23 16:31:57.406
STEP: Waiting for the pdb to be processed 01/12/23 16:31:57.413
STEP: patching the pdb 01/12/23 16:31:59.418
STEP: Waiting for the pdb to be processed 01/12/23 16:31:59.423
STEP: Waiting for the pdb to be deleted 01/12/23 16:32:01.431
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:01.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8504" for this suite. 01/12/23 16:32:01.435
------------------------------
 [SLOW TEST] [6.057 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:31:55.381
    Jan 12 16:31:55.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption 01/12/23 16:31:55.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:31:55.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:31:55.394
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/12/23 16:31:55.397
    STEP: Waiting for the pdb to be processed 01/12/23 16:31:55.4
    STEP: updating the pdb 01/12/23 16:31:57.406
    STEP: Waiting for the pdb to be processed 01/12/23 16:31:57.413
    STEP: patching the pdb 01/12/23 16:31:59.418
    STEP: Waiting for the pdb to be processed 01/12/23 16:31:59.423
    STEP: Waiting for the pdb to be deleted 01/12/23 16:32:01.431
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:01.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8504" for this suite. 01/12/23 16:32:01.435
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:01.439
Jan 12 16:32:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:32:01.44
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:01.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:01.451
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/12/23 16:32:01.453
Jan 12 16:32:01.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6453 api-versions'
Jan 12 16:32:01.527: INFO: stderr: ""
Jan 12 16:32:01.527: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautopilot.k0sproject.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.k0sproject.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:01.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6453" for this suite. 01/12/23 16:32:01.529
------------------------------
 [0.095 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:01.439
    Jan 12 16:32:01.439: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:32:01.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:01.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:01.451
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/12/23 16:32:01.453
    Jan 12 16:32:01.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6453 api-versions'
    Jan 12 16:32:01.527: INFO: stderr: ""
    Jan 12 16:32:01.527: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautopilot.k0sproject.io/v1beta2\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.k0sproject.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:01.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6453" for this suite. 01/12/23 16:32:01.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:01.535
Jan 12 16:32:01.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-pred 01/12/23 16:32:01.536
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:01.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:01.546
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 16:32:01.549: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 16:32:01.552: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 16:32:01.554: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Jan 12 16:32:01.558: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:32:01.558: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:32:01.558: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:32:01.558: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:32:01.558: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container metrics-server ready: true, restart count 0
Jan 12 16:32:01.558: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container e2e ready: true, restart count 0
Jan 12 16:32:01.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:32:01.558: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:32:01.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:32:01.558: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 16:32:01.558: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Jan 12 16:32:01.561: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:32:01.561: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:32:01.561: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:32:01.561: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:32:01.561: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 16:32:01.561: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:32:01.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:32:01.561: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 16:32:01.561
Jan 12 16:32:01.566: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3726" to be "running"
Jan 12 16:32:01.568: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.703772ms
Jan 12 16:32:03.570: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003815305s
Jan 12 16:32:03.570: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 16:32:03.572
STEP: Trying to apply a random label on the found node. 01/12/23 16:32:03.58
STEP: verifying the node has the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc 42 01/12/23 16:32:03.587
STEP: Trying to relaunch the pod, now with labels. 01/12/23 16:32:03.59
Jan 12 16:32:03.595: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3726" to be "not pending"
Jan 12 16:32:03.597: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003593ms
Jan 12 16:32:05.599: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004103329s
Jan 12 16:32:05.599: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc off the node worker-1 01/12/23 16:32:05.601
STEP: verifying the node doesn't have the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc 01/12/23 16:32:05.611
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3726" for this suite. 01/12/23 16:32:05.615
------------------------------
 [4.083 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:01.535
    Jan 12 16:32:01.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-pred 01/12/23 16:32:01.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:01.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:01.546
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 16:32:01.549: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 16:32:01.552: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 16:32:01.554: INFO: 
    Logging pods the apiserver thinks is on node worker-0 before test
    Jan 12 16:32:01.558: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:32:01.558: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 16:32:01.558: INFO: 
    Logging pods the apiserver thinks is on node worker-1 before test
    Jan 12 16:32:01.561: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:32:01.561: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:32:01.561: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/12/23 16:32:01.561
    Jan 12 16:32:01.566: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3726" to be "running"
    Jan 12 16:32:01.568: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.703772ms
    Jan 12 16:32:03.570: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003815305s
    Jan 12 16:32:03.570: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/12/23 16:32:03.572
    STEP: Trying to apply a random label on the found node. 01/12/23 16:32:03.58
    STEP: verifying the node has the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc 42 01/12/23 16:32:03.587
    STEP: Trying to relaunch the pod, now with labels. 01/12/23 16:32:03.59
    Jan 12 16:32:03.595: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3726" to be "not pending"
    Jan 12 16:32:03.597: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003593ms
    Jan 12 16:32:05.599: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.004103329s
    Jan 12 16:32:05.599: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc off the node worker-1 01/12/23 16:32:05.601
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-939a5b2f-931d-4141-a56b-be9acd90c3dc 01/12/23 16:32:05.611
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:05.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3726" for this suite. 01/12/23 16:32:05.615
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:05.619
Jan 12 16:32:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:32:05.62
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:05.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:05.629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/12/23 16:32:05.632
Jan 12 16:32:05.632: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: rename a version 01/12/23 16:32:09.042
STEP: check the new version name is served 01/12/23 16:32:09.054
STEP: check the old version name is removed 01/12/23 16:32:10.43
STEP: check the other version is not changed 01/12/23 16:32:11.129
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:13.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7181" for this suite. 01/12/23 16:32:13.875
------------------------------
 [SLOW TEST] [8.263 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:05.619
    Jan 12 16:32:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:32:05.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:05.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:05.629
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/12/23 16:32:05.632
    Jan 12 16:32:05.632: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: rename a version 01/12/23 16:32:09.042
    STEP: check the new version name is served 01/12/23 16:32:09.054
    STEP: check the old version name is removed 01/12/23 16:32:10.43
    STEP: check the other version is not changed 01/12/23 16:32:11.129
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:13.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7181" for this suite. 01/12/23 16:32:13.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:13.882
Jan 12 16:32:13.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:32:13.883
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:13.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:13.894
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 12 16:32:13.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: creating the pod 01/12/23 16:32:13.897
STEP: submitting the pod to kubernetes 01/12/23 16:32:13.897
Jan 12 16:32:13.904: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780" in namespace "pods-1129" to be "running and ready"
Jan 12 16:32:13.906: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780": Phase="Pending", Reason="", readiness=false. Elapsed: 1.814909ms
Jan 12 16:32:13.906: INFO: The phase of Pod pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:32:15.909: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780": Phase="Running", Reason="", readiness=true. Elapsed: 2.004735722s
Jan 12 16:32:15.909: INFO: The phase of Pod pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780 is Running (Ready = true)
Jan 12 16:32:15.909: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1129" for this suite. 01/12/23 16:32:15.978
------------------------------
 [2.099 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:13.882
    Jan 12 16:32:13.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:32:13.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:13.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:13.894
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 12 16:32:13.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: creating the pod 01/12/23 16:32:13.897
    STEP: submitting the pod to kubernetes 01/12/23 16:32:13.897
    Jan 12 16:32:13.904: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780" in namespace "pods-1129" to be "running and ready"
    Jan 12 16:32:13.906: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780": Phase="Pending", Reason="", readiness=false. Elapsed: 1.814909ms
    Jan 12 16:32:13.906: INFO: The phase of Pod pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:32:15.909: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780": Phase="Running", Reason="", readiness=true. Elapsed: 2.004735722s
    Jan 12 16:32:15.909: INFO: The phase of Pod pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780 is Running (Ready = true)
    Jan 12 16:32:15.909: INFO: Pod "pod-exec-websocket-60841721-337b-452c-a61f-6f6c9b2b6780" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:15.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1129" for this suite. 01/12/23 16:32:15.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:15.983
Jan 12 16:32:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename prestop 01/12/23 16:32:15.984
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:15.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:15.993
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-3414 01/12/23 16:32:15.995
STEP: Waiting for pods to come up. 01/12/23 16:32:16
Jan 12 16:32:16.000: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3414" to be "running"
Jan 12 16:32:16.002: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869832ms
Jan 12 16:32:18.005: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005511618s
Jan 12 16:32:18.006: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-3414 01/12/23 16:32:18.007
Jan 12 16:32:18.011: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3414" to be "running"
Jan 12 16:32:18.012: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677573ms
Jan 12 16:32:20.015: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004521312s
Jan 12 16:32:20.015: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/12/23 16:32:20.015
Jan 12 16:32:25.031: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/12/23 16:32:25.031
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:25.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-3414" for this suite. 01/12/23 16:32:25.041
------------------------------
 [SLOW TEST] [9.064 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:15.983
    Jan 12 16:32:15.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename prestop 01/12/23 16:32:15.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:15.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:15.993
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-3414 01/12/23 16:32:15.995
    STEP: Waiting for pods to come up. 01/12/23 16:32:16
    Jan 12 16:32:16.000: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-3414" to be "running"
    Jan 12 16:32:16.002: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869832ms
    Jan 12 16:32:18.005: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005511618s
    Jan 12 16:32:18.006: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-3414 01/12/23 16:32:18.007
    Jan 12 16:32:18.011: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-3414" to be "running"
    Jan 12 16:32:18.012: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.677573ms
    Jan 12 16:32:20.015: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004521312s
    Jan 12 16:32:20.015: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/12/23 16:32:20.015
    Jan 12 16:32:25.031: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/12/23 16:32:25.031
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:25.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-3414" for this suite. 01/12/23 16:32:25.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:25.048
Jan 12 16:32:25.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:32:25.049
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:25.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:25.06
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 12 16:32:25.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3308" for this suite. 01/12/23 16:32:31.211
------------------------------
 [SLOW TEST] [6.169 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:25.048
    Jan 12 16:32:25.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:32:25.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:25.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:25.06
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 12 16:32:25.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3308" for this suite. 01/12/23 16:32:31.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:31.218
Jan 12 16:32:31.218: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:32:31.219
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:31.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:31.229
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 12 16:32:31.240: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5644 to be scheduled
Jan 12 16:32:31.242: INFO: 1 pods are not scheduled: [runtimeclass-5644/test-runtimeclass-runtimeclass-5644-preconfigured-handler-75dwz(91767344-5c45-4197-8024-754bb0dd627f)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5644" for this suite. 01/12/23 16:32:33.25
------------------------------
 [2.036 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:31.218
    Jan 12 16:32:31.218: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:32:31.219
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:31.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:31.229
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 12 16:32:31.240: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5644 to be scheduled
    Jan 12 16:32:31.242: INFO: 1 pods are not scheduled: [runtimeclass-5644/test-runtimeclass-runtimeclass-5644-preconfigured-handler-75dwz(91767344-5c45-4197-8024-754bb0dd627f)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:33.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5644" for this suite. 01/12/23 16:32:33.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:33.254
Jan 12 16:32:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-runtime 01/12/23 16:32:33.255
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:33.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:33.265
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/12/23 16:32:33.267
STEP: wait for the container to reach Succeeded 01/12/23 16:32:33.272
STEP: get the container status 01/12/23 16:32:36.281
STEP: the container should be terminated 01/12/23 16:32:36.283
STEP: the termination message should be set 01/12/23 16:32:36.283
Jan 12 16:32:36.283: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/12/23 16:32:36.283
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5556" for this suite. 01/12/23 16:32:36.297
------------------------------
 [3.047 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:33.254
    Jan 12 16:32:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-runtime 01/12/23 16:32:33.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:33.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:33.265
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/12/23 16:32:33.267
    STEP: wait for the container to reach Succeeded 01/12/23 16:32:33.272
    STEP: get the container status 01/12/23 16:32:36.281
    STEP: the container should be terminated 01/12/23 16:32:36.283
    STEP: the termination message should be set 01/12/23 16:32:36.283
    Jan 12 16:32:36.283: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/12/23 16:32:36.283
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5556" for this suite. 01/12/23 16:32:36.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:36.304
Jan 12 16:32:36.304: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename configmap 01/12/23 16:32:36.305
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:36.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:36.322
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-bad44323-41e9-4625-a5d1-1dfaf5099556 01/12/23 16:32:36.326
STEP: Creating the pod 01/12/23 16:32:36.33
Jan 12 16:32:36.337: INFO: Waiting up to 5m0s for pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e" in namespace "configmap-6795" to be "running and ready"
Jan 12 16:32:36.339: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162706ms
Jan 12 16:32:36.339: INFO: The phase of Pod pod-configmaps-8075c804-e56c-4286-bd01-de400957772e is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:32:38.342: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004946069s
Jan 12 16:32:38.342: INFO: The phase of Pod pod-configmaps-8075c804-e56c-4286-bd01-de400957772e is Running (Ready = true)
Jan 12 16:32:38.342: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-bad44323-41e9-4625-a5d1-1dfaf5099556 01/12/23 16:32:38.349
STEP: waiting to observe update in volume 01/12/23 16:32:38.352
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:32:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6795" for this suite. 01/12/23 16:32:40.364
------------------------------
 [4.066 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:36.304
    Jan 12 16:32:36.304: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename configmap 01/12/23 16:32:36.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:36.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:36.322
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-bad44323-41e9-4625-a5d1-1dfaf5099556 01/12/23 16:32:36.326
    STEP: Creating the pod 01/12/23 16:32:36.33
    Jan 12 16:32:36.337: INFO: Waiting up to 5m0s for pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e" in namespace "configmap-6795" to be "running and ready"
    Jan 12 16:32:36.339: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162706ms
    Jan 12 16:32:36.339: INFO: The phase of Pod pod-configmaps-8075c804-e56c-4286-bd01-de400957772e is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:32:38.342: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004946069s
    Jan 12 16:32:38.342: INFO: The phase of Pod pod-configmaps-8075c804-e56c-4286-bd01-de400957772e is Running (Ready = true)
    Jan 12 16:32:38.342: INFO: Pod "pod-configmaps-8075c804-e56c-4286-bd01-de400957772e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-bad44323-41e9-4625-a5d1-1dfaf5099556 01/12/23 16:32:38.349
    STEP: waiting to observe update in volume 01/12/23 16:32:38.352
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:32:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6795" for this suite. 01/12/23 16:32:40.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:32:40.372
Jan 12 16:32:40.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-preemption 01/12/23 16:32:40.373
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:40.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:40.383
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 12 16:32:40.395: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 12 16:33:40.410: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/12/23 16:33:40.411
Jan 12 16:33:40.424: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 12 16:33:40.430: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 12 16:33:40.450: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 12 16:33:40.456: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/12/23 16:33:40.456
Jan 12 16:33:40.456: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-186" to be "running"
Jan 12 16:33:40.459: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135725ms
Jan 12 16:33:42.462: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006095147s
Jan 12 16:33:44.463: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.007051884s
Jan 12 16:33:44.463: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 12 16:33:44.463: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
Jan 12 16:33:44.465: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.802385ms
Jan 12 16:33:44.465: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 16:33:44.465: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
Jan 12 16:33:44.466: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535716ms
Jan 12 16:33:46.469: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.003921137s
Jan 12 16:33:46.469: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 12 16:33:46.469: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
Jan 12 16:33:46.470: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.565678ms
Jan 12 16:33:46.471: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/12/23 16:33:46.471
Jan 12 16:33:46.479: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 12 16:33:46.480: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737561ms
Jan 12 16:33:48.484: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005113617s
Jan 12 16:33:48.484: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:33:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-186" for this suite. 01/12/23 16:33:48.522
------------------------------
 [SLOW TEST] [68.153 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:32:40.372
    Jan 12 16:32:40.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-preemption 01/12/23 16:32:40.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:32:40.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:32:40.383
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 12 16:32:40.395: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 12 16:33:40.410: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/12/23 16:33:40.411
    Jan 12 16:33:40.424: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 12 16:33:40.430: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 12 16:33:40.450: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 12 16:33:40.456: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/12/23 16:33:40.456
    Jan 12 16:33:40.456: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-186" to be "running"
    Jan 12 16:33:40.459: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135725ms
    Jan 12 16:33:42.462: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006095147s
    Jan 12 16:33:44.463: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.007051884s
    Jan 12 16:33:44.463: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 12 16:33:44.463: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
    Jan 12 16:33:44.465: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.802385ms
    Jan 12 16:33:44.465: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 16:33:44.465: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
    Jan 12 16:33:44.466: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535716ms
    Jan 12 16:33:46.469: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.003921137s
    Jan 12 16:33:46.469: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 12 16:33:46.469: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-186" to be "running"
    Jan 12 16:33:46.470: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.565678ms
    Jan 12 16:33:46.471: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/12/23 16:33:46.471
    Jan 12 16:33:46.479: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 12 16:33:46.480: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737561ms
    Jan 12 16:33:48.484: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005113617s
    Jan 12 16:33:48.484: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:33:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-186" for this suite. 01/12/23 16:33:48.522
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:33:48.527
Jan 12 16:33:48.527: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 16:33:48.528
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:48.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:48.536
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 12 16:33:48.544: INFO: Waiting up to 2m0s for pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" in namespace "var-expansion-3756" to be "container 0 failed with reason CreateContainerConfigError"
Jan 12 16:33:48.546: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002354ms
Jan 12 16:33:50.548: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004631679s
Jan 12 16:33:50.548: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 12 16:33:50.548: INFO: Deleting pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" in namespace "var-expansion-3756"
Jan 12 16:33:50.552: INFO: Wait up to 5m0s for pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 16:33:52.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3756" for this suite. 01/12/23 16:33:52.558
------------------------------
 [4.037 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:33:48.527
    Jan 12 16:33:48.527: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 16:33:48.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:48.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:48.536
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 12 16:33:48.544: INFO: Waiting up to 2m0s for pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" in namespace "var-expansion-3756" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 12 16:33:48.546: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.002354ms
    Jan 12 16:33:50.548: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004631679s
    Jan 12 16:33:50.548: INFO: Pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 12 16:33:50.548: INFO: Deleting pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" in namespace "var-expansion-3756"
    Jan 12 16:33:50.552: INFO: Wait up to 5m0s for pod "var-expansion-73bfbfd8-4c73-4a99-abe8-7f9831d92b0f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:33:52.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3756" for this suite. 01/12/23 16:33:52.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:33:52.565
Jan 12 16:33:52.565: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename csistoragecapacity 01/12/23 16:33:52.566
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:52.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:52.575
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/12/23 16:33:52.577
STEP: getting /apis/storage.k8s.io 01/12/23 16:33:52.579
STEP: getting /apis/storage.k8s.io/v1 01/12/23 16:33:52.58
STEP: creating 01/12/23 16:33:52.581
STEP: watching 01/12/23 16:33:52.593
Jan 12 16:33:52.593: INFO: starting watch
STEP: getting 01/12/23 16:33:52.598
STEP: listing in namespace 01/12/23 16:33:52.6
STEP: listing across namespaces 01/12/23 16:33:52.602
STEP: patching 01/12/23 16:33:52.603
STEP: updating 01/12/23 16:33:52.606
Jan 12 16:33:52.609: INFO: waiting for watch events with expected annotations in namespace
Jan 12 16:33:52.609: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/12/23 16:33:52.609
STEP: deleting a collection 01/12/23 16:33:52.617
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 12 16:33:52.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-7966" for this suite. 01/12/23 16:33:52.628
------------------------------
 [0.066 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:33:52.565
    Jan 12 16:33:52.565: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename csistoragecapacity 01/12/23 16:33:52.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:52.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:52.575
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/12/23 16:33:52.577
    STEP: getting /apis/storage.k8s.io 01/12/23 16:33:52.579
    STEP: getting /apis/storage.k8s.io/v1 01/12/23 16:33:52.58
    STEP: creating 01/12/23 16:33:52.581
    STEP: watching 01/12/23 16:33:52.593
    Jan 12 16:33:52.593: INFO: starting watch
    STEP: getting 01/12/23 16:33:52.598
    STEP: listing in namespace 01/12/23 16:33:52.6
    STEP: listing across namespaces 01/12/23 16:33:52.602
    STEP: patching 01/12/23 16:33:52.603
    STEP: updating 01/12/23 16:33:52.606
    Jan 12 16:33:52.609: INFO: waiting for watch events with expected annotations in namespace
    Jan 12 16:33:52.609: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/12/23 16:33:52.609
    STEP: deleting a collection 01/12/23 16:33:52.617
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:33:52.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-7966" for this suite. 01/12/23 16:33:52.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:33:52.633
Jan 12 16:33:52.633: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:33:52.633
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:52.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:52.644
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/12/23 16:33:52.647
Jan 12 16:33:52.651: INFO: Waiting up to 5m0s for pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb" in namespace "emptydir-5246" to be "Succeeded or Failed"
Jan 12 16:33:52.653: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803777ms
Jan 12 16:33:54.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004849091s
Jan 12 16:33:56.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004277709s
STEP: Saw pod success 01/12/23 16:33:56.656
Jan 12 16:33:56.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb" satisfied condition "Succeeded or Failed"
Jan 12 16:33:56.658: INFO: Trying to get logs from node worker-0 pod pod-015df0d1-de27-41bb-9829-51ce6e61decb container test-container: <nil>
STEP: delete the pod 01/12/23 16:33:56.669
Jan 12 16:33:56.675: INFO: Waiting for pod pod-015df0d1-de27-41bb-9829-51ce6e61decb to disappear
Jan 12 16:33:56.677: INFO: Pod pod-015df0d1-de27-41bb-9829-51ce6e61decb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:33:56.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5246" for this suite. 01/12/23 16:33:56.679
------------------------------
 [4.052 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:33:52.633
    Jan 12 16:33:52.633: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:33:52.633
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:52.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:52.644
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/12/23 16:33:52.647
    Jan 12 16:33:52.651: INFO: Waiting up to 5m0s for pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb" in namespace "emptydir-5246" to be "Succeeded or Failed"
    Jan 12 16:33:52.653: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.803777ms
    Jan 12 16:33:54.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004849091s
    Jan 12 16:33:56.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004277709s
    STEP: Saw pod success 01/12/23 16:33:56.656
    Jan 12 16:33:56.656: INFO: Pod "pod-015df0d1-de27-41bb-9829-51ce6e61decb" satisfied condition "Succeeded or Failed"
    Jan 12 16:33:56.658: INFO: Trying to get logs from node worker-0 pod pod-015df0d1-de27-41bb-9829-51ce6e61decb container test-container: <nil>
    STEP: delete the pod 01/12/23 16:33:56.669
    Jan 12 16:33:56.675: INFO: Waiting for pod pod-015df0d1-de27-41bb-9829-51ce6e61decb to disappear
    Jan 12 16:33:56.677: INFO: Pod pod-015df0d1-de27-41bb-9829-51ce6e61decb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:33:56.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5246" for this suite. 01/12/23 16:33:56.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:33:56.689
Jan 12 16:33:56.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename cronjob 01/12/23 16:33:56.69
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:56.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:56.698
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/12/23 16:33:56.7
STEP: Ensuring a job is scheduled 01/12/23 16:33:56.706
STEP: Ensuring exactly one is scheduled 01/12/23 16:34:00.71
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 16:34:00.711
STEP: Ensuring the job is replaced with a new one 01/12/23 16:34:00.713
STEP: Removing cronjob 01/12/23 16:35:00.717
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5134" for this suite. 01/12/23 16:35:00.723
------------------------------
 [SLOW TEST] [64.039 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:33:56.689
    Jan 12 16:33:56.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename cronjob 01/12/23 16:33:56.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:33:56.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:33:56.698
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/12/23 16:33:56.7
    STEP: Ensuring a job is scheduled 01/12/23 16:33:56.706
    STEP: Ensuring exactly one is scheduled 01/12/23 16:34:00.71
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/12/23 16:34:00.711
    STEP: Ensuring the job is replaced with a new one 01/12/23 16:34:00.713
    STEP: Removing cronjob 01/12/23 16:35:00.717
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5134" for this suite. 01/12/23 16:35:00.723
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:00.728
Jan 12 16:35:00.728: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:35:00.729
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:00.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:00.745
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 16:35:00.747
Jan 12 16:35:00.752: INFO: Waiting up to 5m0s for pod "pod-9b654a10-a099-4714-97dc-03dc39930869" in namespace "emptydir-4581" to be "Succeeded or Failed"
Jan 12 16:35:00.755: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682538ms
Jan 12 16:35:02.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00517399s
Jan 12 16:35:04.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005367087s
STEP: Saw pod success 01/12/23 16:35:04.757
Jan 12 16:35:04.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869" satisfied condition "Succeeded or Failed"
Jan 12 16:35:04.759: INFO: Trying to get logs from node worker-0 pod pod-9b654a10-a099-4714-97dc-03dc39930869 container test-container: <nil>
STEP: delete the pod 01/12/23 16:35:04.764
Jan 12 16:35:04.772: INFO: Waiting for pod pod-9b654a10-a099-4714-97dc-03dc39930869 to disappear
Jan 12 16:35:04.773: INFO: Pod pod-9b654a10-a099-4714-97dc-03dc39930869 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:04.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4581" for this suite. 01/12/23 16:35:04.776
------------------------------
 [4.052 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:00.728
    Jan 12 16:35:00.728: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:35:00.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:00.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:00.745
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 16:35:00.747
    Jan 12 16:35:00.752: INFO: Waiting up to 5m0s for pod "pod-9b654a10-a099-4714-97dc-03dc39930869" in namespace "emptydir-4581" to be "Succeeded or Failed"
    Jan 12 16:35:00.755: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.682538ms
    Jan 12 16:35:02.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00517399s
    Jan 12 16:35:04.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005367087s
    STEP: Saw pod success 01/12/23 16:35:04.757
    Jan 12 16:35:04.757: INFO: Pod "pod-9b654a10-a099-4714-97dc-03dc39930869" satisfied condition "Succeeded or Failed"
    Jan 12 16:35:04.759: INFO: Trying to get logs from node worker-0 pod pod-9b654a10-a099-4714-97dc-03dc39930869 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:35:04.764
    Jan 12 16:35:04.772: INFO: Waiting for pod pod-9b654a10-a099-4714-97dc-03dc39930869 to disappear
    Jan 12 16:35:04.773: INFO: Pod pod-9b654a10-a099-4714-97dc-03dc39930869 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:04.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4581" for this suite. 01/12/23 16:35:04.776
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:04.78
Jan 12 16:35:04.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 16:35:04.781
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:04.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:04.791
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/12/23 16:35:04.794
STEP: Verify that the required pods have come up 01/12/23 16:35:04.797
Jan 12 16:35:04.799: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 12 16:35:09.802: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/12/23 16:35:09.802
Jan 12 16:35:09.804: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/12/23 16:35:09.804
STEP: DeleteCollection of the ReplicaSets 01/12/23 16:35:09.806
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/12/23 16:35:09.811
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7605" for this suite. 01/12/23 16:35:09.816
------------------------------
 [SLOW TEST] [5.042 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:04.78
    Jan 12 16:35:04.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 16:35:04.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:04.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:04.791
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/12/23 16:35:04.794
    STEP: Verify that the required pods have come up 01/12/23 16:35:04.797
    Jan 12 16:35:04.799: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 12 16:35:09.802: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/12/23 16:35:09.802
    Jan 12 16:35:09.804: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/12/23 16:35:09.804
    STEP: DeleteCollection of the ReplicaSets 01/12/23 16:35:09.806
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/12/23 16:35:09.811
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7605" for this suite. 01/12/23 16:35:09.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:09.825
Jan 12 16:35:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:35:09.825
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:09.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:09.853
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/12/23 16:35:09.855
STEP: Creating a ResourceQuota 01/12/23 16:35:14.858
STEP: Ensuring resource quota status is calculated 01/12/23 16:35:14.862
STEP: Creating a ReplicaSet 01/12/23 16:35:16.865
STEP: Ensuring resource quota status captures replicaset creation 01/12/23 16:35:16.874
STEP: Deleting a ReplicaSet 01/12/23 16:35:18.878
STEP: Ensuring resource quota status released usage 01/12/23 16:35:18.881
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:20.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3580" for this suite. 01/12/23 16:35:20.887
------------------------------
 [SLOW TEST] [11.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:09.825
    Jan 12 16:35:09.825: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:35:09.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:09.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:09.853
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/12/23 16:35:09.855
    STEP: Creating a ResourceQuota 01/12/23 16:35:14.858
    STEP: Ensuring resource quota status is calculated 01/12/23 16:35:14.862
    STEP: Creating a ReplicaSet 01/12/23 16:35:16.865
    STEP: Ensuring resource quota status captures replicaset creation 01/12/23 16:35:16.874
    STEP: Deleting a ReplicaSet 01/12/23 16:35:18.878
    STEP: Ensuring resource quota status released usage 01/12/23 16:35:18.881
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:20.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3580" for this suite. 01/12/23 16:35:20.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:20.892
Jan 12 16:35:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:35:20.892
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:20.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:20.906
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-db99cf9b-a226-4edd-a89c-0bcafd97a367 01/12/23 16:35:20.911
STEP: Creating configMap with name cm-test-opt-upd-7d504dbd-4209-4153-bc54-00150dd9fd67 01/12/23 16:35:20.914
STEP: Creating the pod 01/12/23 16:35:20.917
Jan 12 16:35:20.923: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb" in namespace "projected-4695" to be "running and ready"
Jan 12 16:35:20.925: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78188ms
Jan 12 16:35:20.925: INFO: The phase of Pod pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:35:22.928: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00497934s
Jan 12 16:35:22.928: INFO: The phase of Pod pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb is Running (Ready = true)
Jan 12 16:35:22.928: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-db99cf9b-a226-4edd-a89c-0bcafd97a367 01/12/23 16:35:22.943
STEP: Updating configmap cm-test-opt-upd-7d504dbd-4209-4153-bc54-00150dd9fd67 01/12/23 16:35:22.947
STEP: Creating configMap with name cm-test-opt-create-ee392e7f-2f27-4de1-b335-ac386531ea32 01/12/23 16:35:22.95
STEP: waiting to observe update in volume 01/12/23 16:35:22.954
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4695" for this suite. 01/12/23 16:35:26.983
------------------------------
 [SLOW TEST] [6.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:20.892
    Jan 12 16:35:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:35:20.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:20.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:20.906
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-db99cf9b-a226-4edd-a89c-0bcafd97a367 01/12/23 16:35:20.911
    STEP: Creating configMap with name cm-test-opt-upd-7d504dbd-4209-4153-bc54-00150dd9fd67 01/12/23 16:35:20.914
    STEP: Creating the pod 01/12/23 16:35:20.917
    Jan 12 16:35:20.923: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb" in namespace "projected-4695" to be "running and ready"
    Jan 12 16:35:20.925: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.78188ms
    Jan 12 16:35:20.925: INFO: The phase of Pod pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:35:22.928: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.00497934s
    Jan 12 16:35:22.928: INFO: The phase of Pod pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb is Running (Ready = true)
    Jan 12 16:35:22.928: INFO: Pod "pod-projected-configmaps-fc951bbc-d45c-48d3-a1b9-965bd5dd44bb" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-db99cf9b-a226-4edd-a89c-0bcafd97a367 01/12/23 16:35:22.943
    STEP: Updating configmap cm-test-opt-upd-7d504dbd-4209-4153-bc54-00150dd9fd67 01/12/23 16:35:22.947
    STEP: Creating configMap with name cm-test-opt-create-ee392e7f-2f27-4de1-b335-ac386531ea32 01/12/23 16:35:22.95
    STEP: waiting to observe update in volume 01/12/23 16:35:22.954
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4695" for this suite. 01/12/23 16:35:26.983
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:26.988
Jan 12 16:35:26.988: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:35:26.989
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:26.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:27.001
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 12 16:35:27.012: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5152 to be scheduled
Jan 12 16:35:27.014: INFO: 1 pods are not scheduled: [runtimeclass-5152/test-runtimeclass-runtimeclass-5152-preconfigured-handler-b4wz9(9f7f8686-d57c-4c05-921a-b9daf3bf30cf)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:29.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5152" for this suite. 01/12/23 16:35:29.023
------------------------------
 [2.038 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:26.988
    Jan 12 16:35:26.988: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:35:26.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:26.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:27.001
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 12 16:35:27.012: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5152 to be scheduled
    Jan 12 16:35:27.014: INFO: 1 pods are not scheduled: [runtimeclass-5152/test-runtimeclass-runtimeclass-5152-preconfigured-handler-b4wz9(9f7f8686-d57c-4c05-921a-b9daf3bf30cf)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:29.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5152" for this suite. 01/12/23 16:35:29.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:29.027
Jan 12 16:35:29.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:35:29.028
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:29.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:29.037
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-357-delete-me 01/12/23 16:35:29.044
STEP: Waiting for the RuntimeClass to disappear 01/12/23 16:35:29.047
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:29.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-357" for this suite. 01/12/23 16:35:29.054
------------------------------
 [0.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:29.027
    Jan 12 16:35:29.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename runtimeclass 01/12/23 16:35:29.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:29.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:29.037
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-357-delete-me 01/12/23 16:35:29.044
    STEP: Waiting for the RuntimeClass to disappear 01/12/23 16:35:29.047
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:29.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-357" for this suite. 01/12/23 16:35:29.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:29.059
Jan 12 16:35:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-runtime 01/12/23 16:35:29.06
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:29.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:29.07
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/12/23 16:35:29.072
STEP: wait for the container to reach Failed 01/12/23 16:35:29.077
STEP: get the container status 01/12/23 16:35:33.089
STEP: the container should be terminated 01/12/23 16:35:33.091
STEP: the termination message should be set 01/12/23 16:35:33.091
Jan 12 16:35:33.091: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/12/23 16:35:33.091
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:33.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4362" for this suite. 01/12/23 16:35:33.104
------------------------------
 [4.048 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:29.059
    Jan 12 16:35:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-runtime 01/12/23 16:35:29.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:29.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:29.07
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/12/23 16:35:29.072
    STEP: wait for the container to reach Failed 01/12/23 16:35:29.077
    STEP: get the container status 01/12/23 16:35:33.089
    STEP: the container should be terminated 01/12/23 16:35:33.091
    STEP: the termination message should be set 01/12/23 16:35:33.091
    Jan 12 16:35:33.091: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/12/23 16:35:33.091
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:33.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4362" for this suite. 01/12/23 16:35:33.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:33.109
Jan 12 16:35:33.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:35:33.11
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:33.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:33.122
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 16:35:33.125
Jan 12 16:35:33.131: INFO: Waiting up to 5m0s for pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15" in namespace "emptydir-1716" to be "Succeeded or Failed"
Jan 12 16:35:33.133: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864309ms
Jan 12 16:35:35.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004643541s
Jan 12 16:35:37.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005024094s
STEP: Saw pod success 01/12/23 16:35:37.136
Jan 12 16:35:37.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15" satisfied condition "Succeeded or Failed"
Jan 12 16:35:37.138: INFO: Trying to get logs from node worker-1 pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 container test-container: <nil>
STEP: delete the pod 01/12/23 16:35:37.148
Jan 12 16:35:37.157: INFO: Waiting for pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 to disappear
Jan 12 16:35:37.158: INFO: Pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:37.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1716" for this suite. 01/12/23 16:35:37.161
------------------------------
 [4.055 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:33.109
    Jan 12 16:35:33.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:35:33.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:33.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:33.122
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 16:35:33.125
    Jan 12 16:35:33.131: INFO: Waiting up to 5m0s for pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15" in namespace "emptydir-1716" to be "Succeeded or Failed"
    Jan 12 16:35:33.133: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Pending", Reason="", readiness=false. Elapsed: 1.864309ms
    Jan 12 16:35:35.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004643541s
    Jan 12 16:35:37.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005024094s
    STEP: Saw pod success 01/12/23 16:35:37.136
    Jan 12 16:35:37.136: INFO: Pod "pod-e811e9c3-179f-42a6-8c3d-3b241c647b15" satisfied condition "Succeeded or Failed"
    Jan 12 16:35:37.138: INFO: Trying to get logs from node worker-1 pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:35:37.148
    Jan 12 16:35:37.157: INFO: Waiting for pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 to disappear
    Jan 12 16:35:37.158: INFO: Pod pod-e811e9c3-179f-42a6-8c3d-3b241c647b15 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:37.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1716" for this suite. 01/12/23 16:35:37.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:37.167
Jan 12 16:35:37.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename watch 01/12/23 16:35:37.167
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:37.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:37.177
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/12/23 16:35:37.179
STEP: modifying the configmap once 01/12/23 16:35:37.183
STEP: modifying the configmap a second time 01/12/23 16:35:37.188
STEP: deleting the configmap 01/12/23 16:35:37.193
STEP: creating a watch on configmaps from the resource version returned by the first update 01/12/23 16:35:37.196
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/12/23 16:35:37.197
Jan 12 16:35:37.197: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5786  e21cadbe-e054-4d6a-929a-bb690149f1d7 25787 0 2023-01-12 16:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 16:35:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:35:37.198: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5786  e21cadbe-e054-4d6a-929a-bb690149f1d7 25788 0 2023-01-12 16:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 16:35:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:37.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5786" for this suite. 01/12/23 16:35:37.2
------------------------------
 [0.040 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:37.167
    Jan 12 16:35:37.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename watch 01/12/23 16:35:37.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:37.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:37.177
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/12/23 16:35:37.179
    STEP: modifying the configmap once 01/12/23 16:35:37.183
    STEP: modifying the configmap a second time 01/12/23 16:35:37.188
    STEP: deleting the configmap 01/12/23 16:35:37.193
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/12/23 16:35:37.196
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/12/23 16:35:37.197
    Jan 12 16:35:37.197: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5786  e21cadbe-e054-4d6a-929a-bb690149f1d7 25787 0 2023-01-12 16:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 16:35:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:35:37.198: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5786  e21cadbe-e054-4d6a-929a-bb690149f1d7 25788 0 2023-01-12 16:35:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-12 16:35:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:37.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5786" for this suite. 01/12/23 16:35:37.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:37.21
Jan 12 16:35:37.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-webhook 01/12/23 16:35:37.211
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:37.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:37.222
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/12/23 16:35:37.224
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 16:35:37.681
STEP: Deploying the custom resource conversion webhook pod 01/12/23 16:35:37.685
STEP: Wait for the deployment to be ready 01/12/23 16:35:37.696
Jan 12 16:35:37.702: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:35:39.709
STEP: Verifying the service has paired with the endpoint 01/12/23 16:35:39.717
Jan 12 16:35:40.717: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 12 16:35:40.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Creating a v1 custom resource 01/12/23 16:35:43.289
STEP: v2 custom resource should be converted 01/12/23 16:35:43.292
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:43.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4816" for this suite. 01/12/23 16:35:43.837
------------------------------
 [SLOW TEST] [6.633 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:37.21
    Jan 12 16:35:37.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-webhook 01/12/23 16:35:37.211
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:37.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:37.222
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/12/23 16:35:37.224
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/12/23 16:35:37.681
    STEP: Deploying the custom resource conversion webhook pod 01/12/23 16:35:37.685
    STEP: Wait for the deployment to be ready 01/12/23 16:35:37.696
    Jan 12 16:35:37.702: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:35:39.709
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:35:39.717
    Jan 12 16:35:40.717: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 12 16:35:40.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Creating a v1 custom resource 01/12/23 16:35:43.289
    STEP: v2 custom resource should be converted 01/12/23 16:35:43.292
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:43.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4816" for this suite. 01/12/23 16:35:43.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:43.844
Jan 12 16:35:43.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:35:43.846
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:43.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:43.857
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/12/23 16:35:43.859
Jan 12 16:35:43.865: INFO: Waiting up to 5m0s for pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4" in namespace "pods-6834" to be "running and ready"
Jan 12 16:35:43.866: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671474ms
Jan 12 16:35:43.866: INFO: The phase of Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:35:45.869: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004128318s
Jan 12 16:35:45.869: INFO: The phase of Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 is Running (Ready = true)
Jan 12 16:35:45.869: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4" satisfied condition "running and ready"
Jan 12 16:35:45.872: INFO: Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 has hostIP: 10.0.40.50
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:35:45.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6834" for this suite. 01/12/23 16:35:45.874
------------------------------
 [2.033 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:43.844
    Jan 12 16:35:43.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:35:43.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:43.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:43.857
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/12/23 16:35:43.859
    Jan 12 16:35:43.865: INFO: Waiting up to 5m0s for pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4" in namespace "pods-6834" to be "running and ready"
    Jan 12 16:35:43.866: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671474ms
    Jan 12 16:35:43.866: INFO: The phase of Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:35:45.869: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004128318s
    Jan 12 16:35:45.869: INFO: The phase of Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 is Running (Ready = true)
    Jan 12 16:35:45.869: INFO: Pod "pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4" satisfied condition "running and ready"
    Jan 12 16:35:45.872: INFO: Pod pod-hostip-0e2ef432-e38b-411f-b10b-5b5044ed08b4 has hostIP: 10.0.40.50
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:35:45.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6834" for this suite. 01/12/23 16:35:45.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:35:45.878
Jan 12 16:35:45.878: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 16:35:45.879
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:45.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:45.888
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 12 16:35:45.896: INFO: Waiting up to 5m0s for pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19" in namespace "container-probe-9780" to be "running and ready"
Jan 12 16:35:45.897: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664959ms
Jan 12 16:35:45.897: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:35:47.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 2.004952173s
Jan 12 16:35:47.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:49.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 4.004425295s
Jan 12 16:35:49.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:51.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 6.005020678s
Jan 12 16:35:51.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:53.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 8.004795398s
Jan 12 16:35:53.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:55.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 10.004178762s
Jan 12 16:35:55.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:57.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 12.005524368s
Jan 12 16:35:57.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:35:59.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 14.004741139s
Jan 12 16:35:59.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:36:01.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 16.005974357s
Jan 12 16:36:01.902: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:36:03.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 18.004910807s
Jan 12 16:36:03.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:36:05.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 20.004702274s
Jan 12 16:36:05.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
Jan 12 16:36:07.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=true. Elapsed: 22.006021045s
Jan 12 16:36:07.902: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = true)
Jan 12 16:36:07.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19" satisfied condition "running and ready"
Jan 12 16:36:07.903: INFO: Container started at 2023-01-12 16:35:46 +0000 UTC, pod became ready at 2023-01-12 16:36:06 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 16:36:07.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9780" for this suite. 01/12/23 16:36:07.906
------------------------------
 [SLOW TEST] [22.032 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:35:45.878
    Jan 12 16:35:45.878: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 16:35:45.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:35:45.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:35:45.888
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 12 16:35:45.896: INFO: Waiting up to 5m0s for pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19" in namespace "container-probe-9780" to be "running and ready"
    Jan 12 16:35:45.897: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.664959ms
    Jan 12 16:35:45.897: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:35:47.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 2.004952173s
    Jan 12 16:35:47.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:49.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 4.004425295s
    Jan 12 16:35:49.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:51.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 6.005020678s
    Jan 12 16:35:51.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:53.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 8.004795398s
    Jan 12 16:35:53.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:55.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 10.004178762s
    Jan 12 16:35:55.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:57.901: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 12.005524368s
    Jan 12 16:35:57.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:35:59.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 14.004741139s
    Jan 12 16:35:59.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:36:01.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 16.005974357s
    Jan 12 16:36:01.902: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:36:03.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 18.004910807s
    Jan 12 16:36:03.901: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:36:05.900: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=false. Elapsed: 20.004702274s
    Jan 12 16:36:05.900: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = false)
    Jan 12 16:36:07.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19": Phase="Running", Reason="", readiness=true. Elapsed: 22.006021045s
    Jan 12 16:36:07.902: INFO: The phase of Pod test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19 is Running (Ready = true)
    Jan 12 16:36:07.902: INFO: Pod "test-webserver-3c8ab81b-6ec1-4716-8bca-8d528c0cfa19" satisfied condition "running and ready"
    Jan 12 16:36:07.903: INFO: Container started at 2023-01-12 16:35:46 +0000 UTC, pod became ready at 2023-01-12 16:36:06 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:36:07.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9780" for this suite. 01/12/23 16:36:07.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:36:07.911
Jan 12 16:36:07.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:36:07.912
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:07.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:07.93
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:36:07.94
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:36:08.281
STEP: Deploying the webhook pod 01/12/23 16:36:08.286
STEP: Wait for the deployment to be ready 01/12/23 16:36:08.296
Jan 12 16:36:08.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 16:36:10.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:36:12.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:36:14.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:36:16.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 16:36:18.311
STEP: Verifying the service has paired with the endpoint 01/12/23 16:36:18.32
Jan 12 16:36:19.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/12/23 16:36:19.323
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.339
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/12/23 16:36:19.348
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.356
STEP: Patching a validating webhook configuration's rules to include the create operation 01/12/23 16:36:19.362
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.367
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:36:19.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8149" for this suite. 01/12/23 16:36:19.402
STEP: Destroying namespace "webhook-8149-markers" for this suite. 01/12/23 16:36:19.409
------------------------------
 [SLOW TEST] [11.501 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:36:07.911
    Jan 12 16:36:07.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:36:07.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:07.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:07.93
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:36:07.94
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:36:08.281
    STEP: Deploying the webhook pod 01/12/23 16:36:08.286
    STEP: Wait for the deployment to be ready 01/12/23 16:36:08.296
    Jan 12 16:36:08.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 16:36:10.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:36:12.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:36:14.310: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:36:16.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 36, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 16:36:18.311
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:36:18.32
    Jan 12 16:36:19.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/12/23 16:36:19.323
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.339
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/12/23 16:36:19.348
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.356
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/12/23 16:36:19.362
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:19.367
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:36:19.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8149" for this suite. 01/12/23 16:36:19.402
    STEP: Destroying namespace "webhook-8149-markers" for this suite. 01/12/23 16:36:19.409
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:36:19.413
Jan 12 16:36:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 16:36:19.414
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:19.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:19.425
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/12/23 16:36:19.428
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local;sleep 1; done
 01/12/23 16:36:19.431
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local;sleep 1; done
 01/12/23 16:36:19.431
STEP: creating a pod to probe DNS 01/12/23 16:36:19.431
STEP: submitting the pod to kubernetes 01/12/23 16:36:19.431
Jan 12 16:36:19.439: INFO: Waiting up to 15m0s for pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f" in namespace "dns-8207" to be "running"
Jan 12 16:36:19.441: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145534ms
Jan 12 16:36:21.444: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005034887s
Jan 12 16:36:21.444: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f" satisfied condition "running"
STEP: retrieving the pod 01/12/23 16:36:21.444
STEP: looking for the results for each expected name from probers 01/12/23 16:36:21.446
Jan 12 16:36:21.451: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.454: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.456: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.458: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.461: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.463: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.465: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.468: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:21.468: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:26.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.476: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.481: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:26.490: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:31.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:31.489: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:36.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:36.489: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:41.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:41.490: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:46.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.476: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.481: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.486: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
Jan 12 16:36:46.491: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

Jan 12 16:36:51.490: INFO: DNS probes using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f succeeded

STEP: deleting the pod 01/12/23 16:36:51.49
STEP: deleting the test headless service 01/12/23 16:36:51.498
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 16:36:51.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8207" for this suite. 01/12/23 16:36:51.514
------------------------------
 [SLOW TEST] [32.108 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:36:19.413
    Jan 12 16:36:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 16:36:19.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:19.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:19.425
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/12/23 16:36:19.428
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local;sleep 1; done
     01/12/23 16:36:19.431
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local;sleep 1; done
     01/12/23 16:36:19.431
    STEP: creating a pod to probe DNS 01/12/23 16:36:19.431
    STEP: submitting the pod to kubernetes 01/12/23 16:36:19.431
    Jan 12 16:36:19.439: INFO: Waiting up to 15m0s for pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f" in namespace "dns-8207" to be "running"
    Jan 12 16:36:19.441: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145534ms
    Jan 12 16:36:21.444: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005034887s
    Jan 12 16:36:21.444: INFO: Pod "dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f" satisfied condition "running"
    STEP: retrieving the pod 01/12/23 16:36:21.444
    STEP: looking for the results for each expected name from probers 01/12/23 16:36:21.446
    Jan 12 16:36:21.451: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.454: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.456: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.458: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.461: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.463: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.465: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.468: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:21.468: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:26.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.476: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.481: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:26.490: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:31.472: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:31.489: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:36.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.489: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:36.489: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:41.473: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.475: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.477: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.480: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.482: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.485: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.487: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:41.490: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:46.474: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.476: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.481: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.483: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.486: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.488: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.490: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local from pod dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f: the server could not find the requested resource (get pods dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f)
    Jan 12 16:36:46.491: INFO: Lookups using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8207.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8207.svc.cluster.local jessie_udp@dns-test-service-2.dns-8207.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8207.svc.cluster.local]

    Jan 12 16:36:51.490: INFO: DNS probes using dns-8207/dns-test-7ba78ed8-db33-42cc-9324-38e8445d266f succeeded

    STEP: deleting the pod 01/12/23 16:36:51.49
    STEP: deleting the test headless service 01/12/23 16:36:51.498
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:36:51.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8207" for this suite. 01/12/23 16:36:51.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:36:51.526
Jan 12 16:36:51.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:36:51.527
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:51.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:51.538
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-1a88cc8c-1487-4512-a591-6e8a78539eb8 01/12/23 16:36:51.54
STEP: Creating a pod to test consume configMaps 01/12/23 16:36:51.543
Jan 12 16:36:51.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7" in namespace "projected-1548" to be "Succeeded or Failed"
Jan 12 16:36:51.550: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757362ms
Jan 12 16:36:53.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465213s
Jan 12 16:36:55.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004890221s
STEP: Saw pod success 01/12/23 16:36:55.553
Jan 12 16:36:55.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7" satisfied condition "Succeeded or Failed"
Jan 12 16:36:55.555: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:36:55.561
Jan 12 16:36:55.568: INFO: Waiting for pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 to disappear
Jan 12 16:36:55.569: INFO: Pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:36:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1548" for this suite. 01/12/23 16:36:55.571
------------------------------
 [4.049 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:36:51.526
    Jan 12 16:36:51.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:36:51.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:51.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:51.538
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-1a88cc8c-1487-4512-a591-6e8a78539eb8 01/12/23 16:36:51.54
    STEP: Creating a pod to test consume configMaps 01/12/23 16:36:51.543
    Jan 12 16:36:51.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7" in namespace "projected-1548" to be "Succeeded or Failed"
    Jan 12 16:36:51.550: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757362ms
    Jan 12 16:36:53.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465213s
    Jan 12 16:36:55.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004890221s
    STEP: Saw pod success 01/12/23 16:36:55.553
    Jan 12 16:36:55.553: INFO: Pod "pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7" satisfied condition "Succeeded or Failed"
    Jan 12 16:36:55.555: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:36:55.561
    Jan 12 16:36:55.568: INFO: Waiting for pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 to disappear
    Jan 12 16:36:55.569: INFO: Pod pod-projected-configmaps-765ad09c-1bbc-4bcb-be77-a3d99a9a9ac7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:36:55.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1548" for this suite. 01/12/23 16:36:55.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:36:55.579
Jan 12 16:36:55.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:36:55.579
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:55.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:55.591
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:36:55.6
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:36:56.325
STEP: Deploying the webhook pod 01/12/23 16:36:56.332
STEP: Wait for the deployment to be ready 01/12/23 16:36:56.34
Jan 12 16:36:56.346: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:36:58.351
STEP: Verifying the service has paired with the endpoint 01/12/23 16:36:58.362
Jan 12 16:36:59.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/12/23 16:36:59.414
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:59.452
STEP: Deleting the collection of validation webhooks 01/12/23 16:36:59.491
STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:59.517
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:36:59.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6461" for this suite. 01/12/23 16:36:59.552
STEP: Destroying namespace "webhook-6461-markers" for this suite. 01/12/23 16:36:59.558
------------------------------
 [3.984 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:36:55.579
    Jan 12 16:36:55.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:36:55.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:55.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:55.591
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:36:55.6
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:36:56.325
    STEP: Deploying the webhook pod 01/12/23 16:36:56.332
    STEP: Wait for the deployment to be ready 01/12/23 16:36:56.34
    Jan 12 16:36:56.346: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:36:58.351
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:36:58.362
    Jan 12 16:36:59.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/12/23 16:36:59.414
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:59.452
    STEP: Deleting the collection of validation webhooks 01/12/23 16:36:59.491
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/12/23 16:36:59.517
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:36:59.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6461" for this suite. 01/12/23 16:36:59.552
    STEP: Destroying namespace "webhook-6461-markers" for this suite. 01/12/23 16:36:59.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:36:59.566
Jan 12 16:36:59.566: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:36:59.567
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:59.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:59.579
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:36:59.582
Jan 12 16:36:59.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a" in namespace "downward-api-6603" to be "Succeeded or Failed"
Jan 12 16:36:59.588: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.644147ms
Jan 12 16:37:01.590: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00380993s
Jan 12 16:37:03.591: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004953567s
STEP: Saw pod success 01/12/23 16:37:03.591
Jan 12 16:37:03.592: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a" satisfied condition "Succeeded or Failed"
Jan 12 16:37:03.593: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a container client-container: <nil>
STEP: delete the pod 01/12/23 16:37:03.597
Jan 12 16:37:03.606: INFO: Waiting for pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a to disappear
Jan 12 16:37:03.607: INFO: Pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:03.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6603" for this suite. 01/12/23 16:37:03.609
------------------------------
 [4.047 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:36:59.566
    Jan 12 16:36:59.566: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:36:59.567
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:36:59.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:36:59.579
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:36:59.582
    Jan 12 16:36:59.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a" in namespace "downward-api-6603" to be "Succeeded or Failed"
    Jan 12 16:36:59.588: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.644147ms
    Jan 12 16:37:01.590: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00380993s
    Jan 12 16:37:03.591: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004953567s
    STEP: Saw pod success 01/12/23 16:37:03.591
    Jan 12 16:37:03.592: INFO: Pod "downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a" satisfied condition "Succeeded or Failed"
    Jan 12 16:37:03.593: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a container client-container: <nil>
    STEP: delete the pod 01/12/23 16:37:03.597
    Jan 12 16:37:03.606: INFO: Waiting for pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a to disappear
    Jan 12 16:37:03.607: INFO: Pod downwardapi-volume-7657cefc-d190-470e-8dd1-03111c44628a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:03.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6603" for this suite. 01/12/23 16:37:03.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:03.613
Jan 12 16:37:03.613: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 16:37:03.614
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:03.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:03.626
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/12/23 16:37:03.628
STEP: patching the Namespace 01/12/23 16:37:03.635
STEP: get the Namespace and ensuring it has the label 01/12/23 16:37:03.64
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:03.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9480" for this suite. 01/12/23 16:37:03.643
STEP: Destroying namespace "nspatchtest-73b73d73-51c6-4064-bfec-7137675922ec-4509" for this suite. 01/12/23 16:37:03.647
------------------------------
 [0.037 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:03.613
    Jan 12 16:37:03.613: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 16:37:03.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:03.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:03.626
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/12/23 16:37:03.628
    STEP: patching the Namespace 01/12/23 16:37:03.635
    STEP: get the Namespace and ensuring it has the label 01/12/23 16:37:03.64
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:03.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9480" for this suite. 01/12/23 16:37:03.643
    STEP: Destroying namespace "nspatchtest-73b73d73-51c6-4064-bfec-7137675922ec-4509" for this suite. 01/12/23 16:37:03.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:03.651
Jan 12 16:37:03.651: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:37:03.652
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:03.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:03.661
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7443 01/12/23 16:37:03.663
STEP: creating a selector 01/12/23 16:37:03.663
STEP: Creating the service pods in kubernetes 01/12/23 16:37:03.663
Jan 12 16:37:03.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 16:37:03.680: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7443" to be "running and ready"
Jan 12 16:37:03.683: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779652ms
Jan 12 16:37:03.683: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:37:05.685: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004836042s
Jan 12 16:37:05.685: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:07.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006111362s
Jan 12 16:37:07.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:09.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005762856s
Jan 12 16:37:09.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:11.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00725572s
Jan 12 16:37:11.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:13.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005924553s
Jan 12 16:37:13.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:15.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006833265s
Jan 12 16:37:15.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:17.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007264003s
Jan 12 16:37:17.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:19.685: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005294741s
Jan 12 16:37:19.685: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:21.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006158899s
Jan 12 16:37:21.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:23.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005743287s
Jan 12 16:37:23.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:37:25.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006709918s
Jan 12 16:37:25.687: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 16:37:25.687: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 16:37:25.688: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7443" to be "running and ready"
Jan 12 16:37:25.690: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.557436ms
Jan 12 16:37:25.690: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 16:37:25.690: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 16:37:25.692
Jan 12 16:37:25.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7443" to be "running"
Jan 12 16:37:25.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070647ms
Jan 12 16:37:27.707: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004749419s
Jan 12 16:37:27.707: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 16:37:27.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7443" to be "running"
Jan 12 16:37:27.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.467507ms
Jan 12 16:37:27.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 12 16:37:27.711: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 16:37:27.711: INFO: Going to poll 10.244.0.197 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 12 16:37:27.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:37:27.713: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:37:27.714: INFO: ExecWithOptions: Clientset creation
Jan 12 16:37:27.714: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7443/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 16:37:27.772: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 12 16:37:27.772: INFO: Going to poll 10.244.1.98 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 12 16:37:27.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.98:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:37:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:37:27.774: INFO: ExecWithOptions: Clientset creation
Jan 12 16:37:27.774: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7443/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.98%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 16:37:27.832: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:27.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7443" for this suite. 01/12/23 16:37:27.834
------------------------------
 [SLOW TEST] [24.187 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:03.651
    Jan 12 16:37:03.651: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:37:03.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:03.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:03.661
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7443 01/12/23 16:37:03.663
    STEP: creating a selector 01/12/23 16:37:03.663
    STEP: Creating the service pods in kubernetes 01/12/23 16:37:03.663
    Jan 12 16:37:03.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 16:37:03.680: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7443" to be "running and ready"
    Jan 12 16:37:03.683: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779652ms
    Jan 12 16:37:03.683: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:37:05.685: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004836042s
    Jan 12 16:37:05.685: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:07.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006111362s
    Jan 12 16:37:07.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:09.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005762856s
    Jan 12 16:37:09.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:11.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00725572s
    Jan 12 16:37:11.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:13.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005924553s
    Jan 12 16:37:13.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:15.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.006833265s
    Jan 12 16:37:15.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:17.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007264003s
    Jan 12 16:37:17.687: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:19.685: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005294741s
    Jan 12 16:37:19.685: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:21.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006158899s
    Jan 12 16:37:21.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:23.686: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005743287s
    Jan 12 16:37:23.686: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:37:25.687: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.006709918s
    Jan 12 16:37:25.687: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 16:37:25.687: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 16:37:25.688: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7443" to be "running and ready"
    Jan 12 16:37:25.690: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.557436ms
    Jan 12 16:37:25.690: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 16:37:25.690: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 16:37:25.692
    Jan 12 16:37:25.702: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7443" to be "running"
    Jan 12 16:37:25.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070647ms
    Jan 12 16:37:27.707: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004749419s
    Jan 12 16:37:27.707: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 16:37:27.708: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7443" to be "running"
    Jan 12 16:37:27.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.467507ms
    Jan 12 16:37:27.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 12 16:37:27.711: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 16:37:27.711: INFO: Going to poll 10.244.0.197 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 16:37:27.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.197:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:37:27.713: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:37:27.714: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:37:27.714: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7443/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.197%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 16:37:27.772: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 12 16:37:27.772: INFO: Going to poll 10.244.1.98 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 12 16:37:27.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.98:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7443 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:37:27.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:37:27.774: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:37:27.774: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7443/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.98%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 16:37:27.832: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:27.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7443" for this suite. 01/12/23 16:37:27.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:27.84
Jan 12 16:37:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:37:27.84
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:27.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:27.85
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-a80bafe1-5f1e-4125-8133-7d58684b3fa3 01/12/23 16:37:27.854
STEP: Creating secret with name s-test-opt-upd-a0596bfc-af33-429a-8f39-159e1d29d5d0 01/12/23 16:37:27.857
STEP: Creating the pod 01/12/23 16:37:27.86
Jan 12 16:37:27.865: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5" in namespace "projected-9536" to be "running and ready"
Jan 12 16:37:27.867: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94225ms
Jan 12 16:37:27.867: INFO: The phase of Pod pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:37:29.870: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005165646s
Jan 12 16:37:29.871: INFO: The phase of Pod pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5 is Running (Ready = true)
Jan 12 16:37:29.871: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-a80bafe1-5f1e-4125-8133-7d58684b3fa3 01/12/23 16:37:29.884
STEP: Updating secret s-test-opt-upd-a0596bfc-af33-429a-8f39-159e1d29d5d0 01/12/23 16:37:29.888
STEP: Creating secret with name s-test-opt-create-722d7ce9-047c-4af6-83ea-4dc43ed6d593 01/12/23 16:37:29.891
STEP: waiting to observe update in volume 01/12/23 16:37:29.894
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9536" for this suite. 01/12/23 16:37:33.918
------------------------------
 [SLOW TEST] [6.082 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:27.84
    Jan 12 16:37:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:37:27.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:27.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:27.85
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-a80bafe1-5f1e-4125-8133-7d58684b3fa3 01/12/23 16:37:27.854
    STEP: Creating secret with name s-test-opt-upd-a0596bfc-af33-429a-8f39-159e1d29d5d0 01/12/23 16:37:27.857
    STEP: Creating the pod 01/12/23 16:37:27.86
    Jan 12 16:37:27.865: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5" in namespace "projected-9536" to be "running and ready"
    Jan 12 16:37:27.867: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94225ms
    Jan 12 16:37:27.867: INFO: The phase of Pod pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:37:29.870: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005165646s
    Jan 12 16:37:29.871: INFO: The phase of Pod pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5 is Running (Ready = true)
    Jan 12 16:37:29.871: INFO: Pod "pod-projected-secrets-7b51e8d4-0df1-4806-b8d2-753270a86ab5" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-a80bafe1-5f1e-4125-8133-7d58684b3fa3 01/12/23 16:37:29.884
    STEP: Updating secret s-test-opt-upd-a0596bfc-af33-429a-8f39-159e1d29d5d0 01/12/23 16:37:29.888
    STEP: Creating secret with name s-test-opt-create-722d7ce9-047c-4af6-83ea-4dc43ed6d593 01/12/23 16:37:29.891
    STEP: waiting to observe update in volume 01/12/23 16:37:29.894
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:33.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9536" for this suite. 01/12/23 16:37:33.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:33.923
Jan 12 16:37:33.923: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename namespaces 01/12/23 16:37:33.924
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:33.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:33.933
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/12/23 16:37:33.941
Jan 12 16:37:33.943: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/12/23 16:37:33.943
Jan 12 16:37:33.952: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/12/23 16:37:33.952
Jan 12 16:37:33.960: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:33.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4590" for this suite. 01/12/23 16:37:33.962
------------------------------
 [0.042 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:33.923
    Jan 12 16:37:33.923: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename namespaces 01/12/23 16:37:33.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:33.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:33.933
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/12/23 16:37:33.941
    Jan 12 16:37:33.943: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/12/23 16:37:33.943
    Jan 12 16:37:33.952: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/12/23 16:37:33.952
    Jan 12 16:37:33.960: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:33.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4590" for this suite. 01/12/23 16:37:33.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:33.966
Jan 12 16:37:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:37:33.967
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:33.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:33.975
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-ebc35fd0-2f25-4cfc-8ca2-d04ed34ca9cc 01/12/23 16:37:33.978
STEP: Creating a pod to test consume secrets 01/12/23 16:37:33.981
Jan 12 16:37:33.985: INFO: Waiting up to 5m0s for pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c" in namespace "secrets-8540" to be "Succeeded or Failed"
Jan 12 16:37:33.987: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.613075ms
Jan 12 16:37:35.990: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005199681s
Jan 12 16:37:37.990: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175018s
STEP: Saw pod success 01/12/23 16:37:37.99
Jan 12 16:37:37.991: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c" satisfied condition "Succeeded or Failed"
Jan 12 16:37:37.993: INFO: Trying to get logs from node worker-0 pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:37:38.006
Jan 12 16:37:38.016: INFO: Waiting for pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c to disappear
Jan 12 16:37:38.018: INFO: Pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:38.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8540" for this suite. 01/12/23 16:37:38.024
------------------------------
 [4.062 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:33.966
    Jan 12 16:37:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:37:33.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:33.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:33.975
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-ebc35fd0-2f25-4cfc-8ca2-d04ed34ca9cc 01/12/23 16:37:33.978
    STEP: Creating a pod to test consume secrets 01/12/23 16:37:33.981
    Jan 12 16:37:33.985: INFO: Waiting up to 5m0s for pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c" in namespace "secrets-8540" to be "Succeeded or Failed"
    Jan 12 16:37:33.987: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.613075ms
    Jan 12 16:37:35.990: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005199681s
    Jan 12 16:37:37.990: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175018s
    STEP: Saw pod success 01/12/23 16:37:37.99
    Jan 12 16:37:37.991: INFO: Pod "pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c" satisfied condition "Succeeded or Failed"
    Jan 12 16:37:37.993: INFO: Trying to get logs from node worker-0 pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:37:38.006
    Jan 12 16:37:38.016: INFO: Waiting for pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c to disappear
    Jan 12 16:37:38.018: INFO: Pod pod-secrets-9eddaaad-5ef3-4e9c-8f8d-b810755a7a5c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:38.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8540" for this suite. 01/12/23 16:37:38.024
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:38.029
Jan 12 16:37:38.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 16:37:38.03
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:38.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:38.043
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 12 16:37:38.058: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:37:38.062
Jan 12 16:37:38.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:37:38.069: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:37:39.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:37:39.074: INFO: Node worker-1 is running 0 daemon pod, expected 1
Jan 12 16:37:40.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:37:40.074: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/12/23 16:37:40.08
STEP: Check that daemon pods images are updated. 01/12/23 16:37:40.088
Jan 12 16:37:40.090: INFO: Wrong image for pod: daemon-set-2xqjf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 16:37:40.090: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 16:37:41.095: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 16:37:42.096: INFO: Pod daemon-set-2xtbw is not available
Jan 12 16:37:42.096: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 12 16:37:44.095: INFO: Pod daemon-set-l2tc9 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/12/23 16:37:44.097
Jan 12 16:37:44.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:37:44.101: INFO: Node worker-1 is running 0 daemon pod, expected 1
Jan 12 16:37:45.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:37:45.107: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:37:45.115
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4142, will wait for the garbage collector to delete the pods 01/12/23 16:37:45.115
Jan 12 16:37:45.170: INFO: Deleting DaemonSet.extensions daemon-set took: 3.58338ms
Jan 12 16:37:45.271: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.560227ms
Jan 12 16:37:47.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:37:47.574: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 16:37:47.575: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26704"},"items":null}

Jan 12 16:37:47.577: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26704"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4142" for this suite. 01/12/23 16:37:47.585
------------------------------
 [SLOW TEST] [9.561 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:38.029
    Jan 12 16:37:38.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 16:37:38.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:38.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:38.043
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 12 16:37:38.058: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:37:38.062
    Jan 12 16:37:38.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:37:38.069: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:37:39.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:37:39.074: INFO: Node worker-1 is running 0 daemon pod, expected 1
    Jan 12 16:37:40.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:37:40.074: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/12/23 16:37:40.08
    STEP: Check that daemon pods images are updated. 01/12/23 16:37:40.088
    Jan 12 16:37:40.090: INFO: Wrong image for pod: daemon-set-2xqjf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 16:37:40.090: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 16:37:41.095: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 16:37:42.096: INFO: Pod daemon-set-2xtbw is not available
    Jan 12 16:37:42.096: INFO: Wrong image for pod: daemon-set-qldj4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 12 16:37:44.095: INFO: Pod daemon-set-l2tc9 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/12/23 16:37:44.097
    Jan 12 16:37:44.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:37:44.101: INFO: Node worker-1 is running 0 daemon pod, expected 1
    Jan 12 16:37:45.107: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:37:45.107: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:37:45.115
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4142, will wait for the garbage collector to delete the pods 01/12/23 16:37:45.115
    Jan 12 16:37:45.170: INFO: Deleting DaemonSet.extensions daemon-set took: 3.58338ms
    Jan 12 16:37:45.271: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.560227ms
    Jan 12 16:37:47.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:37:47.574: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 16:37:47.575: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26704"},"items":null}

    Jan 12 16:37:47.577: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26704"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4142" for this suite. 01/12/23 16:37:47.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:47.592
Jan 12 16:37:47.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:37:47.592
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:47.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:47.602
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/12/23 16:37:47.604
Jan 12 16:37:47.611: INFO: Waiting up to 5m0s for pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4" in namespace "projected-1835" to be "running and ready"
Jan 12 16:37:47.613: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850805ms
Jan 12 16:37:47.613: INFO: The phase of Pod labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:37:49.615: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004603014s
Jan 12 16:37:49.615: INFO: The phase of Pod labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4 is Running (Ready = true)
Jan 12 16:37:49.615: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4" satisfied condition "running and ready"
Jan 12 16:37:50.131: INFO: Successfully updated pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1835" for this suite. 01/12/23 16:37:54.15
------------------------------
 [SLOW TEST] [6.562 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:47.592
    Jan 12 16:37:47.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:37:47.592
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:47.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:47.602
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/12/23 16:37:47.604
    Jan 12 16:37:47.611: INFO: Waiting up to 5m0s for pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4" in namespace "projected-1835" to be "running and ready"
    Jan 12 16:37:47.613: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850805ms
    Jan 12 16:37:47.613: INFO: The phase of Pod labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:37:49.615: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004603014s
    Jan 12 16:37:49.615: INFO: The phase of Pod labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4 is Running (Ready = true)
    Jan 12 16:37:49.615: INFO: Pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4" satisfied condition "running and ready"
    Jan 12 16:37:50.131: INFO: Successfully updated pod "labelsupdate63905654-f7e2-4efe-8b12-8edf393b48c4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:54.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1835" for this suite. 01/12/23 16:37:54.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:54.154
Jan 12 16:37:54.154: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 16:37:54.155
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:54.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:54.167
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/12/23 16:37:54.169
Jan 12 16:37:54.174: INFO: Waiting up to 5m0s for pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b" in namespace "var-expansion-6468" to be "Succeeded or Failed"
Jan 12 16:37:54.176: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816764ms
Jan 12 16:37:56.179: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004937046s
Jan 12 16:37:58.178: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004070843s
STEP: Saw pod success 01/12/23 16:37:58.178
Jan 12 16:37:58.178: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b" satisfied condition "Succeeded or Failed"
Jan 12 16:37:58.180: INFO: Trying to get logs from node worker-1 pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b container dapi-container: <nil>
STEP: delete the pod 01/12/23 16:37:58.184
Jan 12 16:37:58.192: INFO: Waiting for pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b to disappear
Jan 12 16:37:58.194: INFO: Pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 16:37:58.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6468" for this suite. 01/12/23 16:37:58.196
------------------------------
 [4.046 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:54.154
    Jan 12 16:37:54.154: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 16:37:54.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:54.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:54.167
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/12/23 16:37:54.169
    Jan 12 16:37:54.174: INFO: Waiting up to 5m0s for pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b" in namespace "var-expansion-6468" to be "Succeeded or Failed"
    Jan 12 16:37:54.176: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816764ms
    Jan 12 16:37:56.179: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004937046s
    Jan 12 16:37:58.178: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004070843s
    STEP: Saw pod success 01/12/23 16:37:58.178
    Jan 12 16:37:58.178: INFO: Pod "var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b" satisfied condition "Succeeded or Failed"
    Jan 12 16:37:58.180: INFO: Trying to get logs from node worker-1 pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b container dapi-container: <nil>
    STEP: delete the pod 01/12/23 16:37:58.184
    Jan 12 16:37:58.192: INFO: Waiting for pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b to disappear
    Jan 12 16:37:58.194: INFO: Pod var-expansion-11e2871d-04ba-48ce-bbd5-513308553a7b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:37:58.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6468" for this suite. 01/12/23 16:37:58.196
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:37:58.202
Jan 12 16:37:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:37:58.203
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:58.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:58.213
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:37:58.215
Jan 12 16:37:58.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db" in namespace "projected-4268" to be "Succeeded or Failed"
Jan 12 16:37:58.223: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692171ms
Jan 12 16:38:00.228: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006731612s
Jan 12 16:38:02.226: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004669127s
STEP: Saw pod success 01/12/23 16:38:02.226
Jan 12 16:38:02.226: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db" satisfied condition "Succeeded or Failed"
Jan 12 16:38:02.228: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db container client-container: <nil>
STEP: delete the pod 01/12/23 16:38:02.232
Jan 12 16:38:02.243: INFO: Waiting for pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db to disappear
Jan 12 16:38:02.245: INFO: Pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:02.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4268" for this suite. 01/12/23 16:38:02.247
------------------------------
 [4.050 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:37:58.202
    Jan 12 16:37:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:37:58.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:37:58.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:37:58.213
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:37:58.215
    Jan 12 16:37:58.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db" in namespace "projected-4268" to be "Succeeded or Failed"
    Jan 12 16:37:58.223: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692171ms
    Jan 12 16:38:00.228: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006731612s
    Jan 12 16:38:02.226: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004669127s
    STEP: Saw pod success 01/12/23 16:38:02.226
    Jan 12 16:38:02.226: INFO: Pod "downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db" satisfied condition "Succeeded or Failed"
    Jan 12 16:38:02.228: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db container client-container: <nil>
    STEP: delete the pod 01/12/23 16:38:02.232
    Jan 12 16:38:02.243: INFO: Waiting for pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db to disappear
    Jan 12 16:38:02.245: INFO: Pod downwardapi-volume-bde64cfb-fef7-40ca-b1b2-242dfe7c32db no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:02.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4268" for this suite. 01/12/23 16:38:02.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:02.253
Jan 12 16:38:02.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:38:02.254
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:02.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:02.268
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-409de7c9-34d2-40ab-ac65-4d9d39ce190b 01/12/23 16:38:02.27
STEP: Creating a pod to test consume secrets 01/12/23 16:38:02.273
Jan 12 16:38:02.278: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a" in namespace "projected-9453" to be "Succeeded or Failed"
Jan 12 16:38:02.280: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.906802ms
Jan 12 16:38:04.283: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005504951s
Jan 12 16:38:06.284: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005876182s
STEP: Saw pod success 01/12/23 16:38:06.284
Jan 12 16:38:06.284: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a" satisfied condition "Succeeded or Failed"
Jan 12 16:38:06.286: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:38:06.29
Jan 12 16:38:06.299: INFO: Waiting for pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a to disappear
Jan 12 16:38:06.302: INFO: Pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:06.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9453" for this suite. 01/12/23 16:38:06.305
------------------------------
 [4.055 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:02.253
    Jan 12 16:38:02.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:38:02.254
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:02.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:02.268
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-409de7c9-34d2-40ab-ac65-4d9d39ce190b 01/12/23 16:38:02.27
    STEP: Creating a pod to test consume secrets 01/12/23 16:38:02.273
    Jan 12 16:38:02.278: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a" in namespace "projected-9453" to be "Succeeded or Failed"
    Jan 12 16:38:02.280: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.906802ms
    Jan 12 16:38:04.283: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005504951s
    Jan 12 16:38:06.284: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005876182s
    STEP: Saw pod success 01/12/23 16:38:06.284
    Jan 12 16:38:06.284: INFO: Pod "pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a" satisfied condition "Succeeded or Failed"
    Jan 12 16:38:06.286: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:38:06.29
    Jan 12 16:38:06.299: INFO: Waiting for pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a to disappear
    Jan 12 16:38:06.302: INFO: Pod pod-projected-secrets-5e280640-ef93-4193-afe9-57b909678c8a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:06.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9453" for this suite. 01/12/23 16:38:06.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:06.31
Jan 12 16:38:06.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:38:06.31
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:06.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:06.322
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 12 16:38:06.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8260" for this suite. 01/12/23 16:38:07.343
------------------------------
 [1.039 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:06.31
    Jan 12 16:38:06.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:38:06.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:06.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:06.322
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 12 16:38:06.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8260" for this suite. 01/12/23 16:38:07.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:07.35
Jan 12 16:38:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 16:38:07.351
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:07.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:07.361
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 16:38:07.366
Jan 12 16:38:07.374: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2657" to be "running and ready"
Jan 12 16:38:07.377: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648746ms
Jan 12 16:38:07.377: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:38:09.379: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004797367s
Jan 12 16:38:09.379: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 16:38:09.379: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/12/23 16:38:09.381
Jan 12 16:38:09.385: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2657" to be "running and ready"
Jan 12 16:38:09.387: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688742ms
Jan 12 16:38:09.387: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:38:11.390: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00490612s
Jan 12 16:38:11.390: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 12 16:38:11.390: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/12/23 16:38:11.392
STEP: delete the pod with lifecycle hook 01/12/23 16:38:11.396
Jan 12 16:38:11.402: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 16:38:11.404: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 12 16:38:13.405: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 16:38:13.407: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 12 16:38:15.405: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 12 16:38:15.407: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:15.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2657" for this suite. 01/12/23 16:38:15.41
------------------------------
 [SLOW TEST] [8.064 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:07.35
    Jan 12 16:38:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 16:38:07.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:07.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:07.361
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 16:38:07.366
    Jan 12 16:38:07.374: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2657" to be "running and ready"
    Jan 12 16:38:07.377: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.648746ms
    Jan 12 16:38:07.377: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:38:09.379: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004797367s
    Jan 12 16:38:09.379: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 16:38:09.379: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/12/23 16:38:09.381
    Jan 12 16:38:09.385: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-2657" to be "running and ready"
    Jan 12 16:38:09.387: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688742ms
    Jan 12 16:38:09.387: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:38:11.390: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00490612s
    Jan 12 16:38:11.390: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 12 16:38:11.390: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/12/23 16:38:11.392
    STEP: delete the pod with lifecycle hook 01/12/23 16:38:11.396
    Jan 12 16:38:11.402: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 16:38:11.404: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 12 16:38:13.405: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 16:38:13.407: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 12 16:38:15.405: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 12 16:38:15.407: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:15.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2657" for this suite. 01/12/23 16:38:15.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:15.416
Jan 12 16:38:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:38:15.417
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:15.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:15.427
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/12/23 16:38:15.429
Jan 12 16:38:15.430: INFO: namespace kubectl-9647
Jan 12 16:38:15.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 create -f -'
Jan 12 16:38:16.144: INFO: stderr: ""
Jan 12 16:38:16.144: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 16:38:16.144
Jan 12 16:38:17.147: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:38:17.147: INFO: Found 0 / 1
Jan 12 16:38:18.148: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:38:18.148: INFO: Found 1 / 1
Jan 12 16:38:18.148: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 12 16:38:18.149: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:38:18.149: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 16:38:18.149: INFO: wait on agnhost-primary startup in kubectl-9647 
Jan 12 16:38:18.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 logs agnhost-primary-bcc79 agnhost-primary'
Jan 12 16:38:18.211: INFO: stderr: ""
Jan 12 16:38:18.211: INFO: stdout: "Paused\n"
STEP: exposing RC 01/12/23 16:38:18.211
Jan 12 16:38:18.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 12 16:38:18.280: INFO: stderr: ""
Jan 12 16:38:18.280: INFO: stdout: "service/rm2 exposed\n"
Jan 12 16:38:18.284: INFO: Service rm2 in namespace kubectl-9647 found.
STEP: exposing service 01/12/23 16:38:20.289
Jan 12 16:38:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 12 16:38:20.358: INFO: stderr: ""
Jan 12 16:38:20.358: INFO: stdout: "service/rm3 exposed\n"
Jan 12 16:38:20.360: INFO: Service rm3 in namespace kubectl-9647 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9647" for this suite. 01/12/23 16:38:22.367
------------------------------
 [SLOW TEST] [6.954 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:15.416
    Jan 12 16:38:15.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:38:15.417
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:15.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:15.427
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/12/23 16:38:15.429
    Jan 12 16:38:15.430: INFO: namespace kubectl-9647
    Jan 12 16:38:15.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 create -f -'
    Jan 12 16:38:16.144: INFO: stderr: ""
    Jan 12 16:38:16.144: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 16:38:16.144
    Jan 12 16:38:17.147: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:38:17.147: INFO: Found 0 / 1
    Jan 12 16:38:18.148: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:38:18.148: INFO: Found 1 / 1
    Jan 12 16:38:18.148: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 12 16:38:18.149: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:38:18.149: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 16:38:18.149: INFO: wait on agnhost-primary startup in kubectl-9647 
    Jan 12 16:38:18.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 logs agnhost-primary-bcc79 agnhost-primary'
    Jan 12 16:38:18.211: INFO: stderr: ""
    Jan 12 16:38:18.211: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/12/23 16:38:18.211
    Jan 12 16:38:18.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 12 16:38:18.280: INFO: stderr: ""
    Jan 12 16:38:18.280: INFO: stdout: "service/rm2 exposed\n"
    Jan 12 16:38:18.284: INFO: Service rm2 in namespace kubectl-9647 found.
    STEP: exposing service 01/12/23 16:38:20.289
    Jan 12 16:38:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-9647 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 12 16:38:20.358: INFO: stderr: ""
    Jan 12 16:38:20.358: INFO: stdout: "service/rm3 exposed\n"
    Jan 12 16:38:20.360: INFO: Service rm3 in namespace kubectl-9647 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9647" for this suite. 01/12/23 16:38:22.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:22.371
Jan 12 16:38:22.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-runtime 01/12/23 16:38:22.372
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:22.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:22.384
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/12/23 16:38:22.386
STEP: wait for the container to reach Succeeded 01/12/23 16:38:22.391
STEP: get the container status 01/12/23 16:38:26.407
STEP: the container should be terminated 01/12/23 16:38:26.409
STEP: the termination message should be set 01/12/23 16:38:26.409
Jan 12 16:38:26.409: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/12/23 16:38:26.409
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2240" for this suite. 01/12/23 16:38:26.423
------------------------------
 [4.055 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:22.371
    Jan 12 16:38:22.371: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-runtime 01/12/23 16:38:22.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:22.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:22.384
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/12/23 16:38:22.386
    STEP: wait for the container to reach Succeeded 01/12/23 16:38:22.391
    STEP: get the container status 01/12/23 16:38:26.407
    STEP: the container should be terminated 01/12/23 16:38:26.409
    STEP: the termination message should be set 01/12/23 16:38:26.409
    Jan 12 16:38:26.409: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/12/23 16:38:26.409
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:26.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2240" for this suite. 01/12/23 16:38:26.423
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:26.427
Jan 12 16:38:26.427: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:38:26.428
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:26.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:26.437
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3ba99ed7-3492-4a24-b04c-dcf8106bd0e8 01/12/23 16:38:26.443
STEP: Creating the pod 01/12/23 16:38:26.448
Jan 12 16:38:26.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4" in namespace "projected-4271" to be "running and ready"
Jan 12 16:38:26.456: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321389ms
Jan 12 16:38:26.456: INFO: The phase of Pod pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:38:28.458: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005516996s
Jan 12 16:38:28.458: INFO: The phase of Pod pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4 is Running (Ready = true)
Jan 12 16:38:28.458: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-3ba99ed7-3492-4a24-b04c-dcf8106bd0e8 01/12/23 16:38:28.464
STEP: waiting to observe update in volume 01/12/23 16:38:28.467
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:30.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4271" for this suite. 01/12/23 16:38:30.478
------------------------------
 [4.055 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:26.427
    Jan 12 16:38:26.427: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:38:26.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:26.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:26.437
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-3ba99ed7-3492-4a24-b04c-dcf8106bd0e8 01/12/23 16:38:26.443
    STEP: Creating the pod 01/12/23 16:38:26.448
    Jan 12 16:38:26.452: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4" in namespace "projected-4271" to be "running and ready"
    Jan 12 16:38:26.456: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321389ms
    Jan 12 16:38:26.456: INFO: The phase of Pod pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:38:28.458: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005516996s
    Jan 12 16:38:28.458: INFO: The phase of Pod pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4 is Running (Ready = true)
    Jan 12 16:38:28.458: INFO: Pod "pod-projected-configmaps-7249912a-cbc0-493f-85bb-3769c72d60d4" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-3ba99ed7-3492-4a24-b04c-dcf8106bd0e8 01/12/23 16:38:28.464
    STEP: waiting to observe update in volume 01/12/23 16:38:28.467
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:30.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4271" for this suite. 01/12/23 16:38:30.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:30.483
Jan 12 16:38:30.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:38:30.484
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:30.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:30.494
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-89f12213-0a16-4da3-9762-02f59ab1435e 01/12/23 16:38:30.497
STEP: Creating a pod to test consume secrets 01/12/23 16:38:30.5
Jan 12 16:38:30.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd" in namespace "projected-3055" to be "Succeeded or Failed"
Jan 12 16:38:30.510: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931546ms
Jan 12 16:38:32.514: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006102041s
Jan 12 16:38:34.513: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005874918s
STEP: Saw pod success 01/12/23 16:38:34.513
Jan 12 16:38:34.513: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd" satisfied condition "Succeeded or Failed"
Jan 12 16:38:34.515: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:38:34.52
Jan 12 16:38:34.530: INFO: Waiting for pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd to disappear
Jan 12 16:38:34.531: INFO: Pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:34.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3055" for this suite. 01/12/23 16:38:34.533
------------------------------
 [4.054 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:30.483
    Jan 12 16:38:30.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:38:30.484
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:30.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:30.494
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-89f12213-0a16-4da3-9762-02f59ab1435e 01/12/23 16:38:30.497
    STEP: Creating a pod to test consume secrets 01/12/23 16:38:30.5
    Jan 12 16:38:30.507: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd" in namespace "projected-3055" to be "Succeeded or Failed"
    Jan 12 16:38:30.510: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.931546ms
    Jan 12 16:38:32.514: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006102041s
    Jan 12 16:38:34.513: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005874918s
    STEP: Saw pod success 01/12/23 16:38:34.513
    Jan 12 16:38:34.513: INFO: Pod "pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd" satisfied condition "Succeeded or Failed"
    Jan 12 16:38:34.515: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:38:34.52
    Jan 12 16:38:34.530: INFO: Waiting for pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd to disappear
    Jan 12 16:38:34.531: INFO: Pod pod-projected-secrets-3537ef6f-0801-4f53-b333-da7489e9b9bd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:34.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3055" for this suite. 01/12/23 16:38:34.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:34.539
Jan 12 16:38:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename controllerrevisions 01/12/23 16:38:34.54
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:34.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:34.554
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-8vzdw-daemon-set" 01/12/23 16:38:34.564
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:38:34.57
Jan 12 16:38:34.574: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
Jan 12 16:38:34.574: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:38:35.578: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
Jan 12 16:38:35.578: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:38:36.579: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 2
Jan 12 16:38:36.579: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-8vzdw-daemon-set
STEP: Confirm DaemonSet "e2e-8vzdw-daemon-set" successfully created with "daemonset-name=e2e-8vzdw-daemon-set" label 01/12/23 16:38:36.581
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8vzdw-daemon-set" 01/12/23 16:38:36.584
Jan 12 16:38:36.586: INFO: Located ControllerRevision: "e2e-8vzdw-daemon-set-7dd984f486"
STEP: Patching ControllerRevision "e2e-8vzdw-daemon-set-7dd984f486" 01/12/23 16:38:36.587
Jan 12 16:38:36.592: INFO: e2e-8vzdw-daemon-set-7dd984f486 has been patched
STEP: Create a new ControllerRevision 01/12/23 16:38:36.592
Jan 12 16:38:36.598: INFO: Created ControllerRevision: e2e-8vzdw-daemon-set-5d6cf4b6dd
STEP: Confirm that there are two ControllerRevisions 01/12/23 16:38:36.598
Jan 12 16:38:36.598: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 16:38:36.600: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-8vzdw-daemon-set-7dd984f486" 01/12/23 16:38:36.6
STEP: Confirm that there is only one ControllerRevision 01/12/23 16:38:36.603
Jan 12 16:38:36.603: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 16:38:36.605: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-8vzdw-daemon-set-5d6cf4b6dd" 01/12/23 16:38:36.606
Jan 12 16:38:36.611: INFO: e2e-8vzdw-daemon-set-5d6cf4b6dd has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/12/23 16:38:36.611
W0112 16:38:36.617101      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/12/23 16:38:36.617
Jan 12 16:38:36.617: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 16:38:37.619: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 16:38:37.621: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8vzdw-daemon-set-5d6cf4b6dd=updated" 01/12/23 16:38:37.621
STEP: Confirm that there is only one ControllerRevision 01/12/23 16:38:37.626
Jan 12 16:38:37.626: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 12 16:38:37.628: INFO: Found 1 ControllerRevisions
Jan 12 16:38:37.629: INFO: ControllerRevision "e2e-8vzdw-daemon-set-764cb66bc7" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-8vzdw-daemon-set" 01/12/23 16:38:37.631
STEP: deleting DaemonSet.extensions e2e-8vzdw-daemon-set in namespace controllerrevisions-3789, will wait for the garbage collector to delete the pods 01/12/23 16:38:37.631
Jan 12 16:38:37.686: INFO: Deleting DaemonSet.extensions e2e-8vzdw-daemon-set took: 3.62303ms
Jan 12 16:38:37.787: INFO: Terminating DaemonSet.extensions e2e-8vzdw-daemon-set pods took: 100.62322ms
Jan 12 16:38:38.889: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
Jan 12 16:38:38.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8vzdw-daemon-set
Jan 12 16:38:38.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27170"},"items":null}

Jan 12 16:38:38.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27170"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:38.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3789" for this suite. 01/12/23 16:38:38.9
------------------------------
 [4.366 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:34.539
    Jan 12 16:38:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename controllerrevisions 01/12/23 16:38:34.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:34.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:34.554
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-8vzdw-daemon-set" 01/12/23 16:38:34.564
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:38:34.57
    Jan 12 16:38:34.574: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
    Jan 12 16:38:34.574: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:38:35.578: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
    Jan 12 16:38:35.578: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:38:36.579: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 2
    Jan 12 16:38:36.579: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-8vzdw-daemon-set
    STEP: Confirm DaemonSet "e2e-8vzdw-daemon-set" successfully created with "daemonset-name=e2e-8vzdw-daemon-set" label 01/12/23 16:38:36.581
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-8vzdw-daemon-set" 01/12/23 16:38:36.584
    Jan 12 16:38:36.586: INFO: Located ControllerRevision: "e2e-8vzdw-daemon-set-7dd984f486"
    STEP: Patching ControllerRevision "e2e-8vzdw-daemon-set-7dd984f486" 01/12/23 16:38:36.587
    Jan 12 16:38:36.592: INFO: e2e-8vzdw-daemon-set-7dd984f486 has been patched
    STEP: Create a new ControllerRevision 01/12/23 16:38:36.592
    Jan 12 16:38:36.598: INFO: Created ControllerRevision: e2e-8vzdw-daemon-set-5d6cf4b6dd
    STEP: Confirm that there are two ControllerRevisions 01/12/23 16:38:36.598
    Jan 12 16:38:36.598: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 16:38:36.600: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-8vzdw-daemon-set-7dd984f486" 01/12/23 16:38:36.6
    STEP: Confirm that there is only one ControllerRevision 01/12/23 16:38:36.603
    Jan 12 16:38:36.603: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 16:38:36.605: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-8vzdw-daemon-set-5d6cf4b6dd" 01/12/23 16:38:36.606
    Jan 12 16:38:36.611: INFO: e2e-8vzdw-daemon-set-5d6cf4b6dd has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/12/23 16:38:36.611
    W0112 16:38:36.617101      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/12/23 16:38:36.617
    Jan 12 16:38:36.617: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 16:38:37.619: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 16:38:37.621: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-8vzdw-daemon-set-5d6cf4b6dd=updated" 01/12/23 16:38:37.621
    STEP: Confirm that there is only one ControllerRevision 01/12/23 16:38:37.626
    Jan 12 16:38:37.626: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 12 16:38:37.628: INFO: Found 1 ControllerRevisions
    Jan 12 16:38:37.629: INFO: ControllerRevision "e2e-8vzdw-daemon-set-764cb66bc7" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-8vzdw-daemon-set" 01/12/23 16:38:37.631
    STEP: deleting DaemonSet.extensions e2e-8vzdw-daemon-set in namespace controllerrevisions-3789, will wait for the garbage collector to delete the pods 01/12/23 16:38:37.631
    Jan 12 16:38:37.686: INFO: Deleting DaemonSet.extensions e2e-8vzdw-daemon-set took: 3.62303ms
    Jan 12 16:38:37.787: INFO: Terminating DaemonSet.extensions e2e-8vzdw-daemon-set pods took: 100.62322ms
    Jan 12 16:38:38.889: INFO: Number of nodes with available pods controlled by daemonset e2e-8vzdw-daemon-set: 0
    Jan 12 16:38:38.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-8vzdw-daemon-set
    Jan 12 16:38:38.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27170"},"items":null}

    Jan 12 16:38:38.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27170"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:38.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3789" for this suite. 01/12/23 16:38:38.9
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:38.905
Jan 12 16:38:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:38:38.906
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:38.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:38.917
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:38:38.919
Jan 12 16:38:38.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 12 16:38:38.986: INFO: stderr: ""
Jan 12 16:38:38.987: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/12/23 16:38:38.987
Jan 12 16:38:38.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 12 16:38:39.166: INFO: stderr: ""
Jan 12 16:38:39.166: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:38:39.166
Jan 12 16:38:39.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 delete pods e2e-test-httpd-pod'
Jan 12 16:38:40.636: INFO: stderr: ""
Jan 12 16:38:40.636: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:38:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6589" for this suite. 01/12/23 16:38:40.639
------------------------------
 [1.737 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:38.905
    Jan 12 16:38:38.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:38:38.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:38.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:38.917
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:38:38.919
    Jan 12 16:38:38.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 12 16:38:38.986: INFO: stderr: ""
    Jan 12 16:38:38.987: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/12/23 16:38:38.987
    Jan 12 16:38:38.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 12 16:38:39.166: INFO: stderr: ""
    Jan 12 16:38:39.166: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/12/23 16:38:39.166
    Jan 12 16:38:39.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-6589 delete pods e2e-test-httpd-pod'
    Jan 12 16:38:40.636: INFO: stderr: ""
    Jan 12 16:38:40.636: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:38:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6589" for this suite. 01/12/23 16:38:40.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:38:40.648
Jan 12 16:38:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:38:40.649
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:40.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:40.662
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 12 16:38:40.673: INFO: created pod
Jan 12 16:38:40.673: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1395" to be "Succeeded or Failed"
Jan 12 16:38:40.675: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632591ms
Jan 12 16:38:42.678: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004909238s
Jan 12 16:38:44.677: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00403974s
STEP: Saw pod success 01/12/23 16:38:44.677
Jan 12 16:38:44.678: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 12 16:39:14.678: INFO: polling logs
Jan 12 16:39:14.684: INFO: Pod logs: 
I0112 16:38:41.197858       1 log.go:198] OK: Got token
I0112 16:38:41.198039       1 log.go:198] validating with in-cluster discovery
I0112 16:38:41.198358       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0112 16:38:41.198434       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1395:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673542120, NotBefore:1673541520, IssuedAt:1673541520, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1395", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b4e661dc-8ffd-4790-a830-ba82609038b2"}}}
I0112 16:38:41.205307       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0112 16:38:41.206526       1 log.go:198] OK: Validated signature on JWT
I0112 16:38:41.206615       1 log.go:198] OK: Got valid claims from token!
I0112 16:38:41.206675       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1395:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673542120, NotBefore:1673541520, IssuedAt:1673541520, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1395", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b4e661dc-8ffd-4790-a830-ba82609038b2"}}}

Jan 12 16:39:14.684: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1395" for this suite. 01/12/23 16:39:14.69
------------------------------
 [SLOW TEST] [34.046 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:38:40.648
    Jan 12 16:38:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:38:40.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:38:40.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:38:40.662
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 12 16:38:40.673: INFO: created pod
    Jan 12 16:38:40.673: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1395" to be "Succeeded or Failed"
    Jan 12 16:38:40.675: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.632591ms
    Jan 12 16:38:42.678: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004909238s
    Jan 12 16:38:44.677: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00403974s
    STEP: Saw pod success 01/12/23 16:38:44.677
    Jan 12 16:38:44.678: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 12 16:39:14.678: INFO: polling logs
    Jan 12 16:39:14.684: INFO: Pod logs: 
    I0112 16:38:41.197858       1 log.go:198] OK: Got token
    I0112 16:38:41.198039       1 log.go:198] validating with in-cluster discovery
    I0112 16:38:41.198358       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0112 16:38:41.198434       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1395:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673542120, NotBefore:1673541520, IssuedAt:1673541520, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1395", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b4e661dc-8ffd-4790-a830-ba82609038b2"}}}
    I0112 16:38:41.205307       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0112 16:38:41.206526       1 log.go:198] OK: Validated signature on JWT
    I0112 16:38:41.206615       1 log.go:198] OK: Got valid claims from token!
    I0112 16:38:41.206675       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1395:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673542120, NotBefore:1673541520, IssuedAt:1673541520, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1395", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b4e661dc-8ffd-4790-a830-ba82609038b2"}}}

    Jan 12 16:39:14.684: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1395" for this suite. 01/12/23 16:39:14.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:14.695
Jan 12 16:39:14.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:39:14.696
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:14.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:14.707
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-f9026d9a-eefc-4d20-a05c-3a29dfad927b 01/12/23 16:39:14.711
STEP: Creating a pod to test consume secrets 01/12/23 16:39:14.717
Jan 12 16:39:14.722: INFO: Waiting up to 5m0s for pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1" in namespace "secrets-3498" to be "Succeeded or Failed"
Jan 12 16:39:14.724: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750394ms
Jan 12 16:39:16.727: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004892166s
Jan 12 16:39:18.726: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004161394s
STEP: Saw pod success 01/12/23 16:39:18.726
Jan 12 16:39:18.726: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1" satisfied condition "Succeeded or Failed"
Jan 12 16:39:18.729: INFO: Trying to get logs from node worker-1 pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:39:18.733
Jan 12 16:39:18.741: INFO: Waiting for pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 to disappear
Jan 12 16:39:18.742: INFO: Pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:18.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3498" for this suite. 01/12/23 16:39:18.744
------------------------------
 [4.053 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:14.695
    Jan 12 16:39:14.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:39:14.696
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:14.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:14.707
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-f9026d9a-eefc-4d20-a05c-3a29dfad927b 01/12/23 16:39:14.711
    STEP: Creating a pod to test consume secrets 01/12/23 16:39:14.717
    Jan 12 16:39:14.722: INFO: Waiting up to 5m0s for pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1" in namespace "secrets-3498" to be "Succeeded or Failed"
    Jan 12 16:39:14.724: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.750394ms
    Jan 12 16:39:16.727: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004892166s
    Jan 12 16:39:18.726: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004161394s
    STEP: Saw pod success 01/12/23 16:39:18.726
    Jan 12 16:39:18.726: INFO: Pod "pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1" satisfied condition "Succeeded or Failed"
    Jan 12 16:39:18.729: INFO: Trying to get logs from node worker-1 pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:39:18.733
    Jan 12 16:39:18.741: INFO: Waiting for pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 to disappear
    Jan 12 16:39:18.742: INFO: Pod pod-secrets-e6ed460c-ea26-4ee9-a9aa-723d41e951f1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:18.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3498" for this suite. 01/12/23 16:39:18.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:18.75
Jan 12 16:39:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename conformance-tests 01/12/23 16:39:18.75
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:18.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:18.762
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/12/23 16:39:18.764
Jan 12 16:39:18.764: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-9660" for this suite. 01/12/23 16:39:18.77
------------------------------
 [0.024 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:18.75
    Jan 12 16:39:18.750: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename conformance-tests 01/12/23 16:39:18.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:18.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:18.762
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/12/23 16:39:18.764
    Jan 12 16:39:18.764: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:18.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-9660" for this suite. 01/12/23 16:39:18.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:18.775
Jan 12 16:39:18.775: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:39:18.776
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:18.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:18.786
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:39:18.788
Jan 12 16:39:18.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7" in namespace "downward-api-5366" to be "Succeeded or Failed"
Jan 12 16:39:18.799: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135776ms
Jan 12 16:39:20.802: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978013s
Jan 12 16:39:22.801: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005588965s
STEP: Saw pod success 01/12/23 16:39:22.801
Jan 12 16:39:22.801: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7" satisfied condition "Succeeded or Failed"
Jan 12 16:39:22.803: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 container client-container: <nil>
STEP: delete the pod 01/12/23 16:39:22.807
Jan 12 16:39:22.816: INFO: Waiting for pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 to disappear
Jan 12 16:39:22.817: INFO: Pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:22.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5366" for this suite. 01/12/23 16:39:22.822
------------------------------
 [4.050 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:18.775
    Jan 12 16:39:18.775: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:39:18.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:18.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:18.786
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:39:18.788
    Jan 12 16:39:18.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7" in namespace "downward-api-5366" to be "Succeeded or Failed"
    Jan 12 16:39:18.799: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135776ms
    Jan 12 16:39:20.802: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005978013s
    Jan 12 16:39:22.801: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005588965s
    STEP: Saw pod success 01/12/23 16:39:22.801
    Jan 12 16:39:22.801: INFO: Pod "downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7" satisfied condition "Succeeded or Failed"
    Jan 12 16:39:22.803: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:39:22.807
    Jan 12 16:39:22.816: INFO: Waiting for pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 to disappear
    Jan 12 16:39:22.817: INFO: Pod downwardapi-volume-242af391-e6d7-4c13-bdd5-160ac446c9d7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:22.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5366" for this suite. 01/12/23 16:39:22.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:22.827
Jan 12 16:39:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 16:39:22.828
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:22.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:22.837
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 12 16:39:22.850: INFO: Waiting up to 5m0s for pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e" in namespace "emptydir-wrapper-8226" to be "running and ready"
Jan 12 16:39:22.852: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174689ms
Jan 12 16:39:22.852: INFO: The phase of Pod pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:39:24.855: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00516374s
Jan 12 16:39:24.855: INFO: The phase of Pod pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e is Running (Ready = true)
Jan 12 16:39:24.855: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/12/23 16:39:24.857
STEP: Cleaning up the configmap 01/12/23 16:39:24.861
STEP: Cleaning up the pod 01/12/23 16:39:24.864
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:24.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8226" for this suite. 01/12/23 16:39:24.876
------------------------------
 [2.053 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:22.827
    Jan 12 16:39:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 16:39:22.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:22.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:22.837
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 12 16:39:22.850: INFO: Waiting up to 5m0s for pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e" in namespace "emptydir-wrapper-8226" to be "running and ready"
    Jan 12 16:39:22.852: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174689ms
    Jan 12 16:39:22.852: INFO: The phase of Pod pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:39:24.855: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00516374s
    Jan 12 16:39:24.855: INFO: The phase of Pod pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e is Running (Ready = true)
    Jan 12 16:39:24.855: INFO: Pod "pod-secrets-aa7976c8-acf7-42bc-b74d-cff2e1fd8e1e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/12/23 16:39:24.857
    STEP: Cleaning up the configmap 01/12/23 16:39:24.861
    STEP: Cleaning up the pod 01/12/23 16:39:24.864
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:24.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8226" for this suite. 01/12/23 16:39:24.876
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:24.88
Jan 12 16:39:24.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:39:24.881
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:24.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:24.892
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 12 16:39:24.902: INFO: Waiting up to 5m0s for pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130" in namespace "svcaccounts-5769" to be "running"
Jan 12 16:39:24.904: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977809ms
Jan 12 16:39:26.908: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130": Phase="Running", Reason="", readiness=true. Elapsed: 2.005553914s
Jan 12 16:39:26.908: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130" satisfied condition "running"
STEP: reading a file in the container 01/12/23 16:39:26.908
Jan 12 16:39:26.908: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/12/23 16:39:27.034
Jan 12 16:39:27.034: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/12/23 16:39:27.149
Jan 12 16:39:27.149: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 12 16:39:27.273: INFO: Got root ca configmap in namespace "svcaccounts-5769"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:27.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5769" for this suite. 01/12/23 16:39:27.277
------------------------------
 [2.400 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:24.88
    Jan 12 16:39:24.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename svcaccounts 01/12/23 16:39:24.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:24.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:24.892
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 12 16:39:24.902: INFO: Waiting up to 5m0s for pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130" in namespace "svcaccounts-5769" to be "running"
    Jan 12 16:39:24.904: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130": Phase="Pending", Reason="", readiness=false. Elapsed: 1.977809ms
    Jan 12 16:39:26.908: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130": Phase="Running", Reason="", readiness=true. Elapsed: 2.005553914s
    Jan 12 16:39:26.908: INFO: Pod "pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130" satisfied condition "running"
    STEP: reading a file in the container 01/12/23 16:39:26.908
    Jan 12 16:39:26.908: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/12/23 16:39:27.034
    Jan 12 16:39:27.034: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/12/23 16:39:27.149
    Jan 12 16:39:27.149: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5769 pod-service-account-a84f0cdf-5889-4dee-a694-b7bf9c422130 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 12 16:39:27.273: INFO: Got root ca configmap in namespace "svcaccounts-5769"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:27.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5769" for this suite. 01/12/23 16:39:27.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:27.281
Jan 12 16:39:27.281: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:39:27.282
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:27.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:27.293
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 12 16:39:27.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: creating the pod 01/12/23 16:39:27.296
STEP: submitting the pod to kubernetes 01/12/23 16:39:27.296
Jan 12 16:39:27.301: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65" in namespace "pods-6380" to be "running and ready"
Jan 12 16:39:27.302: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768046ms
Jan 12 16:39:27.302: INFO: The phase of Pod pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:39:29.305: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65": Phase="Running", Reason="", readiness=true. Elapsed: 2.004246589s
Jan 12 16:39:29.305: INFO: The phase of Pod pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65 is Running (Ready = true)
Jan 12 16:39:29.305: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:39:29.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6380" for this suite. 01/12/23 16:39:29.316
------------------------------
 [2.039 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:27.281
    Jan 12 16:39:27.281: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:39:27.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:27.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:27.293
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 12 16:39:27.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: creating the pod 01/12/23 16:39:27.296
    STEP: submitting the pod to kubernetes 01/12/23 16:39:27.296
    Jan 12 16:39:27.301: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65" in namespace "pods-6380" to be "running and ready"
    Jan 12 16:39:27.302: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768046ms
    Jan 12 16:39:27.302: INFO: The phase of Pod pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:39:29.305: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65": Phase="Running", Reason="", readiness=true. Elapsed: 2.004246589s
    Jan 12 16:39:29.305: INFO: The phase of Pod pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65 is Running (Ready = true)
    Jan 12 16:39:29.305: INFO: Pod "pod-logs-websocket-740d46a9-5e46-4c5f-b4c4-10392b52ea65" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:39:29.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6380" for this suite. 01/12/23 16:39:29.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:39:29.321
Jan 12 16:39:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename var-expansion 01/12/23 16:39:29.322
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:29.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:29.342
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/12/23 16:39:29.345
Jan 12 16:39:29.351: INFO: Waiting up to 2m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112" to be "running"
Jan 12 16:39:29.353: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2319ms
Jan 12 16:39:31.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005599561s
Jan 12 16:39:33.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005393532s
Jan 12 16:39:35.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00452541s
Jan 12 16:39:37.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00584161s
Jan 12 16:39:39.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005180291s
Jan 12 16:39:41.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004723165s
Jan 12 16:39:43.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005858938s
Jan 12 16:39:45.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006250707s
Jan 12 16:39:47.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006739714s
Jan 12 16:39:49.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00536393s
Jan 12 16:39:51.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005854337s
Jan 12 16:39:53.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006134558s
Jan 12 16:39:55.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005458832s
Jan 12 16:39:57.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004715493s
Jan 12 16:39:59.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005985861s
Jan 12 16:40:01.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006420918s
Jan 12 16:40:03.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005579466s
Jan 12 16:40:05.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006203059s
Jan 12 16:40:07.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00662829s
Jan 12 16:40:09.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004867004s
Jan 12 16:40:11.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006246088s
Jan 12 16:40:13.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005441069s
Jan 12 16:40:15.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005629618s
Jan 12 16:40:17.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005337358s
Jan 12 16:40:19.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00507182s
Jan 12 16:40:21.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006951108s
Jan 12 16:40:23.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004605377s
Jan 12 16:40:25.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006031887s
Jan 12 16:40:27.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0053639s
Jan 12 16:40:29.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004739439s
Jan 12 16:40:31.355: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004407073s
Jan 12 16:40:33.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005639873s
Jan 12 16:40:35.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005983427s
Jan 12 16:40:37.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006222498s
Jan 12 16:40:39.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005062195s
Jan 12 16:40:41.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00656431s
Jan 12 16:40:43.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005488372s
Jan 12 16:40:45.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00597272s
Jan 12 16:40:47.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006233179s
Jan 12 16:40:49.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004498534s
Jan 12 16:40:51.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004657058s
Jan 12 16:40:53.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006584627s
Jan 12 16:40:55.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006291083s
Jan 12 16:40:57.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00652377s
Jan 12 16:40:59.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004778778s
Jan 12 16:41:01.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006036761s
Jan 12 16:41:03.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006560322s
Jan 12 16:41:05.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006008964s
Jan 12 16:41:07.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00630812s
Jan 12 16:41:09.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004527565s
Jan 12 16:41:11.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005713563s
Jan 12 16:41:13.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006119964s
Jan 12 16:41:15.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006196165s
Jan 12 16:41:17.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00691319s
Jan 12 16:41:19.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005827088s
Jan 12 16:41:21.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006172209s
Jan 12 16:41:23.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006539574s
Jan 12 16:41:25.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006296868s
Jan 12 16:41:27.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006323301s
Jan 12 16:41:29.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004586587s
Jan 12 16:41:29.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006159343s
STEP: updating the pod 01/12/23 16:41:29.357
Jan 12 16:41:29.867: INFO: Successfully updated pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab"
STEP: waiting for pod running 01/12/23 16:41:29.867
Jan 12 16:41:29.867: INFO: Waiting up to 2m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112" to be "running"
Jan 12 16:41:29.869: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927329ms
Jan 12 16:41:31.873: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.006090778s
Jan 12 16:41:31.873: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" satisfied condition "running"
STEP: deleting the pod gracefully 01/12/23 16:41:31.873
Jan 12 16:41:31.874: INFO: Deleting pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112"
Jan 12 16:41:31.879: INFO: Wait up to 5m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:03.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4112" for this suite. 01/12/23 16:42:03.886
------------------------------
 [SLOW TEST] [154.569 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:39:29.321
    Jan 12 16:39:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename var-expansion 01/12/23 16:39:29.322
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:39:29.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:39:29.342
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/12/23 16:39:29.345
    Jan 12 16:39:29.351: INFO: Waiting up to 2m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112" to be "running"
    Jan 12 16:39:29.353: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2319ms
    Jan 12 16:39:31.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005599561s
    Jan 12 16:39:33.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005393532s
    Jan 12 16:39:35.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00452541s
    Jan 12 16:39:37.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00584161s
    Jan 12 16:39:39.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005180291s
    Jan 12 16:39:41.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004723165s
    Jan 12 16:39:43.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005858938s
    Jan 12 16:39:45.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006250707s
    Jan 12 16:39:47.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006739714s
    Jan 12 16:39:49.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00536393s
    Jan 12 16:39:51.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005854337s
    Jan 12 16:39:53.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006134558s
    Jan 12 16:39:55.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005458832s
    Jan 12 16:39:57.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004715493s
    Jan 12 16:39:59.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005985861s
    Jan 12 16:40:01.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006420918s
    Jan 12 16:40:03.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 34.005579466s
    Jan 12 16:40:05.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006203059s
    Jan 12 16:40:07.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00662829s
    Jan 12 16:40:09.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004867004s
    Jan 12 16:40:11.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006246088s
    Jan 12 16:40:13.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005441069s
    Jan 12 16:40:15.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005629618s
    Jan 12 16:40:17.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005337358s
    Jan 12 16:40:19.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00507182s
    Jan 12 16:40:21.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006951108s
    Jan 12 16:40:23.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 54.004605377s
    Jan 12 16:40:25.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006031887s
    Jan 12 16:40:27.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0053639s
    Jan 12 16:40:29.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004739439s
    Jan 12 16:40:31.355: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004407073s
    Jan 12 16:40:33.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005639873s
    Jan 12 16:40:35.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.005983427s
    Jan 12 16:40:37.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006222498s
    Jan 12 16:40:39.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005062195s
    Jan 12 16:40:41.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00656431s
    Jan 12 16:40:43.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005488372s
    Jan 12 16:40:45.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00597272s
    Jan 12 16:40:47.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006233179s
    Jan 12 16:40:49.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004498534s
    Jan 12 16:40:51.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004657058s
    Jan 12 16:40:53.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006584627s
    Jan 12 16:40:55.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006291083s
    Jan 12 16:40:57.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.00652377s
    Jan 12 16:40:59.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004778778s
    Jan 12 16:41:01.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006036761s
    Jan 12 16:41:03.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006560322s
    Jan 12 16:41:05.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006008964s
    Jan 12 16:41:07.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00630812s
    Jan 12 16:41:09.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004527565s
    Jan 12 16:41:11.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005713563s
    Jan 12 16:41:13.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006119964s
    Jan 12 16:41:15.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006196165s
    Jan 12 16:41:17.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00691319s
    Jan 12 16:41:19.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.005827088s
    Jan 12 16:41:21.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006172209s
    Jan 12 16:41:23.358: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006539574s
    Jan 12 16:41:25.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006296868s
    Jan 12 16:41:27.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006323301s
    Jan 12 16:41:29.356: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004586587s
    Jan 12 16:41:29.357: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006159343s
    STEP: updating the pod 01/12/23 16:41:29.357
    Jan 12 16:41:29.867: INFO: Successfully updated pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab"
    STEP: waiting for pod running 01/12/23 16:41:29.867
    Jan 12 16:41:29.867: INFO: Waiting up to 2m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112" to be "running"
    Jan 12 16:41:29.869: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.927329ms
    Jan 12 16:41:31.873: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab": Phase="Running", Reason="", readiness=true. Elapsed: 2.006090778s
    Jan 12 16:41:31.873: INFO: Pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" satisfied condition "running"
    STEP: deleting the pod gracefully 01/12/23 16:41:31.873
    Jan 12 16:41:31.874: INFO: Deleting pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" in namespace "var-expansion-4112"
    Jan 12 16:41:31.879: INFO: Wait up to 5m0s for pod "var-expansion-ba333f67-8c62-436c-aa39-566a4aff98ab" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:03.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4112" for this suite. 01/12/23 16:42:03.886
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:03.89
Jan 12 16:42:03.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:42:03.891
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:03.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:03.903
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-5c193f76-346c-4a3a-a534-fd0cc734e9b6 01/12/23 16:42:03.905
STEP: Creating a pod to test consume secrets 01/12/23 16:42:03.908
Jan 12 16:42:03.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261" in namespace "projected-7435" to be "Succeeded or Failed"
Jan 12 16:42:03.916: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645948ms
Jan 12 16:42:05.919: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004242338s
Jan 12 16:42:07.920: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0056995s
STEP: Saw pod success 01/12/23 16:42:07.92
Jan 12 16:42:07.920: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261" satisfied condition "Succeeded or Failed"
Jan 12 16:42:07.922: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:42:07.934
Jan 12 16:42:07.943: INFO: Waiting for pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 to disappear
Jan 12 16:42:07.944: INFO: Pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:07.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7435" for this suite. 01/12/23 16:42:07.947
------------------------------
 [4.060 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:03.89
    Jan 12 16:42:03.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:42:03.891
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:03.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:03.903
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-5c193f76-346c-4a3a-a534-fd0cc734e9b6 01/12/23 16:42:03.905
    STEP: Creating a pod to test consume secrets 01/12/23 16:42:03.908
    Jan 12 16:42:03.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261" in namespace "projected-7435" to be "Succeeded or Failed"
    Jan 12 16:42:03.916: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645948ms
    Jan 12 16:42:05.919: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004242338s
    Jan 12 16:42:07.920: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0056995s
    STEP: Saw pod success 01/12/23 16:42:07.92
    Jan 12 16:42:07.920: INFO: Pod "pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261" satisfied condition "Succeeded or Failed"
    Jan 12 16:42:07.922: INFO: Trying to get logs from node worker-1 pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:42:07.934
    Jan 12 16:42:07.943: INFO: Waiting for pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 to disappear
    Jan 12 16:42:07.944: INFO: Pod pod-projected-secrets-f62fed45-b8fd-43e0-8402-ba46f9a92261 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:07.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7435" for this suite. 01/12/23 16:42:07.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:07.951
Jan 12 16:42:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:42:07.952
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:07.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:07.963
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 12 16:42:07.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:11.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8688" for this suite. 01/12/23 16:42:11.066
------------------------------
 [3.118 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:07.951
    Jan 12 16:42:07.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:42:07.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:07.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:07.963
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 12 16:42:07.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:11.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8688" for this suite. 01/12/23 16:42:11.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:11.071
Jan 12 16:42:11.071: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename containers 01/12/23 16:42:11.072
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:11.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:11.081
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 12 16:42:11.088: INFO: Waiting up to 5m0s for pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc" in namespace "containers-6401" to be "running"
Jan 12 16:42:11.090: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675403ms
Jan 12 16:42:13.092: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004329261s
Jan 12 16:42:13.092: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:13.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6401" for this suite. 01/12/23 16:42:13.099
------------------------------
 [2.031 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:11.071
    Jan 12 16:42:11.071: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename containers 01/12/23 16:42:11.072
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:11.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:11.081
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 12 16:42:11.088: INFO: Waiting up to 5m0s for pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc" in namespace "containers-6401" to be "running"
    Jan 12 16:42:11.090: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.675403ms
    Jan 12 16:42:13.092: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004329261s
    Jan 12 16:42:13.092: INFO: Pod "client-containers-b560f1da-3d9e-4ba6-ab0c-fbabb6e309dc" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:13.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6401" for this suite. 01/12/23 16:42:13.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:13.106
Jan 12 16:42:13.106: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 16:42:13.107
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:13.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:13.117
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/12/23 16:42:13.121
STEP: Patching the Job 01/12/23 16:42:13.124
STEP: Watching for Job to be patched 01/12/23 16:42:13.137
Jan 12 16:42:13.139: INFO: Event ADDED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 12 16:42:13.139: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 12 16:42:13.139: INFO: Event MODIFIED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/12/23 16:42:13.139
STEP: Watching for Job to be updated 01/12/23 16:42:13.144
Jan 12 16:42:13.146: INFO: Event MODIFIED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:13.146: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/12/23 16:42:13.146
Jan 12 16:42:13.148: INFO: Job: e2e-mn4v7 as labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched]
STEP: Waiting for job to complete 01/12/23 16:42:13.148
STEP: Delete a job collection with a labelselector 01/12/23 16:42:21.152
STEP: Watching for Job to be deleted 01/12/23 16:42:21.156
Jan 12 16:42:21.157: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 12 16:42:21.158: INFO: Event DELETED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/12/23 16:42:21.158
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:21.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8982" for this suite. 01/12/23 16:42:21.164
------------------------------
 [SLOW TEST] [8.061 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:13.106
    Jan 12 16:42:13.106: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 16:42:13.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:13.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:13.117
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/12/23 16:42:13.121
    STEP: Patching the Job 01/12/23 16:42:13.124
    STEP: Watching for Job to be patched 01/12/23 16:42:13.137
    Jan 12 16:42:13.139: INFO: Event ADDED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 12 16:42:13.139: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 12 16:42:13.139: INFO: Event MODIFIED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/12/23 16:42:13.139
    STEP: Watching for Job to be updated 01/12/23 16:42:13.144
    Jan 12 16:42:13.146: INFO: Event MODIFIED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:13.146: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/12/23 16:42:13.146
    Jan 12 16:42:13.148: INFO: Job: e2e-mn4v7 as labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched]
    STEP: Waiting for job to complete 01/12/23 16:42:13.148
    STEP: Delete a job collection with a labelselector 01/12/23 16:42:21.152
    STEP: Watching for Job to be deleted 01/12/23 16:42:21.156
    Jan 12 16:42:21.157: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:21.158: INFO: Event MODIFIED observed for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 12 16:42:21.158: INFO: Event DELETED found for Job e2e-mn4v7 in namespace job-8982 with labels: map[e2e-job-label:e2e-mn4v7 e2e-mn4v7:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/12/23 16:42:21.158
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:21.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8982" for this suite. 01/12/23 16:42:21.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:21.168
Jan 12 16:42:21.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:42:21.169
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:21.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:21.188
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 12 16:42:21.196: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a" in namespace "kubelet-test-1282" to be "running and ready"
Jan 12 16:42:21.198: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819379ms
Jan 12 16:42:21.198: INFO: The phase of Pod busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:42:23.201: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004474045s
Jan 12 16:42:23.201: INFO: The phase of Pod busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a is Running (Ready = true)
Jan 12 16:42:23.201: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:23.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1282" for this suite. 01/12/23 16:42:23.209
------------------------------
 [2.044 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:21.168
    Jan 12 16:42:21.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:42:21.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:21.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:21.188
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 12 16:42:21.196: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a" in namespace "kubelet-test-1282" to be "running and ready"
    Jan 12 16:42:21.198: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819379ms
    Jan 12 16:42:21.198: INFO: The phase of Pod busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:42:23.201: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a": Phase="Running", Reason="", readiness=true. Elapsed: 2.004474045s
    Jan 12 16:42:23.201: INFO: The phase of Pod busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a is Running (Ready = true)
    Jan 12 16:42:23.201: INFO: Pod "busybox-readonly-fsa636403b-96f9-495a-82b2-2d691443724a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:23.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1282" for this suite. 01/12/23 16:42:23.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:23.213
Jan 12 16:42:23.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:42:23.214
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:23.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:23.225
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-e2681b1f-01d2-42b1-b98d-b0d4d30a7c6d 01/12/23 16:42:23.227
STEP: Creating a pod to test consume secrets 01/12/23 16:42:23.23
Jan 12 16:42:23.234: INFO: Waiting up to 5m0s for pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc" in namespace "secrets-9530" to be "Succeeded or Failed"
Jan 12 16:42:23.236: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914422ms
Jan 12 16:42:25.239: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005028779s
Jan 12 16:42:27.240: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006211696s
STEP: Saw pod success 01/12/23 16:42:27.24
Jan 12 16:42:27.241: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc" satisfied condition "Succeeded or Failed"
Jan 12 16:42:27.242: INFO: Trying to get logs from node worker-1 pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc container secret-volume-test: <nil>
STEP: delete the pod 01/12/23 16:42:27.247
Jan 12 16:42:27.255: INFO: Waiting for pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc to disappear
Jan 12 16:42:27.256: INFO: Pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:27.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9530" for this suite. 01/12/23 16:42:27.258
------------------------------
 [4.049 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:23.213
    Jan 12 16:42:23.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:42:23.214
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:23.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:23.225
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-e2681b1f-01d2-42b1-b98d-b0d4d30a7c6d 01/12/23 16:42:23.227
    STEP: Creating a pod to test consume secrets 01/12/23 16:42:23.23
    Jan 12 16:42:23.234: INFO: Waiting up to 5m0s for pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc" in namespace "secrets-9530" to be "Succeeded or Failed"
    Jan 12 16:42:23.236: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.914422ms
    Jan 12 16:42:25.239: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005028779s
    Jan 12 16:42:27.240: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006211696s
    STEP: Saw pod success 01/12/23 16:42:27.24
    Jan 12 16:42:27.241: INFO: Pod "pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc" satisfied condition "Succeeded or Failed"
    Jan 12 16:42:27.242: INFO: Trying to get logs from node worker-1 pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc container secret-volume-test: <nil>
    STEP: delete the pod 01/12/23 16:42:27.247
    Jan 12 16:42:27.255: INFO: Waiting for pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc to disappear
    Jan 12 16:42:27.256: INFO: Pod pod-secrets-f14e69e5-003e-4e20-82b5-994b608268cc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:27.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9530" for this suite. 01/12/23 16:42:27.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:27.263
Jan 12 16:42:27.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context-test 01/12/23 16:42:27.264
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:27.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:27.276
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 12 16:42:27.282: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c" in namespace "security-context-test-5863" to be "Succeeded or Failed"
Jan 12 16:42:27.284: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654614ms
Jan 12 16:42:29.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004751651s
Jan 12 16:42:31.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004253047s
Jan 12 16:42:31.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:31.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5863" for this suite. 01/12/23 16:42:31.293
------------------------------
 [4.034 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:27.263
    Jan 12 16:42:27.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context-test 01/12/23 16:42:27.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:27.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:27.276
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 12 16:42:27.282: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c" in namespace "security-context-test-5863" to be "Succeeded or Failed"
    Jan 12 16:42:27.284: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654614ms
    Jan 12 16:42:29.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004751651s
    Jan 12 16:42:31.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004253047s
    Jan 12 16:42:31.287: INFO: Pod "alpine-nnp-false-e0d68489-32af-4336-9832-fa5b135f8a5c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:31.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5863" for this suite. 01/12/23 16:42:31.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:31.298
Jan 12 16:42:31.298: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename subpath 01/12/23 16:42:31.299
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:31.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:31.311
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/12/23 16:42:31.313
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-zp2p 01/12/23 16:42:31.32
STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:42:31.32
Jan 12 16:42:31.327: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zp2p" in namespace "subpath-3811" to be "Succeeded or Failed"
Jan 12 16:42:31.329: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741847ms
Jan 12 16:42:33.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005161848s
Jan 12 16:42:35.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.005431878s
Jan 12 16:42:37.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 6.004637507s
Jan 12 16:42:39.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 8.004640324s
Jan 12 16:42:41.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 10.004859066s
Jan 12 16:42:43.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 12.004382395s
Jan 12 16:42:45.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 14.005673629s
Jan 12 16:42:47.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 16.006360086s
Jan 12 16:42:49.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 18.004566995s
Jan 12 16:42:51.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 20.005535794s
Jan 12 16:42:53.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=false. Elapsed: 22.005897241s
Jan 12 16:42:55.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005589265s
STEP: Saw pod success 01/12/23 16:42:55.333
Jan 12 16:42:55.333: INFO: Pod "pod-subpath-test-projected-zp2p" satisfied condition "Succeeded or Failed"
Jan 12 16:42:55.335: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-projected-zp2p container test-container-subpath-projected-zp2p: <nil>
STEP: delete the pod 01/12/23 16:42:55.34
Jan 12 16:42:55.348: INFO: Waiting for pod pod-subpath-test-projected-zp2p to disappear
Jan 12 16:42:55.351: INFO: Pod pod-subpath-test-projected-zp2p no longer exists
STEP: Deleting pod pod-subpath-test-projected-zp2p 01/12/23 16:42:55.351
Jan 12 16:42:55.351: INFO: Deleting pod "pod-subpath-test-projected-zp2p" in namespace "subpath-3811"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:55.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3811" for this suite. 01/12/23 16:42:55.355
------------------------------
 [SLOW TEST] [24.060 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:31.298
    Jan 12 16:42:31.298: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename subpath 01/12/23 16:42:31.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:31.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:31.311
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/12/23 16:42:31.313
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-zp2p 01/12/23 16:42:31.32
    STEP: Creating a pod to test atomic-volume-subpath 01/12/23 16:42:31.32
    Jan 12 16:42:31.327: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zp2p" in namespace "subpath-3811" to be "Succeeded or Failed"
    Jan 12 16:42:31.329: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741847ms
    Jan 12 16:42:33.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 2.005161848s
    Jan 12 16:42:35.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 4.005431878s
    Jan 12 16:42:37.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 6.004637507s
    Jan 12 16:42:39.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 8.004640324s
    Jan 12 16:42:41.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 10.004859066s
    Jan 12 16:42:43.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 12.004382395s
    Jan 12 16:42:45.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 14.005673629s
    Jan 12 16:42:47.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 16.006360086s
    Jan 12 16:42:49.332: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 18.004566995s
    Jan 12 16:42:51.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=true. Elapsed: 20.005535794s
    Jan 12 16:42:53.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Running", Reason="", readiness=false. Elapsed: 22.005897241s
    Jan 12 16:42:55.333: INFO: Pod "pod-subpath-test-projected-zp2p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005589265s
    STEP: Saw pod success 01/12/23 16:42:55.333
    Jan 12 16:42:55.333: INFO: Pod "pod-subpath-test-projected-zp2p" satisfied condition "Succeeded or Failed"
    Jan 12 16:42:55.335: INFO: Trying to get logs from node worker-1 pod pod-subpath-test-projected-zp2p container test-container-subpath-projected-zp2p: <nil>
    STEP: delete the pod 01/12/23 16:42:55.34
    Jan 12 16:42:55.348: INFO: Waiting for pod pod-subpath-test-projected-zp2p to disappear
    Jan 12 16:42:55.351: INFO: Pod pod-subpath-test-projected-zp2p no longer exists
    STEP: Deleting pod pod-subpath-test-projected-zp2p 01/12/23 16:42:55.351
    Jan 12 16:42:55.351: INFO: Deleting pod "pod-subpath-test-projected-zp2p" in namespace "subpath-3811"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:55.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3811" for this suite. 01/12/23 16:42:55.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:55.362
Jan 12 16:42:55.362: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename podtemplate 01/12/23 16:42:55.363
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:55.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:55.372
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 12 16:42:55.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9676" for this suite. 01/12/23 16:42:55.394
------------------------------
 [0.035 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:55.362
    Jan 12 16:42:55.362: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename podtemplate 01/12/23 16:42:55.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:55.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:55.372
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:42:55.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9676" for this suite. 01/12/23 16:42:55.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:42:55.399
Jan 12 16:42:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename init-container 01/12/23 16:42:55.399
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:55.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:55.41
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/12/23 16:42:55.412
Jan 12 16:42:55.413: INFO: PodSpec: initContainers in spec.initContainers
Jan 12 16:43:38.458: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d47f855c-d7f7-4dda-9eae-c51c9ab8bcf3", GenerateName:"", Namespace:"init-container-4435", SelfLink:"", UID:"0745d5d8-f3df-465b-8b29-d5c3da5cc494", ResourceVersion:"28346", Generation:0, CreationTimestamp:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"413074781"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064b42a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 16, 43, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064b42d0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qm79f", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00411f160), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006196ca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003f16c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006196d30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006196d60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006196d68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006196d6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0014ce290), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.42.160", PodIP:"10.244.0.206", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.206"}}, StartTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f1b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f1c00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://86bca7a65e7782a61c16183160db323bbb4db551e326f8c0e5a26d6ce4b01ec4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00411f1e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00411f1c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc006196e6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4435" for this suite. 01/12/23 16:43:38.46
------------------------------
 [SLOW TEST] [43.066 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:42:55.399
    Jan 12 16:42:55.399: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename init-container 01/12/23 16:42:55.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:42:55.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:42:55.41
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/12/23 16:42:55.412
    Jan 12 16:42:55.413: INFO: PodSpec: initContainers in spec.initContainers
    Jan 12 16:43:38.458: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d47f855c-d7f7-4dda-9eae-c51c9ab8bcf3", GenerateName:"", Namespace:"init-container-4435", SelfLink:"", UID:"0745d5d8-f3df-465b-8b29-d5c3da5cc494", ResourceVersion:"28346", Generation:0, CreationTimestamp:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"413074781"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064b42a0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 12, 16, 43, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0064b42d0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qm79f", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00411f160), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qm79f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006196ca8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker-0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003f16c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006196d30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006196d60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006196d68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006196d6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0014ce290), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.42.160", PodIP:"10.244.0.206", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.0.206"}}, StartTime:time.Date(2023, time.January, 12, 16, 42, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f1b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003f1c00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://86bca7a65e7782a61c16183160db323bbb4db551e326f8c0e5a26d6ce4b01ec4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00411f1e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00411f1c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc006196e6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4435" for this suite. 01/12/23 16:43:38.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:38.465
Jan 12 16:43:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 16:43:38.466
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:38.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:38.476
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/12/23 16:43:38.48
Jan 12 16:43:38.480: INFO: Creating simple deployment test-deployment-x7h8v
Jan 12 16:43:38.487: INFO: deployment "test-deployment-x7h8v" doesn't have the required revision set
STEP: Getting /status 01/12/23 16:43:40.496
Jan 12 16:43:40.498: INFO: Deployment test-deployment-x7h8v has Conditions: [{Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/12/23 16:43:40.498
Jan 12 16:43:40.505: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 43, 38, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-x7h8v-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/12/23 16:43:40.505
Jan 12 16:43:40.506: INFO: Observed &Deployment event: ADDED
Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
Jan 12 16:43:40.506: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x7h8v-54bc444df" is progressing.}
Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
Jan 12 16:43:40.507: INFO: Found Deployment test-deployment-x7h8v in namespace deployment-7127 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 16:43:40.507: INFO: Deployment test-deployment-x7h8v has an updated status
STEP: patching the Statefulset Status 01/12/23 16:43:40.507
Jan 12 16:43:40.507: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 16:43:40.511: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/12/23 16:43:40.511
Jan 12 16:43:40.513: INFO: Observed &Deployment event: ADDED
Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x7h8v-54bc444df" is progressing.}
Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
Jan 12 16:43:40.514: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 16:43:40.514: INFO: Observed &Deployment event: MODIFIED
Jan 12 16:43:40.514: INFO: Found deployment test-deployment-x7h8v in namespace deployment-7127 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 12 16:43:40.514: INFO: Deployment test-deployment-x7h8v has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 16:43:40.517: INFO: Deployment "test-deployment-x7h8v":
&Deployment{ObjectMeta:{test-deployment-x7h8v  deployment-7127  ed2f0c24-abab-4469-a201-cf92a2378ef3 28379 1 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-12 16:43:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-12 16:43:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003436bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-x7h8v-54bc444df",LastUpdateTime:2023-01-12 16:43:40 +0000 UTC,LastTransitionTime:2023-01-12 16:43:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 16:43:40.519: INFO: New ReplicaSet "test-deployment-x7h8v-54bc444df" of Deployment "test-deployment-x7h8v":
&ReplicaSet{ObjectMeta:{test-deployment-x7h8v-54bc444df  deployment-7127  6420d839-e9cc-4122-8184-dc4db97cc56d 28372 1 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-x7h8v ed2f0c24-abab-4469-a201-cf92a2378ef3 0xc003437010 0xc003437011}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2f0c24-abab-4469-a201-cf92a2378ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:43:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034370b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:43:40.521: INFO: Pod "test-deployment-x7h8v-54bc444df-67c27" is available:
&Pod{ObjectMeta:{test-deployment-x7h8v-54bc444df-67c27 test-deployment-x7h8v-54bc444df- deployment-7127  bd84ac8d-30f9-4004-ac40-742af5fdf583 28371 0 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-x7h8v-54bc444df 6420d839-e9cc-4122-8184-dc4db97cc56d 0xc003437450 0xc003437451}] [] [{kube-controller-manager Update v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6420d839-e9cc-4122-8184-dc4db97cc56d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:43:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5kp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5kp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.128,StartTime:2023-01-12 16:43:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:43:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e702baa9b2b03d85b256de30a22a438a389661101142e84955d3c6aa598bc93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:40.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7127" for this suite. 01/12/23 16:43:40.524
------------------------------
 [2.065 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:38.465
    Jan 12 16:43:38.465: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 16:43:38.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:38.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:38.476
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/12/23 16:43:38.48
    Jan 12 16:43:38.480: INFO: Creating simple deployment test-deployment-x7h8v
    Jan 12 16:43:38.487: INFO: deployment "test-deployment-x7h8v" doesn't have the required revision set
    STEP: Getting /status 01/12/23 16:43:40.496
    Jan 12 16:43:40.498: INFO: Deployment test-deployment-x7h8v has Conditions: [{Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/12/23 16:43:40.498
    Jan 12 16:43:40.505: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 43, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 43, 38, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-x7h8v-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/12/23 16:43:40.505
    Jan 12 16:43:40.506: INFO: Observed &Deployment event: ADDED
    Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
    Jan 12 16:43:40.506: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
    Jan 12 16:43:40.506: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x7h8v-54bc444df" is progressing.}
    Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
    Jan 12 16:43:40.507: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 16:43:40.507: INFO: Observed Deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
    Jan 12 16:43:40.507: INFO: Found Deployment test-deployment-x7h8v in namespace deployment-7127 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 16:43:40.507: INFO: Deployment test-deployment-x7h8v has an updated status
    STEP: patching the Statefulset Status 01/12/23 16:43:40.507
    Jan 12 16:43:40.507: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 16:43:40.511: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/12/23 16:43:40.511
    Jan 12 16:43:40.513: INFO: Observed &Deployment event: ADDED
    Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
    Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x7h8v-54bc444df"}
    Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 12 16:43:40.513: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:38 +0000 UTC 2023-01-12 16:43:38 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x7h8v-54bc444df" is progressing.}
    Jan 12 16:43:40.513: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
    Jan 12 16:43:40.514: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-12 16:43:39 +0000 UTC 2023-01-12 16:43:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x7h8v-54bc444df" has successfully progressed.}
    Jan 12 16:43:40.514: INFO: Observed deployment test-deployment-x7h8v in namespace deployment-7127 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 16:43:40.514: INFO: Observed &Deployment event: MODIFIED
    Jan 12 16:43:40.514: INFO: Found deployment test-deployment-x7h8v in namespace deployment-7127 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 12 16:43:40.514: INFO: Deployment test-deployment-x7h8v has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 16:43:40.517: INFO: Deployment "test-deployment-x7h8v":
    &Deployment{ObjectMeta:{test-deployment-x7h8v  deployment-7127  ed2f0c24-abab-4469-a201-cf92a2378ef3 28379 1 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-12 16:43:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-12 16:43:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003436bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-x7h8v-54bc444df",LastUpdateTime:2023-01-12 16:43:40 +0000 UTC,LastTransitionTime:2023-01-12 16:43:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 16:43:40.519: INFO: New ReplicaSet "test-deployment-x7h8v-54bc444df" of Deployment "test-deployment-x7h8v":
    &ReplicaSet{ObjectMeta:{test-deployment-x7h8v-54bc444df  deployment-7127  6420d839-e9cc-4122-8184-dc4db97cc56d 28372 1 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-x7h8v ed2f0c24-abab-4469-a201-cf92a2378ef3 0xc003437010 0xc003437011}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed2f0c24-abab-4469-a201-cf92a2378ef3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:43:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034370b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:43:40.521: INFO: Pod "test-deployment-x7h8v-54bc444df-67c27" is available:
    &Pod{ObjectMeta:{test-deployment-x7h8v-54bc444df-67c27 test-deployment-x7h8v-54bc444df- deployment-7127  bd84ac8d-30f9-4004-ac40-742af5fdf583 28371 0 2023-01-12 16:43:38 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-x7h8v-54bc444df 6420d839-e9cc-4122-8184-dc4db97cc56d 0xc003437450 0xc003437451}] [] [{kube-controller-manager Update v1 2023-01-12 16:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6420d839-e9cc-4122-8184-dc4db97cc56d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:43:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.128\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5kp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5kp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.128,StartTime:2023-01-12 16:43:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:43:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0e702baa9b2b03d85b256de30a22a438a389661101142e84955d3c6aa598bc93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.128,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:40.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7127" for this suite. 01/12/23 16:43:40.524
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:40.531
Jan 12 16:43:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replication-controller 01/12/23 16:43:40.532
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:40.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:40.542
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/12/23 16:43:40.546
STEP: waiting for RC to be added 01/12/23 16:43:40.552
STEP: waiting for available Replicas 01/12/23 16:43:40.552
STEP: patching ReplicationController 01/12/23 16:43:41.248
STEP: waiting for RC to be modified 01/12/23 16:43:41.253
STEP: patching ReplicationController status 01/12/23 16:43:41.253
STEP: waiting for RC to be modified 01/12/23 16:43:41.259
STEP: waiting for available Replicas 01/12/23 16:43:41.259
STEP: fetching ReplicationController status 01/12/23 16:43:41.262
STEP: patching ReplicationController scale 01/12/23 16:43:41.264
STEP: waiting for RC to be modified 01/12/23 16:43:41.269
STEP: waiting for ReplicationController's scale to be the max amount 01/12/23 16:43:41.27
STEP: fetching ReplicationController; ensuring that it's patched 01/12/23 16:43:42.469
STEP: updating ReplicationController status 01/12/23 16:43:42.471
STEP: waiting for RC to be modified 01/12/23 16:43:42.475
STEP: listing all ReplicationControllers 01/12/23 16:43:42.475
STEP: checking that ReplicationController has expected values 01/12/23 16:43:42.477
STEP: deleting ReplicationControllers by collection 01/12/23 16:43:42.477
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/12/23 16:43:42.483
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:42.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2785" for this suite. 01/12/23 16:43:42.532
------------------------------
 [2.005 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:40.531
    Jan 12 16:43:40.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replication-controller 01/12/23 16:43:40.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:40.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:40.542
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/12/23 16:43:40.546
    STEP: waiting for RC to be added 01/12/23 16:43:40.552
    STEP: waiting for available Replicas 01/12/23 16:43:40.552
    STEP: patching ReplicationController 01/12/23 16:43:41.248
    STEP: waiting for RC to be modified 01/12/23 16:43:41.253
    STEP: patching ReplicationController status 01/12/23 16:43:41.253
    STEP: waiting for RC to be modified 01/12/23 16:43:41.259
    STEP: waiting for available Replicas 01/12/23 16:43:41.259
    STEP: fetching ReplicationController status 01/12/23 16:43:41.262
    STEP: patching ReplicationController scale 01/12/23 16:43:41.264
    STEP: waiting for RC to be modified 01/12/23 16:43:41.269
    STEP: waiting for ReplicationController's scale to be the max amount 01/12/23 16:43:41.27
    STEP: fetching ReplicationController; ensuring that it's patched 01/12/23 16:43:42.469
    STEP: updating ReplicationController status 01/12/23 16:43:42.471
    STEP: waiting for RC to be modified 01/12/23 16:43:42.475
    STEP: listing all ReplicationControllers 01/12/23 16:43:42.475
    STEP: checking that ReplicationController has expected values 01/12/23 16:43:42.477
    STEP: deleting ReplicationControllers by collection 01/12/23 16:43:42.477
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/12/23 16:43:42.483
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:42.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2785" for this suite. 01/12/23 16:43:42.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:42.537
Jan 12 16:43:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:43:42.538
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:42.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:42.547
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 16:43:42.55
Jan 12 16:43:42.556: INFO: Waiting up to 5m0s for pod "pod-ff251ec5-c622-4919-8433-01b42cf42281" in namespace "emptydir-6037" to be "Succeeded or Failed"
Jan 12 16:43:42.561: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418998ms
Jan 12 16:43:44.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007123432s
Jan 12 16:43:46.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007794918s
STEP: Saw pod success 01/12/23 16:43:46.564
Jan 12 16:43:46.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281" satisfied condition "Succeeded or Failed"
Jan 12 16:43:46.566: INFO: Trying to get logs from node worker-1 pod pod-ff251ec5-c622-4919-8433-01b42cf42281 container test-container: <nil>
STEP: delete the pod 01/12/23 16:43:46.571
Jan 12 16:43:46.583: INFO: Waiting for pod pod-ff251ec5-c622-4919-8433-01b42cf42281 to disappear
Jan 12 16:43:46.585: INFO: Pod pod-ff251ec5-c622-4919-8433-01b42cf42281 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:46.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6037" for this suite. 01/12/23 16:43:46.587
------------------------------
 [4.053 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:42.537
    Jan 12 16:43:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:43:42.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:42.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:42.547
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/12/23 16:43:42.55
    Jan 12 16:43:42.556: INFO: Waiting up to 5m0s for pod "pod-ff251ec5-c622-4919-8433-01b42cf42281" in namespace "emptydir-6037" to be "Succeeded or Failed"
    Jan 12 16:43:42.561: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418998ms
    Jan 12 16:43:44.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007123432s
    Jan 12 16:43:46.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007794918s
    STEP: Saw pod success 01/12/23 16:43:46.564
    Jan 12 16:43:46.564: INFO: Pod "pod-ff251ec5-c622-4919-8433-01b42cf42281" satisfied condition "Succeeded or Failed"
    Jan 12 16:43:46.566: INFO: Trying to get logs from node worker-1 pod pod-ff251ec5-c622-4919-8433-01b42cf42281 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:43:46.571
    Jan 12 16:43:46.583: INFO: Waiting for pod pod-ff251ec5-c622-4919-8433-01b42cf42281 to disappear
    Jan 12 16:43:46.585: INFO: Pod pod-ff251ec5-c622-4919-8433-01b42cf42281 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:46.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6037" for this suite. 01/12/23 16:43:46.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:46.594
Jan 12 16:43:46.594: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pods 01/12/23 16:43:46.595
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:46.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:46.607
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/12/23 16:43:46.609
Jan 12 16:43:46.614: INFO: Waiting up to 5m0s for pod "pod-m4wgp" in namespace "pods-8898" to be "running"
Jan 12 16:43:46.616: INFO: Pod "pod-m4wgp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90928ms
Jan 12 16:43:48.619: INFO: Pod "pod-m4wgp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004934047s
Jan 12 16:43:48.619: INFO: Pod "pod-m4wgp" satisfied condition "running"
STEP: patching /status 01/12/23 16:43:48.619
Jan 12 16:43:48.627: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:48.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8898" for this suite. 01/12/23 16:43:48.63
------------------------------
 [2.040 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:46.594
    Jan 12 16:43:46.594: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pods 01/12/23 16:43:46.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:46.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:46.607
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/12/23 16:43:46.609
    Jan 12 16:43:46.614: INFO: Waiting up to 5m0s for pod "pod-m4wgp" in namespace "pods-8898" to be "running"
    Jan 12 16:43:46.616: INFO: Pod "pod-m4wgp": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90928ms
    Jan 12 16:43:48.619: INFO: Pod "pod-m4wgp": Phase="Running", Reason="", readiness=true. Elapsed: 2.004934047s
    Jan 12 16:43:48.619: INFO: Pod "pod-m4wgp" satisfied condition "running"
    STEP: patching /status 01/12/23 16:43:48.619
    Jan 12 16:43:48.627: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:48.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8898" for this suite. 01/12/23 16:43:48.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:48.635
Jan 12 16:43:48.635: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:43:48.636
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:48.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:48.648
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/12/23 16:43:48.651
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-405" for this suite. 01/12/23 16:43:48.655
------------------------------
 [0.024 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:48.635
    Jan 12 16:43:48.635: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:43:48.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:48.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:48.648
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/12/23 16:43:48.651
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-405" for this suite. 01/12/23 16:43:48.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:48.662
Jan 12 16:43:48.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:43:48.663
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:48.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:48.676
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/12/23 16:43:48.678
Jan 12 16:43:48.683: INFO: Waiting up to 5m0s for pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971" in namespace "projected-4436" to be "running and ready"
Jan 12 16:43:48.685: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971": Phase="Pending", Reason="", readiness=false. Elapsed: 1.719767ms
Jan 12 16:43:48.685: INFO: The phase of Pod annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:43:50.688: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971": Phase="Running", Reason="", readiness=true. Elapsed: 2.0043613s
Jan 12 16:43:50.688: INFO: The phase of Pod annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971 is Running (Ready = true)
Jan 12 16:43:50.688: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971" satisfied condition "running and ready"
Jan 12 16:43:51.206: INFO: Successfully updated pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:55.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4436" for this suite. 01/12/23 16:43:55.224
------------------------------
 [SLOW TEST] [6.566 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:48.662
    Jan 12 16:43:48.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:43:48.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:48.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:48.676
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/12/23 16:43:48.678
    Jan 12 16:43:48.683: INFO: Waiting up to 5m0s for pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971" in namespace "projected-4436" to be "running and ready"
    Jan 12 16:43:48.685: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971": Phase="Pending", Reason="", readiness=false. Elapsed: 1.719767ms
    Jan 12 16:43:48.685: INFO: The phase of Pod annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:43:50.688: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971": Phase="Running", Reason="", readiness=true. Elapsed: 2.0043613s
    Jan 12 16:43:50.688: INFO: The phase of Pod annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971 is Running (Ready = true)
    Jan 12 16:43:50.688: INFO: Pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971" satisfied condition "running and ready"
    Jan 12 16:43:51.206: INFO: Successfully updated pod "annotationupdateb40f7412-a4bf-40a7-be6a-dc61f34a5971"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:55.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4436" for this suite. 01/12/23 16:43:55.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:55.229
Jan 12 16:43:55.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:43:55.23
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:55.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:55.239
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 16:43:55.241
Jan 12 16:43:55.245: INFO: Waiting up to 5m0s for pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b" in namespace "emptydir-9074" to be "Succeeded or Failed"
Jan 12 16:43:55.247: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666608ms
Jan 12 16:43:57.250: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004390532s
Jan 12 16:43:59.249: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003908324s
STEP: Saw pod success 01/12/23 16:43:59.249
Jan 12 16:43:59.250: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b" satisfied condition "Succeeded or Failed"
Jan 12 16:43:59.251: INFO: Trying to get logs from node worker-1 pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b container test-container: <nil>
STEP: delete the pod 01/12/23 16:43:59.255
Jan 12 16:43:59.262: INFO: Waiting for pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b to disappear
Jan 12 16:43:59.264: INFO: Pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:43:59.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9074" for this suite. 01/12/23 16:43:59.266
------------------------------
 [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:55.229
    Jan 12 16:43:55.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:43:55.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:55.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:55.239
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/12/23 16:43:55.241
    Jan 12 16:43:55.245: INFO: Waiting up to 5m0s for pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b" in namespace "emptydir-9074" to be "Succeeded or Failed"
    Jan 12 16:43:55.247: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666608ms
    Jan 12 16:43:57.250: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004390532s
    Jan 12 16:43:59.249: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003908324s
    STEP: Saw pod success 01/12/23 16:43:59.249
    Jan 12 16:43:59.250: INFO: Pod "pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b" satisfied condition "Succeeded or Failed"
    Jan 12 16:43:59.251: INFO: Trying to get logs from node worker-1 pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b container test-container: <nil>
    STEP: delete the pod 01/12/23 16:43:59.255
    Jan 12 16:43:59.262: INFO: Waiting for pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b to disappear
    Jan 12 16:43:59.264: INFO: Pod pod-64a104eb-c3cc-4e34-9759-6c15173d7f5b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:43:59.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9074" for this suite. 01/12/23 16:43:59.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:43:59.273
Jan 12 16:43:59.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename daemonsets 01/12/23 16:43:59.274
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:59.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:59.286
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/12/23 16:43:59.298
STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:43:59.302
Jan 12 16:43:59.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:43:59.307: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:44:00.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 12 16:44:00.312: INFO: Node worker-0 is running 0 daemon pod, expected 1
Jan 12 16:44:01.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 12 16:44:01.317: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/12/23 16:44:01.321
Jan 12 16:44:01.323: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/12/23 16:44:01.323
Jan 12 16:44:01.332: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/12/23 16:44:01.332
Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: ADDED
Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.334: INFO: Found daemon set daemon-set in namespace daemonsets-3877 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 16:44:01.334: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/12/23 16:44:01.334
STEP: watching for the daemon set status to be patched 01/12/23 16:44:01.339
Jan 12 16:44:01.340: INFO: Observed &DaemonSet event: ADDED
Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.341: INFO: Observed daemon set daemon-set in namespace daemonsets-3877 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
Jan 12 16:44:01.341: INFO: Found daemon set daemon-set in namespace daemonsets-3877 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 12 16:44:01.341: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:44:01.345
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3877, will wait for the garbage collector to delete the pods 01/12/23 16:44:01.345
Jan 12 16:44:01.401: INFO: Deleting DaemonSet.extensions daemon-set took: 4.314018ms
Jan 12 16:44:01.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.813666ms
Jan 12 16:44:03.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 12 16:44:03.604: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 12 16:44:03.606: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28663"},"items":null}

Jan 12 16:44:03.607: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28663"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:03.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3877" for this suite. 01/12/23 16:44:03.615
------------------------------
 [4.346 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:43:59.273
    Jan 12 16:43:59.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename daemonsets 01/12/23 16:43:59.274
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:43:59.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:43:59.286
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/12/23 16:43:59.298
    STEP: Check that daemon pods launch on every node of the cluster. 01/12/23 16:43:59.302
    Jan 12 16:43:59.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:43:59.307: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:44:00.312: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 12 16:44:00.312: INFO: Node worker-0 is running 0 daemon pod, expected 1
    Jan 12 16:44:01.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 12 16:44:01.317: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/12/23 16:44:01.321
    Jan 12 16:44:01.323: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/12/23 16:44:01.323
    Jan 12 16:44:01.332: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/12/23 16:44:01.332
    Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: ADDED
    Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.334: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.334: INFO: Found daemon set daemon-set in namespace daemonsets-3877 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 16:44:01.334: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/12/23 16:44:01.334
    STEP: watching for the daemon set status to be patched 01/12/23 16:44:01.339
    Jan 12 16:44:01.340: INFO: Observed &DaemonSet event: ADDED
    Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.341: INFO: Observed daemon set daemon-set in namespace daemonsets-3877 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 12 16:44:01.341: INFO: Observed &DaemonSet event: MODIFIED
    Jan 12 16:44:01.341: INFO: Found daemon set daemon-set in namespace daemonsets-3877 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 12 16:44:01.341: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/12/23 16:44:01.345
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3877, will wait for the garbage collector to delete the pods 01/12/23 16:44:01.345
    Jan 12 16:44:01.401: INFO: Deleting DaemonSet.extensions daemon-set took: 4.314018ms
    Jan 12 16:44:01.502: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.813666ms
    Jan 12 16:44:03.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 12 16:44:03.604: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 12 16:44:03.606: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28663"},"items":null}

    Jan 12 16:44:03.607: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28663"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:03.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3877" for this suite. 01/12/23 16:44:03.615
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:03.62
Jan 12 16:44:03.621: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:44:03.622
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:03.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:03.632
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-csnn2" 01/12/23 16:44:03.636
Jan 12 16:44:03.641: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard cpu limit of 500m
Jan 12 16:44:03.641: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.641
STEP: Confirm /status for "e2e-rq-status-csnn2" resourceQuota via watch 01/12/23 16:44:03.647
Jan 12 16:44:03.648: INFO: observed resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList(nil)
Jan 12 16:44:03.648: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 12 16:44:03.648: INFO: ResourceQuota "e2e-rq-status-csnn2" /status was updated
STEP: Patching hard spec values for cpu & memory 01/12/23 16:44:03.65
Jan 12 16:44:03.653: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard cpu limit of 1
Jan 12 16:44:03.653: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.653
STEP: Confirm /status for "e2e-rq-status-csnn2" resourceQuota via watch 01/12/23 16:44:03.657
Jan 12 16:44:03.658: INFO: observed resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 12 16:44:03.658: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 12 16:44:03.658: INFO: ResourceQuota "e2e-rq-status-csnn2" /status was patched
STEP: Get "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.658
Jan 12 16:44:03.660: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard cpu of 1
Jan 12 16:44:03.660: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-csnn2" /status before checking Spec is unchanged 01/12/23 16:44:03.662
Jan 12 16:44:03.665: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard cpu of 2
Jan 12 16:44:03.665: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard memory of 2Gi
Jan 12 16:44:03.666: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 12 16:44:08.673: INFO: ResourceQuota "e2e-rq-status-csnn2" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1444" for this suite. 01/12/23 16:44:08.676
------------------------------
 [SLOW TEST] [5.060 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:03.62
    Jan 12 16:44:03.621: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:44:03.622
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:03.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:03.632
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-csnn2" 01/12/23 16:44:03.636
    Jan 12 16:44:03.641: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard cpu limit of 500m
    Jan 12 16:44:03.641: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.641
    STEP: Confirm /status for "e2e-rq-status-csnn2" resourceQuota via watch 01/12/23 16:44:03.647
    Jan 12 16:44:03.648: INFO: observed resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList(nil)
    Jan 12 16:44:03.648: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 12 16:44:03.648: INFO: ResourceQuota "e2e-rq-status-csnn2" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/12/23 16:44:03.65
    Jan 12 16:44:03.653: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard cpu limit of 1
    Jan 12 16:44:03.653: INFO: Resource quota "e2e-rq-status-csnn2" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.653
    STEP: Confirm /status for "e2e-rq-status-csnn2" resourceQuota via watch 01/12/23 16:44:03.657
    Jan 12 16:44:03.658: INFO: observed resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 12 16:44:03.658: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 12 16:44:03.658: INFO: ResourceQuota "e2e-rq-status-csnn2" /status was patched
    STEP: Get "e2e-rq-status-csnn2" /status 01/12/23 16:44:03.658
    Jan 12 16:44:03.660: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard cpu of 1
    Jan 12 16:44:03.660: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-csnn2" /status before checking Spec is unchanged 01/12/23 16:44:03.662
    Jan 12 16:44:03.665: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard cpu of 2
    Jan 12 16:44:03.665: INFO: Resourcequota "e2e-rq-status-csnn2" reports status: hard memory of 2Gi
    Jan 12 16:44:03.666: INFO: Found resourceQuota "e2e-rq-status-csnn2" in namespace "resourcequota-1444" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 12 16:44:08.673: INFO: ResourceQuota "e2e-rq-status-csnn2" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:08.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1444" for this suite. 01/12/23 16:44:08.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:08.681
Jan 12 16:44:08.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename secrets 01/12/23 16:44:08.682
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:08.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:08.696
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-8ab0a2ca-fe82-4e8c-a8f8-2e8edce03036 01/12/23 16:44:08.7
STEP: Creating secret with name s-test-opt-upd-012c287f-2e03-4ec6-80d4-143dcafc5d02 01/12/23 16:44:08.704
STEP: Creating the pod 01/12/23 16:44:08.709
Jan 12 16:44:08.717: INFO: Waiting up to 5m0s for pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3" in namespace "secrets-5885" to be "running and ready"
Jan 12 16:44:08.720: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524184ms
Jan 12 16:44:08.720: INFO: The phase of Pod pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:44:10.724: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006534283s
Jan 12 16:44:10.724: INFO: The phase of Pod pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3 is Running (Ready = true)
Jan 12 16:44:10.724: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-8ab0a2ca-fe82-4e8c-a8f8-2e8edce03036 01/12/23 16:44:10.737
STEP: Updating secret s-test-opt-upd-012c287f-2e03-4ec6-80d4-143dcafc5d02 01/12/23 16:44:10.741
STEP: Creating secret with name s-test-opt-create-c96968a1-9e61-415d-a60e-ba9c2bcb9397 01/12/23 16:44:10.744
STEP: waiting to observe update in volume 01/12/23 16:44:10.746
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:14.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5885" for this suite. 01/12/23 16:44:14.773
------------------------------
 [SLOW TEST] [6.096 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:08.681
    Jan 12 16:44:08.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename secrets 01/12/23 16:44:08.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:08.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:08.696
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-8ab0a2ca-fe82-4e8c-a8f8-2e8edce03036 01/12/23 16:44:08.7
    STEP: Creating secret with name s-test-opt-upd-012c287f-2e03-4ec6-80d4-143dcafc5d02 01/12/23 16:44:08.704
    STEP: Creating the pod 01/12/23 16:44:08.709
    Jan 12 16:44:08.717: INFO: Waiting up to 5m0s for pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3" in namespace "secrets-5885" to be "running and ready"
    Jan 12 16:44:08.720: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524184ms
    Jan 12 16:44:08.720: INFO: The phase of Pod pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:44:10.724: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006534283s
    Jan 12 16:44:10.724: INFO: The phase of Pod pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3 is Running (Ready = true)
    Jan 12 16:44:10.724: INFO: Pod "pod-secrets-9c22a65a-8f82-415e-a38b-72c5e17823c3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-8ab0a2ca-fe82-4e8c-a8f8-2e8edce03036 01/12/23 16:44:10.737
    STEP: Updating secret s-test-opt-upd-012c287f-2e03-4ec6-80d4-143dcafc5d02 01/12/23 16:44:10.741
    STEP: Creating secret with name s-test-opt-create-c96968a1-9e61-415d-a60e-ba9c2bcb9397 01/12/23 16:44:10.744
    STEP: waiting to observe update in volume 01/12/23 16:44:10.746
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:14.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5885" for this suite. 01/12/23 16:44:14.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:14.78
Jan 12 16:44:14.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:44:14.781
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:14.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:14.794
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 12 16:44:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6041" for this suite. 01/12/23 16:44:15.332
------------------------------
 [0.557 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:14.78
    Jan 12 16:44:14.780: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename custom-resource-definition 01/12/23 16:44:14.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:14.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:14.794
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 12 16:44:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6041" for this suite. 01/12/23 16:44:15.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:15.338
Jan 12 16:44:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:44:15.338
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:15.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:15.35
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 12 16:44:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:44:16.745
Jan 12 16:44:16.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 create -f -'
Jan 12 16:44:17.335: INFO: stderr: ""
Jan 12 16:44:17.335: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 12 16:44:17.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 delete e2e-test-crd-publish-openapi-5963-crds test-cr'
Jan 12 16:44:17.396: INFO: stderr: ""
Jan 12 16:44:17.396: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 12 16:44:17.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 apply -f -'
Jan 12 16:44:17.582: INFO: stderr: ""
Jan 12 16:44:17.582: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 12 16:44:17.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 delete e2e-test-crd-publish-openapi-5963-crds test-cr'
Jan 12 16:44:17.642: INFO: stderr: ""
Jan 12 16:44:17.642: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/12/23 16:44:17.642
Jan 12 16:44:17.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 explain e2e-test-crd-publish-openapi-5963-crds'
Jan 12 16:44:17.814: INFO: stderr: ""
Jan 12 16:44:17.814: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5963-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:19.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2950" for this suite. 01/12/23 16:44:19.204
------------------------------
 [3.870 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:15.338
    Jan 12 16:44:15.338: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename crd-publish-openapi 01/12/23 16:44:15.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:15.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:15.35
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 12 16:44:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/12/23 16:44:16.745
    Jan 12 16:44:16.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 create -f -'
    Jan 12 16:44:17.335: INFO: stderr: ""
    Jan 12 16:44:17.335: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 12 16:44:17.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 delete e2e-test-crd-publish-openapi-5963-crds test-cr'
    Jan 12 16:44:17.396: INFO: stderr: ""
    Jan 12 16:44:17.396: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 12 16:44:17.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 apply -f -'
    Jan 12 16:44:17.582: INFO: stderr: ""
    Jan 12 16:44:17.582: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 12 16:44:17.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 --namespace=crd-publish-openapi-2950 delete e2e-test-crd-publish-openapi-5963-crds test-cr'
    Jan 12 16:44:17.642: INFO: stderr: ""
    Jan 12 16:44:17.642: INFO: stdout: "e2e-test-crd-publish-openapi-5963-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/12/23 16:44:17.642
    Jan 12 16:44:17.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=crd-publish-openapi-2950 explain e2e-test-crd-publish-openapi-5963-crds'
    Jan 12 16:44:17.814: INFO: stderr: ""
    Jan 12 16:44:17.814: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5963-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:19.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2950" for this suite. 01/12/23 16:44:19.204
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:19.208
Jan 12 16:44:19.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename watch 01/12/23 16:44:19.209
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:19.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:19.218
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/12/23 16:44:19.221
STEP: creating a watch on configmaps with label B 01/12/23 16:44:19.222
STEP: creating a watch on configmaps with label A or B 01/12/23 16:44:19.223
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.224
Jan 12 16:44:19.226: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28796 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:19.227: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28796 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.227
Jan 12 16:44:19.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28797 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:19.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28797 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/12/23 16:44:19.232
Jan 12 16:44:19.236: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28798 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:19.236: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28798 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.236
Jan 12 16:44:19.240: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28799 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:19.240: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28799 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/12/23 16:44:19.24
Jan 12 16:44:19.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28800 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:19.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28800 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/12/23 16:44:29.244
Jan 12 16:44:29.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28851 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:44:29.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28851 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:44:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3661" for this suite. 01/12/23 16:44:39.252
------------------------------
 [SLOW TEST] [20.048 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:19.208
    Jan 12 16:44:19.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename watch 01/12/23 16:44:19.209
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:19.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:19.218
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/12/23 16:44:19.221
    STEP: creating a watch on configmaps with label B 01/12/23 16:44:19.222
    STEP: creating a watch on configmaps with label A or B 01/12/23 16:44:19.223
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.224
    Jan 12 16:44:19.226: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28796 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:19.227: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28796 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.227
    Jan 12 16:44:19.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28797 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:19.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28797 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/12/23 16:44:19.232
    Jan 12 16:44:19.236: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28798 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:19.236: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28798 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/12/23 16:44:19.236
    Jan 12 16:44:19.240: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28799 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:19.240: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3661  736a309b-46d6-462b-b5a1-dbdccf6e7d7c 28799 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/12/23 16:44:19.24
    Jan 12 16:44:19.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28800 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:19.243: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28800 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/12/23 16:44:29.244
    Jan 12 16:44:29.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28851 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:44:29.248: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3661  4fecc794-3234-421b-8fb7-33d8e0e6fdf3 28851 0 2023-01-12 16:44:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-12 16:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:44:39.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3661" for this suite. 01/12/23 16:44:39.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:44:39.257
Jan 12 16:44:39.257: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 16:44:39.258
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:39.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:39.267
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/12/23 16:44:39.27
STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:44:39.511
Jan 12 16:44:39.615: INFO: Pod name wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 16:44:39.615
Jan 12 16:44:39.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:44:39.660: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 45.508971ms
Jan 12 16:44:41.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048235755s
Jan 12 16:44:43.665: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049841477s
Jan 12 16:44:45.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049420092s
Jan 12 16:44:47.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04820649s
Jan 12 16:44:49.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048440239s
Jan 12 16:44:51.665: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049881206s
Jan 12 16:44:53.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Running", Reason="", readiness=true. Elapsed: 14.049446959s
Jan 12 16:44:53.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv" satisfied condition "running"
Jan 12 16:44:53.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:44:53.667: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.089597ms
Jan 12 16:44:53.667: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz" satisfied condition "running"
Jan 12 16:44:53.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:44:53.669: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.061425ms
Jan 12 16:44:53.669: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw" satisfied condition "running"
Jan 12 16:44:53.669: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:44:53.671: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz": Phase="Running", Reason="", readiness=true. Elapsed: 1.945522ms
Jan 12 16:44:53.671: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz" satisfied condition "running"
Jan 12 16:44:53.671: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:44:53.673: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22": Phase="Running", Reason="", readiness=true. Elapsed: 1.87967ms
Jan 12 16:44:53.673: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:44:53.673
Jan 12 16:44:53.730: INFO: Deleting ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd took: 4.516434ms
Jan 12 16:44:53.830: INFO: Terminating ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd pods took: 100.127684ms
STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:44:57.832
Jan 12 16:44:57.848: INFO: Pod name wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e: Found 0 pods out of 5
Jan 12 16:45:02.853: INFO: Pod name wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 16:45:02.853
Jan 12 16:45:02.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:02.855: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.443921ms
Jan 12 16:45:04.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005799949s
Jan 12 16:45:06.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006266459s
Jan 12 16:45:08.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006157837s
Jan 12 16:45:10.858: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005475411s
Jan 12 16:45:12.860: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Running", Reason="", readiness=true. Elapsed: 10.007058108s
Jan 12 16:45:12.860: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf" satisfied condition "running"
Jan 12 16:45:12.860: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:12.862: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm": Phase="Running", Reason="", readiness=true. Elapsed: 1.915861ms
Jan 12 16:45:12.862: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm" satisfied condition "running"
Jan 12 16:45:12.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:12.864: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw": Phase="Running", Reason="", readiness=true. Elapsed: 1.982756ms
Jan 12 16:45:12.864: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw" satisfied condition "running"
Jan 12 16:45:12.864: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:12.866: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps": Phase="Pending", Reason="", readiness=false. Elapsed: 1.882466ms
Jan 12 16:45:14.869: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps": Phase="Running", Reason="", readiness=true. Elapsed: 2.004787343s
Jan 12 16:45:14.869: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps" satisfied condition "running"
Jan 12 16:45:14.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:14.871: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025362ms
Jan 12 16:45:14.871: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:45:14.871
Jan 12 16:45:14.927: INFO: Deleting ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e took: 4.165769ms
Jan 12 16:45:15.028: INFO: Terminating ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e pods took: 100.999973ms
STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:45:17.832
Jan 12 16:45:17.843: INFO: Pod name wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6: Found 0 pods out of 5
Jan 12 16:45:22.851: INFO: Pod name wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/12/23 16:45:22.851
Jan 12 16:45:22.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:22.853: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060782ms
Jan 12 16:45:24.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758341s
Jan 12 16:45:26.856: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004805283s
Jan 12 16:45:28.856: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005205744s
Jan 12 16:45:30.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005863313s
Jan 12 16:45:30.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5" satisfied condition "running"
Jan 12 16:45:30.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:30.859: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.124335ms
Jan 12 16:45:30.859: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k" satisfied condition "running"
Jan 12 16:45:30.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:30.861: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.989096ms
Jan 12 16:45:32.865: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006053724s
Jan 12 16:45:32.865: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb" satisfied condition "running"
Jan 12 16:45:32.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:32.868: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.235379ms
Jan 12 16:45:32.868: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4" satisfied condition "running"
Jan 12 16:45:32.868: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9" in namespace "emptydir-wrapper-8" to be "running"
Jan 12 16:45:32.870: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9": Phase="Running", Reason="", readiness=true. Elapsed: 1.968098ms
Jan 12 16:45:32.870: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:45:32.87
Jan 12 16:45:32.926: INFO: Deleting ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 took: 4.013184ms
Jan 12 16:45:33.027: INFO: Terminating ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 pods took: 100.413951ms
STEP: Cleaning up the configMaps 01/12/23 16:45:36.027
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:45:36.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8" for this suite. 01/12/23 16:45:36.2
------------------------------
 [SLOW TEST] [56.946 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:44:39.257
    Jan 12 16:44:39.257: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir-wrapper 01/12/23 16:44:39.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:44:39.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:44:39.267
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/12/23 16:44:39.27
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:44:39.511
    Jan 12 16:44:39.615: INFO: Pod name wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 16:44:39.615
    Jan 12 16:44:39.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:44:39.660: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 45.508971ms
    Jan 12 16:44:41.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048235755s
    Jan 12 16:44:43.665: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049841477s
    Jan 12 16:44:45.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049420092s
    Jan 12 16:44:47.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04820649s
    Jan 12 16:44:49.663: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048440239s
    Jan 12 16:44:51.665: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049881206s
    Jan 12 16:44:53.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv": Phase="Running", Reason="", readiness=true. Elapsed: 14.049446959s
    Jan 12 16:44:53.664: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-cl5wv" satisfied condition "running"
    Jan 12 16:44:53.664: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:44:53.667: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.089597ms
    Jan 12 16:44:53.667: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-jvgzz" satisfied condition "running"
    Jan 12 16:44:53.667: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:44:53.669: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.061425ms
    Jan 12 16:44:53.669: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-s59qw" satisfied condition "running"
    Jan 12 16:44:53.669: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:44:53.671: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz": Phase="Running", Reason="", readiness=true. Elapsed: 1.945522ms
    Jan 12 16:44:53.671: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-xbghz" satisfied condition "running"
    Jan 12 16:44:53.671: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:44:53.673: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22": Phase="Running", Reason="", readiness=true. Elapsed: 1.87967ms
    Jan 12 16:44:53.673: INFO: Pod "wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd-zhp22" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:44:53.673
    Jan 12 16:44:53.730: INFO: Deleting ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd took: 4.516434ms
    Jan 12 16:44:53.830: INFO: Terminating ReplicationController wrapped-volume-race-572f85ac-7aa1-4c51-b587-f3d2bfd383cd pods took: 100.127684ms
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:44:57.832
    Jan 12 16:44:57.848: INFO: Pod name wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e: Found 0 pods out of 5
    Jan 12 16:45:02.853: INFO: Pod name wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 16:45:02.853
    Jan 12 16:45:02.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:02.855: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.443921ms
    Jan 12 16:45:04.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005799949s
    Jan 12 16:45:06.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006266459s
    Jan 12 16:45:08.859: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006157837s
    Jan 12 16:45:10.858: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005475411s
    Jan 12 16:45:12.860: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf": Phase="Running", Reason="", readiness=true. Elapsed: 10.007058108s
    Jan 12 16:45:12.860: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-4r2lf" satisfied condition "running"
    Jan 12 16:45:12.860: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:12.862: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm": Phase="Running", Reason="", readiness=true. Elapsed: 1.915861ms
    Jan 12 16:45:12.862: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-69lzm" satisfied condition "running"
    Jan 12 16:45:12.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:12.864: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw": Phase="Running", Reason="", readiness=true. Elapsed: 1.982756ms
    Jan 12 16:45:12.864: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-7q9zw" satisfied condition "running"
    Jan 12 16:45:12.864: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:12.866: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps": Phase="Pending", Reason="", readiness=false. Elapsed: 1.882466ms
    Jan 12 16:45:14.869: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps": Phase="Running", Reason="", readiness=true. Elapsed: 2.004787343s
    Jan 12 16:45:14.869: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-td2ps" satisfied condition "running"
    Jan 12 16:45:14.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:14.871: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025362ms
    Jan 12 16:45:14.871: INFO: Pod "wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e-vvfrf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:45:14.871
    Jan 12 16:45:14.927: INFO: Deleting ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e took: 4.165769ms
    Jan 12 16:45:15.028: INFO: Terminating ReplicationController wrapped-volume-race-947e42bc-7970-4d0f-97a5-284ba3a5621e pods took: 100.999973ms
    STEP: Creating RC which spawns configmap-volume pods 01/12/23 16:45:17.832
    Jan 12 16:45:17.843: INFO: Pod name wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6: Found 0 pods out of 5
    Jan 12 16:45:22.851: INFO: Pod name wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/12/23 16:45:22.851
    Jan 12 16:45:22.851: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:22.853: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060782ms
    Jan 12 16:45:24.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005758341s
    Jan 12 16:45:26.856: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004805283s
    Jan 12 16:45:28.856: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005205744s
    Jan 12 16:45:30.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.005863313s
    Jan 12 16:45:30.857: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-54lj5" satisfied condition "running"
    Jan 12 16:45:30.857: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:30.859: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.124335ms
    Jan 12 16:45:30.859: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-74c7k" satisfied condition "running"
    Jan 12 16:45:30.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:30.861: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.989096ms
    Jan 12 16:45:32.865: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006053724s
    Jan 12 16:45:32.865: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-b48kb" satisfied condition "running"
    Jan 12 16:45:32.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:32.868: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.235379ms
    Jan 12 16:45:32.868: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-l9wl4" satisfied condition "running"
    Jan 12 16:45:32.868: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9" in namespace "emptydir-wrapper-8" to be "running"
    Jan 12 16:45:32.870: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9": Phase="Running", Reason="", readiness=true. Elapsed: 1.968098ms
    Jan 12 16:45:32.870: INFO: Pod "wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6-v25v9" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 in namespace emptydir-wrapper-8, will wait for the garbage collector to delete the pods 01/12/23 16:45:32.87
    Jan 12 16:45:32.926: INFO: Deleting ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 took: 4.013184ms
    Jan 12 16:45:33.027: INFO: Terminating ReplicationController wrapped-volume-race-78bb7799-6517-45ab-be46-e42a9cf508d6 pods took: 100.413951ms
    STEP: Cleaning up the configMaps 01/12/23 16:45:36.027
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:45:36.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8" for this suite. 01/12/23 16:45:36.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:45:36.205
Jan 12 16:45:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename sched-pred 01/12/23 16:45:36.205
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:36.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:36.217
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 12 16:45:36.220: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 12 16:45:36.223: INFO: Waiting for terminating namespaces to be deleted...
Jan 12 16:45:36.225: INFO: 
Logging pods the apiserver thinks is on node worker-0 before test
Jan 12 16:45:36.229: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:45:36.229: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:45:36.229: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:45:36.229: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:45:36.229: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container metrics-server ready: true, restart count 0
Jan 12 16:45:36.229: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container e2e ready: true, restart count 0
Jan 12 16:45:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:45:36.229: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:45:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:45:36.229: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 12 16:45:36.229: INFO: 
Logging pods the apiserver thinks is on node worker-1 before test
Jan 12 16:45:36.232: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container coredns ready: true, restart count 0
Jan 12 16:45:36.232: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 12 16:45:36.232: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 12 16:45:36.232: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container kube-router ready: true, restart count 0
Jan 12 16:45:36.232: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 12 16:45:36.232: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
Jan 12 16:45:36.232: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 12 16:45:36.232: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/12/23 16:45:36.232
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17399d8632abf749], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/12/23 16:45:36.249
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:45:37.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5702" for this suite. 01/12/23 16:45:37.251
------------------------------
 [1.050 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:45:36.205
    Jan 12 16:45:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename sched-pred 01/12/23 16:45:36.205
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:36.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:36.217
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 12 16:45:36.220: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 12 16:45:36.223: INFO: Waiting for terminating namespaces to be deleted...
    Jan 12 16:45:36.225: INFO: 
    Logging pods the apiserver thinks is on node worker-0 before test
    Jan 12 16:45:36.229: INFO: coredns-9864b985-9m6mg from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: konnectivity-agent-n82db from kube-system started at 2023-01-12 15:23:04 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: kube-proxy-stxrn from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: kube-router-rtpgj from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: metrics-server-7446cc488c-tsnl2 from kube-system started at 2023-01-12 15:23:17 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: sonobuoy-e2e-job-829dc73999d04348 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container e2e ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-z2vhh from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:45:36.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 12 16:45:36.229: INFO: 
    Logging pods the apiserver thinks is on node worker-1 before test
    Jan 12 16:45:36.232: INFO: coredns-9864b985-zm525 from kube-system started at 2023-01-12 16:08:19 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container coredns ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: konnectivity-agent-xds5c from kube-system started at 2023-01-12 15:23:05 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: kube-proxy-f2rbm from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: kube-router-ppxdq from kube-system started at 2023-01-12 15:23:01 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container kube-router ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: sonobuoy from sonobuoy started at 2023-01-12 15:23:29 +0000 UTC (1 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: sonobuoy-systemd-logs-daemon-set-2db641a4a8f54471-2kzc2 from sonobuoy started at 2023-01-12 15:23:32 +0000 UTC (2 container statuses recorded)
    Jan 12 16:45:36.232: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 12 16:45:36.232: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/12/23 16:45:36.232
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17399d8632abf749], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/12/23 16:45:36.249
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:45:37.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5702" for this suite. 01/12/23 16:45:37.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:45:37.255
Jan 12 16:45:37.256: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:45:37.256
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:37.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:37.268
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/12/23 16:45:37.271
STEP: Counting existing ResourceQuota 01/12/23 16:45:42.273
STEP: Creating a ResourceQuota 01/12/23 16:45:47.276
STEP: Ensuring resource quota status is calculated 01/12/23 16:45:47.282
STEP: Creating a Secret 01/12/23 16:45:49.284
STEP: Ensuring resource quota status captures secret creation 01/12/23 16:45:49.295
STEP: Deleting a secret 01/12/23 16:45:51.297
STEP: Ensuring resource quota status released usage 01/12/23 16:45:51.301
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:45:53.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8912" for this suite. 01/12/23 16:45:53.306
------------------------------
 [SLOW TEST] [16.054 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:45:37.255
    Jan 12 16:45:37.256: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:45:37.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:37.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:37.268
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/12/23 16:45:37.271
    STEP: Counting existing ResourceQuota 01/12/23 16:45:42.273
    STEP: Creating a ResourceQuota 01/12/23 16:45:47.276
    STEP: Ensuring resource quota status is calculated 01/12/23 16:45:47.282
    STEP: Creating a Secret 01/12/23 16:45:49.284
    STEP: Ensuring resource quota status captures secret creation 01/12/23 16:45:49.295
    STEP: Deleting a secret 01/12/23 16:45:51.297
    STEP: Ensuring resource quota status released usage 01/12/23 16:45:51.301
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:45:53.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8912" for this suite. 01/12/23 16:45:53.306
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:45:53.31
Jan 12 16:45:53.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-probe 01/12/23 16:45:53.311
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:53.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:53.32
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 in namespace container-probe-2815 01/12/23 16:45:53.323
Jan 12 16:45:53.330: INFO: Waiting up to 5m0s for pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3" in namespace "container-probe-2815" to be "not pending"
Jan 12 16:45:53.332: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.978223ms
Jan 12 16:45:55.334: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00464433s
Jan 12 16:45:55.335: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3" satisfied condition "not pending"
Jan 12 16:45:55.335: INFO: Started pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 in namespace container-probe-2815
STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:45:55.335
Jan 12 16:45:55.336: INFO: Initial restart count of pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is 0
Jan 12 16:46:15.371: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 1 (20.034812653s elapsed)
Jan 12 16:46:35.407: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 2 (40.070468021s elapsed)
Jan 12 16:46:55.436: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 3 (1m0.10004188s elapsed)
Jan 12 16:47:15.465: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 4 (1m20.129199944s elapsed)
Jan 12 16:48:15.559: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 5 (2m20.222219954s elapsed)
STEP: deleting the pod 01/12/23 16:48:15.559
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2815" for this suite. 01/12/23 16:48:15.57
------------------------------
 [SLOW TEST] [142.264 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:45:53.31
    Jan 12 16:45:53.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-probe 01/12/23 16:45:53.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:45:53.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:45:53.32
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 in namespace container-probe-2815 01/12/23 16:45:53.323
    Jan 12 16:45:53.330: INFO: Waiting up to 5m0s for pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3" in namespace "container-probe-2815" to be "not pending"
    Jan 12 16:45:53.332: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.978223ms
    Jan 12 16:45:55.334: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00464433s
    Jan 12 16:45:55.335: INFO: Pod "liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3" satisfied condition "not pending"
    Jan 12 16:45:55.335: INFO: Started pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 in namespace container-probe-2815
    STEP: checking the pod's current state and verifying that restartCount is present 01/12/23 16:45:55.335
    Jan 12 16:45:55.336: INFO: Initial restart count of pod liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is 0
    Jan 12 16:46:15.371: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 1 (20.034812653s elapsed)
    Jan 12 16:46:35.407: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 2 (40.070468021s elapsed)
    Jan 12 16:46:55.436: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 3 (1m0.10004188s elapsed)
    Jan 12 16:47:15.465: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 4 (1m20.129199944s elapsed)
    Jan 12 16:48:15.559: INFO: Restart count of pod container-probe-2815/liveness-498b0daf-3767-4f6b-b230-c7683eaa51e3 is now 5 (2m20.222219954s elapsed)
    STEP: deleting the pod 01/12/23 16:48:15.559
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2815" for this suite. 01/12/23 16:48:15.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:15.577
Jan 12 16:48:15.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:48:15.578
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:15.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:15.59
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/12/23 16:48:15.592
Jan 12 16:48:15.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5994 create -f -'
Jan 12 16:48:16.118: INFO: stderr: ""
Jan 12 16:48:16.118: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/12/23 16:48:16.118
Jan 12 16:48:17.121: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:48:17.121: INFO: Found 1 / 1
Jan 12 16:48:17.121: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/12/23 16:48:17.121
Jan 12 16:48:17.123: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:48:17.123: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 12 16:48:17.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5994 patch pod agnhost-primary-hqzp6 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 12 16:48:17.188: INFO: stderr: ""
Jan 12 16:48:17.188: INFO: stdout: "pod/agnhost-primary-hqzp6 patched\n"
STEP: checking annotations 01/12/23 16:48:17.188
Jan 12 16:48:17.190: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 12 16:48:17.190: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:17.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5994" for this suite. 01/12/23 16:48:17.193
------------------------------
 [1.619 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:15.577
    Jan 12 16:48:15.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:48:15.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:15.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:15.59
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/12/23 16:48:15.592
    Jan 12 16:48:15.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5994 create -f -'
    Jan 12 16:48:16.118: INFO: stderr: ""
    Jan 12 16:48:16.118: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/12/23 16:48:16.118
    Jan 12 16:48:17.121: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:48:17.121: INFO: Found 1 / 1
    Jan 12 16:48:17.121: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/12/23 16:48:17.121
    Jan 12 16:48:17.123: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:48:17.123: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 12 16:48:17.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-5994 patch pod agnhost-primary-hqzp6 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 12 16:48:17.188: INFO: stderr: ""
    Jan 12 16:48:17.188: INFO: stdout: "pod/agnhost-primary-hqzp6 patched\n"
    STEP: checking annotations 01/12/23 16:48:17.188
    Jan 12 16:48:17.190: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 12 16:48:17.190: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:17.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5994" for this suite. 01/12/23 16:48:17.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:17.199
Jan 12 16:48:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:48:17.199
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:17.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:17.211
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/12/23 16:48:17.213
Jan 12 16:48:17.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 create -f -'
Jan 12 16:48:17.402: INFO: stderr: ""
Jan 12 16:48:17.402: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 16:48:17.402
Jan 12 16:48:17.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 16:48:17.474: INFO: stderr: ""
Jan 12 16:48:17.474: INFO: stdout: "update-demo-nautilus-hjqnl update-demo-nautilus-z8g8s "
Jan 12 16:48:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 16:48:17.535: INFO: stderr: ""
Jan 12 16:48:17.535: INFO: stdout: ""
Jan 12 16:48:17.535: INFO: update-demo-nautilus-hjqnl is created but not running
Jan 12 16:48:22.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 12 16:48:22.611: INFO: stderr: ""
Jan 12 16:48:22.611: INFO: stdout: "update-demo-nautilus-hjqnl update-demo-nautilus-z8g8s "
Jan 12 16:48:22.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 16:48:22.669: INFO: stderr: ""
Jan 12 16:48:22.669: INFO: stdout: "true"
Jan 12 16:48:22.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 16:48:22.727: INFO: stderr: ""
Jan 12 16:48:22.727: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 16:48:22.727: INFO: validating pod update-demo-nautilus-hjqnl
Jan 12 16:48:22.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 16:48:22.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 16:48:22.733: INFO: update-demo-nautilus-hjqnl is verified up and running
Jan 12 16:48:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-z8g8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 12 16:48:22.792: INFO: stderr: ""
Jan 12 16:48:22.792: INFO: stdout: "true"
Jan 12 16:48:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-z8g8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 12 16:48:22.850: INFO: stderr: ""
Jan 12 16:48:22.850: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 12 16:48:22.850: INFO: validating pod update-demo-nautilus-z8g8s
Jan 12 16:48:22.855: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 12 16:48:22.855: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 12 16:48:22.855: INFO: update-demo-nautilus-z8g8s is verified up and running
STEP: using delete to clean up resources 01/12/23 16:48:22.855
Jan 12 16:48:22.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 delete --grace-period=0 --force -f -'
Jan 12 16:48:22.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 12 16:48:22.915: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 12 16:48:22.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get rc,svc -l name=update-demo --no-headers'
Jan 12 16:48:22.997: INFO: stderr: "No resources found in kubectl-8061 namespace.\n"
Jan 12 16:48:22.997: INFO: stdout: ""
Jan 12 16:48:22.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 12 16:48:23.068: INFO: stderr: ""
Jan 12 16:48:23.068: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:23.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8061" for this suite. 01/12/23 16:48:23.07
------------------------------
 [SLOW TEST] [5.875 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:17.199
    Jan 12 16:48:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:48:17.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:17.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:17.211
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/12/23 16:48:17.213
    Jan 12 16:48:17.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 create -f -'
    Jan 12 16:48:17.402: INFO: stderr: ""
    Jan 12 16:48:17.402: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/12/23 16:48:17.402
    Jan 12 16:48:17.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 16:48:17.474: INFO: stderr: ""
    Jan 12 16:48:17.474: INFO: stdout: "update-demo-nautilus-hjqnl update-demo-nautilus-z8g8s "
    Jan 12 16:48:17.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 16:48:17.535: INFO: stderr: ""
    Jan 12 16:48:17.535: INFO: stdout: ""
    Jan 12 16:48:17.535: INFO: update-demo-nautilus-hjqnl is created but not running
    Jan 12 16:48:22.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 12 16:48:22.611: INFO: stderr: ""
    Jan 12 16:48:22.611: INFO: stdout: "update-demo-nautilus-hjqnl update-demo-nautilus-z8g8s "
    Jan 12 16:48:22.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 16:48:22.669: INFO: stderr: ""
    Jan 12 16:48:22.669: INFO: stdout: "true"
    Jan 12 16:48:22.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-hjqnl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 16:48:22.727: INFO: stderr: ""
    Jan 12 16:48:22.727: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 16:48:22.727: INFO: validating pod update-demo-nautilus-hjqnl
    Jan 12 16:48:22.733: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 16:48:22.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 16:48:22.733: INFO: update-demo-nautilus-hjqnl is verified up and running
    Jan 12 16:48:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-z8g8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 12 16:48:22.792: INFO: stderr: ""
    Jan 12 16:48:22.792: INFO: stdout: "true"
    Jan 12 16:48:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods update-demo-nautilus-z8g8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 12 16:48:22.850: INFO: stderr: ""
    Jan 12 16:48:22.850: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 12 16:48:22.850: INFO: validating pod update-demo-nautilus-z8g8s
    Jan 12 16:48:22.855: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 12 16:48:22.855: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 12 16:48:22.855: INFO: update-demo-nautilus-z8g8s is verified up and running
    STEP: using delete to clean up resources 01/12/23 16:48:22.855
    Jan 12 16:48:22.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 delete --grace-period=0 --force -f -'
    Jan 12 16:48:22.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 12 16:48:22.915: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 12 16:48:22.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get rc,svc -l name=update-demo --no-headers'
    Jan 12 16:48:22.997: INFO: stderr: "No resources found in kubectl-8061 namespace.\n"
    Jan 12 16:48:22.997: INFO: stdout: ""
    Jan 12 16:48:22.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8061 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 12 16:48:23.068: INFO: stderr: ""
    Jan 12 16:48:23.068: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:23.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8061" for this suite. 01/12/23 16:48:23.07
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:23.074
Jan 12 16:48:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:48:23.075
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:23.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:23.084
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/12/23 16:48:23.086
Jan 12 16:48:23.093: INFO: Waiting up to 5m0s for pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792" in namespace "downward-api-3838" to be "Succeeded or Failed"
Jan 12 16:48:23.095: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671257ms
Jan 12 16:48:25.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005366041s
Jan 12 16:48:27.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004834945s
STEP: Saw pod success 01/12/23 16:48:27.098
Jan 12 16:48:27.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792" satisfied condition "Succeeded or Failed"
Jan 12 16:48:27.100: INFO: Trying to get logs from node worker-1 pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 container dapi-container: <nil>
STEP: delete the pod 01/12/23 16:48:27.112
Jan 12 16:48:27.121: INFO: Waiting for pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 to disappear
Jan 12 16:48:27.122: INFO: Pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3838" for this suite. 01/12/23 16:48:27.124
------------------------------
 [4.055 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:23.074
    Jan 12 16:48:23.074: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:48:23.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:23.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:23.084
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/12/23 16:48:23.086
    Jan 12 16:48:23.093: INFO: Waiting up to 5m0s for pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792" in namespace "downward-api-3838" to be "Succeeded or Failed"
    Jan 12 16:48:23.095: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671257ms
    Jan 12 16:48:25.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005366041s
    Jan 12 16:48:27.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004834945s
    STEP: Saw pod success 01/12/23 16:48:27.098
    Jan 12 16:48:27.098: INFO: Pod "downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792" satisfied condition "Succeeded or Failed"
    Jan 12 16:48:27.100: INFO: Trying to get logs from node worker-1 pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 container dapi-container: <nil>
    STEP: delete the pod 01/12/23 16:48:27.112
    Jan 12 16:48:27.121: INFO: Waiting for pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 to disappear
    Jan 12 16:48:27.122: INFO: Pod downward-api-6e1408c8-3513-4438-8a21-c6a8235c9792 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:27.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3838" for this suite. 01/12/23 16:48:27.124
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:27.129
Jan 12 16:48:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename gc 01/12/23 16:48:27.13
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:27.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:27.142
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/12/23 16:48:27.146
STEP: create the rc2 01/12/23 16:48:27.149
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/12/23 16:48:32.159
STEP: delete the rc simpletest-rc-to-be-deleted 01/12/23 16:48:32.497
STEP: wait for the rc to be deleted 01/12/23 16:48:32.5
Jan 12 16:48:37.509: INFO: 73 pods remaining
Jan 12 16:48:37.509: INFO: 73 pods has nil DeletionTimestamp
Jan 12 16:48:37.509: INFO: 
STEP: Gathering metrics 01/12/23 16:48:42.507
W0112 16:48:42.510658      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 12 16:48:42.510: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 12 16:48:42.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-278vd" in namespace "gc-6190"
Jan 12 16:48:42.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-29zbd" in namespace "gc-6190"
Jan 12 16:48:42.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-4grt6" in namespace "gc-6190"
Jan 12 16:48:42.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kn97" in namespace "gc-6190"
Jan 12 16:48:42.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qv2f" in namespace "gc-6190"
Jan 12 16:48:42.543: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t6sl" in namespace "gc-6190"
Jan 12 16:48:42.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-522ms" in namespace "gc-6190"
Jan 12 16:48:42.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-526k7" in namespace "gc-6190"
Jan 12 16:48:42.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ck42" in namespace "gc-6190"
Jan 12 16:48:42.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fjh7" in namespace "gc-6190"
Jan 12 16:48:42.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-5scnd" in namespace "gc-6190"
Jan 12 16:48:42.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ss8z" in namespace "gc-6190"
Jan 12 16:48:42.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-66s2h" in namespace "gc-6190"
Jan 12 16:48:42.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-68xqq" in namespace "gc-6190"
Jan 12 16:48:42.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-69mz5" in namespace "gc-6190"
Jan 12 16:48:42.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nw6w" in namespace "gc-6190"
Jan 12 16:48:42.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bdw6" in namespace "gc-6190"
Jan 12 16:48:42.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fw68" in namespace "gc-6190"
Jan 12 16:48:42.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wl47" in namespace "gc-6190"
Jan 12 16:48:42.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-84zx7" in namespace "gc-6190"
Jan 12 16:48:42.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dm8n" in namespace "gc-6190"
Jan 12 16:48:42.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t9hg" in namespace "gc-6190"
Jan 12 16:48:42.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x85l" in namespace "gc-6190"
Jan 12 16:48:42.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z24p" in namespace "gc-6190"
Jan 12 16:48:42.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-95bdk" in namespace "gc-6190"
Jan 12 16:48:42.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-974nj" in namespace "gc-6190"
Jan 12 16:48:42.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-9978b" in namespace "gc-6190"
Jan 12 16:48:42.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cdmh" in namespace "gc-6190"
Jan 12 16:48:42.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dl9r" in namespace "gc-6190"
Jan 12 16:48:42.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jns5" in namespace "gc-6190"
Jan 12 16:48:42.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5fw2" in namespace "gc-6190"
Jan 12 16:48:42.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcwnb" in namespace "gc-6190"
Jan 12 16:48:42.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm24z" in namespace "gc-6190"
Jan 12 16:48:42.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxmkx" in namespace "gc-6190"
Jan 12 16:48:42.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-c42rz" in namespace "gc-6190"
Jan 12 16:48:42.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-c58z4" in namespace "gc-6190"
Jan 12 16:48:42.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-chxzq" in namespace "gc-6190"
Jan 12 16:48:42.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6dv5" in namespace "gc-6190"
Jan 12 16:48:42.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7kl2" in namespace "gc-6190"
Jan 12 16:48:42.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvqkl" in namespace "gc-6190"
Jan 12 16:48:42.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-f48nb" in namespace "gc-6190"
Jan 12 16:48:42.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd759" in namespace "gc-6190"
Jan 12 16:48:42.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgt7w" in namespace "gc-6190"
Jan 12 16:48:42.852: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn25r" in namespace "gc-6190"
Jan 12 16:48:42.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwzsx" in namespace "gc-6190"
Jan 12 16:48:42.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2l5n" in namespace "gc-6190"
Jan 12 16:48:42.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-g86kn" in namespace "gc-6190"
Jan 12 16:48:42.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8h2h" in namespace "gc-6190"
Jan 12 16:48:42.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpkf6" in namespace "gc-6190"
Jan 12 16:48:42.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxz8q" in namespace "gc-6190"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:42.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6190" for this suite. 01/12/23 16:48:42.916
------------------------------
 [SLOW TEST] [15.791 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:27.129
    Jan 12 16:48:27.129: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename gc 01/12/23 16:48:27.13
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:27.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:27.142
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/12/23 16:48:27.146
    STEP: create the rc2 01/12/23 16:48:27.149
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/12/23 16:48:32.159
    STEP: delete the rc simpletest-rc-to-be-deleted 01/12/23 16:48:32.497
    STEP: wait for the rc to be deleted 01/12/23 16:48:32.5
    Jan 12 16:48:37.509: INFO: 73 pods remaining
    Jan 12 16:48:37.509: INFO: 73 pods has nil DeletionTimestamp
    Jan 12 16:48:37.509: INFO: 
    STEP: Gathering metrics 01/12/23 16:48:42.507
    W0112 16:48:42.510658      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 12 16:48:42.510: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 12 16:48:42.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-278vd" in namespace "gc-6190"
    Jan 12 16:48:42.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-29zbd" in namespace "gc-6190"
    Jan 12 16:48:42.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-4grt6" in namespace "gc-6190"
    Jan 12 16:48:42.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kn97" in namespace "gc-6190"
    Jan 12 16:48:42.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qv2f" in namespace "gc-6190"
    Jan 12 16:48:42.543: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t6sl" in namespace "gc-6190"
    Jan 12 16:48:42.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-522ms" in namespace "gc-6190"
    Jan 12 16:48:42.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-526k7" in namespace "gc-6190"
    Jan 12 16:48:42.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ck42" in namespace "gc-6190"
    Jan 12 16:48:42.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fjh7" in namespace "gc-6190"
    Jan 12 16:48:42.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-5scnd" in namespace "gc-6190"
    Jan 12 16:48:42.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ss8z" in namespace "gc-6190"
    Jan 12 16:48:42.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-66s2h" in namespace "gc-6190"
    Jan 12 16:48:42.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-68xqq" in namespace "gc-6190"
    Jan 12 16:48:42.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-69mz5" in namespace "gc-6190"
    Jan 12 16:48:42.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nw6w" in namespace "gc-6190"
    Jan 12 16:48:42.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bdw6" in namespace "gc-6190"
    Jan 12 16:48:42.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-7fw68" in namespace "gc-6190"
    Jan 12 16:48:42.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wl47" in namespace "gc-6190"
    Jan 12 16:48:42.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-84zx7" in namespace "gc-6190"
    Jan 12 16:48:42.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dm8n" in namespace "gc-6190"
    Jan 12 16:48:42.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t9hg" in namespace "gc-6190"
    Jan 12 16:48:42.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x85l" in namespace "gc-6190"
    Jan 12 16:48:42.690: INFO: Deleting pod "simpletest-rc-to-be-deleted-8z24p" in namespace "gc-6190"
    Jan 12 16:48:42.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-95bdk" in namespace "gc-6190"
    Jan 12 16:48:42.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-974nj" in namespace "gc-6190"
    Jan 12 16:48:42.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-9978b" in namespace "gc-6190"
    Jan 12 16:48:42.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cdmh" in namespace "gc-6190"
    Jan 12 16:48:42.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dl9r" in namespace "gc-6190"
    Jan 12 16:48:42.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jns5" in namespace "gc-6190"
    Jan 12 16:48:42.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5fw2" in namespace "gc-6190"
    Jan 12 16:48:42.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcwnb" in namespace "gc-6190"
    Jan 12 16:48:42.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm24z" in namespace "gc-6190"
    Jan 12 16:48:42.765: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxmkx" in namespace "gc-6190"
    Jan 12 16:48:42.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-c42rz" in namespace "gc-6190"
    Jan 12 16:48:42.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-c58z4" in namespace "gc-6190"
    Jan 12 16:48:42.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-chxzq" in namespace "gc-6190"
    Jan 12 16:48:42.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6dv5" in namespace "gc-6190"
    Jan 12 16:48:42.810: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7kl2" in namespace "gc-6190"
    Jan 12 16:48:42.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvqkl" in namespace "gc-6190"
    Jan 12 16:48:42.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-f48nb" in namespace "gc-6190"
    Jan 12 16:48:42.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd759" in namespace "gc-6190"
    Jan 12 16:48:42.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgt7w" in namespace "gc-6190"
    Jan 12 16:48:42.852: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn25r" in namespace "gc-6190"
    Jan 12 16:48:42.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwzsx" in namespace "gc-6190"
    Jan 12 16:48:42.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2l5n" in namespace "gc-6190"
    Jan 12 16:48:42.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-g86kn" in namespace "gc-6190"
    Jan 12 16:48:42.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8h2h" in namespace "gc-6190"
    Jan 12 16:48:42.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-gpkf6" in namespace "gc-6190"
    Jan 12 16:48:42.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxz8q" in namespace "gc-6190"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:42.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6190" for this suite. 01/12/23 16:48:42.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:42.933
Jan 12 16:48:42.933: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:48:42.934
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:42.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:42.956
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:48:42.961
Jan 12 16:48:42.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66" in namespace "projected-4384" to be "Succeeded or Failed"
Jan 12 16:48:42.972: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.947026ms
Jan 12 16:48:44.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004586356s
Jan 12 16:48:46.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005327902s
Jan 12 16:48:48.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004895424s
Jan 12 16:48:50.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005187236s
Jan 12 16:48:52.974: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004486465s
Jan 12 16:48:54.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.00496638s
STEP: Saw pod success 01/12/23 16:48:54.975
Jan 12 16:48:54.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66" satisfied condition "Succeeded or Failed"
Jan 12 16:48:54.977: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 container client-container: <nil>
STEP: delete the pod 01/12/23 16:48:54.982
Jan 12 16:48:54.990: INFO: Waiting for pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 to disappear
Jan 12 16:48:54.992: INFO: Pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:48:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4384" for this suite. 01/12/23 16:48:54.996
------------------------------
 [SLOW TEST] [12.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:42.933
    Jan 12 16:48:42.933: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:48:42.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:42.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:42.956
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:48:42.961
    Jan 12 16:48:42.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66" in namespace "projected-4384" to be "Succeeded or Failed"
    Jan 12 16:48:42.972: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.947026ms
    Jan 12 16:48:44.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004586356s
    Jan 12 16:48:46.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005327902s
    Jan 12 16:48:48.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004895424s
    Jan 12 16:48:50.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005187236s
    Jan 12 16:48:52.974: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004486465s
    Jan 12 16:48:54.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.00496638s
    STEP: Saw pod success 01/12/23 16:48:54.975
    Jan 12 16:48:54.975: INFO: Pod "downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66" satisfied condition "Succeeded or Failed"
    Jan 12 16:48:54.977: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:48:54.982
    Jan 12 16:48:54.990: INFO: Waiting for pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 to disappear
    Jan 12 16:48:54.992: INFO: Pod downwardapi-volume-e54c5e7b-e4e2-4851-a836-ae5068f3dd66 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:48:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4384" for this suite. 01/12/23 16:48:54.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:55
Jan 12 16:48:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption 01/12/23 16:48:55.001
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:55.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:55.013
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:48:55.015
Jan 12 16:48:55.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption-2 01/12/23 16:48:55.016
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:55.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:55.025
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/12/23 16:48:55.031
STEP: Waiting for the pdb to be processed 01/12/23 16:48:57.04
STEP: Waiting for the pdb to be processed 01/12/23 16:48:59.048
STEP: listing a collection of PDBs across all namespaces 01/12/23 16:49:01.052
STEP: listing a collection of PDBs in namespace disruption-9468 01/12/23 16:49:01.054
STEP: deleting a collection of PDBs 01/12/23 16:49:01.056
STEP: Waiting for the PDB collection to be deleted 01/12/23 16:49:01.063
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:01.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:01.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3060" for this suite. 01/12/23 16:49:01.069
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9468" for this suite. 01/12/23 16:49:01.074
------------------------------
 [SLOW TEST] [6.077 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:55
    Jan 12 16:48:55.000: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption 01/12/23 16:48:55.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:55.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:55.013
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:48:55.015
    Jan 12 16:48:55.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption-2 01/12/23 16:48:55.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:48:55.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:48:55.025
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/12/23 16:48:55.031
    STEP: Waiting for the pdb to be processed 01/12/23 16:48:57.04
    STEP: Waiting for the pdb to be processed 01/12/23 16:48:59.048
    STEP: listing a collection of PDBs across all namespaces 01/12/23 16:49:01.052
    STEP: listing a collection of PDBs in namespace disruption-9468 01/12/23 16:49:01.054
    STEP: deleting a collection of PDBs 01/12/23 16:49:01.056
    STEP: Waiting for the PDB collection to be deleted 01/12/23 16:49:01.063
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:01.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:01.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3060" for this suite. 01/12/23 16:49:01.069
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9468" for this suite. 01/12/23 16:49:01.074
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:01.078
Jan 12 16:49:01.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:49:01.079
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:01.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:01.09
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:49:01.092
Jan 12 16:49:01.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633" in namespace "downward-api-1732" to be "Succeeded or Failed"
Jan 12 16:49:01.099: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Pending", Reason="", readiness=false. Elapsed: 1.909538ms
Jan 12 16:49:03.102: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005091381s
Jan 12 16:49:05.101: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004487416s
STEP: Saw pod success 01/12/23 16:49:05.101
Jan 12 16:49:05.102: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633" satisfied condition "Succeeded or Failed"
Jan 12 16:49:05.103: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 container client-container: <nil>
STEP: delete the pod 01/12/23 16:49:05.107
Jan 12 16:49:05.117: INFO: Waiting for pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 to disappear
Jan 12 16:49:05.118: INFO: Pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:05.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1732" for this suite. 01/12/23 16:49:05.12
------------------------------
 [4.049 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:01.078
    Jan 12 16:49:01.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:49:01.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:01.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:01.09
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:49:01.092
    Jan 12 16:49:01.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633" in namespace "downward-api-1732" to be "Succeeded or Failed"
    Jan 12 16:49:01.099: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Pending", Reason="", readiness=false. Elapsed: 1.909538ms
    Jan 12 16:49:03.102: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005091381s
    Jan 12 16:49:05.101: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004487416s
    STEP: Saw pod success 01/12/23 16:49:05.101
    Jan 12 16:49:05.102: INFO: Pod "downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633" satisfied condition "Succeeded or Failed"
    Jan 12 16:49:05.103: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:49:05.107
    Jan 12 16:49:05.117: INFO: Waiting for pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 to disappear
    Jan 12 16:49:05.118: INFO: Pod downwardapi-volume-451d8aa2-5e1d-4eec-83af-1a39a22c3633 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:05.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1732" for this suite. 01/12/23 16:49:05.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:05.127
Jan 12 16:49:05.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename containers 01/12/23 16:49:05.128
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:05.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:05.138
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/12/23 16:49:05.14
Jan 12 16:49:05.146: INFO: Waiting up to 5m0s for pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50" in namespace "containers-7508" to be "Succeeded or Failed"
Jan 12 16:49:05.148: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691925ms
Jan 12 16:49:07.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004332411s
Jan 12 16:49:09.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004540246s
STEP: Saw pod success 01/12/23 16:49:09.151
Jan 12 16:49:09.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50" satisfied condition "Succeeded or Failed"
Jan 12 16:49:09.153: INFO: Trying to get logs from node worker-1 pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:49:09.157
Jan 12 16:49:09.164: INFO: Waiting for pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 to disappear
Jan 12 16:49:09.166: INFO: Pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:09.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7508" for this suite. 01/12/23 16:49:09.168
------------------------------
 [4.044 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:05.127
    Jan 12 16:49:05.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename containers 01/12/23 16:49:05.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:05.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:05.138
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/12/23 16:49:05.14
    Jan 12 16:49:05.146: INFO: Waiting up to 5m0s for pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50" in namespace "containers-7508" to be "Succeeded or Failed"
    Jan 12 16:49:05.148: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691925ms
    Jan 12 16:49:07.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004332411s
    Jan 12 16:49:09.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004540246s
    STEP: Saw pod success 01/12/23 16:49:09.151
    Jan 12 16:49:09.151: INFO: Pod "client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50" satisfied condition "Succeeded or Failed"
    Jan 12 16:49:09.153: INFO: Trying to get logs from node worker-1 pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:49:09.157
    Jan 12 16:49:09.164: INFO: Waiting for pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 to disappear
    Jan 12 16:49:09.166: INFO: Pod client-containers-3d332ad3-df91-4795-b9b2-3a9e3181fd50 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:09.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7508" for this suite. 01/12/23 16:49:09.168
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:09.171
Jan 12 16:49:09.172: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:49:09.172
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:09.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:09.181
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:49:09.191
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:49:09.961
STEP: Deploying the webhook pod 01/12/23 16:49:09.966
STEP: Wait for the deployment to be ready 01/12/23 16:49:09.976
Jan 12 16:49:09.982: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 12 16:49:11.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:49:13.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:49:15.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:49:17.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 12 16:49:19.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/12/23 16:49:21.991
STEP: Verifying the service has paired with the endpoint 01/12/23 16:49:22.001
Jan 12 16:49:23.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 12 16:49:23.003: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-915-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:49:23.511
STEP: Creating a custom resource while v1 is storage version 01/12/23 16:49:23.526
STEP: Patching Custom Resource Definition to set v2 as storage 01/12/23 16:49:25.574
STEP: Patching the custom resource while v2 is storage version 01/12/23 16:49:25.591
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:26.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6316" for this suite. 01/12/23 16:49:26.182
STEP: Destroying namespace "webhook-6316-markers" for this suite. 01/12/23 16:49:26.19
------------------------------
 [SLOW TEST] [17.022 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:09.171
    Jan 12 16:49:09.172: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:49:09.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:09.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:09.181
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:49:09.191
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:49:09.961
    STEP: Deploying the webhook pod 01/12/23 16:49:09.966
    STEP: Wait for the deployment to be ready 01/12/23 16:49:09.976
    Jan 12 16:49:09.982: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 12 16:49:11.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:49:13.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:49:15.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:49:17.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 12 16:49:19.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 12, 16, 49, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 12, 16, 49, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/12/23 16:49:21.991
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:49:22.001
    Jan 12 16:49:23.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 12 16:49:23.003: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-915-crds.webhook.example.com via the AdmissionRegistration API 01/12/23 16:49:23.511
    STEP: Creating a custom resource while v1 is storage version 01/12/23 16:49:23.526
    STEP: Patching Custom Resource Definition to set v2 as storage 01/12/23 16:49:25.574
    STEP: Patching the custom resource while v2 is storage version 01/12/23 16:49:25.591
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:26.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6316" for this suite. 01/12/23 16:49:26.182
    STEP: Destroying namespace "webhook-6316-markers" for this suite. 01/12/23 16:49:26.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:26.197
Jan 12 16:49:26.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename endpointslicemirroring 01/12/23 16:49:26.198
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:26.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:26.211
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/12/23 16:49:26.224
Jan 12 16:49:26.229: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/12/23 16:49:28.232
Jan 12 16:49:28.237: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/12/23 16:49:30.239
Jan 12 16:49:30.248: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:32.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-4772" for this suite. 01/12/23 16:49:32.253
------------------------------
 [SLOW TEST] [6.060 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:26.197
    Jan 12 16:49:26.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename endpointslicemirroring 01/12/23 16:49:26.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:26.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:26.211
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/12/23 16:49:26.224
    Jan 12 16:49:26.229: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/12/23 16:49:28.232
    Jan 12 16:49:28.237: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/12/23 16:49:30.239
    Jan 12 16:49:30.248: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:32.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-4772" for this suite. 01/12/23 16:49:32.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:32.259
Jan 12 16:49:32.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:49:32.26
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:32.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:32.273
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:49:32.282
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:49:32.505
STEP: Deploying the webhook pod 01/12/23 16:49:32.511
STEP: Wait for the deployment to be ready 01/12/23 16:49:32.521
Jan 12 16:49:32.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:49:34.533
STEP: Verifying the service has paired with the endpoint 01/12/23 16:49:34.542
Jan 12 16:49:35.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/12/23 16:49:35.545
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/12/23 16:49:35.546
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 16:49:35.546
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/12/23 16:49:35.546
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/12/23 16:49:35.547
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 16:49:35.548
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 16:49:35.548
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:35.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6648" for this suite. 01/12/23 16:49:35.583
STEP: Destroying namespace "webhook-6648-markers" for this suite. 01/12/23 16:49:35.59
------------------------------
 [3.337 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:32.259
    Jan 12 16:49:32.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:49:32.26
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:32.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:32.273
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:49:32.282
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:49:32.505
    STEP: Deploying the webhook pod 01/12/23 16:49:32.511
    STEP: Wait for the deployment to be ready 01/12/23 16:49:32.521
    Jan 12 16:49:32.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:49:34.533
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:49:34.542
    Jan 12 16:49:35.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/12/23 16:49:35.545
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/12/23 16:49:35.546
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/12/23 16:49:35.546
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/12/23 16:49:35.546
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/12/23 16:49:35.547
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 16:49:35.548
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/12/23 16:49:35.548
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:35.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6648" for this suite. 01/12/23 16:49:35.583
    STEP: Destroying namespace "webhook-6648-markers" for this suite. 01/12/23 16:49:35.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:35.596
Jan 12 16:49:35.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:49:35.597
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:35.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:35.609
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/12/23 16:49:35.611
STEP: Ensuring ResourceQuota status is calculated 01/12/23 16:49:35.614
STEP: Creating a ResourceQuota with not best effort scope 01/12/23 16:49:37.617
STEP: Ensuring ResourceQuota status is calculated 01/12/23 16:49:37.621
STEP: Creating a best-effort pod 01/12/23 16:49:39.624
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/12/23 16:49:39.632
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/12/23 16:49:41.635
STEP: Deleting the pod 01/12/23 16:49:43.637
STEP: Ensuring resource quota status released the pod usage 01/12/23 16:49:43.644
STEP: Creating a not best-effort pod 01/12/23 16:49:45.647
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/12/23 16:49:45.654
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/12/23 16:49:47.656
STEP: Deleting the pod 01/12/23 16:49:49.66
STEP: Ensuring resource quota status released the pod usage 01/12/23 16:49:49.67
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:51.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6111" for this suite. 01/12/23 16:49:51.676
------------------------------
 [SLOW TEST] [16.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:35.596
    Jan 12 16:49:35.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:49:35.597
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:35.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:35.609
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/12/23 16:49:35.611
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 16:49:35.614
    STEP: Creating a ResourceQuota with not best effort scope 01/12/23 16:49:37.617
    STEP: Ensuring ResourceQuota status is calculated 01/12/23 16:49:37.621
    STEP: Creating a best-effort pod 01/12/23 16:49:39.624
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/12/23 16:49:39.632
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/12/23 16:49:41.635
    STEP: Deleting the pod 01/12/23 16:49:43.637
    STEP: Ensuring resource quota status released the pod usage 01/12/23 16:49:43.644
    STEP: Creating a not best-effort pod 01/12/23 16:49:45.647
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/12/23 16:49:45.654
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/12/23 16:49:47.656
    STEP: Deleting the pod 01/12/23 16:49:49.66
    STEP: Ensuring resource quota status released the pod usage 01/12/23 16:49:49.67
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:51.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6111" for this suite. 01/12/23 16:49:51.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:51.682
Jan 12 16:49:51.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:49:51.682
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:51.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:51.695
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2755 01/12/23 16:49:51.697
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 16:49:51.707
STEP: creating service externalsvc in namespace services-2755 01/12/23 16:49:51.708
STEP: creating replication controller externalsvc in namespace services-2755 01/12/23 16:49:51.716
I0112 16:49:51.722166      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2755, replica count: 2
I0112 16:49:54.773534      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/12/23 16:49:54.775
Jan 12 16:49:54.788: INFO: Creating new exec pod
Jan 12 16:49:54.793: INFO: Waiting up to 5m0s for pod "execpodbls5b" in namespace "services-2755" to be "running"
Jan 12 16:49:54.795: INFO: Pod "execpodbls5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.781038ms
Jan 12 16:49:56.798: INFO: Pod "execpodbls5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00436176s
Jan 12 16:49:56.798: INFO: Pod "execpodbls5b" satisfied condition "running"
Jan 12 16:49:56.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-2755 exec execpodbls5b -- /bin/sh -x -c nslookup nodeport-service.services-2755.svc.cluster.local'
Jan 12 16:49:56.933: INFO: stderr: "+ nslookup nodeport-service.services-2755.svc.cluster.local\n"
Jan 12 16:49:56.933: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2755.svc.cluster.local\tcanonical name = externalsvc.services-2755.svc.cluster.local.\nName:\texternalsvc.services-2755.svc.cluster.local\nAddress: 10.100.212.123\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2755, will wait for the garbage collector to delete the pods 01/12/23 16:49:56.933
Jan 12 16:49:56.989: INFO: Deleting ReplicationController externalsvc took: 3.343013ms
Jan 12 16:49:57.089: INFO: Terminating ReplicationController externalsvc pods took: 100.180011ms
Jan 12 16:49:58.903: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:49:58.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2755" for this suite. 01/12/23 16:49:58.914
------------------------------
 [SLOW TEST] [7.238 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:51.682
    Jan 12 16:49:51.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:49:51.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:51.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:51.695
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2755 01/12/23 16:49:51.697
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/12/23 16:49:51.707
    STEP: creating service externalsvc in namespace services-2755 01/12/23 16:49:51.708
    STEP: creating replication controller externalsvc in namespace services-2755 01/12/23 16:49:51.716
    I0112 16:49:51.722166      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2755, replica count: 2
    I0112 16:49:54.773534      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/12/23 16:49:54.775
    Jan 12 16:49:54.788: INFO: Creating new exec pod
    Jan 12 16:49:54.793: INFO: Waiting up to 5m0s for pod "execpodbls5b" in namespace "services-2755" to be "running"
    Jan 12 16:49:54.795: INFO: Pod "execpodbls5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.781038ms
    Jan 12 16:49:56.798: INFO: Pod "execpodbls5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00436176s
    Jan 12 16:49:56.798: INFO: Pod "execpodbls5b" satisfied condition "running"
    Jan 12 16:49:56.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=services-2755 exec execpodbls5b -- /bin/sh -x -c nslookup nodeport-service.services-2755.svc.cluster.local'
    Jan 12 16:49:56.933: INFO: stderr: "+ nslookup nodeport-service.services-2755.svc.cluster.local\n"
    Jan 12 16:49:56.933: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2755.svc.cluster.local\tcanonical name = externalsvc.services-2755.svc.cluster.local.\nName:\texternalsvc.services-2755.svc.cluster.local\nAddress: 10.100.212.123\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2755, will wait for the garbage collector to delete the pods 01/12/23 16:49:56.933
    Jan 12 16:49:56.989: INFO: Deleting ReplicationController externalsvc took: 3.343013ms
    Jan 12 16:49:57.089: INFO: Terminating ReplicationController externalsvc pods took: 100.180011ms
    Jan 12 16:49:58.903: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:49:58.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2755" for this suite. 01/12/23 16:49:58.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:49:58.922
Jan 12 16:49:58.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:49:58.923
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:58.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:58.933
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:49:58.935
Jan 12 16:49:58.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7" in namespace "projected-2008" to be "Succeeded or Failed"
Jan 12 16:49:58.942: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.991514ms
Jan 12 16:50:00.945: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00524804s
Jan 12 16:50:02.946: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005567257s
STEP: Saw pod success 01/12/23 16:50:02.946
Jan 12 16:50:02.946: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7" satisfied condition "Succeeded or Failed"
Jan 12 16:50:02.948: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 container client-container: <nil>
STEP: delete the pod 01/12/23 16:50:02.959
Jan 12 16:50:02.967: INFO: Waiting for pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 to disappear
Jan 12 16:50:02.969: INFO: Pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:02.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2008" for this suite. 01/12/23 16:50:02.971
------------------------------
 [4.053 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:49:58.922
    Jan 12 16:49:58.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:49:58.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:49:58.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:49:58.933
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:49:58.935
    Jan 12 16:49:58.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7" in namespace "projected-2008" to be "Succeeded or Failed"
    Jan 12 16:49:58.942: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.991514ms
    Jan 12 16:50:00.945: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00524804s
    Jan 12 16:50:02.946: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005567257s
    STEP: Saw pod success 01/12/23 16:50:02.946
    Jan 12 16:50:02.946: INFO: Pod "downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7" satisfied condition "Succeeded or Failed"
    Jan 12 16:50:02.948: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:50:02.959
    Jan 12 16:50:02.967: INFO: Waiting for pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 to disappear
    Jan 12 16:50:02.969: INFO: Pod downwardapi-volume-e1630802-2e66-4ba9-8f3d-c7dc8ab2a9a7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:02.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2008" for this suite. 01/12/23 16:50:02.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:02.976
Jan 12 16:50:02.976: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:50:02.977
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:02.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:02.989
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:07.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9881" for this suite. 01/12/23 16:50:07.008
------------------------------
 [4.036 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:02.976
    Jan 12 16:50:02.976: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubelet-test 01/12/23 16:50:02.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:02.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:02.989
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:07.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9881" for this suite. 01/12/23 16:50:07.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:07.015
Jan 12 16:50:07.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename ephemeral-containers-test 01/12/23 16:50:07.015
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:07.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:07.028
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/12/23 16:50:07.03
Jan 12 16:50:07.037: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7963" to be "running and ready"
Jan 12 16:50:07.038: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706228ms
Jan 12 16:50:07.038: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:50:09.041: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004621967s
Jan 12 16:50:09.041: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 12 16:50:09.041: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/12/23 16:50:09.043
Jan 12 16:50:09.053: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7963" to be "container debugger running"
Jan 12 16:50:09.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.643995ms
Jan 12 16:50:11.058: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004552336s
Jan 12 16:50:11.058: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/12/23 16:50:11.058
Jan 12 16:50:11.058: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7963 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:50:11.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:50:11.059: INFO: ExecWithOptions: Clientset creation
Jan 12 16:50:11.059: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-7963/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 12 16:50:11.117: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:11.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-7963" for this suite. 01/12/23 16:50:11.123
------------------------------
 [4.113 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:07.015
    Jan 12 16:50:07.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/12/23 16:50:07.015
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:07.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:07.028
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/12/23 16:50:07.03
    Jan 12 16:50:07.037: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7963" to be "running and ready"
    Jan 12 16:50:07.038: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706228ms
    Jan 12 16:50:07.038: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:50:09.041: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004621967s
    Jan 12 16:50:09.041: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 12 16:50:09.041: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/12/23 16:50:09.043
    Jan 12 16:50:09.053: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-7963" to be "container debugger running"
    Jan 12 16:50:09.055: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.643995ms
    Jan 12 16:50:11.058: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004552336s
    Jan 12 16:50:11.058: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/12/23 16:50:11.058
    Jan 12 16:50:11.058: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7963 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:50:11.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:50:11.059: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:50:11.059: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-7963/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 12 16:50:11.117: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:11.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-7963" for this suite. 01/12/23 16:50:11.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:11.129
Jan 12 16:50:11.129: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:50:11.13
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:11.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:11.141
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:50:11.143
Jan 12 16:50:11.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4" in namespace "downward-api-8059" to be "Succeeded or Failed"
Jan 12 16:50:11.150: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769749ms
Jan 12 16:50:13.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004906351s
Jan 12 16:50:15.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004809171s
STEP: Saw pod success 01/12/23 16:50:15.153
Jan 12 16:50:15.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4" satisfied condition "Succeeded or Failed"
Jan 12 16:50:15.155: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 container client-container: <nil>
STEP: delete the pod 01/12/23 16:50:15.159
Jan 12 16:50:15.170: INFO: Waiting for pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 to disappear
Jan 12 16:50:15.172: INFO: Pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:15.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8059" for this suite. 01/12/23 16:50:15.174
------------------------------
 [4.049 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:11.129
    Jan 12 16:50:11.129: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:50:11.13
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:11.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:11.141
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:50:11.143
    Jan 12 16:50:11.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4" in namespace "downward-api-8059" to be "Succeeded or Failed"
    Jan 12 16:50:11.150: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.769749ms
    Jan 12 16:50:13.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004906351s
    Jan 12 16:50:15.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004809171s
    STEP: Saw pod success 01/12/23 16:50:15.153
    Jan 12 16:50:15.153: INFO: Pod "downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4" satisfied condition "Succeeded or Failed"
    Jan 12 16:50:15.155: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 container client-container: <nil>
    STEP: delete the pod 01/12/23 16:50:15.159
    Jan 12 16:50:15.170: INFO: Waiting for pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 to disappear
    Jan 12 16:50:15.172: INFO: Pod downwardapi-volume-7ae94c15-d112-47e3-8caf-19c70f639fb4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:15.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8059" for this suite. 01/12/23 16:50:15.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:15.179
Jan 12 16:50:15.179: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 16:50:15.18
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:15.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:15.189
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 12 16:50:15.192: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 12 16:50:15.197: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 16:50:20.199: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 16:50:20.2
Jan 12 16:50:20.200: INFO: Creating deployment "test-rolling-update-deployment"
Jan 12 16:50:20.204: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 12 16:50:20.208: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 12 16:50:22.214: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 12 16:50:22.215: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 16:50:22.220: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8634  4b08ee4a-5858-4825-8934-87a14fb77868 32341 1 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055d0aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 16:50:20 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-12 16:50:20 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 12 16:50:22.222: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8634  c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17 32331 1 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4b08ee4a-5858-4825-8934-87a14fb77868 0xc0064906a7 0xc0064906a8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b08ee4a-5858-4825-8934-87a14fb77868\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006490758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:50:22.222: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 12 16:50:22.222: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8634  bfc57456-b2ae-4840-af5e-51198de0130f 32340 2 2023-01-12 16:50:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4b08ee4a-5858-4825-8934-87a14fb77868 0xc006490577 0xc006490578}] [] [{e2e.test Update apps/v1 2023-01-12 16:50:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b08ee4a-5858-4825-8934-87a14fb77868\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006490638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:50:22.224: INFO: Pod "test-rolling-update-deployment-7549d9f46d-nxhk5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-nxhk5 test-rolling-update-deployment-7549d9f46d- deployment-8634  3bb8a764-4686-4b70-9601-c5ec479c6319 32330 0 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17 0xc0055d0e77 0xc0055d0e78}] [] [{kube-controller-manager Update v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgr9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgr9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.15,StartTime:2023-01-12 16:50:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:50:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e00567d92441e7e1d07ba6bffe2340992aca0b9da11b6efb591f1bdaca6078fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8634" for this suite. 01/12/23 16:50:22.226
------------------------------
 [SLOW TEST] [7.051 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:15.179
    Jan 12 16:50:15.179: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 16:50:15.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:15.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:15.189
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 12 16:50:15.192: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 12 16:50:15.197: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 16:50:20.199: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 16:50:20.2
    Jan 12 16:50:20.200: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 12 16:50:20.204: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 12 16:50:20.208: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 12 16:50:22.214: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 12 16:50:22.215: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 16:50:22.220: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8634  4b08ee4a-5858-4825-8934-87a14fb77868 32341 1 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055d0aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-12 16:50:20 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-12 16:50:20 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 12 16:50:22.222: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8634  c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17 32331 1 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4b08ee4a-5858-4825-8934-87a14fb77868 0xc0064906a7 0xc0064906a8}] [] [{kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b08ee4a-5858-4825-8934-87a14fb77868\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006490758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:50:22.222: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 12 16:50:22.222: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8634  bfc57456-b2ae-4840-af5e-51198de0130f 32340 2 2023-01-12 16:50:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4b08ee4a-5858-4825-8934-87a14fb77868 0xc006490577 0xc006490578}] [] [{e2e.test Update apps/v1 2023-01-12 16:50:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b08ee4a-5858-4825-8934-87a14fb77868\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006490638 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:50:22.224: INFO: Pod "test-rolling-update-deployment-7549d9f46d-nxhk5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-nxhk5 test-rolling-update-deployment-7549d9f46d- deployment-8634  3bb8a764-4686-4b70-9601-c5ec479c6319 32330 0 2023-01-12 16:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17 0xc0055d0e77 0xc0055d0e78}] [] [{kube-controller-manager Update v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8d8c44b-07bd-45a2-8bfc-eb9f218d9e17\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgr9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgr9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:50:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.42.160,PodIP:10.244.0.15,StartTime:2023-01-12 16:50:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:50:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e00567d92441e7e1d07ba6bffe2340992aca0b9da11b6efb591f1bdaca6078fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8634" for this suite. 01/12/23 16:50:22.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:22.231
Jan 12 16:50:22.231: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename projected 01/12/23 16:50:22.232
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:22.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:22.241
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-ce30d84f-33fe-4a5b-bfb5-1ecbdbafa0c3 01/12/23 16:50:22.244
STEP: Creating a pod to test consume configMaps 01/12/23 16:50:22.246
Jan 12 16:50:22.251: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553" in namespace "projected-3030" to be "Succeeded or Failed"
Jan 12 16:50:22.252: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572963ms
Jan 12 16:50:24.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003892592s
Jan 12 16:50:26.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004248361s
STEP: Saw pod success 01/12/23 16:50:26.255
Jan 12 16:50:26.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553" satisfied condition "Succeeded or Failed"
Jan 12 16:50:26.257: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 container agnhost-container: <nil>
STEP: delete the pod 01/12/23 16:50:26.262
Jan 12 16:50:26.270: INFO: Waiting for pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 to disappear
Jan 12 16:50:26.272: INFO: Pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 12 16:50:26.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3030" for this suite. 01/12/23 16:50:26.274
------------------------------
 [4.046 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:22.231
    Jan 12 16:50:22.231: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename projected 01/12/23 16:50:22.232
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:22.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:22.241
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-ce30d84f-33fe-4a5b-bfb5-1ecbdbafa0c3 01/12/23 16:50:22.244
    STEP: Creating a pod to test consume configMaps 01/12/23 16:50:22.246
    Jan 12 16:50:22.251: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553" in namespace "projected-3030" to be "Succeeded or Failed"
    Jan 12 16:50:22.252: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572963ms
    Jan 12 16:50:24.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003892592s
    Jan 12 16:50:26.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004248361s
    STEP: Saw pod success 01/12/23 16:50:26.255
    Jan 12 16:50:26.255: INFO: Pod "pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553" satisfied condition "Succeeded or Failed"
    Jan 12 16:50:26.257: INFO: Trying to get logs from node worker-1 pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 container agnhost-container: <nil>
    STEP: delete the pod 01/12/23 16:50:26.262
    Jan 12 16:50:26.270: INFO: Waiting for pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 to disappear
    Jan 12 16:50:26.272: INFO: Pod pod-projected-configmaps-28d01cc5-b595-4975-8219-49b64743a553 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:50:26.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3030" for this suite. 01/12/23 16:50:26.274
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:50:26.278
Jan 12 16:50:26.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename job 01/12/23 16:50:26.279
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:26.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:26.291
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/12/23 16:50:26.293
STEP: Ensuring active pods == parallelism 01/12/23 16:50:26.296
STEP: delete a job 01/12/23 16:50:28.299
STEP: deleting Job.batch foo in namespace job-4121, will wait for the garbage collector to delete the pods 01/12/23 16:50:28.3
Jan 12 16:50:28.355: INFO: Deleting Job.batch foo took: 3.734591ms
Jan 12 16:50:28.455: INFO: Terminating Job.batch foo pods took: 100.112568ms
STEP: Ensuring job was deleted 01/12/23 16:51:00.856
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4121" for this suite. 01/12/23 16:51:00.86
------------------------------
 [SLOW TEST] [34.586 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:50:26.278
    Jan 12 16:50:26.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename job 01/12/23 16:50:26.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:50:26.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:50:26.291
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/12/23 16:50:26.293
    STEP: Ensuring active pods == parallelism 01/12/23 16:50:26.296
    STEP: delete a job 01/12/23 16:50:28.299
    STEP: deleting Job.batch foo in namespace job-4121, will wait for the garbage collector to delete the pods 01/12/23 16:50:28.3
    Jan 12 16:50:28.355: INFO: Deleting Job.batch foo took: 3.734591ms
    Jan 12 16:50:28.455: INFO: Terminating Job.batch foo pods took: 100.112568ms
    STEP: Ensuring job was deleted 01/12/23 16:51:00.856
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:00.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4121" for this suite. 01/12/23 16:51:00.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:00.866
Jan 12 16:51:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 16:51:00.867
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:00.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:00.878
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/12/23 16:51:00.88
Jan 12 16:51:00.887: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1450" to be "running and ready"
Jan 12 16:51:00.889: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007153ms
Jan 12 16:51:00.889: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:51:02.892: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004448587s
Jan 12 16:51:02.892: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 12 16:51:02.892: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/12/23 16:51:02.893
STEP: Then the orphan pod is adopted 01/12/23 16:51:02.897
STEP: When the matched label of one of its pods change 01/12/23 16:51:03.901
Jan 12 16:51:03.903: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/12/23 16:51:03.912
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:04.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1450" for this suite. 01/12/23 16:51:04.919
------------------------------
 [4.056 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:00.866
    Jan 12 16:51:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 16:51:00.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:00.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:00.878
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/12/23 16:51:00.88
    Jan 12 16:51:00.887: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1450" to be "running and ready"
    Jan 12 16:51:00.889: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007153ms
    Jan 12 16:51:00.889: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:51:02.892: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004448587s
    Jan 12 16:51:02.892: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 12 16:51:02.892: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/12/23 16:51:02.893
    STEP: Then the orphan pod is adopted 01/12/23 16:51:02.897
    STEP: When the matched label of one of its pods change 01/12/23 16:51:03.901
    Jan 12 16:51:03.903: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/12/23 16:51:03.912
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:04.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1450" for this suite. 01/12/23 16:51:04.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:04.924
Jan 12 16:51:04.924: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename webhook 01/12/23 16:51:04.924
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:04.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:04.934
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/12/23 16:51:04.945
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:51:05.5
STEP: Deploying the webhook pod 01/12/23 16:51:05.506
STEP: Wait for the deployment to be ready 01/12/23 16:51:05.515
Jan 12 16:51:05.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/12/23 16:51:07.527
STEP: Verifying the service has paired with the endpoint 01/12/23 16:51:07.537
Jan 12 16:51:08.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/12/23 16:51:08.539
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:08.539
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/12/23 16:51:08.558
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/12/23 16:51:09.565
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:09.565
STEP: Having no error when timeout is longer than webhook latency 01/12/23 16:51:10.591
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:10.591
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/12/23 16:51:15.616
STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:15.616
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-551" for this suite. 01/12/23 16:51:20.664
STEP: Destroying namespace "webhook-551-markers" for this suite. 01/12/23 16:51:20.668
------------------------------
 [SLOW TEST] [15.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:04.924
    Jan 12 16:51:04.924: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename webhook 01/12/23 16:51:04.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:04.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:04.934
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/12/23 16:51:04.945
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/12/23 16:51:05.5
    STEP: Deploying the webhook pod 01/12/23 16:51:05.506
    STEP: Wait for the deployment to be ready 01/12/23 16:51:05.515
    Jan 12 16:51:05.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/12/23 16:51:07.527
    STEP: Verifying the service has paired with the endpoint 01/12/23 16:51:07.537
    Jan 12 16:51:08.537: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/12/23 16:51:08.539
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:08.539
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/12/23 16:51:08.558
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/12/23 16:51:09.565
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:09.565
    STEP: Having no error when timeout is longer than webhook latency 01/12/23 16:51:10.591
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:10.591
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/12/23 16:51:15.616
    STEP: Registering slow webhook via the AdmissionRegistration API 01/12/23 16:51:15.616
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-551" for this suite. 01/12/23 16:51:20.664
    STEP: Destroying namespace "webhook-551-markers" for this suite. 01/12/23 16:51:20.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:20.673
Jan 12 16:51:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename kubectl 01/12/23 16:51:20.674
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:20.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:20.688
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/12/23 16:51:20.691
Jan 12 16:51:20.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 create -f -'
Jan 12 16:51:21.466: INFO: stderr: ""
Jan 12 16:51:21.466: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/12/23 16:51:21.466
Jan 12 16:51:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 diff -f -'
Jan 12 16:51:21.655: INFO: rc: 1
Jan 12 16:51:21.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 delete -f -'
Jan 12 16:51:21.713: INFO: stderr: ""
Jan 12 16:51:21.713: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:21.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8652" for this suite. 01/12/23 16:51:21.716
------------------------------
 [1.046 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:20.673
    Jan 12 16:51:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename kubectl 01/12/23 16:51:20.674
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:20.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:20.688
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/12/23 16:51:20.691
    Jan 12 16:51:20.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 create -f -'
    Jan 12 16:51:21.466: INFO: stderr: ""
    Jan 12 16:51:21.466: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/12/23 16:51:21.466
    Jan 12 16:51:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 diff -f -'
    Jan 12 16:51:21.655: INFO: rc: 1
    Jan 12 16:51:21.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4021033506 --namespace=kubectl-8652 delete -f -'
    Jan 12 16:51:21.713: INFO: stderr: ""
    Jan 12 16:51:21.713: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:21.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8652" for this suite. 01/12/23 16:51:21.716
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:21.719
Jan 12 16:51:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:51:21.72
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:21.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:21.729
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8899 01/12/23 16:51:21.731
STEP: creating a selector 01/12/23 16:51:21.732
STEP: Creating the service pods in kubernetes 01/12/23 16:51:21.732
Jan 12 16:51:21.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 12 16:51:21.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8899" to be "running and ready"
Jan 12 16:51:21.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.066161ms
Jan 12 16:51:21.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:51:23.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01034104s
Jan 12 16:51:23.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:51:25.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010576204s
Jan 12 16:51:25.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:51:27.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010870468s
Jan 12 16:51:27.759: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:51:29.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010312604s
Jan 12 16:51:29.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:51:31.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00998835s
Jan 12 16:51:31.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 12 16:51:33.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009823451s
Jan 12 16:51:33.758: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 12 16:51:33.758: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 12 16:51:33.759: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8899" to be "running and ready"
Jan 12 16:51:33.761: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.548001ms
Jan 12 16:51:33.761: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 12 16:51:33.761: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/12/23 16:51:33.763
Jan 12 16:51:33.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8899" to be "running"
Jan 12 16:51:33.768: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695944ms
Jan 12 16:51:35.771: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004354468s
Jan 12 16:51:35.771: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 12 16:51:35.772: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 12 16:51:35.772: INFO: Breadth first check of 10.244.0.18 on host 10.0.42.160...
Jan 12 16:51:35.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.210:9080/dial?request=hostname&protocol=http&host=10.244.0.18&port=8083&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:51:35.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:51:35.775: INFO: ExecWithOptions: Clientset creation
Jan 12 16:51:35.775: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.210%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.18%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 16:51:35.832: INFO: Waiting for responses: map[]
Jan 12 16:51:35.832: INFO: reached 10.244.0.18 after 0/1 tries
Jan 12 16:51:35.832: INFO: Breadth first check of 10.244.1.209 on host 10.0.40.50...
Jan 12 16:51:35.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.210:9080/dial?request=hostname&protocol=http&host=10.244.1.209&port=8083&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:51:35.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:51:35.834: INFO: ExecWithOptions: Clientset creation
Jan 12 16:51:35.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.210%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.209%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 12 16:51:35.895: INFO: Waiting for responses: map[]
Jan 12 16:51:35.895: INFO: reached 10.244.1.209 after 0/1 tries
Jan 12 16:51:35.895: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:35.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8899" for this suite. 01/12/23 16:51:35.898
------------------------------
 [SLOW TEST] [14.182 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:21.719
    Jan 12 16:51:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename pod-network-test 01/12/23 16:51:21.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:21.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:21.729
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8899 01/12/23 16:51:21.731
    STEP: creating a selector 01/12/23 16:51:21.732
    STEP: Creating the service pods in kubernetes 01/12/23 16:51:21.732
    Jan 12 16:51:21.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 12 16:51:21.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8899" to be "running and ready"
    Jan 12 16:51:21.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.066161ms
    Jan 12 16:51:21.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:51:23.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01034104s
    Jan 12 16:51:23.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:51:25.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010576204s
    Jan 12 16:51:25.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:51:27.759: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010870468s
    Jan 12 16:51:27.759: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:51:29.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010312604s
    Jan 12 16:51:29.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:51:31.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00998835s
    Jan 12 16:51:31.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 12 16:51:33.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009823451s
    Jan 12 16:51:33.758: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 12 16:51:33.758: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 12 16:51:33.759: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8899" to be "running and ready"
    Jan 12 16:51:33.761: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.548001ms
    Jan 12 16:51:33.761: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 12 16:51:33.761: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/12/23 16:51:33.763
    Jan 12 16:51:33.766: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8899" to be "running"
    Jan 12 16:51:33.768: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.695944ms
    Jan 12 16:51:35.771: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004354468s
    Jan 12 16:51:35.771: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 12 16:51:35.772: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 12 16:51:35.772: INFO: Breadth first check of 10.244.0.18 on host 10.0.42.160...
    Jan 12 16:51:35.774: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.210:9080/dial?request=hostname&protocol=http&host=10.244.0.18&port=8083&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:51:35.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:51:35.775: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:51:35.775: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.210%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.18%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 16:51:35.832: INFO: Waiting for responses: map[]
    Jan 12 16:51:35.832: INFO: reached 10.244.0.18 after 0/1 tries
    Jan 12 16:51:35.832: INFO: Breadth first check of 10.244.1.209 on host 10.0.40.50...
    Jan 12 16:51:35.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.210:9080/dial?request=hostname&protocol=http&host=10.244.1.209&port=8083&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:51:35.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:51:35.834: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:51:35.834: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.210%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.209%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 12 16:51:35.895: INFO: Waiting for responses: map[]
    Jan 12 16:51:35.895: INFO: reached 10.244.1.209 after 0/1 tries
    Jan 12 16:51:35.895: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:35.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8899" for this suite. 01/12/23 16:51:35.898
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:35.902
Jan 12 16:51:35.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename replicaset 01/12/23 16:51:35.902
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:35.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:35.915
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 12 16:51:35.924: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 12 16:51:40.926: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 16:51:40.926
STEP: Scaling up "test-rs" replicaset  01/12/23 16:51:40.926
Jan 12 16:51:40.932: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/12/23 16:51:40.932
W0112 16:51:40.940631      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 12 16:51:40.942: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 16:51:40.955: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 16:51:40.968: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 16:51:40.975: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
Jan 12 16:51:41.857: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 2, AvailableReplicas 2
Jan 12 16:51:42.044: INFO: observed Replicaset test-rs in namespace replicaset-668 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-668" for this suite. 01/12/23 16:51:42.046
------------------------------
 [SLOW TEST] [6.150 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:35.902
    Jan 12 16:51:35.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename replicaset 01/12/23 16:51:35.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:35.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:35.915
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 12 16:51:35.924: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 12 16:51:40.926: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 16:51:40.926
    STEP: Scaling up "test-rs" replicaset  01/12/23 16:51:40.926
    Jan 12 16:51:40.932: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/12/23 16:51:40.932
    W0112 16:51:40.940631      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 12 16:51:40.942: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 16:51:40.955: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 16:51:40.968: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 16:51:40.975: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 1, AvailableReplicas 1
    Jan 12 16:51:41.857: INFO: observed ReplicaSet test-rs in namespace replicaset-668 with ReadyReplicas 2, AvailableReplicas 2
    Jan 12 16:51:42.044: INFO: observed Replicaset test-rs in namespace replicaset-668 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:42.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-668" for this suite. 01/12/23 16:51:42.046
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:42.052
Jan 12 16:51:42.052: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename watch 01/12/23 16:51:42.053
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:42.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:42.065
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/12/23 16:51:42.067
STEP: creating a new configmap 01/12/23 16:51:42.068
STEP: modifying the configmap once 01/12/23 16:51:42.071
STEP: changing the label value of the configmap 01/12/23 16:51:42.076
STEP: Expecting to observe a delete notification for the watched object 01/12/23 16:51:42.083
Jan 12 16:51:42.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32876 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:51:42.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32877 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:51:42.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32878 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/12/23 16:51:42.084
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/12/23 16:51:42.089
STEP: changing the label value of the configmap back 01/12/23 16:51:52.089
STEP: modifying the configmap a third time 01/12/23 16:51:52.095
STEP: deleting the configmap 01/12/23 16:51:52.1
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/12/23 16:51:52.103
Jan 12 16:51:52.103: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32935 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:51:52.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32936 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 12 16:51:52.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32937 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:51:52.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3158" for this suite. 01/12/23 16:51:52.106
------------------------------
 [SLOW TEST] [10.057 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:42.052
    Jan 12 16:51:42.052: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename watch 01/12/23 16:51:42.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:42.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:42.065
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/12/23 16:51:42.067
    STEP: creating a new configmap 01/12/23 16:51:42.068
    STEP: modifying the configmap once 01/12/23 16:51:42.071
    STEP: changing the label value of the configmap 01/12/23 16:51:42.076
    STEP: Expecting to observe a delete notification for the watched object 01/12/23 16:51:42.083
    Jan 12 16:51:42.083: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32876 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:51:42.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32877 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:51:42.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32878 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/12/23 16:51:42.084
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/12/23 16:51:42.089
    STEP: changing the label value of the configmap back 01/12/23 16:51:52.089
    STEP: modifying the configmap a third time 01/12/23 16:51:52.095
    STEP: deleting the configmap 01/12/23 16:51:52.1
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/12/23 16:51:52.103
    Jan 12 16:51:52.103: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32935 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:51:52.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32936 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 12 16:51:52.104: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3158  1c6e4dec-bb15-40de-b87e-7ccebd7468b9 32937 0 2023-01-12 16:51:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-12 16:51:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:51:52.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3158" for this suite. 01/12/23 16:51:52.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:51:52.111
Jan 12 16:51:52.111: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:51:52.112
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:52.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:52.124
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/12/23 16:51:52.126
STEP: Creating a ResourceQuota 01/12/23 16:51:57.129
STEP: Ensuring resource quota status is calculated 01/12/23 16:51:57.132
STEP: Creating a Pod that fits quota 01/12/23 16:51:59.135
STEP: Ensuring ResourceQuota status captures the pod usage 01/12/23 16:51:59.146
STEP: Not allowing a pod to be created that exceeds remaining quota 01/12/23 16:52:01.149
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/12/23 16:52:01.151
STEP: Ensuring a pod cannot update its resource requirements 01/12/23 16:52:01.153
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/12/23 16:52:01.155
STEP: Deleting the pod 01/12/23 16:52:03.159
STEP: Ensuring resource quota status released the pod usage 01/12/23 16:52:03.17
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8292" for this suite. 01/12/23 16:52:05.174
------------------------------
 [SLOW TEST] [13.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:51:52.111
    Jan 12 16:51:52.111: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:51:52.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:51:52.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:51:52.124
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/12/23 16:51:52.126
    STEP: Creating a ResourceQuota 01/12/23 16:51:57.129
    STEP: Ensuring resource quota status is calculated 01/12/23 16:51:57.132
    STEP: Creating a Pod that fits quota 01/12/23 16:51:59.135
    STEP: Ensuring ResourceQuota status captures the pod usage 01/12/23 16:51:59.146
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/12/23 16:52:01.149
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/12/23 16:52:01.151
    STEP: Ensuring a pod cannot update its resource requirements 01/12/23 16:52:01.153
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/12/23 16:52:01.155
    STEP: Deleting the pod 01/12/23 16:52:03.159
    STEP: Ensuring resource quota status released the pod usage 01/12/23 16:52:03.17
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8292" for this suite. 01/12/23 16:52:05.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:05.179
Jan 12 16:52:05.179: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:52:05.18
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:05.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:05.189
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 16:52:05.192
Jan 12 16:52:05.196: INFO: Waiting up to 5m0s for pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d" in namespace "emptydir-6521" to be "Succeeded or Failed"
Jan 12 16:52:05.198: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61682ms
Jan 12 16:52:07.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00495542s
Jan 12 16:52:09.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004564953s
STEP: Saw pod success 01/12/23 16:52:09.201
Jan 12 16:52:09.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d" satisfied condition "Succeeded or Failed"
Jan 12 16:52:09.203: INFO: Trying to get logs from node worker-1 pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d container test-container: <nil>
STEP: delete the pod 01/12/23 16:52:09.213
Jan 12 16:52:09.222: INFO: Waiting for pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d to disappear
Jan 12 16:52:09.223: INFO: Pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:09.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6521" for this suite. 01/12/23 16:52:09.225
------------------------------
 [4.050 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:05.179
    Jan 12 16:52:05.179: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:52:05.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:05.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:05.189
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/12/23 16:52:05.192
    Jan 12 16:52:05.196: INFO: Waiting up to 5m0s for pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d" in namespace "emptydir-6521" to be "Succeeded or Failed"
    Jan 12 16:52:05.198: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.61682ms
    Jan 12 16:52:07.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00495542s
    Jan 12 16:52:09.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004564953s
    STEP: Saw pod success 01/12/23 16:52:09.201
    Jan 12 16:52:09.201: INFO: Pod "pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d" satisfied condition "Succeeded or Failed"
    Jan 12 16:52:09.203: INFO: Trying to get logs from node worker-1 pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d container test-container: <nil>
    STEP: delete the pod 01/12/23 16:52:09.213
    Jan 12 16:52:09.222: INFO: Waiting for pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d to disappear
    Jan 12 16:52:09.223: INFO: Pod pod-fb2c2bd8-55dc-46dd-ae1c-65c886e1dd2d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:09.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6521" for this suite. 01/12/23 16:52:09.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:09.23
Jan 12 16:52:09.230: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 16:52:09.231
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:09.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:09.244
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/12/23 16:52:09.248
Jan 12 16:52:09.255: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1693" to be "running and ready"
Jan 12 16:52:09.256: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596257ms
Jan 12 16:52:09.257: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:52:11.259: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004069611s
Jan 12 16:52:11.259: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 12 16:52:11.259: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/12/23 16:52:11.261
Jan 12 16:52:11.264: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1693" to be "running and ready"
Jan 12 16:52:11.266: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.739212ms
Jan 12 16:52:11.266: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:52:13.269: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005143295s
Jan 12 16:52:13.269: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 12 16:52:13.269: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/12/23 16:52:13.271
Jan 12 16:52:13.276: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 12 16:52:13.278: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 12 16:52:15.279: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 12 16:52:15.282: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/12/23 16:52:15.282
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:15.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1693" for this suite. 01/12/23 16:52:15.294
------------------------------
 [SLOW TEST] [6.068 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:09.23
    Jan 12 16:52:09.230: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/12/23 16:52:09.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:09.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:09.244
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/12/23 16:52:09.248
    Jan 12 16:52:09.255: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1693" to be "running and ready"
    Jan 12 16:52:09.256: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596257ms
    Jan 12 16:52:09.257: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:52:11.259: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004069611s
    Jan 12 16:52:11.259: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 12 16:52:11.259: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/12/23 16:52:11.261
    Jan 12 16:52:11.264: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-1693" to be "running and ready"
    Jan 12 16:52:11.266: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.739212ms
    Jan 12 16:52:11.266: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:52:13.269: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005143295s
    Jan 12 16:52:13.269: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 12 16:52:13.269: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/12/23 16:52:13.271
    Jan 12 16:52:13.276: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 12 16:52:13.278: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 12 16:52:15.279: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 12 16:52:15.282: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/12/23 16:52:15.282
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:15.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1693" for this suite. 01/12/23 16:52:15.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:15.299
Jan 12 16:52:15.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:52:15.3
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:15.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:15.31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:52:15.312
Jan 12 16:52:15.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b" in namespace "downward-api-1214" to be "Succeeded or Failed"
Jan 12 16:52:15.319: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.720116ms
Jan 12 16:52:17.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004997622s
Jan 12 16:52:19.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004785278s
STEP: Saw pod success 01/12/23 16:52:19.322
Jan 12 16:52:19.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b" satisfied condition "Succeeded or Failed"
Jan 12 16:52:19.324: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b container client-container: <nil>
STEP: delete the pod 01/12/23 16:52:19.328
Jan 12 16:52:19.335: INFO: Waiting for pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b to disappear
Jan 12 16:52:19.337: INFO: Pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1214" for this suite. 01/12/23 16:52:19.339
------------------------------
 [4.044 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:15.299
    Jan 12 16:52:15.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:52:15.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:15.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:15.31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:52:15.312
    Jan 12 16:52:15.317: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b" in namespace "downward-api-1214" to be "Succeeded or Failed"
    Jan 12 16:52:15.319: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.720116ms
    Jan 12 16:52:17.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004997622s
    Jan 12 16:52:19.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004785278s
    STEP: Saw pod success 01/12/23 16:52:19.322
    Jan 12 16:52:19.322: INFO: Pod "downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b" satisfied condition "Succeeded or Failed"
    Jan 12 16:52:19.324: INFO: Trying to get logs from node worker-1 pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b container client-container: <nil>
    STEP: delete the pod 01/12/23 16:52:19.328
    Jan 12 16:52:19.335: INFO: Waiting for pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b to disappear
    Jan 12 16:52:19.337: INFO: Pod downwardapi-volume-10e5483e-731c-4077-853d-43efb584676b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:19.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1214" for this suite. 01/12/23 16:52:19.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:19.345
Jan 12 16:52:19.345: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename dns 01/12/23 16:52:19.346
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:19.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:19.359
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/12/23 16:52:19.361
Jan 12 16:52:19.366: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9968  0ab0137a-b937-42a1-b4e2-9cb2bdb7a9d9 33093 0 2023-01-12 16:52:19 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-12 16:52:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pw47m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pw47m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 12 16:52:19.366: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9968" to be "running and ready"
Jan 12 16:52:19.368: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70746ms
Jan 12 16:52:19.368: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 12 16:52:21.371: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005065896s
Jan 12 16:52:21.371: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 12 16:52:21.371: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/12/23 16:52:21.371
Jan 12 16:52:21.372: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9968 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:52:21.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:52:21.372: INFO: ExecWithOptions: Clientset creation
Jan 12 16:52:21.372: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9968/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/12/23 16:52:21.438
Jan 12 16:52:21.438: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9968 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 12 16:52:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
Jan 12 16:52:21.439: INFO: ExecWithOptions: Clientset creation
Jan 12 16:52:21.439: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9968/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 12 16:52:21.524: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:21.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9968" for this suite. 01/12/23 16:52:21.536
------------------------------
 [2.195 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:19.345
    Jan 12 16:52:19.345: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename dns 01/12/23 16:52:19.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:19.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:19.359
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/12/23 16:52:19.361
    Jan 12 16:52:19.366: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9968  0ab0137a-b937-42a1-b4e2-9cb2bdb7a9d9 33093 0 2023-01-12 16:52:19 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-12 16:52:19 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pw47m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pw47m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 12 16:52:19.366: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-9968" to be "running and ready"
    Jan 12 16:52:19.368: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70746ms
    Jan 12 16:52:19.368: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 12 16:52:21.371: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.005065896s
    Jan 12 16:52:21.371: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 12 16:52:21.371: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/12/23 16:52:21.371
    Jan 12 16:52:21.372: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9968 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:52:21.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:52:21.372: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:52:21.372: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9968/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/12/23 16:52:21.438
    Jan 12 16:52:21.438: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9968 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 12 16:52:21.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    Jan 12 16:52:21.439: INFO: ExecWithOptions: Clientset creation
    Jan 12 16:52:21.439: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9968/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 12 16:52:21.524: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:21.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9968" for this suite. 01/12/23 16:52:21.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:21.542
Jan 12 16:52:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename resourcequota 01/12/23 16:52:21.543
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:21.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:21.553
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/12/23 16:52:21.555
STEP: Getting a ResourceQuota 01/12/23 16:52:21.558
STEP: Updating a ResourceQuota 01/12/23 16:52:21.56
STEP: Verifying a ResourceQuota was modified 01/12/23 16:52:21.565
STEP: Deleting a ResourceQuota 01/12/23 16:52:21.567
STEP: Verifying the deleted ResourceQuota 01/12/23 16:52:21.572
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:21.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8659" for this suite. 01/12/23 16:52:21.575
------------------------------
 [0.037 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:21.542
    Jan 12 16:52:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename resourcequota 01/12/23 16:52:21.543
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:21.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:21.553
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/12/23 16:52:21.555
    STEP: Getting a ResourceQuota 01/12/23 16:52:21.558
    STEP: Updating a ResourceQuota 01/12/23 16:52:21.56
    STEP: Verifying a ResourceQuota was modified 01/12/23 16:52:21.565
    STEP: Deleting a ResourceQuota 01/12/23 16:52:21.567
    STEP: Verifying the deleted ResourceQuota 01/12/23 16:52:21.572
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:21.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8659" for this suite. 01/12/23 16:52:21.575
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:21.579
Jan 12 16:52:21.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename watch 01/12/23 16:52:21.58
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:21.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:21.589
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/12/23 16:52:21.591
STEP: starting a background goroutine to produce watch events 01/12/23 16:52:21.593
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/12/23 16:52:21.593
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:24.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6426" for this suite. 01/12/23 16:52:24.433
------------------------------
 [2.906 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:21.579
    Jan 12 16:52:21.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename watch 01/12/23 16:52:21.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:21.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:21.589
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/12/23 16:52:21.591
    STEP: starting a background goroutine to produce watch events 01/12/23 16:52:21.593
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/12/23 16:52:21.593
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:24.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6426" for this suite. 01/12/23 16:52:24.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:24.488
Jan 12 16:52:24.488: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename security-context 01/12/23 16:52:24.489
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:24.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:24.512
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 16:52:24.515
Jan 12 16:52:24.522: INFO: Waiting up to 5m0s for pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd" in namespace "security-context-4111" to be "Succeeded or Failed"
Jan 12 16:52:24.524: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183974ms
Jan 12 16:52:26.527: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004826188s
Jan 12 16:52:28.526: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004638272s
STEP: Saw pod success 01/12/23 16:52:28.526
Jan 12 16:52:28.527: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd" satisfied condition "Succeeded or Failed"
Jan 12 16:52:28.528: INFO: Trying to get logs from node worker-1 pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd container test-container: <nil>
STEP: delete the pod 01/12/23 16:52:28.533
Jan 12 16:52:28.541: INFO: Waiting for pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd to disappear
Jan 12 16:52:28.543: INFO: Pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:28.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4111" for this suite. 01/12/23 16:52:28.546
------------------------------
 [4.062 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:24.488
    Jan 12 16:52:24.488: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename security-context 01/12/23 16:52:24.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:24.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:24.512
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/12/23 16:52:24.515
    Jan 12 16:52:24.522: INFO: Waiting up to 5m0s for pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd" in namespace "security-context-4111" to be "Succeeded or Failed"
    Jan 12 16:52:24.524: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.183974ms
    Jan 12 16:52:26.527: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004826188s
    Jan 12 16:52:28.526: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004638272s
    STEP: Saw pod success 01/12/23 16:52:28.526
    Jan 12 16:52:28.527: INFO: Pod "security-context-d673293a-f44f-47a7-9433-028ec39a19fd" satisfied condition "Succeeded or Failed"
    Jan 12 16:52:28.528: INFO: Trying to get logs from node worker-1 pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd container test-container: <nil>
    STEP: delete the pod 01/12/23 16:52:28.533
    Jan 12 16:52:28.541: INFO: Waiting for pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd to disappear
    Jan 12 16:52:28.543: INFO: Pod security-context-d673293a-f44f-47a7-9433-028ec39a19fd no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:28.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4111" for this suite. 01/12/23 16:52:28.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:28.551
Jan 12 16:52:28.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename services 01/12/23 16:52:28.552
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:28.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:28.563
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:28.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2631" for this suite. 01/12/23 16:52:28.57
------------------------------
 [0.022 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:28.551
    Jan 12 16:52:28.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename services 01/12/23 16:52:28.552
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:28.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:28.563
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:28.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2631" for this suite. 01/12/23 16:52:28.57
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:28.574
Jan 12 16:52:28.575: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename statefulset 01/12/23 16:52:28.575
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:28.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:28.586
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8391 01/12/23 16:52:28.589
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8391 01/12/23 16:52:28.593
Jan 12 16:52:28.600: INFO: Found 0 stateful pods, waiting for 1
Jan 12 16:52:38.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 12 16:52:48.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/12/23 16:52:48.608
STEP: Getting /status 01/12/23 16:52:48.612
Jan 12 16:52:48.617: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/12/23 16:52:48.617
Jan 12 16:52:48.624: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/12/23 16:52:48.624
Jan 12 16:52:48.625: INFO: Observed &StatefulSet event: ADDED
Jan 12 16:52:48.625: INFO: Found Statefulset ss in namespace statefulset-8391 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 16:52:48.625: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/12/23 16:52:48.625
Jan 12 16:52:48.625: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 12 16:52:48.631: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/12/23 16:52:48.631
Jan 12 16:52:48.632: INFO: Observed &StatefulSet event: ADDED
Jan 12 16:52:48.632: INFO: Observed Statefulset ss in namespace statefulset-8391 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 12 16:52:48.632: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 12 16:52:48.632: INFO: Deleting all statefulset in ns statefulset-8391
Jan 12 16:52:48.634: INFO: Scaling statefulset ss to 0
Jan 12 16:52:58.649: INFO: Waiting for statefulset status.replicas updated to 0
Jan 12 16:52:58.652: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 12 16:52:58.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8391" for this suite. 01/12/23 16:52:58.661
------------------------------
 [SLOW TEST] [30.092 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:28.574
    Jan 12 16:52:28.575: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename statefulset 01/12/23 16:52:28.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:28.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:28.586
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8391 01/12/23 16:52:28.589
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8391 01/12/23 16:52:28.593
    Jan 12 16:52:28.600: INFO: Found 0 stateful pods, waiting for 1
    Jan 12 16:52:38.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 12 16:52:48.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/12/23 16:52:48.608
    STEP: Getting /status 01/12/23 16:52:48.612
    Jan 12 16:52:48.617: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/12/23 16:52:48.617
    Jan 12 16:52:48.624: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/12/23 16:52:48.624
    Jan 12 16:52:48.625: INFO: Observed &StatefulSet event: ADDED
    Jan 12 16:52:48.625: INFO: Found Statefulset ss in namespace statefulset-8391 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 16:52:48.625: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/12/23 16:52:48.625
    Jan 12 16:52:48.625: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 12 16:52:48.631: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/12/23 16:52:48.631
    Jan 12 16:52:48.632: INFO: Observed &StatefulSet event: ADDED
    Jan 12 16:52:48.632: INFO: Observed Statefulset ss in namespace statefulset-8391 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 12 16:52:48.632: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 12 16:52:48.632: INFO: Deleting all statefulset in ns statefulset-8391
    Jan 12 16:52:48.634: INFO: Scaling statefulset ss to 0
    Jan 12 16:52:58.649: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 12 16:52:58.652: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:52:58.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8391" for this suite. 01/12/23 16:52:58.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:52:58.667
Jan 12 16:52:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename disruption 01/12/23 16:52:58.668
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:58.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:58.68
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/12/23 16:52:58.682
STEP: Waiting for the pdb to be processed 01/12/23 16:52:58.686
STEP: First trying to evict a pod which shouldn't be evictable 01/12/23 16:53:00.694
STEP: Waiting for all pods to be running 01/12/23 16:53:00.694
Jan 12 16:53:00.696: INFO: pods: 0 < 3
STEP: locating a running pod 01/12/23 16:53:02.699
STEP: Updating the pdb to allow a pod to be evicted 01/12/23 16:53:02.704
STEP: Waiting for the pdb to be processed 01/12/23 16:53:02.711
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 16:53:04.715
STEP: Waiting for all pods to be running 01/12/23 16:53:04.715
STEP: Waiting for the pdb to observed all healthy pods 01/12/23 16:53:04.717
STEP: Patching the pdb to disallow a pod to be evicted 01/12/23 16:53:04.734
STEP: Waiting for the pdb to be processed 01/12/23 16:53:04.753
STEP: Waiting for all pods to be running 01/12/23 16:53:06.757
STEP: locating a running pod 01/12/23 16:53:06.759
STEP: Deleting the pdb to allow a pod to be evicted 01/12/23 16:53:06.764
STEP: Waiting for the pdb to be deleted 01/12/23 16:53:06.768
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 16:53:06.769
STEP: Waiting for all pods to be running 01/12/23 16:53:06.769
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 12 16:53:06.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2809" for this suite. 01/12/23 16:53:06.783
------------------------------
 [SLOW TEST] [8.124 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:52:58.667
    Jan 12 16:52:58.667: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename disruption 01/12/23 16:52:58.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:52:58.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:52:58.68
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/12/23 16:52:58.682
    STEP: Waiting for the pdb to be processed 01/12/23 16:52:58.686
    STEP: First trying to evict a pod which shouldn't be evictable 01/12/23 16:53:00.694
    STEP: Waiting for all pods to be running 01/12/23 16:53:00.694
    Jan 12 16:53:00.696: INFO: pods: 0 < 3
    STEP: locating a running pod 01/12/23 16:53:02.699
    STEP: Updating the pdb to allow a pod to be evicted 01/12/23 16:53:02.704
    STEP: Waiting for the pdb to be processed 01/12/23 16:53:02.711
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 16:53:04.715
    STEP: Waiting for all pods to be running 01/12/23 16:53:04.715
    STEP: Waiting for the pdb to observed all healthy pods 01/12/23 16:53:04.717
    STEP: Patching the pdb to disallow a pod to be evicted 01/12/23 16:53:04.734
    STEP: Waiting for the pdb to be processed 01/12/23 16:53:04.753
    STEP: Waiting for all pods to be running 01/12/23 16:53:06.757
    STEP: locating a running pod 01/12/23 16:53:06.759
    STEP: Deleting the pdb to allow a pod to be evicted 01/12/23 16:53:06.764
    STEP: Waiting for the pdb to be deleted 01/12/23 16:53:06.768
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/12/23 16:53:06.769
    STEP: Waiting for all pods to be running 01/12/23 16:53:06.769
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:53:06.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2809" for this suite. 01/12/23 16:53:06.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:53:06.792
Jan 12 16:53:06.792: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename downward-api 01/12/23 16:53:06.793
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:06.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:06.806
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/12/23 16:53:06.808
Jan 12 16:53:06.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b" in namespace "downward-api-7746" to be "Succeeded or Failed"
Jan 12 16:53:06.815: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565054ms
Jan 12 16:53:08.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004773477s
Jan 12 16:53:10.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004653308s
STEP: Saw pod success 01/12/23 16:53:10.818
Jan 12 16:53:10.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b" satisfied condition "Succeeded or Failed"
Jan 12 16:53:10.820: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b container client-container: <nil>
STEP: delete the pod 01/12/23 16:53:10.824
Jan 12 16:53:10.832: INFO: Waiting for pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b to disappear
Jan 12 16:53:10.836: INFO: Pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 12 16:53:10.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7746" for this suite. 01/12/23 16:53:10.838
------------------------------
 [4.050 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:53:06.792
    Jan 12 16:53:06.792: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename downward-api 01/12/23 16:53:06.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:06.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:06.806
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/12/23 16:53:06.808
    Jan 12 16:53:06.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b" in namespace "downward-api-7746" to be "Succeeded or Failed"
    Jan 12 16:53:06.815: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565054ms
    Jan 12 16:53:08.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004773477s
    Jan 12 16:53:10.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004653308s
    STEP: Saw pod success 01/12/23 16:53:10.818
    Jan 12 16:53:10.818: INFO: Pod "downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b" satisfied condition "Succeeded or Failed"
    Jan 12 16:53:10.820: INFO: Trying to get logs from node worker-0 pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b container client-container: <nil>
    STEP: delete the pod 01/12/23 16:53:10.824
    Jan 12 16:53:10.832: INFO: Waiting for pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b to disappear
    Jan 12 16:53:10.836: INFO: Pod downwardapi-volume-8a471dd8-0802-40d1-ac83-93da3c0dc51b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:53:10.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7746" for this suite. 01/12/23 16:53:10.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:53:10.85
Jan 12 16:53:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename emptydir 01/12/23 16:53:10.851
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:10.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:10.862
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 16:53:10.865
Jan 12 16:53:10.870: INFO: Waiting up to 5m0s for pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694" in namespace "emptydir-4758" to be "Succeeded or Failed"
Jan 12 16:53:10.872: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24999ms
Jan 12 16:53:12.874: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004696024s
Jan 12 16:53:14.875: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005099766s
STEP: Saw pod success 01/12/23 16:53:14.875
Jan 12 16:53:14.875: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694" satisfied condition "Succeeded or Failed"
Jan 12 16:53:14.877: INFO: Trying to get logs from node worker-1 pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 container test-container: <nil>
STEP: delete the pod 01/12/23 16:53:14.881
Jan 12 16:53:14.890: INFO: Waiting for pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 to disappear
Jan 12 16:53:14.892: INFO: Pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 12 16:53:14.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4758" for this suite. 01/12/23 16:53:14.894
------------------------------
 [4.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:53:10.85
    Jan 12 16:53:10.850: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename emptydir 01/12/23 16:53:10.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:10.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:10.862
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/12/23 16:53:10.865
    Jan 12 16:53:10.870: INFO: Waiting up to 5m0s for pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694" in namespace "emptydir-4758" to be "Succeeded or Failed"
    Jan 12 16:53:10.872: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24999ms
    Jan 12 16:53:12.874: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004696024s
    Jan 12 16:53:14.875: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005099766s
    STEP: Saw pod success 01/12/23 16:53:14.875
    Jan 12 16:53:14.875: INFO: Pod "pod-f47424ae-a2ea-491f-bc9a-43ff30b61694" satisfied condition "Succeeded or Failed"
    Jan 12 16:53:14.877: INFO: Trying to get logs from node worker-1 pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 container test-container: <nil>
    STEP: delete the pod 01/12/23 16:53:14.881
    Jan 12 16:53:14.890: INFO: Waiting for pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 to disappear
    Jan 12 16:53:14.892: INFO: Pod pod-f47424ae-a2ea-491f-bc9a-43ff30b61694 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:53:14.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4758" for this suite. 01/12/23 16:53:14.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/12/23 16:53:14.899
Jan 12 16:53:14.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
STEP: Building a namespace api object, basename deployment 01/12/23 16:53:14.9
STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:14.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:14.911
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 12 16:53:14.919: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 12 16:53:19.922: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/12/23 16:53:19.922
Jan 12 16:53:19.923: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/12/23 16:53:19.93
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 12 16:53:19.937: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6187  bf819eab-de3c-482d-8167-81dffe1949e3 33670 1 2023-01-12 16:53:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-12 16:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 12 16:53:19.938: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 12 16:53:19.938: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 12 16:53:19.938: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6187  b883217c-19ac-45a1-a46f-0896a0529acd 33671 1 2023-01-12 16:53:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment bf819eab-de3c-482d-8167-81dffe1949e3 0xc005e7a737 0xc005e7a738}] [] [{e2e.test Update apps/v1 2023-01-12 16:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:53:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 16:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"bf819eab-de3c-482d-8167-81dffe1949e3\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e7a7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 12 16:53:19.942: INFO: Pod "test-cleanup-controller-2c864" is available:
&Pod{ObjectMeta:{test-cleanup-controller-2c864 test-cleanup-controller- deployment-6187  ca4c4830-5780-46d5-a9b2-b86ec4e4bd9d 33657 0 2023-01-12 16:53:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b883217c-19ac-45a1-a46f-0896a0529acd 0xc005e7aaf7 0xc005e7aaf8}] [] [{kube-controller-manager Update v1 2023-01-12 16:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b883217c-19ac-45a1-a46f-0896a0529acd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:53:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67ffs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67ffs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.223,StartTime:2023-01-12 16:53:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:53:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ae49a6c217c382d37e41956e92a8df1f7dc46285507ffc657cd3fd76fd16dad1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 12 16:53:19.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6187" for this suite. 01/12/23 16:53:19.946
------------------------------
 [SLOW TEST] [5.057 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/12/23 16:53:14.899
    Jan 12 16:53:14.899: INFO: >>> kubeConfig: /tmp/kubeconfig-4021033506
    STEP: Building a namespace api object, basename deployment 01/12/23 16:53:14.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/12/23 16:53:14.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/12/23 16:53:14.911
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 12 16:53:14.919: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 12 16:53:19.922: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/12/23 16:53:19.922
    Jan 12 16:53:19.923: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/12/23 16:53:19.93
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 12 16:53:19.937: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6187  bf819eab-de3c-482d-8167-81dffe1949e3 33670 1 2023-01-12 16:53:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-12 16:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e7a418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 12 16:53:19.938: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 12 16:53:19.938: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 12 16:53:19.938: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6187  b883217c-19ac-45a1-a46f-0896a0529acd 33671 1 2023-01-12 16:53:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment bf819eab-de3c-482d-8167-81dffe1949e3 0xc005e7a737 0xc005e7a738}] [] [{e2e.test Update apps/v1 2023-01-12 16:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-12 16:53:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-12 16:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"bf819eab-de3c-482d-8167-81dffe1949e3\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e7a7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 12 16:53:19.942: INFO: Pod "test-cleanup-controller-2c864" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-2c864 test-cleanup-controller- deployment-6187  ca4c4830-5780-46d5-a9b2-b86ec4e4bd9d 33657 0 2023-01-12 16:53:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b883217c-19ac-45a1-a46f-0896a0529acd 0xc005e7aaf7 0xc005e7aaf8}] [] [{kube-controller-manager Update v1 2023-01-12 16:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b883217c-19ac-45a1-a46f-0896a0529acd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-12 16:53:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67ffs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67ffs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-12 16:53:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.40.50,PodIP:10.244.1.223,StartTime:2023-01-12 16:53:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-12 16:53:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ae49a6c217c382d37e41956e92a8df1f7dc46285507ffc657cd3fd76fd16dad1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 12 16:53:19.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6187" for this suite. 01/12/23 16:53:19.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 12 16:53:19.959: INFO: Running AfterSuite actions on node 1
Jan 12 16:53:19.959: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 12 16:53:19.959: INFO: Running AfterSuite actions on node 1
    Jan 12 16:53:19.959: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.094 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5371.054 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h29m31.370634336s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

